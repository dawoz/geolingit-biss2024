{
  "best_metric": 0.22524040937423706,
  "best_model_checkpoint": "LLaMinerva/LLaMA/checkpoint-1710",
  "epoch": 4.0,
  "eval_steps": 500,
  "global_step": 1710,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0023391812865497076,
      "grad_norm": 1.4306095838546753,
      "learning_rate": 7.025761124121779e-07,
      "loss": 2.386,
      "step": 1
    },
    {
      "epoch": 0.004678362573099415,
      "grad_norm": 1.3843284845352173,
      "learning_rate": 1.4051522248243558e-06,
      "loss": 2.3541,
      "step": 2
    },
    {
      "epoch": 0.007017543859649123,
      "grad_norm": 1.323738932609558,
      "learning_rate": 2.107728337236534e-06,
      "loss": 2.3711,
      "step": 3
    },
    {
      "epoch": 0.00935672514619883,
      "grad_norm": 1.3000459671020508,
      "learning_rate": 2.8103044496487116e-06,
      "loss": 2.4047,
      "step": 4
    },
    {
      "epoch": 0.011695906432748537,
      "grad_norm": 1.4253416061401367,
      "learning_rate": 3.5128805620608897e-06,
      "loss": 2.3807,
      "step": 5
    },
    {
      "epoch": 0.014035087719298246,
      "grad_norm": 1.3897935152053833,
      "learning_rate": 4.215456674473068e-06,
      "loss": 2.3661,
      "step": 6
    },
    {
      "epoch": 0.016374269005847954,
      "grad_norm": 1.358989953994751,
      "learning_rate": 4.9180327868852455e-06,
      "loss": 2.4451,
      "step": 7
    },
    {
      "epoch": 0.01871345029239766,
      "grad_norm": 1.4573681354522705,
      "learning_rate": 5.620608899297423e-06,
      "loss": 2.4296,
      "step": 8
    },
    {
      "epoch": 0.021052631578947368,
      "grad_norm": 1.400085687637329,
      "learning_rate": 6.323185011709601e-06,
      "loss": 2.3973,
      "step": 9
    },
    {
      "epoch": 0.023391812865497075,
      "grad_norm": 1.4273765087127686,
      "learning_rate": 7.025761124121779e-06,
      "loss": 2.3034,
      "step": 10
    },
    {
      "epoch": 0.025730994152046785,
      "grad_norm": 1.4506750106811523,
      "learning_rate": 7.728337236533957e-06,
      "loss": 2.3448,
      "step": 11
    },
    {
      "epoch": 0.028070175438596492,
      "grad_norm": 1.4477540254592896,
      "learning_rate": 8.430913348946136e-06,
      "loss": 2.2972,
      "step": 12
    },
    {
      "epoch": 0.0304093567251462,
      "grad_norm": 1.3708553314208984,
      "learning_rate": 9.133489461358312e-06,
      "loss": 2.306,
      "step": 13
    },
    {
      "epoch": 0.03274853801169591,
      "grad_norm": 1.4014590978622437,
      "learning_rate": 9.836065573770491e-06,
      "loss": 2.3576,
      "step": 14
    },
    {
      "epoch": 0.03508771929824561,
      "grad_norm": 1.4470701217651367,
      "learning_rate": 1.0538641686182668e-05,
      "loss": 2.2967,
      "step": 15
    },
    {
      "epoch": 0.03742690058479532,
      "grad_norm": 1.3958741426467896,
      "learning_rate": 1.1241217798594846e-05,
      "loss": 2.3203,
      "step": 16
    },
    {
      "epoch": 0.03976608187134503,
      "grad_norm": 1.3431909084320068,
      "learning_rate": 1.1943793911007025e-05,
      "loss": 2.2999,
      "step": 17
    },
    {
      "epoch": 0.042105263157894736,
      "grad_norm": 1.4144020080566406,
      "learning_rate": 1.2646370023419202e-05,
      "loss": 2.2098,
      "step": 18
    },
    {
      "epoch": 0.044444444444444446,
      "grad_norm": 1.4368573427200317,
      "learning_rate": 1.334894613583138e-05,
      "loss": 2.2626,
      "step": 19
    },
    {
      "epoch": 0.04678362573099415,
      "grad_norm": 1.4755048751831055,
      "learning_rate": 1.4051522248243559e-05,
      "loss": 2.2031,
      "step": 20
    },
    {
      "epoch": 0.04912280701754386,
      "grad_norm": 1.515049934387207,
      "learning_rate": 1.4754098360655736e-05,
      "loss": 2.2177,
      "step": 21
    },
    {
      "epoch": 0.05146198830409357,
      "grad_norm": 1.5169371366500854,
      "learning_rate": 1.5456674473067914e-05,
      "loss": 2.1413,
      "step": 22
    },
    {
      "epoch": 0.05380116959064327,
      "grad_norm": 1.4261071681976318,
      "learning_rate": 1.6159250585480093e-05,
      "loss": 2.1641,
      "step": 23
    },
    {
      "epoch": 0.056140350877192984,
      "grad_norm": 1.4440652132034302,
      "learning_rate": 1.686182669789227e-05,
      "loss": 2.1194,
      "step": 24
    },
    {
      "epoch": 0.05847953216374269,
      "grad_norm": 1.497520089149475,
      "learning_rate": 1.756440281030445e-05,
      "loss": 2.0722,
      "step": 25
    },
    {
      "epoch": 0.0608187134502924,
      "grad_norm": 1.5569618940353394,
      "learning_rate": 1.8266978922716625e-05,
      "loss": 2.0396,
      "step": 26
    },
    {
      "epoch": 0.06315789473684211,
      "grad_norm": 1.6468968391418457,
      "learning_rate": 1.8969555035128803e-05,
      "loss": 1.9606,
      "step": 27
    },
    {
      "epoch": 0.06549707602339182,
      "grad_norm": 1.53485107421875,
      "learning_rate": 1.9672131147540982e-05,
      "loss": 1.9695,
      "step": 28
    },
    {
      "epoch": 0.06783625730994151,
      "grad_norm": 1.6037205457687378,
      "learning_rate": 2.037470725995316e-05,
      "loss": 1.9706,
      "step": 29
    },
    {
      "epoch": 0.07017543859649122,
      "grad_norm": 1.5494742393493652,
      "learning_rate": 2.1077283372365335e-05,
      "loss": 1.9768,
      "step": 30
    },
    {
      "epoch": 0.07251461988304093,
      "grad_norm": 1.6133923530578613,
      "learning_rate": 2.1779859484777514e-05,
      "loss": 1.917,
      "step": 31
    },
    {
      "epoch": 0.07485380116959064,
      "grad_norm": 1.6675537824630737,
      "learning_rate": 2.2482435597189693e-05,
      "loss": 1.8233,
      "step": 32
    },
    {
      "epoch": 0.07719298245614035,
      "grad_norm": 1.6955828666687012,
      "learning_rate": 2.318501170960187e-05,
      "loss": 1.775,
      "step": 33
    },
    {
      "epoch": 0.07953216374269007,
      "grad_norm": 1.7579412460327148,
      "learning_rate": 2.388758782201405e-05,
      "loss": 1.7184,
      "step": 34
    },
    {
      "epoch": 0.08187134502923976,
      "grad_norm": 1.8365693092346191,
      "learning_rate": 2.4590163934426225e-05,
      "loss": 1.6323,
      "step": 35
    },
    {
      "epoch": 0.08421052631578947,
      "grad_norm": 1.843304991722107,
      "learning_rate": 2.5292740046838403e-05,
      "loss": 1.6149,
      "step": 36
    },
    {
      "epoch": 0.08654970760233918,
      "grad_norm": 1.8776614665985107,
      "learning_rate": 2.5995316159250582e-05,
      "loss": 1.5868,
      "step": 37
    },
    {
      "epoch": 0.08888888888888889,
      "grad_norm": 1.9316155910491943,
      "learning_rate": 2.669789227166276e-05,
      "loss": 1.4858,
      "step": 38
    },
    {
      "epoch": 0.0912280701754386,
      "grad_norm": 1.938605785369873,
      "learning_rate": 2.740046838407494e-05,
      "loss": 1.4254,
      "step": 39
    },
    {
      "epoch": 0.0935672514619883,
      "grad_norm": 1.898074984550476,
      "learning_rate": 2.8103044496487117e-05,
      "loss": 1.324,
      "step": 40
    },
    {
      "epoch": 0.09590643274853801,
      "grad_norm": 1.9881398677825928,
      "learning_rate": 2.8805620608899293e-05,
      "loss": 1.2262,
      "step": 41
    },
    {
      "epoch": 0.09824561403508772,
      "grad_norm": 1.9419429302215576,
      "learning_rate": 2.950819672131147e-05,
      "loss": 1.1464,
      "step": 42
    },
    {
      "epoch": 0.10058479532163743,
      "grad_norm": 1.827988624572754,
      "learning_rate": 3.021077283372365e-05,
      "loss": 1.1047,
      "step": 43
    },
    {
      "epoch": 0.10292397660818714,
      "grad_norm": 1.8009049892425537,
      "learning_rate": 3.091334894613583e-05,
      "loss": 1.0083,
      "step": 44
    },
    {
      "epoch": 0.10526315789473684,
      "grad_norm": 1.61443030834198,
      "learning_rate": 3.161592505854801e-05,
      "loss": 0.9558,
      "step": 45
    },
    {
      "epoch": 0.10760233918128655,
      "grad_norm": 1.6005187034606934,
      "learning_rate": 3.2318501170960185e-05,
      "loss": 0.9,
      "step": 46
    },
    {
      "epoch": 0.10994152046783626,
      "grad_norm": 1.483568549156189,
      "learning_rate": 3.3021077283372364e-05,
      "loss": 0.8273,
      "step": 47
    },
    {
      "epoch": 0.11228070175438597,
      "grad_norm": 1.4527947902679443,
      "learning_rate": 3.372365339578454e-05,
      "loss": 0.7825,
      "step": 48
    },
    {
      "epoch": 0.11461988304093568,
      "grad_norm": 1.3458753824234009,
      "learning_rate": 3.442622950819672e-05,
      "loss": 0.6789,
      "step": 49
    },
    {
      "epoch": 0.11695906432748537,
      "grad_norm": 1.086005449295044,
      "learning_rate": 3.51288056206089e-05,
      "loss": 0.662,
      "step": 50
    },
    {
      "epoch": 0.11929824561403508,
      "grad_norm": 0.9267519116401672,
      "learning_rate": 3.583138173302107e-05,
      "loss": 0.6372,
      "step": 51
    },
    {
      "epoch": 0.1216374269005848,
      "grad_norm": 0.9496387839317322,
      "learning_rate": 3.653395784543325e-05,
      "loss": 0.5484,
      "step": 52
    },
    {
      "epoch": 0.1239766081871345,
      "grad_norm": 0.9137327671051025,
      "learning_rate": 3.723653395784543e-05,
      "loss": 0.4827,
      "step": 53
    },
    {
      "epoch": 0.12631578947368421,
      "grad_norm": 0.7862095236778259,
      "learning_rate": 3.7939110070257607e-05,
      "loss": 0.5069,
      "step": 54
    },
    {
      "epoch": 0.1286549707602339,
      "grad_norm": 0.6803898811340332,
      "learning_rate": 3.8641686182669785e-05,
      "loss": 0.5504,
      "step": 55
    },
    {
      "epoch": 0.13099415204678364,
      "grad_norm": 0.644611120223999,
      "learning_rate": 3.9344262295081964e-05,
      "loss": 0.5507,
      "step": 56
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 0.6711792349815369,
      "learning_rate": 4.004683840749414e-05,
      "loss": 0.4562,
      "step": 57
    },
    {
      "epoch": 0.13567251461988303,
      "grad_norm": 0.5895348787307739,
      "learning_rate": 4.074941451990632e-05,
      "loss": 0.4876,
      "step": 58
    },
    {
      "epoch": 0.13801169590643275,
      "grad_norm": 0.6154860258102417,
      "learning_rate": 4.145199063231849e-05,
      "loss": 0.4486,
      "step": 59
    },
    {
      "epoch": 0.14035087719298245,
      "grad_norm": 0.677345871925354,
      "learning_rate": 4.215456674473067e-05,
      "loss": 0.4827,
      "step": 60
    },
    {
      "epoch": 0.14269005847953217,
      "grad_norm": 0.8004550933837891,
      "learning_rate": 4.285714285714285e-05,
      "loss": 0.4375,
      "step": 61
    },
    {
      "epoch": 0.14502923976608187,
      "grad_norm": 0.819705069065094,
      "learning_rate": 4.355971896955503e-05,
      "loss": 0.4443,
      "step": 62
    },
    {
      "epoch": 0.14736842105263157,
      "grad_norm": 0.8477626442909241,
      "learning_rate": 4.4262295081967207e-05,
      "loss": 0.4887,
      "step": 63
    },
    {
      "epoch": 0.1497076023391813,
      "grad_norm": 0.7171208262443542,
      "learning_rate": 4.4964871194379385e-05,
      "loss": 0.3952,
      "step": 64
    },
    {
      "epoch": 0.15204678362573099,
      "grad_norm": 0.7436864972114563,
      "learning_rate": 4.5667447306791564e-05,
      "loss": 0.431,
      "step": 65
    },
    {
      "epoch": 0.1543859649122807,
      "grad_norm": 0.5279985070228577,
      "learning_rate": 4.637002341920374e-05,
      "loss": 0.3575,
      "step": 66
    },
    {
      "epoch": 0.1567251461988304,
      "grad_norm": 0.5877094268798828,
      "learning_rate": 4.707259953161592e-05,
      "loss": 0.3713,
      "step": 67
    },
    {
      "epoch": 0.15906432748538013,
      "grad_norm": 0.6235076189041138,
      "learning_rate": 4.77751756440281e-05,
      "loss": 0.504,
      "step": 68
    },
    {
      "epoch": 0.16140350877192983,
      "grad_norm": 0.6021946668624878,
      "learning_rate": 4.847775175644028e-05,
      "loss": 0.4352,
      "step": 69
    },
    {
      "epoch": 0.16374269005847952,
      "grad_norm": 0.534774124622345,
      "learning_rate": 4.918032786885245e-05,
      "loss": 0.4011,
      "step": 70
    },
    {
      "epoch": 0.16608187134502925,
      "grad_norm": 0.783649742603302,
      "learning_rate": 4.988290398126463e-05,
      "loss": 0.3257,
      "step": 71
    },
    {
      "epoch": 0.16842105263157894,
      "grad_norm": 0.8962121605873108,
      "learning_rate": 5.0585480093676807e-05,
      "loss": 0.3956,
      "step": 72
    },
    {
      "epoch": 0.17076023391812867,
      "grad_norm": 0.9246845841407776,
      "learning_rate": 5.1288056206088985e-05,
      "loss": 0.3927,
      "step": 73
    },
    {
      "epoch": 0.17309941520467836,
      "grad_norm": 0.6391623020172119,
      "learning_rate": 5.1990632318501164e-05,
      "loss": 0.4137,
      "step": 74
    },
    {
      "epoch": 0.17543859649122806,
      "grad_norm": 0.634492039680481,
      "learning_rate": 5.269320843091334e-05,
      "loss": 0.3309,
      "step": 75
    },
    {
      "epoch": 0.17777777777777778,
      "grad_norm": 0.5230917930603027,
      "learning_rate": 5.339578454332552e-05,
      "loss": 0.3262,
      "step": 76
    },
    {
      "epoch": 0.18011695906432748,
      "grad_norm": 0.5794457197189331,
      "learning_rate": 5.40983606557377e-05,
      "loss": 0.3387,
      "step": 77
    },
    {
      "epoch": 0.1824561403508772,
      "grad_norm": 0.7706198692321777,
      "learning_rate": 5.480093676814988e-05,
      "loss": 0.3628,
      "step": 78
    },
    {
      "epoch": 0.1847953216374269,
      "grad_norm": 0.5979105830192566,
      "learning_rate": 5.5503512880562056e-05,
      "loss": 0.2912,
      "step": 79
    },
    {
      "epoch": 0.1871345029239766,
      "grad_norm": 0.7409761548042297,
      "learning_rate": 5.6206088992974235e-05,
      "loss": 0.3757,
      "step": 80
    },
    {
      "epoch": 0.18947368421052632,
      "grad_norm": 0.5364107489585876,
      "learning_rate": 5.6908665105386407e-05,
      "loss": 0.3327,
      "step": 81
    },
    {
      "epoch": 0.19181286549707602,
      "grad_norm": 0.6606488823890686,
      "learning_rate": 5.7611241217798585e-05,
      "loss": 0.4134,
      "step": 82
    },
    {
      "epoch": 0.19415204678362574,
      "grad_norm": 0.44536739587783813,
      "learning_rate": 5.8313817330210764e-05,
      "loss": 0.3178,
      "step": 83
    },
    {
      "epoch": 0.19649122807017544,
      "grad_norm": 0.3524056077003479,
      "learning_rate": 5.901639344262294e-05,
      "loss": 0.3606,
      "step": 84
    },
    {
      "epoch": 0.19883040935672514,
      "grad_norm": 0.463692843914032,
      "learning_rate": 5.971896955503512e-05,
      "loss": 0.3419,
      "step": 85
    },
    {
      "epoch": 0.20116959064327486,
      "grad_norm": 0.48036304116249084,
      "learning_rate": 6.04215456674473e-05,
      "loss": 0.3185,
      "step": 86
    },
    {
      "epoch": 0.20350877192982456,
      "grad_norm": 0.5142580270767212,
      "learning_rate": 6.112412177985948e-05,
      "loss": 0.2941,
      "step": 87
    },
    {
      "epoch": 0.20584795321637428,
      "grad_norm": 0.6356299519538879,
      "learning_rate": 6.182669789227166e-05,
      "loss": 0.2949,
      "step": 88
    },
    {
      "epoch": 0.20818713450292398,
      "grad_norm": 0.537380576133728,
      "learning_rate": 6.252927400468383e-05,
      "loss": 0.2952,
      "step": 89
    },
    {
      "epoch": 0.21052631578947367,
      "grad_norm": 0.4168287515640259,
      "learning_rate": 6.323185011709601e-05,
      "loss": 0.4059,
      "step": 90
    },
    {
      "epoch": 0.2128654970760234,
      "grad_norm": 0.4222019910812378,
      "learning_rate": 6.393442622950819e-05,
      "loss": 0.2892,
      "step": 91
    },
    {
      "epoch": 0.2152046783625731,
      "grad_norm": 0.31904223561286926,
      "learning_rate": 6.463700234192037e-05,
      "loss": 0.3195,
      "step": 92
    },
    {
      "epoch": 0.21754385964912282,
      "grad_norm": 0.331831157207489,
      "learning_rate": 6.533957845433255e-05,
      "loss": 0.2436,
      "step": 93
    },
    {
      "epoch": 0.2198830409356725,
      "grad_norm": 0.4414081871509552,
      "learning_rate": 6.604215456674473e-05,
      "loss": 0.3375,
      "step": 94
    },
    {
      "epoch": 0.2222222222222222,
      "grad_norm": 0.5866371393203735,
      "learning_rate": 6.67447306791569e-05,
      "loss": 0.3788,
      "step": 95
    },
    {
      "epoch": 0.22456140350877193,
      "grad_norm": 0.3661201596260071,
      "learning_rate": 6.744730679156908e-05,
      "loss": 0.2982,
      "step": 96
    },
    {
      "epoch": 0.22690058479532163,
      "grad_norm": 0.3820642828941345,
      "learning_rate": 6.814988290398126e-05,
      "loss": 0.2823,
      "step": 97
    },
    {
      "epoch": 0.22923976608187135,
      "grad_norm": 0.6507378816604614,
      "learning_rate": 6.885245901639344e-05,
      "loss": 0.3311,
      "step": 98
    },
    {
      "epoch": 0.23157894736842105,
      "grad_norm": 0.5076301097869873,
      "learning_rate": 6.955503512880562e-05,
      "loss": 0.3684,
      "step": 99
    },
    {
      "epoch": 0.23391812865497075,
      "grad_norm": 0.577716052532196,
      "learning_rate": 7.02576112412178e-05,
      "loss": 0.4254,
      "step": 100
    },
    {
      "epoch": 0.23625730994152047,
      "grad_norm": 0.577597439289093,
      "learning_rate": 7.096018735362998e-05,
      "loss": 0.3029,
      "step": 101
    },
    {
      "epoch": 0.23859649122807017,
      "grad_norm": 0.4219337999820709,
      "learning_rate": 7.166276346604214e-05,
      "loss": 0.3082,
      "step": 102
    },
    {
      "epoch": 0.2409356725146199,
      "grad_norm": 0.5172156095504761,
      "learning_rate": 7.236533957845432e-05,
      "loss": 0.2715,
      "step": 103
    },
    {
      "epoch": 0.2432748538011696,
      "grad_norm": 0.4388159513473511,
      "learning_rate": 7.30679156908665e-05,
      "loss": 0.3685,
      "step": 104
    },
    {
      "epoch": 0.24561403508771928,
      "grad_norm": 0.44052553176879883,
      "learning_rate": 7.377049180327868e-05,
      "loss": 0.3301,
      "step": 105
    },
    {
      "epoch": 0.247953216374269,
      "grad_norm": 0.35001930594444275,
      "learning_rate": 7.447306791569086e-05,
      "loss": 0.3142,
      "step": 106
    },
    {
      "epoch": 0.25029239766081873,
      "grad_norm": 0.39933305978775024,
      "learning_rate": 7.517564402810303e-05,
      "loss": 0.2978,
      "step": 107
    },
    {
      "epoch": 0.25263157894736843,
      "grad_norm": 0.41643548011779785,
      "learning_rate": 7.587822014051521e-05,
      "loss": 0.3101,
      "step": 108
    },
    {
      "epoch": 0.2549707602339181,
      "grad_norm": 0.40143197774887085,
      "learning_rate": 7.658079625292739e-05,
      "loss": 0.3549,
      "step": 109
    },
    {
      "epoch": 0.2573099415204678,
      "grad_norm": 0.32728227972984314,
      "learning_rate": 7.728337236533957e-05,
      "loss": 0.2674,
      "step": 110
    },
    {
      "epoch": 0.2596491228070175,
      "grad_norm": 0.3449554145336151,
      "learning_rate": 7.798594847775175e-05,
      "loss": 0.315,
      "step": 111
    },
    {
      "epoch": 0.26198830409356727,
      "grad_norm": 0.4375527501106262,
      "learning_rate": 7.868852459016393e-05,
      "loss": 0.3348,
      "step": 112
    },
    {
      "epoch": 0.26432748538011697,
      "grad_norm": 0.5052425265312195,
      "learning_rate": 7.93911007025761e-05,
      "loss": 0.3855,
      "step": 113
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 0.5234168767929077,
      "learning_rate": 8.009367681498828e-05,
      "loss": 0.4546,
      "step": 114
    },
    {
      "epoch": 0.26900584795321636,
      "grad_norm": 0.4496801495552063,
      "learning_rate": 8.079625292740046e-05,
      "loss": 0.3535,
      "step": 115
    },
    {
      "epoch": 0.27134502923976606,
      "grad_norm": 0.4848123788833618,
      "learning_rate": 8.149882903981264e-05,
      "loss": 0.322,
      "step": 116
    },
    {
      "epoch": 0.2736842105263158,
      "grad_norm": 0.3914705216884613,
      "learning_rate": 8.220140515222482e-05,
      "loss": 0.3334,
      "step": 117
    },
    {
      "epoch": 0.2760233918128655,
      "grad_norm": 0.36329108476638794,
      "learning_rate": 8.290398126463698e-05,
      "loss": 0.3889,
      "step": 118
    },
    {
      "epoch": 0.2783625730994152,
      "grad_norm": 0.3948042392730713,
      "learning_rate": 8.360655737704916e-05,
      "loss": 0.3321,
      "step": 119
    },
    {
      "epoch": 0.2807017543859649,
      "grad_norm": 0.4534004330635071,
      "learning_rate": 8.430913348946134e-05,
      "loss": 0.2674,
      "step": 120
    },
    {
      "epoch": 0.2830409356725146,
      "grad_norm": 0.3946778178215027,
      "learning_rate": 8.501170960187352e-05,
      "loss": 0.2599,
      "step": 121
    },
    {
      "epoch": 0.28538011695906434,
      "grad_norm": 0.3896239995956421,
      "learning_rate": 8.57142857142857e-05,
      "loss": 0.318,
      "step": 122
    },
    {
      "epoch": 0.28771929824561404,
      "grad_norm": 0.42113733291625977,
      "learning_rate": 8.641686182669788e-05,
      "loss": 0.3226,
      "step": 123
    },
    {
      "epoch": 0.29005847953216374,
      "grad_norm": 0.4344361424446106,
      "learning_rate": 8.711943793911006e-05,
      "loss": 0.3059,
      "step": 124
    },
    {
      "epoch": 0.29239766081871343,
      "grad_norm": 0.602556586265564,
      "learning_rate": 8.782201405152223e-05,
      "loss": 0.3962,
      "step": 125
    },
    {
      "epoch": 0.29473684210526313,
      "grad_norm": 0.4703039526939392,
      "learning_rate": 8.852459016393441e-05,
      "loss": 0.352,
      "step": 126
    },
    {
      "epoch": 0.2970760233918129,
      "grad_norm": 0.4935537576675415,
      "learning_rate": 8.922716627634659e-05,
      "loss": 0.3152,
      "step": 127
    },
    {
      "epoch": 0.2994152046783626,
      "grad_norm": 0.4223690330982208,
      "learning_rate": 8.992974238875877e-05,
      "loss": 0.2898,
      "step": 128
    },
    {
      "epoch": 0.3017543859649123,
      "grad_norm": 0.5188520550727844,
      "learning_rate": 9.063231850117095e-05,
      "loss": 0.3183,
      "step": 129
    },
    {
      "epoch": 0.30409356725146197,
      "grad_norm": 0.3591254949569702,
      "learning_rate": 9.133489461358313e-05,
      "loss": 0.2947,
      "step": 130
    },
    {
      "epoch": 0.3064327485380117,
      "grad_norm": 0.33778059482574463,
      "learning_rate": 9.20374707259953e-05,
      "loss": 0.3356,
      "step": 131
    },
    {
      "epoch": 0.3087719298245614,
      "grad_norm": 0.4786590337753296,
      "learning_rate": 9.274004683840748e-05,
      "loss": 0.2726,
      "step": 132
    },
    {
      "epoch": 0.3111111111111111,
      "grad_norm": 0.5651459693908691,
      "learning_rate": 9.344262295081966e-05,
      "loss": 0.2712,
      "step": 133
    },
    {
      "epoch": 0.3134502923976608,
      "grad_norm": 0.40319427847862244,
      "learning_rate": 9.414519906323184e-05,
      "loss": 0.2552,
      "step": 134
    },
    {
      "epoch": 0.3157894736842105,
      "grad_norm": 0.3911469876766205,
      "learning_rate": 9.484777517564402e-05,
      "loss": 0.3366,
      "step": 135
    },
    {
      "epoch": 0.31812865497076026,
      "grad_norm": 0.5847146511077881,
      "learning_rate": 9.55503512880562e-05,
      "loss": 0.3671,
      "step": 136
    },
    {
      "epoch": 0.32046783625730996,
      "grad_norm": 0.4795289933681488,
      "learning_rate": 9.625292740046838e-05,
      "loss": 0.2067,
      "step": 137
    },
    {
      "epoch": 0.32280701754385965,
      "grad_norm": 0.4528527557849884,
      "learning_rate": 9.695550351288056e-05,
      "loss": 0.3177,
      "step": 138
    },
    {
      "epoch": 0.32514619883040935,
      "grad_norm": 0.48073750734329224,
      "learning_rate": 9.765807962529272e-05,
      "loss": 0.3636,
      "step": 139
    },
    {
      "epoch": 0.32748538011695905,
      "grad_norm": 0.5440852642059326,
      "learning_rate": 9.83606557377049e-05,
      "loss": 0.3981,
      "step": 140
    },
    {
      "epoch": 0.3298245614035088,
      "grad_norm": 0.5694328546524048,
      "learning_rate": 9.906323185011708e-05,
      "loss": 0.3328,
      "step": 141
    },
    {
      "epoch": 0.3321637426900585,
      "grad_norm": 0.6190544366836548,
      "learning_rate": 9.976580796252926e-05,
      "loss": 0.2964,
      "step": 142
    },
    {
      "epoch": 0.3345029239766082,
      "grad_norm": 0.4364238381385803,
      "learning_rate": 0.00010046838407494143,
      "loss": 0.2927,
      "step": 143
    },
    {
      "epoch": 0.3368421052631579,
      "grad_norm": 0.4689282178878784,
      "learning_rate": 0.00010117096018735361,
      "loss": 0.3275,
      "step": 144
    },
    {
      "epoch": 0.3391812865497076,
      "grad_norm": 0.45780521631240845,
      "learning_rate": 0.00010187353629976579,
      "loss": 0.2458,
      "step": 145
    },
    {
      "epoch": 0.34152046783625734,
      "grad_norm": 0.5542861223220825,
      "learning_rate": 0.00010257611241217797,
      "loss": 0.3571,
      "step": 146
    },
    {
      "epoch": 0.34385964912280703,
      "grad_norm": 0.4915057122707367,
      "learning_rate": 0.00010327868852459015,
      "loss": 0.3384,
      "step": 147
    },
    {
      "epoch": 0.34619883040935673,
      "grad_norm": 0.5115144848823547,
      "learning_rate": 0.00010398126463700233,
      "loss": 0.341,
      "step": 148
    },
    {
      "epoch": 0.3485380116959064,
      "grad_norm": 0.5788445472717285,
      "learning_rate": 0.0001046838407494145,
      "loss": 0.3459,
      "step": 149
    },
    {
      "epoch": 0.3508771929824561,
      "grad_norm": 0.4382838010787964,
      "learning_rate": 0.00010538641686182668,
      "loss": 0.2554,
      "step": 150
    },
    {
      "epoch": 0.3532163742690059,
      "grad_norm": 0.39688819646835327,
      "learning_rate": 0.00010608899297423886,
      "loss": 0.3197,
      "step": 151
    },
    {
      "epoch": 0.35555555555555557,
      "grad_norm": 0.36771631240844727,
      "learning_rate": 0.00010679156908665104,
      "loss": 0.2872,
      "step": 152
    },
    {
      "epoch": 0.35789473684210527,
      "grad_norm": 0.4410354793071747,
      "learning_rate": 0.00010749414519906322,
      "loss": 0.2777,
      "step": 153
    },
    {
      "epoch": 0.36023391812865496,
      "grad_norm": 0.451204389333725,
      "learning_rate": 0.0001081967213114754,
      "loss": 0.3125,
      "step": 154
    },
    {
      "epoch": 0.36257309941520466,
      "grad_norm": 0.5183623433113098,
      "learning_rate": 0.00010889929742388758,
      "loss": 0.3189,
      "step": 155
    },
    {
      "epoch": 0.3649122807017544,
      "grad_norm": 0.46266013383865356,
      "learning_rate": 0.00010960187353629976,
      "loss": 0.3497,
      "step": 156
    },
    {
      "epoch": 0.3672514619883041,
      "grad_norm": 0.4449625015258789,
      "learning_rate": 0.00011030444964871193,
      "loss": 0.3174,
      "step": 157
    },
    {
      "epoch": 0.3695906432748538,
      "grad_norm": 0.4182477593421936,
      "learning_rate": 0.00011100702576112411,
      "loss": 0.2929,
      "step": 158
    },
    {
      "epoch": 0.3719298245614035,
      "grad_norm": 0.3415277302265167,
      "learning_rate": 0.00011170960187353629,
      "loss": 0.3373,
      "step": 159
    },
    {
      "epoch": 0.3742690058479532,
      "grad_norm": 0.523028552532196,
      "learning_rate": 0.00011241217798594847,
      "loss": 0.2989,
      "step": 160
    },
    {
      "epoch": 0.37660818713450295,
      "grad_norm": 0.30025777220726013,
      "learning_rate": 0.00011311475409836063,
      "loss": 0.3028,
      "step": 161
    },
    {
      "epoch": 0.37894736842105264,
      "grad_norm": 0.42316606640815735,
      "learning_rate": 0.00011381733021077281,
      "loss": 0.3032,
      "step": 162
    },
    {
      "epoch": 0.38128654970760234,
      "grad_norm": 0.42142874002456665,
      "learning_rate": 0.00011451990632318499,
      "loss": 0.3029,
      "step": 163
    },
    {
      "epoch": 0.38362573099415204,
      "grad_norm": 0.38606539368629456,
      "learning_rate": 0.00011522248243559717,
      "loss": 0.2786,
      "step": 164
    },
    {
      "epoch": 0.38596491228070173,
      "grad_norm": 0.37873706221580505,
      "learning_rate": 0.00011592505854800935,
      "loss": 0.2339,
      "step": 165
    },
    {
      "epoch": 0.3883040935672515,
      "grad_norm": 0.4845566749572754,
      "learning_rate": 0.00011662763466042153,
      "loss": 0.2963,
      "step": 166
    },
    {
      "epoch": 0.3906432748538012,
      "grad_norm": 0.37086358666419983,
      "learning_rate": 0.0001173302107728337,
      "loss": 0.2858,
      "step": 167
    },
    {
      "epoch": 0.3929824561403509,
      "grad_norm": 0.4479604661464691,
      "learning_rate": 0.00011803278688524588,
      "loss": 0.3169,
      "step": 168
    },
    {
      "epoch": 0.3953216374269006,
      "grad_norm": 0.4554665684700012,
      "learning_rate": 0.00011873536299765806,
      "loss": 0.3253,
      "step": 169
    },
    {
      "epoch": 0.39766081871345027,
      "grad_norm": 0.5109367966651917,
      "learning_rate": 0.00011943793911007024,
      "loss": 0.3158,
      "step": 170
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.3769114315509796,
      "learning_rate": 0.00012014051522248242,
      "loss": 0.2765,
      "step": 171
    },
    {
      "epoch": 0.4023391812865497,
      "grad_norm": 0.5141635537147522,
      "learning_rate": 0.0001208430913348946,
      "loss": 0.3298,
      "step": 172
    },
    {
      "epoch": 0.4046783625730994,
      "grad_norm": 0.33726632595062256,
      "learning_rate": 0.00012154566744730678,
      "loss": 0.2859,
      "step": 173
    },
    {
      "epoch": 0.4070175438596491,
      "grad_norm": 0.4448893368244171,
      "learning_rate": 0.00012224824355971896,
      "loss": 0.3433,
      "step": 174
    },
    {
      "epoch": 0.4093567251461988,
      "grad_norm": 0.33489757776260376,
      "learning_rate": 0.00012295081967213115,
      "loss": 0.2887,
      "step": 175
    },
    {
      "epoch": 0.41169590643274856,
      "grad_norm": 0.5036230683326721,
      "learning_rate": 0.0001236533957845433,
      "loss": 0.366,
      "step": 176
    },
    {
      "epoch": 0.41403508771929826,
      "grad_norm": 0.5711928009986877,
      "learning_rate": 0.0001243559718969555,
      "loss": 0.2711,
      "step": 177
    },
    {
      "epoch": 0.41637426900584795,
      "grad_norm": 0.5631723403930664,
      "learning_rate": 0.00012505854800936767,
      "loss": 0.1986,
      "step": 178
    },
    {
      "epoch": 0.41871345029239765,
      "grad_norm": 0.5220308899879456,
      "learning_rate": 0.00012576112412177986,
      "loss": 0.3497,
      "step": 179
    },
    {
      "epoch": 0.42105263157894735,
      "grad_norm": 0.5041213631629944,
      "learning_rate": 0.00012646370023419203,
      "loss": 0.3456,
      "step": 180
    },
    {
      "epoch": 0.4233918128654971,
      "grad_norm": 0.47398531436920166,
      "learning_rate": 0.00012716627634660422,
      "loss": 0.318,
      "step": 181
    },
    {
      "epoch": 0.4257309941520468,
      "grad_norm": 0.5366800427436829,
      "learning_rate": 0.00012786885245901638,
      "loss": 0.3588,
      "step": 182
    },
    {
      "epoch": 0.4280701754385965,
      "grad_norm": 0.6290855407714844,
      "learning_rate": 0.00012857142857142855,
      "loss": 0.3445,
      "step": 183
    },
    {
      "epoch": 0.4304093567251462,
      "grad_norm": 0.4664190709590912,
      "learning_rate": 0.00012927400468384074,
      "loss": 0.3113,
      "step": 184
    },
    {
      "epoch": 0.4327485380116959,
      "grad_norm": 0.5228801369667053,
      "learning_rate": 0.0001299765807962529,
      "loss": 0.262,
      "step": 185
    },
    {
      "epoch": 0.43508771929824563,
      "grad_norm": 0.36389026045799255,
      "learning_rate": 0.0001306791569086651,
      "loss": 0.2865,
      "step": 186
    },
    {
      "epoch": 0.43742690058479533,
      "grad_norm": 0.4274851381778717,
      "learning_rate": 0.00013138173302107726,
      "loss": 0.2543,
      "step": 187
    },
    {
      "epoch": 0.439766081871345,
      "grad_norm": 0.41874048113822937,
      "learning_rate": 0.00013208430913348945,
      "loss": 0.3812,
      "step": 188
    },
    {
      "epoch": 0.4421052631578947,
      "grad_norm": 0.4466554522514343,
      "learning_rate": 0.00013278688524590162,
      "loss": 0.3691,
      "step": 189
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 0.4569456875324249,
      "learning_rate": 0.0001334894613583138,
      "loss": 0.3399,
      "step": 190
    },
    {
      "epoch": 0.44678362573099417,
      "grad_norm": 0.36179840564727783,
      "learning_rate": 0.00013419203747072598,
      "loss": 0.3413,
      "step": 191
    },
    {
      "epoch": 0.44912280701754387,
      "grad_norm": 0.5080217123031616,
      "learning_rate": 0.00013489461358313817,
      "loss": 0.3649,
      "step": 192
    },
    {
      "epoch": 0.45146198830409356,
      "grad_norm": 0.475171834230423,
      "learning_rate": 0.00013559718969555033,
      "loss": 0.3164,
      "step": 193
    },
    {
      "epoch": 0.45380116959064326,
      "grad_norm": 0.4558517038822174,
      "learning_rate": 0.00013629976580796253,
      "loss": 0.3349,
      "step": 194
    },
    {
      "epoch": 0.45614035087719296,
      "grad_norm": 0.4865415394306183,
      "learning_rate": 0.0001370023419203747,
      "loss": 0.2336,
      "step": 195
    },
    {
      "epoch": 0.4584795321637427,
      "grad_norm": 0.3909963071346283,
      "learning_rate": 0.00013770491803278688,
      "loss": 0.2866,
      "step": 196
    },
    {
      "epoch": 0.4608187134502924,
      "grad_norm": 0.4660194218158722,
      "learning_rate": 0.00013840749414519905,
      "loss": 0.2547,
      "step": 197
    },
    {
      "epoch": 0.4631578947368421,
      "grad_norm": 0.527320921421051,
      "learning_rate": 0.00013911007025761124,
      "loss": 0.3403,
      "step": 198
    },
    {
      "epoch": 0.4654970760233918,
      "grad_norm": 0.48803019523620605,
      "learning_rate": 0.0001398126463700234,
      "loss": 0.2667,
      "step": 199
    },
    {
      "epoch": 0.4678362573099415,
      "grad_norm": 0.5550084710121155,
      "learning_rate": 0.0001405152224824356,
      "loss": 0.2876,
      "step": 200
    },
    {
      "epoch": 0.47017543859649125,
      "grad_norm": 0.5402237176895142,
      "learning_rate": 0.00014121779859484776,
      "loss": 0.2996,
      "step": 201
    },
    {
      "epoch": 0.47251461988304094,
      "grad_norm": 0.5052096843719482,
      "learning_rate": 0.00014192037470725995,
      "loss": 0.3533,
      "step": 202
    },
    {
      "epoch": 0.47485380116959064,
      "grad_norm": 0.5003495216369629,
      "learning_rate": 0.00014262295081967212,
      "loss": 0.3345,
      "step": 203
    },
    {
      "epoch": 0.47719298245614034,
      "grad_norm": 0.5049117803573608,
      "learning_rate": 0.00014332552693208428,
      "loss": 0.3718,
      "step": 204
    },
    {
      "epoch": 0.47953216374269003,
      "grad_norm": 0.580350935459137,
      "learning_rate": 0.00014402810304449648,
      "loss": 0.2562,
      "step": 205
    },
    {
      "epoch": 0.4818713450292398,
      "grad_norm": 0.5161653757095337,
      "learning_rate": 0.00014473067915690864,
      "loss": 0.2688,
      "step": 206
    },
    {
      "epoch": 0.4842105263157895,
      "grad_norm": 0.3924468755722046,
      "learning_rate": 0.00014543325526932083,
      "loss": 0.3311,
      "step": 207
    },
    {
      "epoch": 0.4865497076023392,
      "grad_norm": 0.38626596331596375,
      "learning_rate": 0.000146135831381733,
      "loss": 0.3193,
      "step": 208
    },
    {
      "epoch": 0.4888888888888889,
      "grad_norm": 0.4724261462688446,
      "learning_rate": 0.0001468384074941452,
      "loss": 0.3305,
      "step": 209
    },
    {
      "epoch": 0.49122807017543857,
      "grad_norm": 0.4696318805217743,
      "learning_rate": 0.00014754098360655736,
      "loss": 0.313,
      "step": 210
    },
    {
      "epoch": 0.4935672514619883,
      "grad_norm": 0.4435808062553406,
      "learning_rate": 0.00014824355971896955,
      "loss": 0.3065,
      "step": 211
    },
    {
      "epoch": 0.495906432748538,
      "grad_norm": 0.42788779735565186,
      "learning_rate": 0.0001489461358313817,
      "loss": 0.2379,
      "step": 212
    },
    {
      "epoch": 0.4982456140350877,
      "grad_norm": 0.44415217638015747,
      "learning_rate": 0.0001496487119437939,
      "loss": 0.256,
      "step": 213
    },
    {
      "epoch": 0.5005847953216375,
      "grad_norm": 0.8387413620948792,
      "learning_rate": 0.00015035128805620607,
      "loss": 0.3341,
      "step": 214
    },
    {
      "epoch": 0.5029239766081871,
      "grad_norm": 0.5046880841255188,
      "learning_rate": 0.00015105386416861826,
      "loss": 0.2742,
      "step": 215
    },
    {
      "epoch": 0.5052631578947369,
      "grad_norm": 0.5436409115791321,
      "learning_rate": 0.00015175644028103043,
      "loss": 0.2824,
      "step": 216
    },
    {
      "epoch": 0.5076023391812865,
      "grad_norm": 0.4819660484790802,
      "learning_rate": 0.00015245901639344262,
      "loss": 0.3421,
      "step": 217
    },
    {
      "epoch": 0.5099415204678363,
      "grad_norm": 0.4409058094024658,
      "learning_rate": 0.00015316159250585478,
      "loss": 0.285,
      "step": 218
    },
    {
      "epoch": 0.512280701754386,
      "grad_norm": 0.39279767870903015,
      "learning_rate": 0.00015386416861826698,
      "loss": 0.288,
      "step": 219
    },
    {
      "epoch": 0.5146198830409356,
      "grad_norm": 0.4048663079738617,
      "learning_rate": 0.00015456674473067914,
      "loss": 0.3155,
      "step": 220
    },
    {
      "epoch": 0.5169590643274854,
      "grad_norm": 0.45693281292915344,
      "learning_rate": 0.00015526932084309133,
      "loss": 0.1867,
      "step": 221
    },
    {
      "epoch": 0.519298245614035,
      "grad_norm": 0.4569353461265564,
      "learning_rate": 0.0001559718969555035,
      "loss": 0.3682,
      "step": 222
    },
    {
      "epoch": 0.5216374269005848,
      "grad_norm": 0.39656728506088257,
      "learning_rate": 0.0001566744730679157,
      "loss": 0.3134,
      "step": 223
    },
    {
      "epoch": 0.5239766081871345,
      "grad_norm": 0.570990800857544,
      "learning_rate": 0.00015737704918032785,
      "loss": 0.3374,
      "step": 224
    },
    {
      "epoch": 0.5263157894736842,
      "grad_norm": 0.49385926127433777,
      "learning_rate": 0.00015807962529274005,
      "loss": 0.3298,
      "step": 225
    },
    {
      "epoch": 0.5286549707602339,
      "grad_norm": 0.7807092070579529,
      "learning_rate": 0.0001587822014051522,
      "loss": 0.3863,
      "step": 226
    },
    {
      "epoch": 0.5309941520467836,
      "grad_norm": 0.46367672085762024,
      "learning_rate": 0.0001594847775175644,
      "loss": 0.3216,
      "step": 227
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 0.4751381278038025,
      "learning_rate": 0.00016018735362997657,
      "loss": 0.3614,
      "step": 228
    },
    {
      "epoch": 0.5356725146198831,
      "grad_norm": 0.4110915958881378,
      "learning_rate": 0.00016088992974238876,
      "loss": 0.2993,
      "step": 229
    },
    {
      "epoch": 0.5380116959064327,
      "grad_norm": 0.45811304450035095,
      "learning_rate": 0.00016159250585480093,
      "loss": 0.2414,
      "step": 230
    },
    {
      "epoch": 0.5403508771929825,
      "grad_norm": 0.5938429832458496,
      "learning_rate": 0.00016229508196721312,
      "loss": 0.3127,
      "step": 231
    },
    {
      "epoch": 0.5426900584795321,
      "grad_norm": 0.3412817418575287,
      "learning_rate": 0.00016299765807962528,
      "loss": 0.2361,
      "step": 232
    },
    {
      "epoch": 0.5450292397660819,
      "grad_norm": 0.5427737236022949,
      "learning_rate": 0.00016370023419203747,
      "loss": 0.3143,
      "step": 233
    },
    {
      "epoch": 0.5473684210526316,
      "grad_norm": 0.46460792422294617,
      "learning_rate": 0.00016440281030444964,
      "loss": 0.2756,
      "step": 234
    },
    {
      "epoch": 0.5497076023391813,
      "grad_norm": 0.6054436564445496,
      "learning_rate": 0.0001651053864168618,
      "loss": 0.3233,
      "step": 235
    },
    {
      "epoch": 0.552046783625731,
      "grad_norm": 0.5137290358543396,
      "learning_rate": 0.00016580796252927397,
      "loss": 0.371,
      "step": 236
    },
    {
      "epoch": 0.5543859649122806,
      "grad_norm": 0.6413848400115967,
      "learning_rate": 0.00016651053864168616,
      "loss": 0.3541,
      "step": 237
    },
    {
      "epoch": 0.5567251461988304,
      "grad_norm": 0.37920868396759033,
      "learning_rate": 0.00016721311475409833,
      "loss": 0.3328,
      "step": 238
    },
    {
      "epoch": 0.5590643274853802,
      "grad_norm": 0.3790728747844696,
      "learning_rate": 0.00016791569086651052,
      "loss": 0.2809,
      "step": 239
    },
    {
      "epoch": 0.5614035087719298,
      "grad_norm": 0.5431953072547913,
      "learning_rate": 0.00016861826697892268,
      "loss": 0.3238,
      "step": 240
    },
    {
      "epoch": 0.5637426900584795,
      "grad_norm": 0.40113937854766846,
      "learning_rate": 0.00016932084309133488,
      "loss": 0.2661,
      "step": 241
    },
    {
      "epoch": 0.5660818713450292,
      "grad_norm": 0.4102403521537781,
      "learning_rate": 0.00017002341920374704,
      "loss": 0.2647,
      "step": 242
    },
    {
      "epoch": 0.5684210526315789,
      "grad_norm": 0.4536903202533722,
      "learning_rate": 0.00017072599531615923,
      "loss": 0.2811,
      "step": 243
    },
    {
      "epoch": 0.5707602339181287,
      "grad_norm": 0.6753126978874207,
      "learning_rate": 0.0001714285714285714,
      "loss": 0.3686,
      "step": 244
    },
    {
      "epoch": 0.5730994152046783,
      "grad_norm": 0.5232048630714417,
      "learning_rate": 0.0001721311475409836,
      "loss": 0.3142,
      "step": 245
    },
    {
      "epoch": 0.5754385964912281,
      "grad_norm": 0.42435258626937866,
      "learning_rate": 0.00017283372365339576,
      "loss": 0.2502,
      "step": 246
    },
    {
      "epoch": 0.5777777777777777,
      "grad_norm": 0.39423274993896484,
      "learning_rate": 0.00017353629976580795,
      "loss": 0.3025,
      "step": 247
    },
    {
      "epoch": 0.5801169590643275,
      "grad_norm": 0.5361171364784241,
      "learning_rate": 0.0001742388758782201,
      "loss": 0.3383,
      "step": 248
    },
    {
      "epoch": 0.5824561403508772,
      "grad_norm": 0.5192535519599915,
      "learning_rate": 0.0001749414519906323,
      "loss": 0.2462,
      "step": 249
    },
    {
      "epoch": 0.5847953216374269,
      "grad_norm": 0.7017741799354553,
      "learning_rate": 0.00017564402810304447,
      "loss": 0.3175,
      "step": 250
    },
    {
      "epoch": 0.5871345029239766,
      "grad_norm": 0.39445364475250244,
      "learning_rate": 0.00017634660421545666,
      "loss": 0.2005,
      "step": 251
    },
    {
      "epoch": 0.5894736842105263,
      "grad_norm": 0.4473083019256592,
      "learning_rate": 0.00017704918032786883,
      "loss": 0.3169,
      "step": 252
    },
    {
      "epoch": 0.591812865497076,
      "grad_norm": 0.4378367066383362,
      "learning_rate": 0.00017775175644028102,
      "loss": 0.2724,
      "step": 253
    },
    {
      "epoch": 0.5941520467836258,
      "grad_norm": 0.4959297478199005,
      "learning_rate": 0.00017845433255269318,
      "loss": 0.2896,
      "step": 254
    },
    {
      "epoch": 0.5964912280701754,
      "grad_norm": 0.4592922329902649,
      "learning_rate": 0.00017915690866510538,
      "loss": 0.3181,
      "step": 255
    },
    {
      "epoch": 0.5988304093567252,
      "grad_norm": 0.8368072509765625,
      "learning_rate": 0.00017985948477751754,
      "loss": 0.3425,
      "step": 256
    },
    {
      "epoch": 0.6011695906432749,
      "grad_norm": 0.579425036907196,
      "learning_rate": 0.00018056206088992973,
      "loss": 0.2552,
      "step": 257
    },
    {
      "epoch": 0.6035087719298246,
      "grad_norm": 0.6009561419487,
      "learning_rate": 0.0001812646370023419,
      "loss": 0.3016,
      "step": 258
    },
    {
      "epoch": 0.6058479532163743,
      "grad_norm": 0.4273008108139038,
      "learning_rate": 0.0001819672131147541,
      "loss": 0.3082,
      "step": 259
    },
    {
      "epoch": 0.6081871345029239,
      "grad_norm": 0.3020242154598236,
      "learning_rate": 0.00018266978922716625,
      "loss": 0.2059,
      "step": 260
    },
    {
      "epoch": 0.6105263157894737,
      "grad_norm": 0.5683671832084656,
      "learning_rate": 0.00018337236533957845,
      "loss": 0.3637,
      "step": 261
    },
    {
      "epoch": 0.6128654970760234,
      "grad_norm": 0.4456746578216553,
      "learning_rate": 0.0001840749414519906,
      "loss": 0.2526,
      "step": 262
    },
    {
      "epoch": 0.6152046783625731,
      "grad_norm": 0.47630012035369873,
      "learning_rate": 0.0001847775175644028,
      "loss": 0.3281,
      "step": 263
    },
    {
      "epoch": 0.6175438596491228,
      "grad_norm": 0.4759986996650696,
      "learning_rate": 0.00018548009367681497,
      "loss": 0.2962,
      "step": 264
    },
    {
      "epoch": 0.6198830409356725,
      "grad_norm": 0.6318939328193665,
      "learning_rate": 0.00018618266978922716,
      "loss": 0.208,
      "step": 265
    },
    {
      "epoch": 0.6222222222222222,
      "grad_norm": 0.690245509147644,
      "learning_rate": 0.00018688524590163933,
      "loss": 0.4133,
      "step": 266
    },
    {
      "epoch": 0.624561403508772,
      "grad_norm": 0.4028525948524475,
      "learning_rate": 0.00018758782201405152,
      "loss": 0.2269,
      "step": 267
    },
    {
      "epoch": 0.6269005847953216,
      "grad_norm": 0.4352359175682068,
      "learning_rate": 0.00018829039812646368,
      "loss": 0.262,
      "step": 268
    },
    {
      "epoch": 0.6292397660818714,
      "grad_norm": 0.41938725113868713,
      "learning_rate": 0.00018899297423887587,
      "loss": 0.2631,
      "step": 269
    },
    {
      "epoch": 0.631578947368421,
      "grad_norm": 0.3927726149559021,
      "learning_rate": 0.00018969555035128804,
      "loss": 0.2355,
      "step": 270
    },
    {
      "epoch": 0.6339181286549708,
      "grad_norm": 0.3540746867656708,
      "learning_rate": 0.00019039812646370023,
      "loss": 0.2686,
      "step": 271
    },
    {
      "epoch": 0.6362573099415205,
      "grad_norm": 0.4013313949108124,
      "learning_rate": 0.0001911007025761124,
      "loss": 0.3375,
      "step": 272
    },
    {
      "epoch": 0.6385964912280702,
      "grad_norm": 0.4574218988418579,
      "learning_rate": 0.0001918032786885246,
      "loss": 0.3072,
      "step": 273
    },
    {
      "epoch": 0.6409356725146199,
      "grad_norm": 0.35463738441467285,
      "learning_rate": 0.00019250585480093675,
      "loss": 0.3146,
      "step": 274
    },
    {
      "epoch": 0.6432748538011696,
      "grad_norm": 0.4288035035133362,
      "learning_rate": 0.00019320843091334895,
      "loss": 0.2391,
      "step": 275
    },
    {
      "epoch": 0.6456140350877193,
      "grad_norm": 0.6283718347549438,
      "learning_rate": 0.0001939110070257611,
      "loss": 0.2257,
      "step": 276
    },
    {
      "epoch": 0.6479532163742691,
      "grad_norm": 0.3865564465522766,
      "learning_rate": 0.0001946135831381733,
      "loss": 0.2802,
      "step": 277
    },
    {
      "epoch": 0.6502923976608187,
      "grad_norm": 0.3219755291938782,
      "learning_rate": 0.00019531615925058544,
      "loss": 0.2587,
      "step": 278
    },
    {
      "epoch": 0.6526315789473685,
      "grad_norm": 0.4228743016719818,
      "learning_rate": 0.00019601873536299763,
      "loss": 0.3084,
      "step": 279
    },
    {
      "epoch": 0.6549707602339181,
      "grad_norm": 0.34016141295433044,
      "learning_rate": 0.0001967213114754098,
      "loss": 0.2871,
      "step": 280
    },
    {
      "epoch": 0.6573099415204678,
      "grad_norm": 0.3850212097167969,
      "learning_rate": 0.000197423887587822,
      "loss": 0.3321,
      "step": 281
    },
    {
      "epoch": 0.6596491228070176,
      "grad_norm": 0.6185659170150757,
      "learning_rate": 0.00019812646370023416,
      "loss": 0.3798,
      "step": 282
    },
    {
      "epoch": 0.6619883040935672,
      "grad_norm": 0.41310006380081177,
      "learning_rate": 0.00019882903981264635,
      "loss": 0.3021,
      "step": 283
    },
    {
      "epoch": 0.664327485380117,
      "grad_norm": 0.3557858467102051,
      "learning_rate": 0.0001995316159250585,
      "loss": 0.2794,
      "step": 284
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.34972184896469116,
      "learning_rate": 0.0002002341920374707,
      "loss": 0.3598,
      "step": 285
    },
    {
      "epoch": 0.6690058479532164,
      "grad_norm": 0.38515859842300415,
      "learning_rate": 0.00020093676814988287,
      "loss": 0.3235,
      "step": 286
    },
    {
      "epoch": 0.6713450292397661,
      "grad_norm": 0.35326889157295227,
      "learning_rate": 0.00020163934426229506,
      "loss": 0.3548,
      "step": 287
    },
    {
      "epoch": 0.6736842105263158,
      "grad_norm": 0.4438682794570923,
      "learning_rate": 0.00020234192037470723,
      "loss": 0.2713,
      "step": 288
    },
    {
      "epoch": 0.6760233918128655,
      "grad_norm": 0.3906956911087036,
      "learning_rate": 0.00020304449648711942,
      "loss": 0.2532,
      "step": 289
    },
    {
      "epoch": 0.6783625730994152,
      "grad_norm": 0.3351839780807495,
      "learning_rate": 0.00020374707259953158,
      "loss": 0.3014,
      "step": 290
    },
    {
      "epoch": 0.6807017543859649,
      "grad_norm": 0.4573776423931122,
      "learning_rate": 0.00020444964871194378,
      "loss": 0.2488,
      "step": 291
    },
    {
      "epoch": 0.6830409356725147,
      "grad_norm": 0.5291275382041931,
      "learning_rate": 0.00020515222482435594,
      "loss": 0.2825,
      "step": 292
    },
    {
      "epoch": 0.6853801169590643,
      "grad_norm": 0.35089680552482605,
      "learning_rate": 0.00020585480093676813,
      "loss": 0.2173,
      "step": 293
    },
    {
      "epoch": 0.6877192982456141,
      "grad_norm": 0.4245279133319855,
      "learning_rate": 0.0002065573770491803,
      "loss": 0.3128,
      "step": 294
    },
    {
      "epoch": 0.6900584795321637,
      "grad_norm": 0.47726911306381226,
      "learning_rate": 0.0002072599531615925,
      "loss": 0.3105,
      "step": 295
    },
    {
      "epoch": 0.6923976608187135,
      "grad_norm": 0.8418300747871399,
      "learning_rate": 0.00020796252927400465,
      "loss": 0.3011,
      "step": 296
    },
    {
      "epoch": 0.6947368421052632,
      "grad_norm": 0.5969954133033752,
      "learning_rate": 0.00020866510538641685,
      "loss": 0.3162,
      "step": 297
    },
    {
      "epoch": 0.6970760233918128,
      "grad_norm": 0.45440801978111267,
      "learning_rate": 0.000209367681498829,
      "loss": 0.2649,
      "step": 298
    },
    {
      "epoch": 0.6994152046783626,
      "grad_norm": 0.49218878149986267,
      "learning_rate": 0.0002100702576112412,
      "loss": 0.3,
      "step": 299
    },
    {
      "epoch": 0.7017543859649122,
      "grad_norm": 0.49978548288345337,
      "learning_rate": 0.00021077283372365337,
      "loss": 0.251,
      "step": 300
    },
    {
      "epoch": 0.704093567251462,
      "grad_norm": 0.45126795768737793,
      "learning_rate": 0.00021147540983606556,
      "loss": 0.2881,
      "step": 301
    },
    {
      "epoch": 0.7064327485380117,
      "grad_norm": 0.35138800740242004,
      "learning_rate": 0.00021217798594847773,
      "loss": 0.2546,
      "step": 302
    },
    {
      "epoch": 0.7087719298245614,
      "grad_norm": 0.36785975098609924,
      "learning_rate": 0.00021288056206088992,
      "loss": 0.2726,
      "step": 303
    },
    {
      "epoch": 0.7111111111111111,
      "grad_norm": 0.44960272312164307,
      "learning_rate": 0.00021358313817330208,
      "loss": 0.2615,
      "step": 304
    },
    {
      "epoch": 0.7134502923976608,
      "grad_norm": 0.3858162462711334,
      "learning_rate": 0.00021428571428571427,
      "loss": 0.2989,
      "step": 305
    },
    {
      "epoch": 0.7157894736842105,
      "grad_norm": 0.3801867961883545,
      "learning_rate": 0.00021498829039812644,
      "loss": 0.2353,
      "step": 306
    },
    {
      "epoch": 0.7181286549707603,
      "grad_norm": 0.41549450159072876,
      "learning_rate": 0.00021569086651053863,
      "loss": 0.2696,
      "step": 307
    },
    {
      "epoch": 0.7204678362573099,
      "grad_norm": 0.401153028011322,
      "learning_rate": 0.0002163934426229508,
      "loss": 0.2303,
      "step": 308
    },
    {
      "epoch": 0.7228070175438597,
      "grad_norm": 0.4926559031009674,
      "learning_rate": 0.000217096018735363,
      "loss": 0.2968,
      "step": 309
    },
    {
      "epoch": 0.7251461988304093,
      "grad_norm": 0.48736777901649475,
      "learning_rate": 0.00021779859484777515,
      "loss": 0.3425,
      "step": 310
    },
    {
      "epoch": 0.7274853801169591,
      "grad_norm": 0.3741879463195801,
      "learning_rate": 0.00021850117096018735,
      "loss": 0.2823,
      "step": 311
    },
    {
      "epoch": 0.7298245614035088,
      "grad_norm": 0.40943795442581177,
      "learning_rate": 0.0002192037470725995,
      "loss": 0.234,
      "step": 312
    },
    {
      "epoch": 0.7321637426900585,
      "grad_norm": 0.35258522629737854,
      "learning_rate": 0.0002199063231850117,
      "loss": 0.265,
      "step": 313
    },
    {
      "epoch": 0.7345029239766082,
      "grad_norm": 0.4653862714767456,
      "learning_rate": 0.00022060889929742387,
      "loss": 0.2574,
      "step": 314
    },
    {
      "epoch": 0.7368421052631579,
      "grad_norm": 0.38888582587242126,
      "learning_rate": 0.00022131147540983606,
      "loss": 0.2999,
      "step": 315
    },
    {
      "epoch": 0.7391812865497076,
      "grad_norm": 0.4036332666873932,
      "learning_rate": 0.00022201405152224822,
      "loss": 0.2942,
      "step": 316
    },
    {
      "epoch": 0.7415204678362574,
      "grad_norm": 0.29926797747612,
      "learning_rate": 0.00022271662763466042,
      "loss": 0.2874,
      "step": 317
    },
    {
      "epoch": 0.743859649122807,
      "grad_norm": 0.29593294858932495,
      "learning_rate": 0.00022341920374707258,
      "loss": 0.2457,
      "step": 318
    },
    {
      "epoch": 0.7461988304093568,
      "grad_norm": 0.5856612920761108,
      "learning_rate": 0.00022412177985948477,
      "loss": 0.3383,
      "step": 319
    },
    {
      "epoch": 0.7485380116959064,
      "grad_norm": 0.28519466519355774,
      "learning_rate": 0.00022482435597189694,
      "loss": 0.2161,
      "step": 320
    },
    {
      "epoch": 0.7508771929824561,
      "grad_norm": 0.3380579948425293,
      "learning_rate": 0.0002255269320843091,
      "loss": 0.2388,
      "step": 321
    },
    {
      "epoch": 0.7532163742690059,
      "grad_norm": 0.396226704120636,
      "learning_rate": 0.00022622950819672127,
      "loss": 0.2786,
      "step": 322
    },
    {
      "epoch": 0.7555555555555555,
      "grad_norm": 0.3908284902572632,
      "learning_rate": 0.00022693208430913346,
      "loss": 0.2732,
      "step": 323
    },
    {
      "epoch": 0.7578947368421053,
      "grad_norm": 0.30436205863952637,
      "learning_rate": 0.00022763466042154563,
      "loss": 0.2176,
      "step": 324
    },
    {
      "epoch": 0.7602339181286549,
      "grad_norm": 0.35717999935150146,
      "learning_rate": 0.00022833723653395782,
      "loss": 0.21,
      "step": 325
    },
    {
      "epoch": 0.7625730994152047,
      "grad_norm": 0.34554216265678406,
      "learning_rate": 0.00022903981264636998,
      "loss": 0.2362,
      "step": 326
    },
    {
      "epoch": 0.7649122807017544,
      "grad_norm": 0.3767659068107605,
      "learning_rate": 0.00022974238875878218,
      "loss": 0.2393,
      "step": 327
    },
    {
      "epoch": 0.7672514619883041,
      "grad_norm": 0.47310441732406616,
      "learning_rate": 0.00023044496487119434,
      "loss": 0.3137,
      "step": 328
    },
    {
      "epoch": 0.7695906432748538,
      "grad_norm": 0.2883976995944977,
      "learning_rate": 0.00023114754098360653,
      "loss": 0.2685,
      "step": 329
    },
    {
      "epoch": 0.7719298245614035,
      "grad_norm": 0.4203339219093323,
      "learning_rate": 0.0002318501170960187,
      "loss": 0.2407,
      "step": 330
    },
    {
      "epoch": 0.7742690058479532,
      "grad_norm": 0.3448198735713959,
      "learning_rate": 0.0002325526932084309,
      "loss": 0.2959,
      "step": 331
    },
    {
      "epoch": 0.776608187134503,
      "grad_norm": 0.4707280993461609,
      "learning_rate": 0.00023325526932084305,
      "loss": 0.3067,
      "step": 332
    },
    {
      "epoch": 0.7789473684210526,
      "grad_norm": 0.4038759469985962,
      "learning_rate": 0.00023395784543325525,
      "loss": 0.2865,
      "step": 333
    },
    {
      "epoch": 0.7812865497076024,
      "grad_norm": 0.33791232109069824,
      "learning_rate": 0.0002346604215456674,
      "loss": 0.2715,
      "step": 334
    },
    {
      "epoch": 0.783625730994152,
      "grad_norm": 0.43753328919410706,
      "learning_rate": 0.0002353629976580796,
      "loss": 0.2256,
      "step": 335
    },
    {
      "epoch": 0.7859649122807018,
      "grad_norm": 0.35320448875427246,
      "learning_rate": 0.00023606557377049177,
      "loss": 0.2233,
      "step": 336
    },
    {
      "epoch": 0.7883040935672515,
      "grad_norm": 0.36819493770599365,
      "learning_rate": 0.00023676814988290396,
      "loss": 0.2616,
      "step": 337
    },
    {
      "epoch": 0.7906432748538011,
      "grad_norm": 0.39631718397140503,
      "learning_rate": 0.00023747072599531613,
      "loss": 0.2642,
      "step": 338
    },
    {
      "epoch": 0.7929824561403509,
      "grad_norm": 0.4340569078922272,
      "learning_rate": 0.00023817330210772832,
      "loss": 0.3367,
      "step": 339
    },
    {
      "epoch": 0.7953216374269005,
      "grad_norm": 0.36092618107795715,
      "learning_rate": 0.00023887587822014048,
      "loss": 0.2348,
      "step": 340
    },
    {
      "epoch": 0.7976608187134503,
      "grad_norm": 0.42236554622650146,
      "learning_rate": 0.00023957845433255267,
      "loss": 0.2237,
      "step": 341
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.46747612953186035,
      "learning_rate": 0.00024028103044496484,
      "loss": 0.186,
      "step": 342
    },
    {
      "epoch": 0.8023391812865497,
      "grad_norm": 0.40207573771476746,
      "learning_rate": 0.00024098360655737703,
      "loss": 0.271,
      "step": 343
    },
    {
      "epoch": 0.8046783625730994,
      "grad_norm": 0.4159904718399048,
      "learning_rate": 0.0002416861826697892,
      "loss": 0.236,
      "step": 344
    },
    {
      "epoch": 0.8070175438596491,
      "grad_norm": 0.4249405264854431,
      "learning_rate": 0.0002423887587822014,
      "loss": 0.3091,
      "step": 345
    },
    {
      "epoch": 0.8093567251461988,
      "grad_norm": 0.450784295797348,
      "learning_rate": 0.00024309133489461355,
      "loss": 0.3063,
      "step": 346
    },
    {
      "epoch": 0.8116959064327486,
      "grad_norm": 0.3935176134109497,
      "learning_rate": 0.00024379391100702575,
      "loss": 0.2666,
      "step": 347
    },
    {
      "epoch": 0.8140350877192982,
      "grad_norm": 0.4165562093257904,
      "learning_rate": 0.0002444964871194379,
      "loss": 0.2738,
      "step": 348
    },
    {
      "epoch": 0.816374269005848,
      "grad_norm": 0.4014893174171448,
      "learning_rate": 0.0002451990632318501,
      "loss": 0.2341,
      "step": 349
    },
    {
      "epoch": 0.8187134502923976,
      "grad_norm": 0.39979806542396545,
      "learning_rate": 0.0002459016393442623,
      "loss": 0.222,
      "step": 350
    },
    {
      "epoch": 0.8210526315789474,
      "grad_norm": 0.5036360025405884,
      "learning_rate": 0.00024660421545667446,
      "loss": 0.3056,
      "step": 351
    },
    {
      "epoch": 0.8233918128654971,
      "grad_norm": 0.3734471797943115,
      "learning_rate": 0.0002473067915690866,
      "loss": 0.264,
      "step": 352
    },
    {
      "epoch": 0.8257309941520468,
      "grad_norm": 0.34338340163230896,
      "learning_rate": 0.0002480093676814988,
      "loss": 0.1741,
      "step": 353
    },
    {
      "epoch": 0.8280701754385965,
      "grad_norm": 0.2881353497505188,
      "learning_rate": 0.000248711943793911,
      "loss": 0.2354,
      "step": 354
    },
    {
      "epoch": 0.8304093567251462,
      "grad_norm": 0.4998178482055664,
      "learning_rate": 0.0002494145199063232,
      "loss": 0.3088,
      "step": 355
    },
    {
      "epoch": 0.8327485380116959,
      "grad_norm": 0.5556555986404419,
      "learning_rate": 0.00025011709601873534,
      "loss": 0.3506,
      "step": 356
    },
    {
      "epoch": 0.8350877192982457,
      "grad_norm": 0.45360055565834045,
      "learning_rate": 0.00025081967213114756,
      "loss": 0.2723,
      "step": 357
    },
    {
      "epoch": 0.8374269005847953,
      "grad_norm": 0.3271198868751526,
      "learning_rate": 0.0002515222482435597,
      "loss": 0.2027,
      "step": 358
    },
    {
      "epoch": 0.839766081871345,
      "grad_norm": 0.366318941116333,
      "learning_rate": 0.0002522248243559719,
      "loss": 0.2577,
      "step": 359
    },
    {
      "epoch": 0.8421052631578947,
      "grad_norm": 0.33095723390579224,
      "learning_rate": 0.00025292740046838405,
      "loss": 0.281,
      "step": 360
    },
    {
      "epoch": 0.8444444444444444,
      "grad_norm": 0.41615593433380127,
      "learning_rate": 0.00025362997658079627,
      "loss": 0.2378,
      "step": 361
    },
    {
      "epoch": 0.8467836257309942,
      "grad_norm": 0.26637977361679077,
      "learning_rate": 0.00025433255269320844,
      "loss": 0.2126,
      "step": 362
    },
    {
      "epoch": 0.8491228070175438,
      "grad_norm": 0.3074035346508026,
      "learning_rate": 0.00025503512880562055,
      "loss": 0.1491,
      "step": 363
    },
    {
      "epoch": 0.8514619883040936,
      "grad_norm": 0.29712268710136414,
      "learning_rate": 0.00025573770491803277,
      "loss": 0.2415,
      "step": 364
    },
    {
      "epoch": 0.8538011695906432,
      "grad_norm": 0.5308188796043396,
      "learning_rate": 0.00025644028103044493,
      "loss": 0.3141,
      "step": 365
    },
    {
      "epoch": 0.856140350877193,
      "grad_norm": 0.38063549995422363,
      "learning_rate": 0.0002571428571428571,
      "loss": 0.27,
      "step": 366
    },
    {
      "epoch": 0.8584795321637427,
      "grad_norm": 0.3152669668197632,
      "learning_rate": 0.00025784543325526926,
      "loss": 0.2166,
      "step": 367
    },
    {
      "epoch": 0.8608187134502924,
      "grad_norm": 0.33344244956970215,
      "learning_rate": 0.0002585480093676815,
      "loss": 0.2681,
      "step": 368
    },
    {
      "epoch": 0.8631578947368421,
      "grad_norm": 0.35615670680999756,
      "learning_rate": 0.00025925058548009365,
      "loss": 0.2556,
      "step": 369
    },
    {
      "epoch": 0.8654970760233918,
      "grad_norm": 0.39786267280578613,
      "learning_rate": 0.0002599531615925058,
      "loss": 0.2899,
      "step": 370
    },
    {
      "epoch": 0.8678362573099415,
      "grad_norm": 0.3850170075893402,
      "learning_rate": 0.000260655737704918,
      "loss": 0.2302,
      "step": 371
    },
    {
      "epoch": 0.8701754385964913,
      "grad_norm": 0.3992738127708435,
      "learning_rate": 0.0002613583138173302,
      "loss": 0.2445,
      "step": 372
    },
    {
      "epoch": 0.8725146198830409,
      "grad_norm": 0.3433757722377777,
      "learning_rate": 0.00026206088992974236,
      "loss": 0.2347,
      "step": 373
    },
    {
      "epoch": 0.8748538011695907,
      "grad_norm": 0.4602271318435669,
      "learning_rate": 0.0002627634660421545,
      "loss": 0.2895,
      "step": 374
    },
    {
      "epoch": 0.8771929824561403,
      "grad_norm": 0.6535781621932983,
      "learning_rate": 0.0002634660421545667,
      "loss": 0.3289,
      "step": 375
    },
    {
      "epoch": 0.87953216374269,
      "grad_norm": 0.3985896110534668,
      "learning_rate": 0.0002641686182669789,
      "loss": 0.2751,
      "step": 376
    },
    {
      "epoch": 0.8818713450292398,
      "grad_norm": 0.38914456963539124,
      "learning_rate": 0.0002648711943793911,
      "loss": 0.2415,
      "step": 377
    },
    {
      "epoch": 0.8842105263157894,
      "grad_norm": 0.3316648602485657,
      "learning_rate": 0.00026557377049180324,
      "loss": 0.2339,
      "step": 378
    },
    {
      "epoch": 0.8865497076023392,
      "grad_norm": 0.33254581689834595,
      "learning_rate": 0.0002662763466042154,
      "loss": 0.2509,
      "step": 379
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 0.3486878573894501,
      "learning_rate": 0.0002669789227166276,
      "loss": 0.2485,
      "step": 380
    },
    {
      "epoch": 0.8912280701754386,
      "grad_norm": 0.4144381284713745,
      "learning_rate": 0.0002676814988290398,
      "loss": 0.2409,
      "step": 381
    },
    {
      "epoch": 0.8935672514619883,
      "grad_norm": 0.40506526827812195,
      "learning_rate": 0.00026838407494145195,
      "loss": 0.2851,
      "step": 382
    },
    {
      "epoch": 0.895906432748538,
      "grad_norm": 0.3411840796470642,
      "learning_rate": 0.0002690866510538641,
      "loss": 0.213,
      "step": 383
    },
    {
      "epoch": 0.8982456140350877,
      "grad_norm": 0.4372701942920685,
      "learning_rate": 0.00026978922716627634,
      "loss": 0.1917,
      "step": 384
    },
    {
      "epoch": 0.9005847953216374,
      "grad_norm": 0.474136084318161,
      "learning_rate": 0.0002704918032786885,
      "loss": 0.3131,
      "step": 385
    },
    {
      "epoch": 0.9029239766081871,
      "grad_norm": 0.41759517788887024,
      "learning_rate": 0.00027119437939110067,
      "loss": 0.2339,
      "step": 386
    },
    {
      "epoch": 0.9052631578947369,
      "grad_norm": 0.4203130602836609,
      "learning_rate": 0.00027189695550351283,
      "loss": 0.2898,
      "step": 387
    },
    {
      "epoch": 0.9076023391812865,
      "grad_norm": 0.35422778129577637,
      "learning_rate": 0.00027259953161592505,
      "loss": 0.2392,
      "step": 388
    },
    {
      "epoch": 0.9099415204678363,
      "grad_norm": 0.32858747243881226,
      "learning_rate": 0.0002733021077283372,
      "loss": 0.2201,
      "step": 389
    },
    {
      "epoch": 0.9122807017543859,
      "grad_norm": 0.34126871824264526,
      "learning_rate": 0.0002740046838407494,
      "loss": 0.2507,
      "step": 390
    },
    {
      "epoch": 0.9146198830409357,
      "grad_norm": 0.39995789527893066,
      "learning_rate": 0.00027470725995316155,
      "loss": 0.2569,
      "step": 391
    },
    {
      "epoch": 0.9169590643274854,
      "grad_norm": 0.48995548486709595,
      "learning_rate": 0.00027540983606557377,
      "loss": 0.2753,
      "step": 392
    },
    {
      "epoch": 0.9192982456140351,
      "grad_norm": 0.42655149102211,
      "learning_rate": 0.00027611241217798593,
      "loss": 0.3187,
      "step": 393
    },
    {
      "epoch": 0.9216374269005848,
      "grad_norm": 0.3829422891139984,
      "learning_rate": 0.0002768149882903981,
      "loss": 0.2288,
      "step": 394
    },
    {
      "epoch": 0.9239766081871345,
      "grad_norm": 0.32396021485328674,
      "learning_rate": 0.0002775175644028103,
      "loss": 0.2153,
      "step": 395
    },
    {
      "epoch": 0.9263157894736842,
      "grad_norm": 0.35108309984207153,
      "learning_rate": 0.0002782201405152225,
      "loss": 0.2856,
      "step": 396
    },
    {
      "epoch": 0.928654970760234,
      "grad_norm": 0.3699961304664612,
      "learning_rate": 0.00027892271662763465,
      "loss": 0.2244,
      "step": 397
    },
    {
      "epoch": 0.9309941520467836,
      "grad_norm": 0.46012628078460693,
      "learning_rate": 0.0002796252927400468,
      "loss": 0.2752,
      "step": 398
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 0.3617975413799286,
      "learning_rate": 0.00028032786885245903,
      "loss": 0.2034,
      "step": 399
    },
    {
      "epoch": 0.935672514619883,
      "grad_norm": 0.36910945177078247,
      "learning_rate": 0.0002810304449648712,
      "loss": 0.2593,
      "step": 400
    },
    {
      "epoch": 0.9380116959064327,
      "grad_norm": 0.3276163339614868,
      "learning_rate": 0.00028173302107728336,
      "loss": 0.2359,
      "step": 401
    },
    {
      "epoch": 0.9403508771929825,
      "grad_norm": 0.3532077670097351,
      "learning_rate": 0.0002824355971896955,
      "loss": 0.2414,
      "step": 402
    },
    {
      "epoch": 0.9426900584795321,
      "grad_norm": 0.38236382603645325,
      "learning_rate": 0.00028313817330210774,
      "loss": 0.2164,
      "step": 403
    },
    {
      "epoch": 0.9450292397660819,
      "grad_norm": 0.4742293953895569,
      "learning_rate": 0.0002838407494145199,
      "loss": 0.3188,
      "step": 404
    },
    {
      "epoch": 0.9473684210526315,
      "grad_norm": 0.35439202189445496,
      "learning_rate": 0.0002845433255269321,
      "loss": 0.1997,
      "step": 405
    },
    {
      "epoch": 0.9497076023391813,
      "grad_norm": 0.4122264087200165,
      "learning_rate": 0.00028524590163934424,
      "loss": 0.2414,
      "step": 406
    },
    {
      "epoch": 0.952046783625731,
      "grad_norm": 0.37811923027038574,
      "learning_rate": 0.0002859484777517564,
      "loss": 0.1719,
      "step": 407
    },
    {
      "epoch": 0.9543859649122807,
      "grad_norm": 0.3871244192123413,
      "learning_rate": 0.00028665105386416857,
      "loss": 0.2036,
      "step": 408
    },
    {
      "epoch": 0.9567251461988304,
      "grad_norm": 0.35355791449546814,
      "learning_rate": 0.00028735362997658073,
      "loss": 0.231,
      "step": 409
    },
    {
      "epoch": 0.9590643274853801,
      "grad_norm": 0.41989293694496155,
      "learning_rate": 0.00028805620608899295,
      "loss": 0.2468,
      "step": 410
    },
    {
      "epoch": 0.9614035087719298,
      "grad_norm": 0.34428712725639343,
      "learning_rate": 0.0002887587822014051,
      "loss": 0.2224,
      "step": 411
    },
    {
      "epoch": 0.9637426900584796,
      "grad_norm": 0.3432953655719757,
      "learning_rate": 0.0002894613583138173,
      "loss": 0.1691,
      "step": 412
    },
    {
      "epoch": 0.9660818713450292,
      "grad_norm": 0.45503178238868713,
      "learning_rate": 0.00029016393442622945,
      "loss": 0.2269,
      "step": 413
    },
    {
      "epoch": 0.968421052631579,
      "grad_norm": 0.5141614079475403,
      "learning_rate": 0.00029086651053864167,
      "loss": 0.2862,
      "step": 414
    },
    {
      "epoch": 0.9707602339181286,
      "grad_norm": 0.27300676703453064,
      "learning_rate": 0.00029156908665105383,
      "loss": 0.153,
      "step": 415
    },
    {
      "epoch": 0.9730994152046784,
      "grad_norm": 0.3060816526412964,
      "learning_rate": 0.000292271662763466,
      "loss": 0.2025,
      "step": 416
    },
    {
      "epoch": 0.9754385964912281,
      "grad_norm": 0.4576477110385895,
      "learning_rate": 0.00029297423887587816,
      "loss": 0.2993,
      "step": 417
    },
    {
      "epoch": 0.9777777777777777,
      "grad_norm": 0.30800187587738037,
      "learning_rate": 0.0002936768149882904,
      "loss": 0.2542,
      "step": 418
    },
    {
      "epoch": 0.9801169590643275,
      "grad_norm": 0.3769973814487457,
      "learning_rate": 0.00029437939110070255,
      "loss": 0.2573,
      "step": 419
    },
    {
      "epoch": 0.9824561403508771,
      "grad_norm": 0.32194778323173523,
      "learning_rate": 0.0002950819672131147,
      "loss": 0.292,
      "step": 420
    },
    {
      "epoch": 0.9847953216374269,
      "grad_norm": 0.37808898091316223,
      "learning_rate": 0.0002957845433255269,
      "loss": 0.2556,
      "step": 421
    },
    {
      "epoch": 0.9871345029239766,
      "grad_norm": 0.3007400631904602,
      "learning_rate": 0.0002964871194379391,
      "loss": 0.2236,
      "step": 422
    },
    {
      "epoch": 0.9894736842105263,
      "grad_norm": 0.41582974791526794,
      "learning_rate": 0.00029718969555035126,
      "loss": 0.2765,
      "step": 423
    },
    {
      "epoch": 0.991812865497076,
      "grad_norm": 0.42285770177841187,
      "learning_rate": 0.0002978922716627634,
      "loss": 0.1964,
      "step": 424
    },
    {
      "epoch": 0.9941520467836257,
      "grad_norm": 0.43786582350730896,
      "learning_rate": 0.0002985948477751756,
      "loss": 0.2465,
      "step": 425
    },
    {
      "epoch": 0.9964912280701754,
      "grad_norm": 0.3762390613555908,
      "learning_rate": 0.0002992974238875878,
      "loss": 0.1463,
      "step": 426
    },
    {
      "epoch": 0.9988304093567252,
      "grad_norm": 0.38420483469963074,
      "learning_rate": 0.0003,
      "loss": 0.241,
      "step": 427
    },
    {
      "epoch": 0.9988304093567252,
      "eval_loss": 0.3344041109085083,
      "eval_runtime": 124.9101,
      "eval_samples_per_second": 4.419,
      "eval_steps_per_second": 0.552,
      "step": 427
    },
    {
      "epoch": 1.001169590643275,
      "grad_norm": 0.41398826241493225,
      "learning_rate": 0.0002999219359875097,
      "loss": 0.1962,
      "step": 428
    },
    {
      "epoch": 1.0035087719298246,
      "grad_norm": 0.44515353441238403,
      "learning_rate": 0.0002998438719750195,
      "loss": 0.259,
      "step": 429
    },
    {
      "epoch": 1.0058479532163742,
      "grad_norm": 0.4824354350566864,
      "learning_rate": 0.00029976580796252925,
      "loss": 0.2651,
      "step": 430
    },
    {
      "epoch": 1.008187134502924,
      "grad_norm": 0.3244799077510834,
      "learning_rate": 0.000299687743950039,
      "loss": 0.2266,
      "step": 431
    },
    {
      "epoch": 1.0105263157894737,
      "grad_norm": 0.4208761751651764,
      "learning_rate": 0.00029960967993754875,
      "loss": 0.2899,
      "step": 432
    },
    {
      "epoch": 1.0128654970760234,
      "grad_norm": 0.3957078456878662,
      "learning_rate": 0.00029953161592505853,
      "loss": 0.2258,
      "step": 433
    },
    {
      "epoch": 1.015204678362573,
      "grad_norm": 0.3524191677570343,
      "learning_rate": 0.00029945355191256825,
      "loss": 0.2193,
      "step": 434
    },
    {
      "epoch": 1.0175438596491229,
      "grad_norm": 0.539068877696991,
      "learning_rate": 0.00029937548790007803,
      "loss": 0.262,
      "step": 435
    },
    {
      "epoch": 1.0198830409356725,
      "grad_norm": 0.4561743438243866,
      "learning_rate": 0.0002992974238875878,
      "loss": 0.2548,
      "step": 436
    },
    {
      "epoch": 1.0222222222222221,
      "grad_norm": 0.31637969613075256,
      "learning_rate": 0.00029921935987509753,
      "loss": 0.2411,
      "step": 437
    },
    {
      "epoch": 1.024561403508772,
      "grad_norm": 0.3036755919456482,
      "learning_rate": 0.0002991412958626073,
      "loss": 0.2998,
      "step": 438
    },
    {
      "epoch": 1.0269005847953216,
      "grad_norm": 0.36797794699668884,
      "learning_rate": 0.0002990632318501171,
      "loss": 0.3176,
      "step": 439
    },
    {
      "epoch": 1.0292397660818713,
      "grad_norm": 0.4130527079105377,
      "learning_rate": 0.0002989851678376268,
      "loss": 0.3306,
      "step": 440
    },
    {
      "epoch": 1.0315789473684212,
      "grad_norm": 0.3042704463005066,
      "learning_rate": 0.0002989071038251366,
      "loss": 0.269,
      "step": 441
    },
    {
      "epoch": 1.0339181286549708,
      "grad_norm": 0.3341429829597473,
      "learning_rate": 0.00029882903981264637,
      "loss": 0.2799,
      "step": 442
    },
    {
      "epoch": 1.0362573099415204,
      "grad_norm": 0.3459418714046478,
      "learning_rate": 0.0002987509758001561,
      "loss": 0.2464,
      "step": 443
    },
    {
      "epoch": 1.03859649122807,
      "grad_norm": 0.2969030737876892,
      "learning_rate": 0.00029867291178766587,
      "loss": 0.1547,
      "step": 444
    },
    {
      "epoch": 1.04093567251462,
      "grad_norm": 0.33472713828086853,
      "learning_rate": 0.0002985948477751756,
      "loss": 0.2835,
      "step": 445
    },
    {
      "epoch": 1.0432748538011696,
      "grad_norm": 0.39182162284851074,
      "learning_rate": 0.00029851678376268537,
      "loss": 0.2616,
      "step": 446
    },
    {
      "epoch": 1.0456140350877192,
      "grad_norm": 0.34597834944725037,
      "learning_rate": 0.00029843871975019514,
      "loss": 0.2422,
      "step": 447
    },
    {
      "epoch": 1.047953216374269,
      "grad_norm": 0.34296509623527527,
      "learning_rate": 0.00029836065573770487,
      "loss": 0.2459,
      "step": 448
    },
    {
      "epoch": 1.0502923976608187,
      "grad_norm": 0.2637760043144226,
      "learning_rate": 0.00029828259172521465,
      "loss": 0.17,
      "step": 449
    },
    {
      "epoch": 1.0526315789473684,
      "grad_norm": 0.2215050309896469,
      "learning_rate": 0.0002982045277127244,
      "loss": 0.1747,
      "step": 450
    },
    {
      "epoch": 1.0549707602339182,
      "grad_norm": 0.35109615325927734,
      "learning_rate": 0.00029812646370023415,
      "loss": 0.2027,
      "step": 451
    },
    {
      "epoch": 1.0573099415204679,
      "grad_norm": 0.392010897397995,
      "learning_rate": 0.0002980483996877439,
      "loss": 0.2377,
      "step": 452
    },
    {
      "epoch": 1.0596491228070175,
      "grad_norm": 0.332209974527359,
      "learning_rate": 0.0002979703356752537,
      "loss": 0.2052,
      "step": 453
    },
    {
      "epoch": 1.0619883040935671,
      "grad_norm": 0.7579382061958313,
      "learning_rate": 0.0002978922716627634,
      "loss": 0.2424,
      "step": 454
    },
    {
      "epoch": 1.064327485380117,
      "grad_norm": 0.44388213753700256,
      "learning_rate": 0.0002978142076502732,
      "loss": 0.2218,
      "step": 455
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 0.37335488200187683,
      "learning_rate": 0.000297736143637783,
      "loss": 0.2575,
      "step": 456
    },
    {
      "epoch": 1.0690058479532163,
      "grad_norm": 0.5514776706695557,
      "learning_rate": 0.0002976580796252927,
      "loss": 0.2808,
      "step": 457
    },
    {
      "epoch": 1.0713450292397662,
      "grad_norm": 0.4050874710083008,
      "learning_rate": 0.0002975800156128025,
      "loss": 0.2578,
      "step": 458
    },
    {
      "epoch": 1.0736842105263158,
      "grad_norm": 0.3457324206829071,
      "learning_rate": 0.00029750195160031226,
      "loss": 0.2624,
      "step": 459
    },
    {
      "epoch": 1.0760233918128654,
      "grad_norm": 0.33347049355506897,
      "learning_rate": 0.000297423887587822,
      "loss": 0.1644,
      "step": 460
    },
    {
      "epoch": 1.0783625730994153,
      "grad_norm": 0.3571535646915436,
      "learning_rate": 0.00029734582357533176,
      "loss": 0.2834,
      "step": 461
    },
    {
      "epoch": 1.080701754385965,
      "grad_norm": 0.40429580211639404,
      "learning_rate": 0.00029726775956284154,
      "loss": 0.2295,
      "step": 462
    },
    {
      "epoch": 1.0830409356725146,
      "grad_norm": 0.3330526351928711,
      "learning_rate": 0.00029718969555035126,
      "loss": 0.1732,
      "step": 463
    },
    {
      "epoch": 1.0853801169590644,
      "grad_norm": 0.357528954744339,
      "learning_rate": 0.000297111631537861,
      "loss": 0.253,
      "step": 464
    },
    {
      "epoch": 1.087719298245614,
      "grad_norm": 0.33141565322875977,
      "learning_rate": 0.00029703356752537076,
      "loss": 0.2447,
      "step": 465
    },
    {
      "epoch": 1.0900584795321637,
      "grad_norm": 0.2521098256111145,
      "learning_rate": 0.00029695550351288054,
      "loss": 0.2059,
      "step": 466
    },
    {
      "epoch": 1.0923976608187134,
      "grad_norm": 0.42194560170173645,
      "learning_rate": 0.00029687743950039026,
      "loss": 0.2171,
      "step": 467
    },
    {
      "epoch": 1.0947368421052632,
      "grad_norm": 0.2840796411037445,
      "learning_rate": 0.00029679937548790004,
      "loss": 0.1892,
      "step": 468
    },
    {
      "epoch": 1.0970760233918129,
      "grad_norm": 0.3863750696182251,
      "learning_rate": 0.0002967213114754098,
      "loss": 0.2665,
      "step": 469
    },
    {
      "epoch": 1.0994152046783625,
      "grad_norm": 0.3181089162826538,
      "learning_rate": 0.00029664324746291954,
      "loss": 0.2089,
      "step": 470
    },
    {
      "epoch": 1.1017543859649124,
      "grad_norm": 0.3880065381526947,
      "learning_rate": 0.0002965651834504293,
      "loss": 0.2524,
      "step": 471
    },
    {
      "epoch": 1.104093567251462,
      "grad_norm": 0.4236178398132324,
      "learning_rate": 0.0002964871194379391,
      "loss": 0.2266,
      "step": 472
    },
    {
      "epoch": 1.1064327485380117,
      "grad_norm": 0.29544785618782043,
      "learning_rate": 0.0002964090554254488,
      "loss": 0.1973,
      "step": 473
    },
    {
      "epoch": 1.1087719298245613,
      "grad_norm": 0.3571181893348694,
      "learning_rate": 0.0002963309914129586,
      "loss": 0.2078,
      "step": 474
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 0.33724597096443176,
      "learning_rate": 0.0002962529274004684,
      "loss": 0.2551,
      "step": 475
    },
    {
      "epoch": 1.1134502923976608,
      "grad_norm": 0.32552850246429443,
      "learning_rate": 0.0002961748633879781,
      "loss": 0.1858,
      "step": 476
    },
    {
      "epoch": 1.1157894736842104,
      "grad_norm": 0.37542298436164856,
      "learning_rate": 0.0002960967993754879,
      "loss": 0.2481,
      "step": 477
    },
    {
      "epoch": 1.1181286549707603,
      "grad_norm": 0.32940101623535156,
      "learning_rate": 0.00029601873536299765,
      "loss": 0.2034,
      "step": 478
    },
    {
      "epoch": 1.12046783625731,
      "grad_norm": 0.33121657371520996,
      "learning_rate": 0.0002959406713505074,
      "loss": 0.2091,
      "step": 479
    },
    {
      "epoch": 1.1228070175438596,
      "grad_norm": 0.2980024516582489,
      "learning_rate": 0.00029586260733801715,
      "loss": 0.1882,
      "step": 480
    },
    {
      "epoch": 1.1251461988304094,
      "grad_norm": 0.3171154260635376,
      "learning_rate": 0.0002957845433255269,
      "loss": 0.1841,
      "step": 481
    },
    {
      "epoch": 1.127485380116959,
      "grad_norm": 0.3828687369823456,
      "learning_rate": 0.00029570647931303665,
      "loss": 0.2193,
      "step": 482
    },
    {
      "epoch": 1.1298245614035087,
      "grad_norm": 0.3238588869571686,
      "learning_rate": 0.00029562841530054643,
      "loss": 0.2052,
      "step": 483
    },
    {
      "epoch": 1.1321637426900586,
      "grad_norm": 0.4171507954597473,
      "learning_rate": 0.00029555035128805615,
      "loss": 0.2439,
      "step": 484
    },
    {
      "epoch": 1.1345029239766082,
      "grad_norm": 0.3904205858707428,
      "learning_rate": 0.00029547228727556593,
      "loss": 0.2113,
      "step": 485
    },
    {
      "epoch": 1.1368421052631579,
      "grad_norm": 0.39696142077445984,
      "learning_rate": 0.0002953942232630757,
      "loss": 0.2825,
      "step": 486
    },
    {
      "epoch": 1.1391812865497075,
      "grad_norm": 0.4181166887283325,
      "learning_rate": 0.00029531615925058543,
      "loss": 0.2656,
      "step": 487
    },
    {
      "epoch": 1.1415204678362574,
      "grad_norm": 0.3784923851490021,
      "learning_rate": 0.0002952380952380952,
      "loss": 0.2284,
      "step": 488
    },
    {
      "epoch": 1.143859649122807,
      "grad_norm": 0.38203561305999756,
      "learning_rate": 0.000295160031225605,
      "loss": 0.253,
      "step": 489
    },
    {
      "epoch": 1.1461988304093567,
      "grad_norm": 0.40119192004203796,
      "learning_rate": 0.0002950819672131147,
      "loss": 0.2292,
      "step": 490
    },
    {
      "epoch": 1.1485380116959065,
      "grad_norm": 0.34419146180152893,
      "learning_rate": 0.0002950039032006245,
      "loss": 0.2481,
      "step": 491
    },
    {
      "epoch": 1.1508771929824562,
      "grad_norm": 0.39827725291252136,
      "learning_rate": 0.00029492583918813427,
      "loss": 0.2287,
      "step": 492
    },
    {
      "epoch": 1.1532163742690058,
      "grad_norm": 0.33879518508911133,
      "learning_rate": 0.000294847775175644,
      "loss": 0.1656,
      "step": 493
    },
    {
      "epoch": 1.1555555555555554,
      "grad_norm": 0.3808956742286682,
      "learning_rate": 0.00029476971116315377,
      "loss": 0.2012,
      "step": 494
    },
    {
      "epoch": 1.1578947368421053,
      "grad_norm": 0.43113842606544495,
      "learning_rate": 0.00029469164715066354,
      "loss": 0.2104,
      "step": 495
    },
    {
      "epoch": 1.160233918128655,
      "grad_norm": 0.28956443071365356,
      "learning_rate": 0.00029461358313817327,
      "loss": 0.1705,
      "step": 496
    },
    {
      "epoch": 1.1625730994152046,
      "grad_norm": 0.28727832436561584,
      "learning_rate": 0.00029453551912568304,
      "loss": 0.1775,
      "step": 497
    },
    {
      "epoch": 1.1649122807017545,
      "grad_norm": 0.35777491331100464,
      "learning_rate": 0.0002944574551131928,
      "loss": 0.2408,
      "step": 498
    },
    {
      "epoch": 1.167251461988304,
      "grad_norm": 0.4051152765750885,
      "learning_rate": 0.00029437939110070255,
      "loss": 0.3033,
      "step": 499
    },
    {
      "epoch": 1.1695906432748537,
      "grad_norm": 0.35794615745544434,
      "learning_rate": 0.0002943013270882123,
      "loss": 0.295,
      "step": 500
    },
    {
      "epoch": 1.1719298245614036,
      "grad_norm": 0.382638543844223,
      "learning_rate": 0.00029422326307572205,
      "loss": 0.2587,
      "step": 501
    },
    {
      "epoch": 1.1742690058479532,
      "grad_norm": 0.32201656699180603,
      "learning_rate": 0.0002941451990632318,
      "loss": 0.229,
      "step": 502
    },
    {
      "epoch": 1.1766081871345029,
      "grad_norm": 0.2686212360858917,
      "learning_rate": 0.0002940671350507416,
      "loss": 0.2257,
      "step": 503
    },
    {
      "epoch": 1.1789473684210527,
      "grad_norm": 0.26908811926841736,
      "learning_rate": 0.0002939890710382513,
      "loss": 0.2098,
      "step": 504
    },
    {
      "epoch": 1.1812865497076024,
      "grad_norm": 0.3212386965751648,
      "learning_rate": 0.0002939110070257611,
      "loss": 0.2435,
      "step": 505
    },
    {
      "epoch": 1.183625730994152,
      "grad_norm": 0.2964070439338684,
      "learning_rate": 0.0002938329430132709,
      "loss": 0.2438,
      "step": 506
    },
    {
      "epoch": 1.1859649122807017,
      "grad_norm": 0.2688436508178711,
      "learning_rate": 0.0002937548790007806,
      "loss": 0.1522,
      "step": 507
    },
    {
      "epoch": 1.1883040935672515,
      "grad_norm": 0.338693231344223,
      "learning_rate": 0.0002936768149882904,
      "loss": 0.2278,
      "step": 508
    },
    {
      "epoch": 1.1906432748538012,
      "grad_norm": 0.24957357347011566,
      "learning_rate": 0.00029359875097580016,
      "loss": 0.1309,
      "step": 509
    },
    {
      "epoch": 1.1929824561403508,
      "grad_norm": 0.2806257903575897,
      "learning_rate": 0.0002935206869633099,
      "loss": 0.1337,
      "step": 510
    },
    {
      "epoch": 1.1953216374269007,
      "grad_norm": 0.42179352045059204,
      "learning_rate": 0.00029344262295081966,
      "loss": 0.2439,
      "step": 511
    },
    {
      "epoch": 1.1976608187134503,
      "grad_norm": 0.570030152797699,
      "learning_rate": 0.00029336455893832944,
      "loss": 0.1922,
      "step": 512
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.5412673354148865,
      "learning_rate": 0.00029328649492583916,
      "loss": 0.2321,
      "step": 513
    },
    {
      "epoch": 1.2023391812865496,
      "grad_norm": 0.38620713353157043,
      "learning_rate": 0.00029320843091334894,
      "loss": 0.2272,
      "step": 514
    },
    {
      "epoch": 1.2046783625730995,
      "grad_norm": 0.4563421607017517,
      "learning_rate": 0.0002931303669008587,
      "loss": 0.1914,
      "step": 515
    },
    {
      "epoch": 1.207017543859649,
      "grad_norm": 0.44636327028274536,
      "learning_rate": 0.00029305230288836844,
      "loss": 0.271,
      "step": 516
    },
    {
      "epoch": 1.2093567251461987,
      "grad_norm": 0.3772982954978943,
      "learning_rate": 0.00029297423887587816,
      "loss": 0.2863,
      "step": 517
    },
    {
      "epoch": 1.2116959064327486,
      "grad_norm": 0.40283432602882385,
      "learning_rate": 0.000292896174863388,
      "loss": 0.2641,
      "step": 518
    },
    {
      "epoch": 1.2140350877192982,
      "grad_norm": 0.3228025734424591,
      "learning_rate": 0.0002928181108508977,
      "loss": 0.2226,
      "step": 519
    },
    {
      "epoch": 1.2163742690058479,
      "grad_norm": 0.32849159836769104,
      "learning_rate": 0.00029274004683840744,
      "loss": 0.2118,
      "step": 520
    },
    {
      "epoch": 1.2187134502923977,
      "grad_norm": 0.3551590144634247,
      "learning_rate": 0.0002926619828259172,
      "loss": 0.1493,
      "step": 521
    },
    {
      "epoch": 1.2210526315789474,
      "grad_norm": 0.34167590737342834,
      "learning_rate": 0.000292583918813427,
      "loss": 0.2271,
      "step": 522
    },
    {
      "epoch": 1.223391812865497,
      "grad_norm": 0.26313886046409607,
      "learning_rate": 0.0002925058548009367,
      "loss": 0.1649,
      "step": 523
    },
    {
      "epoch": 1.225730994152047,
      "grad_norm": 0.22623668611049652,
      "learning_rate": 0.0002924277907884465,
      "loss": 0.1455,
      "step": 524
    },
    {
      "epoch": 1.2280701754385965,
      "grad_norm": 0.30810296535491943,
      "learning_rate": 0.0002923497267759563,
      "loss": 0.2023,
      "step": 525
    },
    {
      "epoch": 1.2304093567251462,
      "grad_norm": 0.3322948217391968,
      "learning_rate": 0.000292271662763466,
      "loss": 0.1854,
      "step": 526
    },
    {
      "epoch": 1.2327485380116958,
      "grad_norm": 0.3744688928127289,
      "learning_rate": 0.0002921935987509758,
      "loss": 0.2593,
      "step": 527
    },
    {
      "epoch": 1.2350877192982457,
      "grad_norm": 0.3828964829444885,
      "learning_rate": 0.00029211553473848555,
      "loss": 0.2301,
      "step": 528
    },
    {
      "epoch": 1.2374269005847953,
      "grad_norm": 0.2767253816127777,
      "learning_rate": 0.0002920374707259953,
      "loss": 0.1999,
      "step": 529
    },
    {
      "epoch": 1.239766081871345,
      "grad_norm": 0.3184651732444763,
      "learning_rate": 0.00029195940671350505,
      "loss": 0.1497,
      "step": 530
    },
    {
      "epoch": 1.2421052631578948,
      "grad_norm": 0.4454036056995392,
      "learning_rate": 0.00029188134270101483,
      "loss": 0.2446,
      "step": 531
    },
    {
      "epoch": 1.2444444444444445,
      "grad_norm": 0.3524395823478699,
      "learning_rate": 0.00029180327868852455,
      "loss": 0.1716,
      "step": 532
    },
    {
      "epoch": 1.246783625730994,
      "grad_norm": 0.4090137481689453,
      "learning_rate": 0.00029172521467603433,
      "loss": 0.216,
      "step": 533
    },
    {
      "epoch": 1.2491228070175437,
      "grad_norm": 0.36596357822418213,
      "learning_rate": 0.0002916471506635441,
      "loss": 0.2001,
      "step": 534
    },
    {
      "epoch": 1.2514619883040936,
      "grad_norm": 0.3151891827583313,
      "learning_rate": 0.00029156908665105383,
      "loss": 0.198,
      "step": 535
    },
    {
      "epoch": 1.2538011695906432,
      "grad_norm": 0.28136107325553894,
      "learning_rate": 0.0002914910226385636,
      "loss": 0.1397,
      "step": 536
    },
    {
      "epoch": 1.256140350877193,
      "grad_norm": 0.41280338168144226,
      "learning_rate": 0.00029141295862607333,
      "loss": 0.2408,
      "step": 537
    },
    {
      "epoch": 1.2584795321637428,
      "grad_norm": 0.43900108337402344,
      "learning_rate": 0.0002913348946135831,
      "loss": 0.2382,
      "step": 538
    },
    {
      "epoch": 1.2608187134502924,
      "grad_norm": 0.490885466337204,
      "learning_rate": 0.0002912568306010929,
      "loss": 0.2443,
      "step": 539
    },
    {
      "epoch": 1.263157894736842,
      "grad_norm": 0.47279927134513855,
      "learning_rate": 0.0002911787665886026,
      "loss": 0.2838,
      "step": 540
    },
    {
      "epoch": 1.265497076023392,
      "grad_norm": 0.3771021366119385,
      "learning_rate": 0.0002911007025761124,
      "loss": 0.2314,
      "step": 541
    },
    {
      "epoch": 1.2678362573099415,
      "grad_norm": 0.4864875078201294,
      "learning_rate": 0.00029102263856362217,
      "loss": 0.2425,
      "step": 542
    },
    {
      "epoch": 1.2701754385964912,
      "grad_norm": 0.3744930326938629,
      "learning_rate": 0.0002909445745511319,
      "loss": 0.201,
      "step": 543
    },
    {
      "epoch": 1.272514619883041,
      "grad_norm": 0.3287687301635742,
      "learning_rate": 0.00029086651053864167,
      "loss": 0.2087,
      "step": 544
    },
    {
      "epoch": 1.2748538011695907,
      "grad_norm": 0.330539733171463,
      "learning_rate": 0.00029078844652615144,
      "loss": 0.2051,
      "step": 545
    },
    {
      "epoch": 1.2771929824561403,
      "grad_norm": 0.33857351541519165,
      "learning_rate": 0.00029071038251366117,
      "loss": 0.2574,
      "step": 546
    },
    {
      "epoch": 1.2795321637426902,
      "grad_norm": 0.3633592128753662,
      "learning_rate": 0.00029063231850117094,
      "loss": 0.2632,
      "step": 547
    },
    {
      "epoch": 1.2818713450292398,
      "grad_norm": 0.36634746193885803,
      "learning_rate": 0.0002905542544886807,
      "loss": 0.2513,
      "step": 548
    },
    {
      "epoch": 1.2842105263157895,
      "grad_norm": 0.33245933055877686,
      "learning_rate": 0.00029047619047619045,
      "loss": 0.2184,
      "step": 549
    },
    {
      "epoch": 1.286549707602339,
      "grad_norm": 0.2731856405735016,
      "learning_rate": 0.0002903981264637002,
      "loss": 0.1889,
      "step": 550
    },
    {
      "epoch": 1.2888888888888888,
      "grad_norm": 0.41323795914649963,
      "learning_rate": 0.00029032006245121,
      "loss": 0.2461,
      "step": 551
    },
    {
      "epoch": 1.2912280701754386,
      "grad_norm": 0.22982096672058105,
      "learning_rate": 0.0002902419984387197,
      "loss": 0.1741,
      "step": 552
    },
    {
      "epoch": 1.2935672514619883,
      "grad_norm": 0.3355451226234436,
      "learning_rate": 0.00029016393442622945,
      "loss": 0.216,
      "step": 553
    },
    {
      "epoch": 1.295906432748538,
      "grad_norm": 0.2758011817932129,
      "learning_rate": 0.0002900858704137393,
      "loss": 0.1558,
      "step": 554
    },
    {
      "epoch": 1.2982456140350878,
      "grad_norm": 0.3291817903518677,
      "learning_rate": 0.000290007806401249,
      "loss": 0.1939,
      "step": 555
    },
    {
      "epoch": 1.3005847953216374,
      "grad_norm": 0.4069708287715912,
      "learning_rate": 0.0002899297423887587,
      "loss": 0.2091,
      "step": 556
    },
    {
      "epoch": 1.302923976608187,
      "grad_norm": 0.3176850974559784,
      "learning_rate": 0.0002898516783762685,
      "loss": 0.209,
      "step": 557
    },
    {
      "epoch": 1.305263157894737,
      "grad_norm": 0.33677729964256287,
      "learning_rate": 0.0002897736143637783,
      "loss": 0.1607,
      "step": 558
    },
    {
      "epoch": 1.3076023391812865,
      "grad_norm": 0.4095131456851959,
      "learning_rate": 0.000289695550351288,
      "loss": 0.2629,
      "step": 559
    },
    {
      "epoch": 1.3099415204678362,
      "grad_norm": 0.3517284095287323,
      "learning_rate": 0.0002896174863387978,
      "loss": 0.178,
      "step": 560
    },
    {
      "epoch": 1.312280701754386,
      "grad_norm": 0.3989841341972351,
      "learning_rate": 0.00028953942232630756,
      "loss": 0.2559,
      "step": 561
    },
    {
      "epoch": 1.3146198830409357,
      "grad_norm": 0.3251431882381439,
      "learning_rate": 0.0002894613583138173,
      "loss": 0.1944,
      "step": 562
    },
    {
      "epoch": 1.3169590643274853,
      "grad_norm": 0.4218730032444,
      "learning_rate": 0.00028938329430132706,
      "loss": 0.2451,
      "step": 563
    },
    {
      "epoch": 1.3192982456140352,
      "grad_norm": 0.3228503167629242,
      "learning_rate": 0.00028930523028883684,
      "loss": 0.2249,
      "step": 564
    },
    {
      "epoch": 1.3216374269005848,
      "grad_norm": 0.3532530665397644,
      "learning_rate": 0.00028922716627634656,
      "loss": 0.1893,
      "step": 565
    },
    {
      "epoch": 1.3239766081871345,
      "grad_norm": 0.4366673231124878,
      "learning_rate": 0.00028914910226385634,
      "loss": 0.2564,
      "step": 566
    },
    {
      "epoch": 1.3263157894736843,
      "grad_norm": 0.2781785726547241,
      "learning_rate": 0.0002890710382513661,
      "loss": 0.2091,
      "step": 567
    },
    {
      "epoch": 1.328654970760234,
      "grad_norm": 0.34564635157585144,
      "learning_rate": 0.00028899297423887584,
      "loss": 0.2502,
      "step": 568
    },
    {
      "epoch": 1.3309941520467836,
      "grad_norm": 0.4302021861076355,
      "learning_rate": 0.0002889149102263856,
      "loss": 0.2987,
      "step": 569
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 0.2849520444869995,
      "learning_rate": 0.0002888368462138954,
      "loss": 0.1681,
      "step": 570
    },
    {
      "epoch": 1.3356725146198831,
      "grad_norm": 0.27384260296821594,
      "learning_rate": 0.0002887587822014051,
      "loss": 0.2217,
      "step": 571
    },
    {
      "epoch": 1.3380116959064328,
      "grad_norm": 0.31411898136138916,
      "learning_rate": 0.0002886807181889149,
      "loss": 0.2063,
      "step": 572
    },
    {
      "epoch": 1.3403508771929824,
      "grad_norm": 0.3019144833087921,
      "learning_rate": 0.0002886026541764246,
      "loss": 0.2231,
      "step": 573
    },
    {
      "epoch": 1.342690058479532,
      "grad_norm": 0.3269272744655609,
      "learning_rate": 0.0002885245901639344,
      "loss": 0.2638,
      "step": 574
    },
    {
      "epoch": 1.345029239766082,
      "grad_norm": 0.3174206614494324,
      "learning_rate": 0.0002884465261514442,
      "loss": 0.2338,
      "step": 575
    },
    {
      "epoch": 1.3473684210526315,
      "grad_norm": 0.2517681419849396,
      "learning_rate": 0.0002883684621389539,
      "loss": 0.1095,
      "step": 576
    },
    {
      "epoch": 1.3497076023391812,
      "grad_norm": 0.40571534633636475,
      "learning_rate": 0.0002882903981264637,
      "loss": 0.2702,
      "step": 577
    },
    {
      "epoch": 1.352046783625731,
      "grad_norm": 0.3157341182231903,
      "learning_rate": 0.00028821233411397345,
      "loss": 0.2259,
      "step": 578
    },
    {
      "epoch": 1.3543859649122807,
      "grad_norm": 0.25192174315452576,
      "learning_rate": 0.0002881342701014832,
      "loss": 0.1863,
      "step": 579
    },
    {
      "epoch": 1.3567251461988303,
      "grad_norm": 0.2902292013168335,
      "learning_rate": 0.00028805620608899295,
      "loss": 0.1805,
      "step": 580
    },
    {
      "epoch": 1.3590643274853802,
      "grad_norm": 0.2869679033756256,
      "learning_rate": 0.00028797814207650273,
      "loss": 0.2111,
      "step": 581
    },
    {
      "epoch": 1.3614035087719298,
      "grad_norm": 0.38085848093032837,
      "learning_rate": 0.00028790007806401245,
      "loss": 0.2173,
      "step": 582
    },
    {
      "epoch": 1.3637426900584795,
      "grad_norm": 0.2648739814758301,
      "learning_rate": 0.00028782201405152223,
      "loss": 0.1345,
      "step": 583
    },
    {
      "epoch": 1.3660818713450293,
      "grad_norm": 0.3386087417602539,
      "learning_rate": 0.000287743950039032,
      "loss": 0.2133,
      "step": 584
    },
    {
      "epoch": 1.368421052631579,
      "grad_norm": 0.4473266303539276,
      "learning_rate": 0.00028766588602654173,
      "loss": 0.2717,
      "step": 585
    },
    {
      "epoch": 1.3707602339181286,
      "grad_norm": 0.3728618323802948,
      "learning_rate": 0.0002875878220140515,
      "loss": 0.206,
      "step": 586
    },
    {
      "epoch": 1.3730994152046785,
      "grad_norm": 0.31546351313591003,
      "learning_rate": 0.0002875097580015613,
      "loss": 0.2147,
      "step": 587
    },
    {
      "epoch": 1.3754385964912281,
      "grad_norm": 0.454201877117157,
      "learning_rate": 0.000287431693989071,
      "loss": 0.2487,
      "step": 588
    },
    {
      "epoch": 1.3777777777777778,
      "grad_norm": 0.3524230718612671,
      "learning_rate": 0.00028735362997658073,
      "loss": 0.2216,
      "step": 589
    },
    {
      "epoch": 1.3801169590643274,
      "grad_norm": 0.35446465015411377,
      "learning_rate": 0.0002872755659640905,
      "loss": 0.1715,
      "step": 590
    },
    {
      "epoch": 1.3824561403508773,
      "grad_norm": 0.3779980540275574,
      "learning_rate": 0.0002871975019516003,
      "loss": 0.2113,
      "step": 591
    },
    {
      "epoch": 1.384795321637427,
      "grad_norm": 0.44190460443496704,
      "learning_rate": 0.00028711943793911,
      "loss": 0.2582,
      "step": 592
    },
    {
      "epoch": 1.3871345029239766,
      "grad_norm": 0.31391701102256775,
      "learning_rate": 0.0002870413739266198,
      "loss": 0.2432,
      "step": 593
    },
    {
      "epoch": 1.3894736842105262,
      "grad_norm": 0.2846074104309082,
      "learning_rate": 0.00028696330991412957,
      "loss": 0.1781,
      "step": 594
    },
    {
      "epoch": 1.391812865497076,
      "grad_norm": 0.36469465494155884,
      "learning_rate": 0.0002868852459016393,
      "loss": 0.1839,
      "step": 595
    },
    {
      "epoch": 1.3941520467836257,
      "grad_norm": 0.2469574213027954,
      "learning_rate": 0.00028680718188914907,
      "loss": 0.1729,
      "step": 596
    },
    {
      "epoch": 1.3964912280701753,
      "grad_norm": 0.3380779027938843,
      "learning_rate": 0.00028672911787665884,
      "loss": 0.1978,
      "step": 597
    },
    {
      "epoch": 1.3988304093567252,
      "grad_norm": 0.4195411503314972,
      "learning_rate": 0.00028665105386416857,
      "loss": 0.1862,
      "step": 598
    },
    {
      "epoch": 1.4011695906432748,
      "grad_norm": 0.3937232494354248,
      "learning_rate": 0.00028657298985167835,
      "loss": 0.2729,
      "step": 599
    },
    {
      "epoch": 1.4035087719298245,
      "grad_norm": 0.4270554482936859,
      "learning_rate": 0.0002864949258391881,
      "loss": 0.1944,
      "step": 600
    },
    {
      "epoch": 1.4058479532163743,
      "grad_norm": 0.35207322239875793,
      "learning_rate": 0.00028641686182669785,
      "loss": 0.2322,
      "step": 601
    },
    {
      "epoch": 1.408187134502924,
      "grad_norm": 0.2822466790676117,
      "learning_rate": 0.0002863387978142076,
      "loss": 0.199,
      "step": 602
    },
    {
      "epoch": 1.4105263157894736,
      "grad_norm": 0.3615182936191559,
      "learning_rate": 0.0002862607338017174,
      "loss": 0.1855,
      "step": 603
    },
    {
      "epoch": 1.4128654970760235,
      "grad_norm": 0.3498174846172333,
      "learning_rate": 0.0002861826697892271,
      "loss": 0.2092,
      "step": 604
    },
    {
      "epoch": 1.4152046783625731,
      "grad_norm": 0.30269867181777954,
      "learning_rate": 0.0002861046057767369,
      "loss": 0.1917,
      "step": 605
    },
    {
      "epoch": 1.4175438596491228,
      "grad_norm": 0.40103697776794434,
      "learning_rate": 0.0002860265417642467,
      "loss": 0.2592,
      "step": 606
    },
    {
      "epoch": 1.4198830409356726,
      "grad_norm": 0.4063367247581482,
      "learning_rate": 0.0002859484777517564,
      "loss": 0.2492,
      "step": 607
    },
    {
      "epoch": 1.4222222222222223,
      "grad_norm": 0.3967605233192444,
      "learning_rate": 0.0002858704137392662,
      "loss": 0.1861,
      "step": 608
    },
    {
      "epoch": 1.424561403508772,
      "grad_norm": 0.3418210446834564,
      "learning_rate": 0.0002857923497267759,
      "loss": 0.1938,
      "step": 609
    },
    {
      "epoch": 1.4269005847953216,
      "grad_norm": 0.419321745634079,
      "learning_rate": 0.0002857142857142857,
      "loss": 0.2504,
      "step": 610
    },
    {
      "epoch": 1.4292397660818714,
      "grad_norm": 0.3998107314109802,
      "learning_rate": 0.00028563622170179546,
      "loss": 0.2507,
      "step": 611
    },
    {
      "epoch": 1.431578947368421,
      "grad_norm": 0.35979577898979187,
      "learning_rate": 0.0002855581576893052,
      "loss": 0.2345,
      "step": 612
    },
    {
      "epoch": 1.4339181286549707,
      "grad_norm": 0.32387205958366394,
      "learning_rate": 0.00028548009367681496,
      "loss": 0.1506,
      "step": 613
    },
    {
      "epoch": 1.4362573099415203,
      "grad_norm": 0.34301555156707764,
      "learning_rate": 0.00028540202966432474,
      "loss": 0.2145,
      "step": 614
    },
    {
      "epoch": 1.4385964912280702,
      "grad_norm": 0.389242023229599,
      "learning_rate": 0.00028532396565183446,
      "loss": 0.2842,
      "step": 615
    },
    {
      "epoch": 1.4409356725146198,
      "grad_norm": 0.36687979102134705,
      "learning_rate": 0.00028524590163934424,
      "loss": 0.1846,
      "step": 616
    },
    {
      "epoch": 1.4432748538011695,
      "grad_norm": 0.3755941390991211,
      "learning_rate": 0.000285167837626854,
      "loss": 0.2295,
      "step": 617
    },
    {
      "epoch": 1.4456140350877194,
      "grad_norm": 0.3923777937889099,
      "learning_rate": 0.00028508977361436374,
      "loss": 0.2149,
      "step": 618
    },
    {
      "epoch": 1.447953216374269,
      "grad_norm": 0.26401278376579285,
      "learning_rate": 0.0002850117096018735,
      "loss": 0.1766,
      "step": 619
    },
    {
      "epoch": 1.4502923976608186,
      "grad_norm": 0.3276418447494507,
      "learning_rate": 0.0002849336455893833,
      "loss": 0.2581,
      "step": 620
    },
    {
      "epoch": 1.4526315789473685,
      "grad_norm": 0.3251068890094757,
      "learning_rate": 0.000284855581576893,
      "loss": 0.2205,
      "step": 621
    },
    {
      "epoch": 1.4549707602339181,
      "grad_norm": 0.3968236446380615,
      "learning_rate": 0.0002847775175644028,
      "loss": 0.257,
      "step": 622
    },
    {
      "epoch": 1.4573099415204678,
      "grad_norm": 0.3444656729698181,
      "learning_rate": 0.00028469945355191257,
      "loss": 0.1688,
      "step": 623
    },
    {
      "epoch": 1.4596491228070176,
      "grad_norm": 0.2660878300666809,
      "learning_rate": 0.0002846213895394223,
      "loss": 0.1809,
      "step": 624
    },
    {
      "epoch": 1.4619883040935673,
      "grad_norm": 0.30196601152420044,
      "learning_rate": 0.0002845433255269321,
      "loss": 0.1965,
      "step": 625
    },
    {
      "epoch": 1.464327485380117,
      "grad_norm": 0.36222633719444275,
      "learning_rate": 0.0002844652615144418,
      "loss": 0.247,
      "step": 626
    },
    {
      "epoch": 1.4666666666666668,
      "grad_norm": 0.403432160615921,
      "learning_rate": 0.0002843871975019516,
      "loss": 0.2675,
      "step": 627
    },
    {
      "epoch": 1.4690058479532164,
      "grad_norm": 0.3229057788848877,
      "learning_rate": 0.00028430913348946135,
      "loss": 0.2274,
      "step": 628
    },
    {
      "epoch": 1.471345029239766,
      "grad_norm": 0.23792992532253265,
      "learning_rate": 0.0002842310694769711,
      "loss": 0.132,
      "step": 629
    },
    {
      "epoch": 1.4736842105263157,
      "grad_norm": 0.3935653269290924,
      "learning_rate": 0.00028415300546448085,
      "loss": 0.2157,
      "step": 630
    },
    {
      "epoch": 1.4760233918128656,
      "grad_norm": 0.4712948203086853,
      "learning_rate": 0.00028407494145199063,
      "loss": 0.2712,
      "step": 631
    },
    {
      "epoch": 1.4783625730994152,
      "grad_norm": 0.3230997323989868,
      "learning_rate": 0.00028399687743950035,
      "loss": 0.1878,
      "step": 632
    },
    {
      "epoch": 1.4807017543859649,
      "grad_norm": 0.4972115159034729,
      "learning_rate": 0.00028391881342701013,
      "loss": 0.2491,
      "step": 633
    },
    {
      "epoch": 1.4830409356725145,
      "grad_norm": 0.4033598303794861,
      "learning_rate": 0.0002838407494145199,
      "loss": 0.2691,
      "step": 634
    },
    {
      "epoch": 1.4853801169590644,
      "grad_norm": 0.32605451345443726,
      "learning_rate": 0.00028376268540202963,
      "loss": 0.2009,
      "step": 635
    },
    {
      "epoch": 1.487719298245614,
      "grad_norm": 0.28312110900878906,
      "learning_rate": 0.0002836846213895394,
      "loss": 0.2053,
      "step": 636
    },
    {
      "epoch": 1.4900584795321636,
      "grad_norm": 0.339213103055954,
      "learning_rate": 0.0002836065573770492,
      "loss": 0.2187,
      "step": 637
    },
    {
      "epoch": 1.4923976608187135,
      "grad_norm": 0.3683432936668396,
      "learning_rate": 0.0002835284933645589,
      "loss": 0.2594,
      "step": 638
    },
    {
      "epoch": 1.4947368421052631,
      "grad_norm": 0.3491817116737366,
      "learning_rate": 0.0002834504293520687,
      "loss": 0.2004,
      "step": 639
    },
    {
      "epoch": 1.4970760233918128,
      "grad_norm": 0.34197109937667847,
      "learning_rate": 0.00028337236533957846,
      "loss": 0.2115,
      "step": 640
    },
    {
      "epoch": 1.4994152046783626,
      "grad_norm": 0.2597975432872772,
      "learning_rate": 0.0002832943013270882,
      "loss": 0.2029,
      "step": 641
    },
    {
      "epoch": 1.5017543859649123,
      "grad_norm": 0.295147180557251,
      "learning_rate": 0.0002832162373145979,
      "loss": 0.2098,
      "step": 642
    },
    {
      "epoch": 1.504093567251462,
      "grad_norm": 0.2678796350955963,
      "learning_rate": 0.00028313817330210774,
      "loss": 0.1997,
      "step": 643
    },
    {
      "epoch": 1.5064327485380118,
      "grad_norm": 0.29357969760894775,
      "learning_rate": 0.00028306010928961747,
      "loss": 0.1787,
      "step": 644
    },
    {
      "epoch": 1.5087719298245614,
      "grad_norm": 0.24661022424697876,
      "learning_rate": 0.0002829820452771272,
      "loss": 0.1773,
      "step": 645
    },
    {
      "epoch": 1.511111111111111,
      "grad_norm": 0.2560795843601227,
      "learning_rate": 0.00028290398126463697,
      "loss": 0.1475,
      "step": 646
    },
    {
      "epoch": 1.513450292397661,
      "grad_norm": 0.2659006416797638,
      "learning_rate": 0.00028282591725214674,
      "loss": 0.157,
      "step": 647
    },
    {
      "epoch": 1.5157894736842106,
      "grad_norm": 0.3076705038547516,
      "learning_rate": 0.00028274785323965647,
      "loss": 0.1939,
      "step": 648
    },
    {
      "epoch": 1.5181286549707602,
      "grad_norm": 0.3615298867225647,
      "learning_rate": 0.00028266978922716625,
      "loss": 0.267,
      "step": 649
    },
    {
      "epoch": 1.52046783625731,
      "grad_norm": 0.3026818633079529,
      "learning_rate": 0.000282591725214676,
      "loss": 0.2074,
      "step": 650
    },
    {
      "epoch": 1.5228070175438595,
      "grad_norm": 0.47915390133857727,
      "learning_rate": 0.00028251366120218575,
      "loss": 0.2642,
      "step": 651
    },
    {
      "epoch": 1.5251461988304094,
      "grad_norm": 0.4077409505844116,
      "learning_rate": 0.0002824355971896955,
      "loss": 0.2232,
      "step": 652
    },
    {
      "epoch": 1.5274853801169592,
      "grad_norm": 0.33040422201156616,
      "learning_rate": 0.0002823575331772053,
      "loss": 0.2074,
      "step": 653
    },
    {
      "epoch": 1.5298245614035086,
      "grad_norm": 0.6676313877105713,
      "learning_rate": 0.000282279469164715,
      "loss": 0.1591,
      "step": 654
    },
    {
      "epoch": 1.5321637426900585,
      "grad_norm": 0.2871656119823456,
      "learning_rate": 0.0002822014051522248,
      "loss": 0.1639,
      "step": 655
    },
    {
      "epoch": 1.5345029239766081,
      "grad_norm": 0.4446089267730713,
      "learning_rate": 0.0002821233411397346,
      "loss": 0.2059,
      "step": 656
    },
    {
      "epoch": 1.5368421052631578,
      "grad_norm": 0.27288809418678284,
      "learning_rate": 0.0002820452771272443,
      "loss": 0.1746,
      "step": 657
    },
    {
      "epoch": 1.5391812865497077,
      "grad_norm": 0.4132358431816101,
      "learning_rate": 0.0002819672131147541,
      "loss": 0.2228,
      "step": 658
    },
    {
      "epoch": 1.5415204678362573,
      "grad_norm": 0.2859112024307251,
      "learning_rate": 0.00028188914910226386,
      "loss": 0.1724,
      "step": 659
    },
    {
      "epoch": 1.543859649122807,
      "grad_norm": 0.35444119572639465,
      "learning_rate": 0.0002818110850897736,
      "loss": 0.2278,
      "step": 660
    },
    {
      "epoch": 1.5461988304093568,
      "grad_norm": 0.3331533968448639,
      "learning_rate": 0.00028173302107728336,
      "loss": 0.1697,
      "step": 661
    },
    {
      "epoch": 1.5485380116959064,
      "grad_norm": 0.3366592228412628,
      "learning_rate": 0.0002816549570647931,
      "loss": 0.2054,
      "step": 662
    },
    {
      "epoch": 1.550877192982456,
      "grad_norm": 0.3361016511917114,
      "learning_rate": 0.00028157689305230286,
      "loss": 0.248,
      "step": 663
    },
    {
      "epoch": 1.553216374269006,
      "grad_norm": 0.28595370054244995,
      "learning_rate": 0.00028149882903981264,
      "loss": 0.1922,
      "step": 664
    },
    {
      "epoch": 1.5555555555555556,
      "grad_norm": 0.2520449459552765,
      "learning_rate": 0.00028142076502732236,
      "loss": 0.1802,
      "step": 665
    },
    {
      "epoch": 1.5578947368421052,
      "grad_norm": 0.39043328166007996,
      "learning_rate": 0.00028134270101483214,
      "loss": 0.2242,
      "step": 666
    },
    {
      "epoch": 1.560233918128655,
      "grad_norm": 0.40432968735694885,
      "learning_rate": 0.0002812646370023419,
      "loss": 0.2417,
      "step": 667
    },
    {
      "epoch": 1.5625730994152047,
      "grad_norm": 0.36974838376045227,
      "learning_rate": 0.00028118657298985164,
      "loss": 0.2183,
      "step": 668
    },
    {
      "epoch": 1.5649122807017544,
      "grad_norm": 0.29053205251693726,
      "learning_rate": 0.0002811085089773614,
      "loss": 0.2055,
      "step": 669
    },
    {
      "epoch": 1.5672514619883042,
      "grad_norm": 0.2742806673049927,
      "learning_rate": 0.0002810304449648712,
      "loss": 0.1645,
      "step": 670
    },
    {
      "epoch": 1.5695906432748536,
      "grad_norm": 0.3071119785308838,
      "learning_rate": 0.0002809523809523809,
      "loss": 0.1411,
      "step": 671
    },
    {
      "epoch": 1.5719298245614035,
      "grad_norm": 0.3817741274833679,
      "learning_rate": 0.0002808743169398907,
      "loss": 0.2093,
      "step": 672
    },
    {
      "epoch": 1.5742690058479534,
      "grad_norm": 0.4099532663822174,
      "learning_rate": 0.00028079625292740047,
      "loss": 0.2416,
      "step": 673
    },
    {
      "epoch": 1.5766081871345028,
      "grad_norm": 0.3713088631629944,
      "learning_rate": 0.0002807181889149102,
      "loss": 0.1789,
      "step": 674
    },
    {
      "epoch": 1.5789473684210527,
      "grad_norm": 0.3870660960674286,
      "learning_rate": 0.00028064012490242,
      "loss": 0.1908,
      "step": 675
    },
    {
      "epoch": 1.5812865497076023,
      "grad_norm": 0.27259960770606995,
      "learning_rate": 0.00028056206088992975,
      "loss": 0.2005,
      "step": 676
    },
    {
      "epoch": 1.583625730994152,
      "grad_norm": 0.41182541847229004,
      "learning_rate": 0.0002804839968774395,
      "loss": 0.2344,
      "step": 677
    },
    {
      "epoch": 1.5859649122807018,
      "grad_norm": 0.35171931982040405,
      "learning_rate": 0.0002804059328649492,
      "loss": 0.2471,
      "step": 678
    },
    {
      "epoch": 1.5883040935672514,
      "grad_norm": 0.3114983141422272,
      "learning_rate": 0.00028032786885245903,
      "loss": 0.2351,
      "step": 679
    },
    {
      "epoch": 1.590643274853801,
      "grad_norm": 0.24852466583251953,
      "learning_rate": 0.00028024980483996875,
      "loss": 0.1486,
      "step": 680
    },
    {
      "epoch": 1.592982456140351,
      "grad_norm": 0.2864631116390228,
      "learning_rate": 0.0002801717408274785,
      "loss": 0.1953,
      "step": 681
    },
    {
      "epoch": 1.5953216374269006,
      "grad_norm": 0.26581907272338867,
      "learning_rate": 0.00028009367681498825,
      "loss": 0.212,
      "step": 682
    },
    {
      "epoch": 1.5976608187134502,
      "grad_norm": 0.4260837137699127,
      "learning_rate": 0.00028001561280249803,
      "loss": 0.2925,
      "step": 683
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.4113549292087555,
      "learning_rate": 0.00027993754879000775,
      "loss": 0.2539,
      "step": 684
    },
    {
      "epoch": 1.6023391812865497,
      "grad_norm": 0.26399633288383484,
      "learning_rate": 0.00027985948477751753,
      "loss": 0.2318,
      "step": 685
    },
    {
      "epoch": 1.6046783625730994,
      "grad_norm": 0.309451699256897,
      "learning_rate": 0.0002797814207650273,
      "loss": 0.1975,
      "step": 686
    },
    {
      "epoch": 1.6070175438596492,
      "grad_norm": 0.38323888182640076,
      "learning_rate": 0.00027970335675253703,
      "loss": 0.2588,
      "step": 687
    },
    {
      "epoch": 1.6093567251461989,
      "grad_norm": 0.33256709575653076,
      "learning_rate": 0.0002796252927400468,
      "loss": 0.1786,
      "step": 688
    },
    {
      "epoch": 1.6116959064327485,
      "grad_norm": 0.2725392282009125,
      "learning_rate": 0.0002795472287275566,
      "loss": 0.2063,
      "step": 689
    },
    {
      "epoch": 1.6140350877192984,
      "grad_norm": 0.2684658467769623,
      "learning_rate": 0.0002794691647150663,
      "loss": 0.1597,
      "step": 690
    },
    {
      "epoch": 1.6163742690058478,
      "grad_norm": 0.29420697689056396,
      "learning_rate": 0.0002793911007025761,
      "loss": 0.1478,
      "step": 691
    },
    {
      "epoch": 1.6187134502923977,
      "grad_norm": 0.3428266644477844,
      "learning_rate": 0.00027931303669008587,
      "loss": 0.2078,
      "step": 692
    },
    {
      "epoch": 1.6210526315789475,
      "grad_norm": 0.31248027086257935,
      "learning_rate": 0.0002792349726775956,
      "loss": 0.2086,
      "step": 693
    },
    {
      "epoch": 1.623391812865497,
      "grad_norm": 0.3471300005912781,
      "learning_rate": 0.00027915690866510537,
      "loss": 0.1972,
      "step": 694
    },
    {
      "epoch": 1.6257309941520468,
      "grad_norm": 0.46041417121887207,
      "learning_rate": 0.00027907884465261514,
      "loss": 0.1764,
      "step": 695
    },
    {
      "epoch": 1.6280701754385964,
      "grad_norm": 0.4824969470500946,
      "learning_rate": 0.00027900078064012487,
      "loss": 0.2657,
      "step": 696
    },
    {
      "epoch": 1.630409356725146,
      "grad_norm": 0.31538018584251404,
      "learning_rate": 0.00027892271662763465,
      "loss": 0.186,
      "step": 697
    },
    {
      "epoch": 1.632748538011696,
      "grad_norm": 0.41921114921569824,
      "learning_rate": 0.00027884465261514437,
      "loss": 0.24,
      "step": 698
    },
    {
      "epoch": 1.6350877192982456,
      "grad_norm": 0.2569272816181183,
      "learning_rate": 0.00027876658860265415,
      "loss": 0.1541,
      "step": 699
    },
    {
      "epoch": 1.6374269005847952,
      "grad_norm": 0.2768537700176239,
      "learning_rate": 0.0002786885245901639,
      "loss": 0.21,
      "step": 700
    },
    {
      "epoch": 1.639766081871345,
      "grad_norm": 0.23033811151981354,
      "learning_rate": 0.00027861046057767365,
      "loss": 0.1197,
      "step": 701
    },
    {
      "epoch": 1.6421052631578947,
      "grad_norm": 0.25952640175819397,
      "learning_rate": 0.0002785323965651834,
      "loss": 0.1468,
      "step": 702
    },
    {
      "epoch": 1.6444444444444444,
      "grad_norm": 0.3535110652446747,
      "learning_rate": 0.0002784543325526932,
      "loss": 0.2269,
      "step": 703
    },
    {
      "epoch": 1.6467836257309942,
      "grad_norm": 0.24888062477111816,
      "learning_rate": 0.0002783762685402029,
      "loss": 0.1497,
      "step": 704
    },
    {
      "epoch": 1.6491228070175439,
      "grad_norm": 0.3180290460586548,
      "learning_rate": 0.0002782982045277127,
      "loss": 0.1938,
      "step": 705
    },
    {
      "epoch": 1.6514619883040935,
      "grad_norm": 0.3255116045475006,
      "learning_rate": 0.0002782201405152225,
      "loss": 0.2293,
      "step": 706
    },
    {
      "epoch": 1.6538011695906434,
      "grad_norm": 0.2831626236438751,
      "learning_rate": 0.0002781420765027322,
      "loss": 0.2064,
      "step": 707
    },
    {
      "epoch": 1.656140350877193,
      "grad_norm": 0.2943427860736847,
      "learning_rate": 0.000278064012490242,
      "loss": 0.1988,
      "step": 708
    },
    {
      "epoch": 1.6584795321637427,
      "grad_norm": 0.3007170855998993,
      "learning_rate": 0.00027798594847775176,
      "loss": 0.1856,
      "step": 709
    },
    {
      "epoch": 1.6608187134502925,
      "grad_norm": 0.3423573672771454,
      "learning_rate": 0.0002779078844652615,
      "loss": 0.1872,
      "step": 710
    },
    {
      "epoch": 1.663157894736842,
      "grad_norm": 0.3802594542503357,
      "learning_rate": 0.00027782982045277126,
      "loss": 0.2293,
      "step": 711
    },
    {
      "epoch": 1.6654970760233918,
      "grad_norm": 0.3674415051937103,
      "learning_rate": 0.00027775175644028104,
      "loss": 0.2368,
      "step": 712
    },
    {
      "epoch": 1.6678362573099417,
      "grad_norm": 0.3656840920448303,
      "learning_rate": 0.00027767369242779076,
      "loss": 0.2393,
      "step": 713
    },
    {
      "epoch": 1.670175438596491,
      "grad_norm": 0.3586183190345764,
      "learning_rate": 0.0002775956284153005,
      "loss": 0.2675,
      "step": 714
    },
    {
      "epoch": 1.672514619883041,
      "grad_norm": 0.30855315923690796,
      "learning_rate": 0.0002775175644028103,
      "loss": 0.199,
      "step": 715
    },
    {
      "epoch": 1.6748538011695906,
      "grad_norm": 0.3203003704547882,
      "learning_rate": 0.00027743950039032004,
      "loss": 0.2295,
      "step": 716
    },
    {
      "epoch": 1.6771929824561402,
      "grad_norm": 0.2929080128669739,
      "learning_rate": 0.00027736143637782976,
      "loss": 0.1742,
      "step": 717
    },
    {
      "epoch": 1.67953216374269,
      "grad_norm": 0.2857760190963745,
      "learning_rate": 0.00027728337236533954,
      "loss": 0.1321,
      "step": 718
    },
    {
      "epoch": 1.6818713450292397,
      "grad_norm": 0.25656649470329285,
      "learning_rate": 0.0002772053083528493,
      "loss": 0.1701,
      "step": 719
    },
    {
      "epoch": 1.6842105263157894,
      "grad_norm": 0.3089071214199066,
      "learning_rate": 0.00027712724434035904,
      "loss": 0.1766,
      "step": 720
    },
    {
      "epoch": 1.6865497076023392,
      "grad_norm": 0.2630336284637451,
      "learning_rate": 0.0002770491803278688,
      "loss": 0.1686,
      "step": 721
    },
    {
      "epoch": 1.6888888888888889,
      "grad_norm": 0.3067742586135864,
      "learning_rate": 0.0002769711163153786,
      "loss": 0.2276,
      "step": 722
    },
    {
      "epoch": 1.6912280701754385,
      "grad_norm": 0.3156285881996155,
      "learning_rate": 0.0002768930523028883,
      "loss": 0.2327,
      "step": 723
    },
    {
      "epoch": 1.6935672514619884,
      "grad_norm": 0.2826021909713745,
      "learning_rate": 0.0002768149882903981,
      "loss": 0.1542,
      "step": 724
    },
    {
      "epoch": 1.695906432748538,
      "grad_norm": 0.2801690697669983,
      "learning_rate": 0.0002767369242779079,
      "loss": 0.1857,
      "step": 725
    },
    {
      "epoch": 1.6982456140350877,
      "grad_norm": 0.39478304982185364,
      "learning_rate": 0.0002766588602654176,
      "loss": 0.2601,
      "step": 726
    },
    {
      "epoch": 1.7005847953216375,
      "grad_norm": 0.33757686614990234,
      "learning_rate": 0.0002765807962529274,
      "loss": 0.1515,
      "step": 727
    },
    {
      "epoch": 1.7029239766081872,
      "grad_norm": 0.29922789335250854,
      "learning_rate": 0.00027650273224043715,
      "loss": 0.1882,
      "step": 728
    },
    {
      "epoch": 1.7052631578947368,
      "grad_norm": 0.30597007274627686,
      "learning_rate": 0.0002764246682279469,
      "loss": 0.1977,
      "step": 729
    },
    {
      "epoch": 1.7076023391812867,
      "grad_norm": 0.3705897331237793,
      "learning_rate": 0.00027634660421545665,
      "loss": 0.1748,
      "step": 730
    },
    {
      "epoch": 1.709941520467836,
      "grad_norm": 0.3475993275642395,
      "learning_rate": 0.00027626854020296643,
      "loss": 0.2264,
      "step": 731
    },
    {
      "epoch": 1.712280701754386,
      "grad_norm": 0.31151309609413147,
      "learning_rate": 0.00027619047619047615,
      "loss": 0.1901,
      "step": 732
    },
    {
      "epoch": 1.7146198830409358,
      "grad_norm": 0.32986417412757874,
      "learning_rate": 0.00027611241217798593,
      "loss": 0.2196,
      "step": 733
    },
    {
      "epoch": 1.7169590643274852,
      "grad_norm": 0.2458714246749878,
      "learning_rate": 0.00027603434816549565,
      "loss": 0.1701,
      "step": 734
    },
    {
      "epoch": 1.719298245614035,
      "grad_norm": 0.34873247146606445,
      "learning_rate": 0.00027595628415300543,
      "loss": 0.1896,
      "step": 735
    },
    {
      "epoch": 1.7216374269005847,
      "grad_norm": 0.3691248893737793,
      "learning_rate": 0.0002758782201405152,
      "loss": 0.2517,
      "step": 736
    },
    {
      "epoch": 1.7239766081871344,
      "grad_norm": 0.23140498995780945,
      "learning_rate": 0.00027580015612802493,
      "loss": 0.1505,
      "step": 737
    },
    {
      "epoch": 1.7263157894736842,
      "grad_norm": 0.3398946523666382,
      "learning_rate": 0.0002757220921155347,
      "loss": 0.2721,
      "step": 738
    },
    {
      "epoch": 1.7286549707602339,
      "grad_norm": 0.3369152545928955,
      "learning_rate": 0.0002756440281030445,
      "loss": 0.2032,
      "step": 739
    },
    {
      "epoch": 1.7309941520467835,
      "grad_norm": 0.31912505626678467,
      "learning_rate": 0.0002755659640905542,
      "loss": 0.1812,
      "step": 740
    },
    {
      "epoch": 1.7333333333333334,
      "grad_norm": 0.2898522615432739,
      "learning_rate": 0.000275487900078064,
      "loss": 0.1842,
      "step": 741
    },
    {
      "epoch": 1.735672514619883,
      "grad_norm": 0.3779204487800598,
      "learning_rate": 0.00027540983606557377,
      "loss": 0.2135,
      "step": 742
    },
    {
      "epoch": 1.7380116959064327,
      "grad_norm": 0.27825722098350525,
      "learning_rate": 0.0002753317720530835,
      "loss": 0.1581,
      "step": 743
    },
    {
      "epoch": 1.7403508771929825,
      "grad_norm": 0.3466607332229614,
      "learning_rate": 0.00027525370804059327,
      "loss": 0.2342,
      "step": 744
    },
    {
      "epoch": 1.7426900584795322,
      "grad_norm": 0.2867234945297241,
      "learning_rate": 0.00027517564402810304,
      "loss": 0.1664,
      "step": 745
    },
    {
      "epoch": 1.7450292397660818,
      "grad_norm": 0.28926539421081543,
      "learning_rate": 0.00027509758001561277,
      "loss": 0.2099,
      "step": 746
    },
    {
      "epoch": 1.7473684210526317,
      "grad_norm": 0.3013129234313965,
      "learning_rate": 0.00027501951600312255,
      "loss": 0.1904,
      "step": 747
    },
    {
      "epoch": 1.7497076023391813,
      "grad_norm": 0.41089391708374023,
      "learning_rate": 0.0002749414519906323,
      "loss": 0.258,
      "step": 748
    },
    {
      "epoch": 1.752046783625731,
      "grad_norm": 0.4357188045978546,
      "learning_rate": 0.00027486338797814205,
      "loss": 0.2353,
      "step": 749
    },
    {
      "epoch": 1.7543859649122808,
      "grad_norm": 0.2842419445514679,
      "learning_rate": 0.0002747853239656518,
      "loss": 0.184,
      "step": 750
    },
    {
      "epoch": 1.7567251461988302,
      "grad_norm": 0.31304991245269775,
      "learning_rate": 0.00027470725995316155,
      "loss": 0.1818,
      "step": 751
    },
    {
      "epoch": 1.75906432748538,
      "grad_norm": 0.32381296157836914,
      "learning_rate": 0.0002746291959406713,
      "loss": 0.1797,
      "step": 752
    },
    {
      "epoch": 1.76140350877193,
      "grad_norm": 0.3481011688709259,
      "learning_rate": 0.0002745511319281811,
      "loss": 0.186,
      "step": 753
    },
    {
      "epoch": 1.7637426900584794,
      "grad_norm": 0.26030027866363525,
      "learning_rate": 0.0002744730679156908,
      "loss": 0.1244,
      "step": 754
    },
    {
      "epoch": 1.7660818713450293,
      "grad_norm": 0.27209559082984924,
      "learning_rate": 0.0002743950039032006,
      "loss": 0.1699,
      "step": 755
    },
    {
      "epoch": 1.768421052631579,
      "grad_norm": 0.42937755584716797,
      "learning_rate": 0.0002743169398907104,
      "loss": 0.287,
      "step": 756
    },
    {
      "epoch": 1.7707602339181285,
      "grad_norm": 0.31238943338394165,
      "learning_rate": 0.0002742388758782201,
      "loss": 0.1524,
      "step": 757
    },
    {
      "epoch": 1.7730994152046784,
      "grad_norm": 0.5081039071083069,
      "learning_rate": 0.0002741608118657299,
      "loss": 0.336,
      "step": 758
    },
    {
      "epoch": 1.775438596491228,
      "grad_norm": 0.33667245507240295,
      "learning_rate": 0.00027408274785323966,
      "loss": 0.247,
      "step": 759
    },
    {
      "epoch": 1.7777777777777777,
      "grad_norm": 0.34430864453315735,
      "learning_rate": 0.0002740046838407494,
      "loss": 0.2309,
      "step": 760
    },
    {
      "epoch": 1.7801169590643275,
      "grad_norm": 0.4118152856826782,
      "learning_rate": 0.00027392661982825916,
      "loss": 0.2269,
      "step": 761
    },
    {
      "epoch": 1.7824561403508772,
      "grad_norm": 0.33980098366737366,
      "learning_rate": 0.00027384855581576894,
      "loss": 0.1813,
      "step": 762
    },
    {
      "epoch": 1.7847953216374268,
      "grad_norm": 0.3327943682670593,
      "learning_rate": 0.00027377049180327866,
      "loss": 0.1695,
      "step": 763
    },
    {
      "epoch": 1.7871345029239767,
      "grad_norm": 0.22785259783267975,
      "learning_rate": 0.00027369242779078844,
      "loss": 0.1581,
      "step": 764
    },
    {
      "epoch": 1.7894736842105263,
      "grad_norm": 0.34676143527030945,
      "learning_rate": 0.0002736143637782982,
      "loss": 0.2408,
      "step": 765
    },
    {
      "epoch": 1.791812865497076,
      "grad_norm": 0.3006179630756378,
      "learning_rate": 0.00027353629976580794,
      "loss": 0.2218,
      "step": 766
    },
    {
      "epoch": 1.7941520467836258,
      "grad_norm": 0.24007010459899902,
      "learning_rate": 0.0002734582357533177,
      "loss": 0.1311,
      "step": 767
    },
    {
      "epoch": 1.7964912280701755,
      "grad_norm": 0.3075335919857025,
      "learning_rate": 0.0002733801717408275,
      "loss": 0.224,
      "step": 768
    },
    {
      "epoch": 1.7988304093567251,
      "grad_norm": 0.24172474443912506,
      "learning_rate": 0.0002733021077283372,
      "loss": 0.1481,
      "step": 769
    },
    {
      "epoch": 1.801169590643275,
      "grad_norm": 0.2703005075454712,
      "learning_rate": 0.00027322404371584694,
      "loss": 0.1839,
      "step": 770
    },
    {
      "epoch": 1.8035087719298246,
      "grad_norm": 0.23152795433998108,
      "learning_rate": 0.0002731459797033567,
      "loss": 0.1508,
      "step": 771
    },
    {
      "epoch": 1.8058479532163743,
      "grad_norm": 0.3277990520000458,
      "learning_rate": 0.0002730679156908665,
      "loss": 0.2444,
      "step": 772
    },
    {
      "epoch": 1.8081871345029241,
      "grad_norm": 0.3368080258369446,
      "learning_rate": 0.0002729898516783762,
      "loss": 0.1699,
      "step": 773
    },
    {
      "epoch": 1.8105263157894735,
      "grad_norm": 0.400138258934021,
      "learning_rate": 0.000272911787665886,
      "loss": 0.2546,
      "step": 774
    },
    {
      "epoch": 1.8128654970760234,
      "grad_norm": 0.2690369188785553,
      "learning_rate": 0.0002728337236533958,
      "loss": 0.1622,
      "step": 775
    },
    {
      "epoch": 1.8152046783625733,
      "grad_norm": 0.3386155664920807,
      "learning_rate": 0.0002727556596409055,
      "loss": 0.2263,
      "step": 776
    },
    {
      "epoch": 1.8175438596491227,
      "grad_norm": 0.3257107138633728,
      "learning_rate": 0.0002726775956284153,
      "loss": 0.2342,
      "step": 777
    },
    {
      "epoch": 1.8198830409356725,
      "grad_norm": 0.23548221588134766,
      "learning_rate": 0.00027259953161592505,
      "loss": 0.1154,
      "step": 778
    },
    {
      "epoch": 1.8222222222222222,
      "grad_norm": 0.35295572876930237,
      "learning_rate": 0.0002725214676034348,
      "loss": 0.1889,
      "step": 779
    },
    {
      "epoch": 1.8245614035087718,
      "grad_norm": 0.3377903997898102,
      "learning_rate": 0.00027244340359094455,
      "loss": 0.227,
      "step": 780
    },
    {
      "epoch": 1.8269005847953217,
      "grad_norm": 0.3514731228351593,
      "learning_rate": 0.00027236533957845433,
      "loss": 0.2394,
      "step": 781
    },
    {
      "epoch": 1.8292397660818713,
      "grad_norm": 0.32704293727874756,
      "learning_rate": 0.00027228727556596405,
      "loss": 0.1714,
      "step": 782
    },
    {
      "epoch": 1.831578947368421,
      "grad_norm": 0.39971157908439636,
      "learning_rate": 0.00027220921155347383,
      "loss": 0.2166,
      "step": 783
    },
    {
      "epoch": 1.8339181286549708,
      "grad_norm": 0.33826911449432373,
      "learning_rate": 0.0002721311475409836,
      "loss": 0.2057,
      "step": 784
    },
    {
      "epoch": 1.8362573099415205,
      "grad_norm": 0.35327237844467163,
      "learning_rate": 0.00027205308352849333,
      "loss": 0.2519,
      "step": 785
    },
    {
      "epoch": 1.8385964912280701,
      "grad_norm": 0.4090031683444977,
      "learning_rate": 0.0002719750195160031,
      "loss": 0.1855,
      "step": 786
    },
    {
      "epoch": 1.84093567251462,
      "grad_norm": 0.3452863395214081,
      "learning_rate": 0.00027189695550351283,
      "loss": 0.1819,
      "step": 787
    },
    {
      "epoch": 1.8432748538011696,
      "grad_norm": 0.3997403085231781,
      "learning_rate": 0.0002718188914910226,
      "loss": 0.2704,
      "step": 788
    },
    {
      "epoch": 1.8456140350877193,
      "grad_norm": 0.43697217106819153,
      "learning_rate": 0.0002717408274785324,
      "loss": 0.2657,
      "step": 789
    },
    {
      "epoch": 1.8479532163742691,
      "grad_norm": 0.4019642174243927,
      "learning_rate": 0.0002716627634660421,
      "loss": 0.2017,
      "step": 790
    },
    {
      "epoch": 1.8502923976608188,
      "grad_norm": 0.36806684732437134,
      "learning_rate": 0.0002715846994535519,
      "loss": 0.2156,
      "step": 791
    },
    {
      "epoch": 1.8526315789473684,
      "grad_norm": 0.28846463561058044,
      "learning_rate": 0.00027150663544106167,
      "loss": 0.198,
      "step": 792
    },
    {
      "epoch": 1.8549707602339183,
      "grad_norm": 0.3756432831287384,
      "learning_rate": 0.0002714285714285714,
      "loss": 0.2033,
      "step": 793
    },
    {
      "epoch": 1.8573099415204677,
      "grad_norm": 0.2874682545661926,
      "learning_rate": 0.00027135050741608117,
      "loss": 0.2239,
      "step": 794
    },
    {
      "epoch": 1.8596491228070176,
      "grad_norm": 0.3049606680870056,
      "learning_rate": 0.00027127244340359094,
      "loss": 0.206,
      "step": 795
    },
    {
      "epoch": 1.8619883040935674,
      "grad_norm": 0.35157135128974915,
      "learning_rate": 0.00027119437939110067,
      "loss": 0.2081,
      "step": 796
    },
    {
      "epoch": 1.8643274853801168,
      "grad_norm": 0.4909668266773224,
      "learning_rate": 0.00027111631537861045,
      "loss": 0.1994,
      "step": 797
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 0.29858189821243286,
      "learning_rate": 0.0002710382513661202,
      "loss": 0.2135,
      "step": 798
    },
    {
      "epoch": 1.8690058479532163,
      "grad_norm": 0.3280254900455475,
      "learning_rate": 0.00027096018735362995,
      "loss": 0.2016,
      "step": 799
    },
    {
      "epoch": 1.871345029239766,
      "grad_norm": 0.34349676966667175,
      "learning_rate": 0.0002708821233411397,
      "loss": 0.2359,
      "step": 800
    },
    {
      "epoch": 1.8736842105263158,
      "grad_norm": 0.33604323863983154,
      "learning_rate": 0.0002708040593286495,
      "loss": 0.2653,
      "step": 801
    },
    {
      "epoch": 1.8760233918128655,
      "grad_norm": 0.30364274978637695,
      "learning_rate": 0.0002707259953161592,
      "loss": 0.196,
      "step": 802
    },
    {
      "epoch": 1.8783625730994151,
      "grad_norm": 0.30275923013687134,
      "learning_rate": 0.00027064793130366895,
      "loss": 0.226,
      "step": 803
    },
    {
      "epoch": 1.880701754385965,
      "grad_norm": 0.3671393394470215,
      "learning_rate": 0.0002705698672911788,
      "loss": 0.21,
      "step": 804
    },
    {
      "epoch": 1.8830409356725146,
      "grad_norm": 0.2528945803642273,
      "learning_rate": 0.0002704918032786885,
      "loss": 0.1599,
      "step": 805
    },
    {
      "epoch": 1.8853801169590643,
      "grad_norm": 0.3241606652736664,
      "learning_rate": 0.0002704137392661982,
      "loss": 0.2056,
      "step": 806
    },
    {
      "epoch": 1.8877192982456141,
      "grad_norm": 0.3053920865058899,
      "learning_rate": 0.000270335675253708,
      "loss": 0.1585,
      "step": 807
    },
    {
      "epoch": 1.8900584795321638,
      "grad_norm": 0.39160749316215515,
      "learning_rate": 0.0002702576112412178,
      "loss": 0.2173,
      "step": 808
    },
    {
      "epoch": 1.8923976608187134,
      "grad_norm": 0.2730060815811157,
      "learning_rate": 0.0002701795472287275,
      "loss": 0.1817,
      "step": 809
    },
    {
      "epoch": 1.8947368421052633,
      "grad_norm": 0.28908827900886536,
      "learning_rate": 0.0002701014832162373,
      "loss": 0.1915,
      "step": 810
    },
    {
      "epoch": 1.897076023391813,
      "grad_norm": 0.425711989402771,
      "learning_rate": 0.00027002341920374706,
      "loss": 0.2283,
      "step": 811
    },
    {
      "epoch": 1.8994152046783626,
      "grad_norm": 0.410805344581604,
      "learning_rate": 0.0002699453551912568,
      "loss": 0.2545,
      "step": 812
    },
    {
      "epoch": 1.9017543859649124,
      "grad_norm": 0.315761923789978,
      "learning_rate": 0.00026986729117876656,
      "loss": 0.2041,
      "step": 813
    },
    {
      "epoch": 1.9040935672514618,
      "grad_norm": 0.3825232684612274,
      "learning_rate": 0.00026978922716627634,
      "loss": 0.179,
      "step": 814
    },
    {
      "epoch": 1.9064327485380117,
      "grad_norm": 0.37721702456474304,
      "learning_rate": 0.00026971116315378606,
      "loss": 0.2562,
      "step": 815
    },
    {
      "epoch": 1.9087719298245616,
      "grad_norm": 0.32517582178115845,
      "learning_rate": 0.00026963309914129584,
      "loss": 0.2068,
      "step": 816
    },
    {
      "epoch": 1.911111111111111,
      "grad_norm": 0.3002636134624481,
      "learning_rate": 0.0002695550351288056,
      "loss": 0.1601,
      "step": 817
    },
    {
      "epoch": 1.9134502923976608,
      "grad_norm": 0.2876092195510864,
      "learning_rate": 0.00026947697111631534,
      "loss": 0.2372,
      "step": 818
    },
    {
      "epoch": 1.9157894736842105,
      "grad_norm": 0.2818314731121063,
      "learning_rate": 0.0002693989071038251,
      "loss": 0.2075,
      "step": 819
    },
    {
      "epoch": 1.9181286549707601,
      "grad_norm": 0.2982136607170105,
      "learning_rate": 0.0002693208430913349,
      "loss": 0.2117,
      "step": 820
    },
    {
      "epoch": 1.92046783625731,
      "grad_norm": 0.2754707634449005,
      "learning_rate": 0.0002692427790788446,
      "loss": 0.1838,
      "step": 821
    },
    {
      "epoch": 1.9228070175438596,
      "grad_norm": 0.31459084153175354,
      "learning_rate": 0.0002691647150663544,
      "loss": 0.2183,
      "step": 822
    },
    {
      "epoch": 1.9251461988304093,
      "grad_norm": 0.2748194932937622,
      "learning_rate": 0.0002690866510538641,
      "loss": 0.1973,
      "step": 823
    },
    {
      "epoch": 1.9274853801169591,
      "grad_norm": 0.31572484970092773,
      "learning_rate": 0.0002690085870413739,
      "loss": 0.1915,
      "step": 824
    },
    {
      "epoch": 1.9298245614035088,
      "grad_norm": 0.3251532018184662,
      "learning_rate": 0.0002689305230288837,
      "loss": 0.2448,
      "step": 825
    },
    {
      "epoch": 1.9321637426900584,
      "grad_norm": 0.43171268701553345,
      "learning_rate": 0.0002688524590163934,
      "loss": 0.2541,
      "step": 826
    },
    {
      "epoch": 1.9345029239766083,
      "grad_norm": 0.29459434747695923,
      "learning_rate": 0.0002687743950039032,
      "loss": 0.1879,
      "step": 827
    },
    {
      "epoch": 1.936842105263158,
      "grad_norm": 0.38598868250846863,
      "learning_rate": 0.00026869633099141295,
      "loss": 0.2376,
      "step": 828
    },
    {
      "epoch": 1.9391812865497076,
      "grad_norm": 0.27688339352607727,
      "learning_rate": 0.0002686182669789227,
      "loss": 0.181,
      "step": 829
    },
    {
      "epoch": 1.9415204678362574,
      "grad_norm": 0.35911667346954346,
      "learning_rate": 0.00026854020296643245,
      "loss": 0.1562,
      "step": 830
    },
    {
      "epoch": 1.943859649122807,
      "grad_norm": 0.3748202621936798,
      "learning_rate": 0.00026846213895394223,
      "loss": 0.2497,
      "step": 831
    },
    {
      "epoch": 1.9461988304093567,
      "grad_norm": 0.2737657129764557,
      "learning_rate": 0.00026838407494145195,
      "loss": 0.185,
      "step": 832
    },
    {
      "epoch": 1.9485380116959066,
      "grad_norm": 0.3753931224346161,
      "learning_rate": 0.00026830601092896173,
      "loss": 0.2471,
      "step": 833
    },
    {
      "epoch": 1.950877192982456,
      "grad_norm": 0.3367230296134949,
      "learning_rate": 0.0002682279469164715,
      "loss": 0.1781,
      "step": 834
    },
    {
      "epoch": 1.9532163742690059,
      "grad_norm": 0.30468788743019104,
      "learning_rate": 0.00026814988290398123,
      "loss": 0.1731,
      "step": 835
    },
    {
      "epoch": 1.9555555555555557,
      "grad_norm": 0.4510633945465088,
      "learning_rate": 0.000268071818891491,
      "loss": 0.1711,
      "step": 836
    },
    {
      "epoch": 1.9578947368421051,
      "grad_norm": 0.3532131314277649,
      "learning_rate": 0.0002679937548790008,
      "loss": 0.2006,
      "step": 837
    },
    {
      "epoch": 1.960233918128655,
      "grad_norm": 0.3347228765487671,
      "learning_rate": 0.0002679156908665105,
      "loss": 0.2431,
      "step": 838
    },
    {
      "epoch": 1.9625730994152046,
      "grad_norm": 0.2527044117450714,
      "learning_rate": 0.00026783762685402023,
      "loss": 0.1924,
      "step": 839
    },
    {
      "epoch": 1.9649122807017543,
      "grad_norm": 0.32114362716674805,
      "learning_rate": 0.00026775956284153007,
      "loss": 0.1709,
      "step": 840
    },
    {
      "epoch": 1.9672514619883041,
      "grad_norm": 0.3320917785167694,
      "learning_rate": 0.0002676814988290398,
      "loss": 0.1355,
      "step": 841
    },
    {
      "epoch": 1.9695906432748538,
      "grad_norm": 0.2670159935951233,
      "learning_rate": 0.0002676034348165495,
      "loss": 0.1428,
      "step": 842
    },
    {
      "epoch": 1.9719298245614034,
      "grad_norm": 0.5057196617126465,
      "learning_rate": 0.0002675253708040593,
      "loss": 0.2413,
      "step": 843
    },
    {
      "epoch": 1.9742690058479533,
      "grad_norm": 0.5195797681808472,
      "learning_rate": 0.00026744730679156907,
      "loss": 0.1879,
      "step": 844
    },
    {
      "epoch": 1.976608187134503,
      "grad_norm": 0.2998170554637909,
      "learning_rate": 0.0002673692427790788,
      "loss": 0.1772,
      "step": 845
    },
    {
      "epoch": 1.9789473684210526,
      "grad_norm": 0.2713130712509155,
      "learning_rate": 0.00026729117876658857,
      "loss": 0.1544,
      "step": 846
    },
    {
      "epoch": 1.9812865497076024,
      "grad_norm": 0.43566474318504333,
      "learning_rate": 0.00026721311475409835,
      "loss": 0.2603,
      "step": 847
    },
    {
      "epoch": 1.983625730994152,
      "grad_norm": 0.22881007194519043,
      "learning_rate": 0.00026713505074160807,
      "loss": 0.1259,
      "step": 848
    },
    {
      "epoch": 1.9859649122807017,
      "grad_norm": 0.3795115053653717,
      "learning_rate": 0.00026705698672911785,
      "loss": 0.2358,
      "step": 849
    },
    {
      "epoch": 1.9883040935672516,
      "grad_norm": 0.3900098204612732,
      "learning_rate": 0.0002669789227166276,
      "loss": 0.2816,
      "step": 850
    },
    {
      "epoch": 1.9906432748538012,
      "grad_norm": 0.34009021520614624,
      "learning_rate": 0.00026690085870413735,
      "loss": 0.1774,
      "step": 851
    },
    {
      "epoch": 1.9929824561403509,
      "grad_norm": 0.36113572120666504,
      "learning_rate": 0.0002668227946916471,
      "loss": 0.226,
      "step": 852
    },
    {
      "epoch": 1.9953216374269007,
      "grad_norm": 0.28866615891456604,
      "learning_rate": 0.0002667447306791569,
      "loss": 0.2049,
      "step": 853
    },
    {
      "epoch": 1.9976608187134501,
      "grad_norm": 0.45624497532844543,
      "learning_rate": 0.0002666666666666666,
      "loss": 0.2875,
      "step": 854
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.45934218168258667,
      "learning_rate": 0.0002665886026541764,
      "loss": 0.1707,
      "step": 855
    },
    {
      "epoch": 2.0,
      "eval_loss": 0.2567705810070038,
      "eval_runtime": 125.1516,
      "eval_samples_per_second": 4.411,
      "eval_steps_per_second": 0.551,
      "step": 855
    },
    {
      "epoch": 2.00233918128655,
      "grad_norm": 0.21586869657039642,
      "learning_rate": 0.0002665105386416862,
      "loss": 0.1106,
      "step": 856
    },
    {
      "epoch": 2.0046783625730993,
      "grad_norm": 0.35527265071868896,
      "learning_rate": 0.0002664324746291959,
      "loss": 0.2073,
      "step": 857
    },
    {
      "epoch": 2.007017543859649,
      "grad_norm": 0.35268348455429077,
      "learning_rate": 0.0002663544106167057,
      "loss": 0.168,
      "step": 858
    },
    {
      "epoch": 2.009356725146199,
      "grad_norm": 0.2851477265357971,
      "learning_rate": 0.0002662763466042154,
      "loss": 0.1263,
      "step": 859
    },
    {
      "epoch": 2.0116959064327484,
      "grad_norm": 0.29962998628616333,
      "learning_rate": 0.0002661982825917252,
      "loss": 0.1362,
      "step": 860
    },
    {
      "epoch": 2.0140350877192983,
      "grad_norm": 0.2794876992702484,
      "learning_rate": 0.00026612021857923496,
      "loss": 0.1466,
      "step": 861
    },
    {
      "epoch": 2.016374269005848,
      "grad_norm": 0.3527912497520447,
      "learning_rate": 0.0002660421545667447,
      "loss": 0.1921,
      "step": 862
    },
    {
      "epoch": 2.0187134502923976,
      "grad_norm": 0.2842937707901001,
      "learning_rate": 0.00026596409055425446,
      "loss": 0.1578,
      "step": 863
    },
    {
      "epoch": 2.0210526315789474,
      "grad_norm": 0.3729068338871002,
      "learning_rate": 0.00026588602654176424,
      "loss": 0.1889,
      "step": 864
    },
    {
      "epoch": 2.023391812865497,
      "grad_norm": 0.3746669590473175,
      "learning_rate": 0.00026580796252927396,
      "loss": 0.2238,
      "step": 865
    },
    {
      "epoch": 2.0257309941520467,
      "grad_norm": 0.30299532413482666,
      "learning_rate": 0.00026572989851678374,
      "loss": 0.1821,
      "step": 866
    },
    {
      "epoch": 2.0280701754385966,
      "grad_norm": 0.25299450755119324,
      "learning_rate": 0.0002656518345042935,
      "loss": 0.1695,
      "step": 867
    },
    {
      "epoch": 2.030409356725146,
      "grad_norm": 0.31847915053367615,
      "learning_rate": 0.00026557377049180324,
      "loss": 0.1528,
      "step": 868
    },
    {
      "epoch": 2.032748538011696,
      "grad_norm": 0.31698721647262573,
      "learning_rate": 0.000265495706479313,
      "loss": 0.173,
      "step": 869
    },
    {
      "epoch": 2.0350877192982457,
      "grad_norm": 0.3412579596042633,
      "learning_rate": 0.0002654176424668228,
      "loss": 0.2352,
      "step": 870
    },
    {
      "epoch": 2.037426900584795,
      "grad_norm": 0.251315176486969,
      "learning_rate": 0.0002653395784543325,
      "loss": 0.1346,
      "step": 871
    },
    {
      "epoch": 2.039766081871345,
      "grad_norm": 0.39477622509002686,
      "learning_rate": 0.0002652615144418423,
      "loss": 0.1696,
      "step": 872
    },
    {
      "epoch": 2.042105263157895,
      "grad_norm": 0.3051072359085083,
      "learning_rate": 0.00026518345042935207,
      "loss": 0.2144,
      "step": 873
    },
    {
      "epoch": 2.0444444444444443,
      "grad_norm": 0.3553225100040436,
      "learning_rate": 0.0002651053864168618,
      "loss": 0.1953,
      "step": 874
    },
    {
      "epoch": 2.046783625730994,
      "grad_norm": 0.3508021831512451,
      "learning_rate": 0.0002650273224043715,
      "loss": 0.1574,
      "step": 875
    },
    {
      "epoch": 2.049122807017544,
      "grad_norm": 0.3258472681045532,
      "learning_rate": 0.00026494925839188135,
      "loss": 0.1775,
      "step": 876
    },
    {
      "epoch": 2.0514619883040934,
      "grad_norm": 0.2968529760837555,
      "learning_rate": 0.0002648711943793911,
      "loss": 0.163,
      "step": 877
    },
    {
      "epoch": 2.0538011695906433,
      "grad_norm": 0.357378214597702,
      "learning_rate": 0.00026479313036690085,
      "loss": 0.1958,
      "step": 878
    },
    {
      "epoch": 2.056140350877193,
      "grad_norm": 0.36802420020103455,
      "learning_rate": 0.0002647150663544106,
      "loss": 0.2424,
      "step": 879
    },
    {
      "epoch": 2.0584795321637426,
      "grad_norm": 0.3622174859046936,
      "learning_rate": 0.00026463700234192035,
      "loss": 0.1606,
      "step": 880
    },
    {
      "epoch": 2.0608187134502924,
      "grad_norm": 0.32033801078796387,
      "learning_rate": 0.00026455893832943013,
      "loss": 0.1033,
      "step": 881
    },
    {
      "epoch": 2.0631578947368423,
      "grad_norm": 0.30374059081077576,
      "learning_rate": 0.00026448087431693985,
      "loss": 0.1474,
      "step": 882
    },
    {
      "epoch": 2.0654970760233917,
      "grad_norm": 0.3980700671672821,
      "learning_rate": 0.00026440281030444963,
      "loss": 0.2378,
      "step": 883
    },
    {
      "epoch": 2.0678362573099416,
      "grad_norm": 0.4235166013240814,
      "learning_rate": 0.0002643247462919594,
      "loss": 0.2165,
      "step": 884
    },
    {
      "epoch": 2.0701754385964914,
      "grad_norm": 0.35599786043167114,
      "learning_rate": 0.00026424668227946913,
      "loss": 0.205,
      "step": 885
    },
    {
      "epoch": 2.072514619883041,
      "grad_norm": 0.4136371910572052,
      "learning_rate": 0.0002641686182669789,
      "loss": 0.1899,
      "step": 886
    },
    {
      "epoch": 2.0748538011695907,
      "grad_norm": 0.22160184383392334,
      "learning_rate": 0.0002640905542544887,
      "loss": 0.1214,
      "step": 887
    },
    {
      "epoch": 2.07719298245614,
      "grad_norm": 0.29247215390205383,
      "learning_rate": 0.0002640124902419984,
      "loss": 0.1597,
      "step": 888
    },
    {
      "epoch": 2.07953216374269,
      "grad_norm": 0.38689160346984863,
      "learning_rate": 0.0002639344262295082,
      "loss": 0.187,
      "step": 889
    },
    {
      "epoch": 2.08187134502924,
      "grad_norm": 0.31428825855255127,
      "learning_rate": 0.00026385636221701797,
      "loss": 0.1936,
      "step": 890
    },
    {
      "epoch": 2.0842105263157893,
      "grad_norm": 0.2765038311481476,
      "learning_rate": 0.0002637782982045277,
      "loss": 0.1516,
      "step": 891
    },
    {
      "epoch": 2.086549707602339,
      "grad_norm": 0.3559015393257141,
      "learning_rate": 0.00026370023419203747,
      "loss": 0.2041,
      "step": 892
    },
    {
      "epoch": 2.088888888888889,
      "grad_norm": 0.3600005507469177,
      "learning_rate": 0.00026362217017954724,
      "loss": 0.1581,
      "step": 893
    },
    {
      "epoch": 2.0912280701754384,
      "grad_norm": 0.41168835759162903,
      "learning_rate": 0.00026354410616705697,
      "loss": 0.1914,
      "step": 894
    },
    {
      "epoch": 2.0935672514619883,
      "grad_norm": 0.3668798804283142,
      "learning_rate": 0.0002634660421545667,
      "loss": 0.2034,
      "step": 895
    },
    {
      "epoch": 2.095906432748538,
      "grad_norm": 0.3119400143623352,
      "learning_rate": 0.00026338797814207647,
      "loss": 0.1478,
      "step": 896
    },
    {
      "epoch": 2.0982456140350876,
      "grad_norm": 0.33016273379325867,
      "learning_rate": 0.00026330991412958625,
      "loss": 0.1934,
      "step": 897
    },
    {
      "epoch": 2.1005847953216374,
      "grad_norm": 0.28601229190826416,
      "learning_rate": 0.00026323185011709597,
      "loss": 0.1676,
      "step": 898
    },
    {
      "epoch": 2.1029239766081873,
      "grad_norm": 0.30858734250068665,
      "learning_rate": 0.00026315378610460575,
      "loss": 0.154,
      "step": 899
    },
    {
      "epoch": 2.1052631578947367,
      "grad_norm": 0.3669334650039673,
      "learning_rate": 0.0002630757220921155,
      "loss": 0.1904,
      "step": 900
    },
    {
      "epoch": 2.1076023391812866,
      "grad_norm": 0.3025450110435486,
      "learning_rate": 0.00026299765807962525,
      "loss": 0.1552,
      "step": 901
    },
    {
      "epoch": 2.1099415204678365,
      "grad_norm": 0.3445025086402893,
      "learning_rate": 0.000262919594067135,
      "loss": 0.1979,
      "step": 902
    },
    {
      "epoch": 2.112280701754386,
      "grad_norm": 0.4377128481864929,
      "learning_rate": 0.0002628415300546448,
      "loss": 0.2248,
      "step": 903
    },
    {
      "epoch": 2.1146198830409357,
      "grad_norm": 0.3596755862236023,
      "learning_rate": 0.0002627634660421545,
      "loss": 0.208,
      "step": 904
    },
    {
      "epoch": 2.116959064327485,
      "grad_norm": 0.36087703704833984,
      "learning_rate": 0.0002626854020296643,
      "loss": 0.2112,
      "step": 905
    },
    {
      "epoch": 2.119298245614035,
      "grad_norm": 0.307037353515625,
      "learning_rate": 0.0002626073380171741,
      "loss": 0.1548,
      "step": 906
    },
    {
      "epoch": 2.121637426900585,
      "grad_norm": 0.23480795323848724,
      "learning_rate": 0.0002625292740046838,
      "loss": 0.1133,
      "step": 907
    },
    {
      "epoch": 2.1239766081871343,
      "grad_norm": 0.3928609788417816,
      "learning_rate": 0.0002624512099921936,
      "loss": 0.2133,
      "step": 908
    },
    {
      "epoch": 2.126315789473684,
      "grad_norm": 0.38995325565338135,
      "learning_rate": 0.00026237314597970336,
      "loss": 0.2029,
      "step": 909
    },
    {
      "epoch": 2.128654970760234,
      "grad_norm": 0.3238731324672699,
      "learning_rate": 0.0002622950819672131,
      "loss": 0.1713,
      "step": 910
    },
    {
      "epoch": 2.1309941520467834,
      "grad_norm": 0.2944492697715759,
      "learning_rate": 0.00026221701795472286,
      "loss": 0.1722,
      "step": 911
    },
    {
      "epoch": 2.1333333333333333,
      "grad_norm": 0.3298560678958893,
      "learning_rate": 0.00026213895394223264,
      "loss": 0.1565,
      "step": 912
    },
    {
      "epoch": 2.135672514619883,
      "grad_norm": 0.3424236476421356,
      "learning_rate": 0.00026206088992974236,
      "loss": 0.1781,
      "step": 913
    },
    {
      "epoch": 2.1380116959064326,
      "grad_norm": 0.2955920100212097,
      "learning_rate": 0.00026198282591725214,
      "loss": 0.2009,
      "step": 914
    },
    {
      "epoch": 2.1403508771929824,
      "grad_norm": 0.29437094926834106,
      "learning_rate": 0.00026190476190476186,
      "loss": 0.2002,
      "step": 915
    },
    {
      "epoch": 2.1426900584795323,
      "grad_norm": 0.3491678833961487,
      "learning_rate": 0.00026182669789227164,
      "loss": 0.1973,
      "step": 916
    },
    {
      "epoch": 2.1450292397660817,
      "grad_norm": 0.3613814413547516,
      "learning_rate": 0.0002617486338797814,
      "loss": 0.2169,
      "step": 917
    },
    {
      "epoch": 2.1473684210526316,
      "grad_norm": 0.3155651092529297,
      "learning_rate": 0.00026167056986729114,
      "loss": 0.223,
      "step": 918
    },
    {
      "epoch": 2.1497076023391815,
      "grad_norm": 0.26727616786956787,
      "learning_rate": 0.0002615925058548009,
      "loss": 0.1479,
      "step": 919
    },
    {
      "epoch": 2.152046783625731,
      "grad_norm": 0.29615476727485657,
      "learning_rate": 0.0002615144418423107,
      "loss": 0.1952,
      "step": 920
    },
    {
      "epoch": 2.1543859649122807,
      "grad_norm": 0.2817157804965973,
      "learning_rate": 0.0002614363778298204,
      "loss": 0.1888,
      "step": 921
    },
    {
      "epoch": 2.1567251461988306,
      "grad_norm": 0.3104028105735779,
      "learning_rate": 0.0002613583138173302,
      "loss": 0.2318,
      "step": 922
    },
    {
      "epoch": 2.15906432748538,
      "grad_norm": 0.37733742594718933,
      "learning_rate": 0.00026128024980483997,
      "loss": 0.2432,
      "step": 923
    },
    {
      "epoch": 2.16140350877193,
      "grad_norm": 0.3124203681945801,
      "learning_rate": 0.0002612021857923497,
      "loss": 0.197,
      "step": 924
    },
    {
      "epoch": 2.1637426900584797,
      "grad_norm": 0.32476484775543213,
      "learning_rate": 0.0002611241217798595,
      "loss": 0.1777,
      "step": 925
    },
    {
      "epoch": 2.166081871345029,
      "grad_norm": 0.3462870419025421,
      "learning_rate": 0.00026104605776736925,
      "loss": 0.1967,
      "step": 926
    },
    {
      "epoch": 2.168421052631579,
      "grad_norm": 0.2962518036365509,
      "learning_rate": 0.000260967993754879,
      "loss": 0.1713,
      "step": 927
    },
    {
      "epoch": 2.170760233918129,
      "grad_norm": 0.31557324528694153,
      "learning_rate": 0.00026088992974238875,
      "loss": 0.1349,
      "step": 928
    },
    {
      "epoch": 2.1730994152046783,
      "grad_norm": 0.3871714770793915,
      "learning_rate": 0.00026081186572989853,
      "loss": 0.242,
      "step": 929
    },
    {
      "epoch": 2.175438596491228,
      "grad_norm": 0.3581230938434601,
      "learning_rate": 0.00026073380171740825,
      "loss": 0.1507,
      "step": 930
    },
    {
      "epoch": 2.1777777777777776,
      "grad_norm": 0.33491620421409607,
      "learning_rate": 0.000260655737704918,
      "loss": 0.1593,
      "step": 931
    },
    {
      "epoch": 2.1801169590643275,
      "grad_norm": 0.3278449475765228,
      "learning_rate": 0.00026057767369242775,
      "loss": 0.2013,
      "step": 932
    },
    {
      "epoch": 2.1824561403508773,
      "grad_norm": 0.3203575611114502,
      "learning_rate": 0.00026049960967993753,
      "loss": 0.1475,
      "step": 933
    },
    {
      "epoch": 2.1847953216374267,
      "grad_norm": 0.3326045274734497,
      "learning_rate": 0.00026042154566744725,
      "loss": 0.1794,
      "step": 934
    },
    {
      "epoch": 2.1871345029239766,
      "grad_norm": 0.40501245856285095,
      "learning_rate": 0.00026034348165495703,
      "loss": 0.1805,
      "step": 935
    },
    {
      "epoch": 2.1894736842105265,
      "grad_norm": 0.34237849712371826,
      "learning_rate": 0.0002602654176424668,
      "loss": 0.1482,
      "step": 936
    },
    {
      "epoch": 2.191812865497076,
      "grad_norm": 0.360757052898407,
      "learning_rate": 0.00026018735362997653,
      "loss": 0.1952,
      "step": 937
    },
    {
      "epoch": 2.1941520467836257,
      "grad_norm": 0.42680272459983826,
      "learning_rate": 0.0002601092896174863,
      "loss": 0.2069,
      "step": 938
    },
    {
      "epoch": 2.1964912280701756,
      "grad_norm": 0.3852192163467407,
      "learning_rate": 0.0002600312256049961,
      "loss": 0.2261,
      "step": 939
    },
    {
      "epoch": 2.198830409356725,
      "grad_norm": 0.29220837354660034,
      "learning_rate": 0.0002599531615925058,
      "loss": 0.178,
      "step": 940
    },
    {
      "epoch": 2.201169590643275,
      "grad_norm": 0.2482595294713974,
      "learning_rate": 0.0002598750975800156,
      "loss": 0.116,
      "step": 941
    },
    {
      "epoch": 2.2035087719298247,
      "grad_norm": 0.45775705575942993,
      "learning_rate": 0.00025979703356752537,
      "loss": 0.2226,
      "step": 942
    },
    {
      "epoch": 2.205847953216374,
      "grad_norm": 0.3243720829486847,
      "learning_rate": 0.0002597189695550351,
      "loss": 0.147,
      "step": 943
    },
    {
      "epoch": 2.208187134502924,
      "grad_norm": 0.2882055640220642,
      "learning_rate": 0.00025964090554254487,
      "loss": 0.209,
      "step": 944
    },
    {
      "epoch": 2.2105263157894735,
      "grad_norm": 0.3241584002971649,
      "learning_rate": 0.00025956284153005464,
      "loss": 0.1815,
      "step": 945
    },
    {
      "epoch": 2.2128654970760233,
      "grad_norm": 0.3041273057460785,
      "learning_rate": 0.00025948477751756437,
      "loss": 0.1814,
      "step": 946
    },
    {
      "epoch": 2.215204678362573,
      "grad_norm": 0.24135132133960724,
      "learning_rate": 0.00025940671350507415,
      "loss": 0.1322,
      "step": 947
    },
    {
      "epoch": 2.2175438596491226,
      "grad_norm": 0.29497483372688293,
      "learning_rate": 0.00025932864949258387,
      "loss": 0.1755,
      "step": 948
    },
    {
      "epoch": 2.2198830409356725,
      "grad_norm": 0.336194783449173,
      "learning_rate": 0.00025925058548009365,
      "loss": 0.1626,
      "step": 949
    },
    {
      "epoch": 2.2222222222222223,
      "grad_norm": 0.2643418312072754,
      "learning_rate": 0.0002591725214676034,
      "loss": 0.1284,
      "step": 950
    },
    {
      "epoch": 2.2245614035087717,
      "grad_norm": 0.2760993540287018,
      "learning_rate": 0.00025909445745511315,
      "loss": 0.1792,
      "step": 951
    },
    {
      "epoch": 2.2269005847953216,
      "grad_norm": 0.3662554621696472,
      "learning_rate": 0.0002590163934426229,
      "loss": 0.2565,
      "step": 952
    },
    {
      "epoch": 2.2292397660818715,
      "grad_norm": 0.27460968494415283,
      "learning_rate": 0.0002589383294301327,
      "loss": 0.1623,
      "step": 953
    },
    {
      "epoch": 2.231578947368421,
      "grad_norm": 0.2537413239479065,
      "learning_rate": 0.0002588602654176424,
      "loss": 0.1609,
      "step": 954
    },
    {
      "epoch": 2.2339181286549707,
      "grad_norm": 0.29294613003730774,
      "learning_rate": 0.0002587822014051522,
      "loss": 0.1633,
      "step": 955
    },
    {
      "epoch": 2.2362573099415206,
      "grad_norm": 0.3786076009273529,
      "learning_rate": 0.000258704137392662,
      "loss": 0.2039,
      "step": 956
    },
    {
      "epoch": 2.23859649122807,
      "grad_norm": 0.3022863566875458,
      "learning_rate": 0.0002586260733801717,
      "loss": 0.1804,
      "step": 957
    },
    {
      "epoch": 2.24093567251462,
      "grad_norm": 0.2846871614456177,
      "learning_rate": 0.0002585480093676815,
      "loss": 0.1744,
      "step": 958
    },
    {
      "epoch": 2.2432748538011698,
      "grad_norm": 0.3604409098625183,
      "learning_rate": 0.00025846994535519126,
      "loss": 0.2186,
      "step": 959
    },
    {
      "epoch": 2.245614035087719,
      "grad_norm": 0.3293013274669647,
      "learning_rate": 0.000258391881342701,
      "loss": 0.1645,
      "step": 960
    },
    {
      "epoch": 2.247953216374269,
      "grad_norm": 0.45405814051628113,
      "learning_rate": 0.00025831381733021076,
      "loss": 0.1875,
      "step": 961
    },
    {
      "epoch": 2.250292397660819,
      "grad_norm": 0.3593733310699463,
      "learning_rate": 0.00025823575331772054,
      "loss": 0.1376,
      "step": 962
    },
    {
      "epoch": 2.2526315789473683,
      "grad_norm": 0.4063454270362854,
      "learning_rate": 0.00025815768930523026,
      "loss": 0.1724,
      "step": 963
    },
    {
      "epoch": 2.254970760233918,
      "grad_norm": 0.36564311385154724,
      "learning_rate": 0.00025807962529274004,
      "loss": 0.1618,
      "step": 964
    },
    {
      "epoch": 2.257309941520468,
      "grad_norm": 0.5027701258659363,
      "learning_rate": 0.0002580015612802498,
      "loss": 0.1958,
      "step": 965
    },
    {
      "epoch": 2.2596491228070175,
      "grad_norm": 0.5067701935768127,
      "learning_rate": 0.00025792349726775954,
      "loss": 0.2452,
      "step": 966
    },
    {
      "epoch": 2.2619883040935673,
      "grad_norm": 0.34000110626220703,
      "learning_rate": 0.00025784543325526926,
      "loss": 0.1461,
      "step": 967
    },
    {
      "epoch": 2.264327485380117,
      "grad_norm": 0.3026914894580841,
      "learning_rate": 0.00025776736924277904,
      "loss": 0.1938,
      "step": 968
    },
    {
      "epoch": 2.2666666666666666,
      "grad_norm": 0.32739385962486267,
      "learning_rate": 0.0002576893052302888,
      "loss": 0.1366,
      "step": 969
    },
    {
      "epoch": 2.2690058479532165,
      "grad_norm": 0.3049500286579132,
      "learning_rate": 0.00025761124121779854,
      "loss": 0.1736,
      "step": 970
    },
    {
      "epoch": 2.271345029239766,
      "grad_norm": 0.3427545130252838,
      "learning_rate": 0.0002575331772053083,
      "loss": 0.209,
      "step": 971
    },
    {
      "epoch": 2.2736842105263158,
      "grad_norm": 0.27513760328292847,
      "learning_rate": 0.0002574551131928181,
      "loss": 0.1431,
      "step": 972
    },
    {
      "epoch": 2.2760233918128656,
      "grad_norm": 0.3207380175590515,
      "learning_rate": 0.0002573770491803278,
      "loss": 0.1783,
      "step": 973
    },
    {
      "epoch": 2.278362573099415,
      "grad_norm": 0.4232273995876312,
      "learning_rate": 0.0002572989851678376,
      "loss": 0.1883,
      "step": 974
    },
    {
      "epoch": 2.280701754385965,
      "grad_norm": 0.3480762541294098,
      "learning_rate": 0.0002572209211553474,
      "loss": 0.1904,
      "step": 975
    },
    {
      "epoch": 2.2830409356725148,
      "grad_norm": 0.3047369420528412,
      "learning_rate": 0.0002571428571428571,
      "loss": 0.1457,
      "step": 976
    },
    {
      "epoch": 2.285380116959064,
      "grad_norm": 0.31112605333328247,
      "learning_rate": 0.0002570647931303669,
      "loss": 0.1608,
      "step": 977
    },
    {
      "epoch": 2.287719298245614,
      "grad_norm": 0.3595544099807739,
      "learning_rate": 0.00025698672911787665,
      "loss": 0.1783,
      "step": 978
    },
    {
      "epoch": 2.290058479532164,
      "grad_norm": 0.31009596586227417,
      "learning_rate": 0.0002569086651053864,
      "loss": 0.1938,
      "step": 979
    },
    {
      "epoch": 2.2923976608187133,
      "grad_norm": 0.35319042205810547,
      "learning_rate": 0.00025683060109289615,
      "loss": 0.1869,
      "step": 980
    },
    {
      "epoch": 2.294736842105263,
      "grad_norm": 0.42302176356315613,
      "learning_rate": 0.00025675253708040593,
      "loss": 0.2398,
      "step": 981
    },
    {
      "epoch": 2.297076023391813,
      "grad_norm": 0.35589301586151123,
      "learning_rate": 0.00025667447306791565,
      "loss": 0.2274,
      "step": 982
    },
    {
      "epoch": 2.2994152046783625,
      "grad_norm": 0.28878116607666016,
      "learning_rate": 0.00025659640905542543,
      "loss": 0.1761,
      "step": 983
    },
    {
      "epoch": 2.3017543859649123,
      "grad_norm": 0.3616693913936615,
      "learning_rate": 0.00025651834504293515,
      "loss": 0.1703,
      "step": 984
    },
    {
      "epoch": 2.3040935672514617,
      "grad_norm": 0.30836498737335205,
      "learning_rate": 0.00025644028103044493,
      "loss": 0.1718,
      "step": 985
    },
    {
      "epoch": 2.3064327485380116,
      "grad_norm": 0.3608560860157013,
      "learning_rate": 0.0002563622170179547,
      "loss": 0.1695,
      "step": 986
    },
    {
      "epoch": 2.3087719298245615,
      "grad_norm": 0.3253852128982544,
      "learning_rate": 0.00025628415300546443,
      "loss": 0.1773,
      "step": 987
    },
    {
      "epoch": 2.311111111111111,
      "grad_norm": 0.30061855912208557,
      "learning_rate": 0.0002562060889929742,
      "loss": 0.1633,
      "step": 988
    },
    {
      "epoch": 2.3134502923976608,
      "grad_norm": 0.30144715309143066,
      "learning_rate": 0.000256128024980484,
      "loss": 0.1869,
      "step": 989
    },
    {
      "epoch": 2.3157894736842106,
      "grad_norm": 0.35518258810043335,
      "learning_rate": 0.0002560499609679937,
      "loss": 0.2053,
      "step": 990
    },
    {
      "epoch": 2.31812865497076,
      "grad_norm": 0.3613813519477844,
      "learning_rate": 0.0002559718969555035,
      "loss": 0.1861,
      "step": 991
    },
    {
      "epoch": 2.32046783625731,
      "grad_norm": 0.3342948853969574,
      "learning_rate": 0.00025589383294301327,
      "loss": 0.1627,
      "step": 992
    },
    {
      "epoch": 2.3228070175438598,
      "grad_norm": 0.3074239194393158,
      "learning_rate": 0.000255815768930523,
      "loss": 0.1531,
      "step": 993
    },
    {
      "epoch": 2.325146198830409,
      "grad_norm": 0.3022809326648712,
      "learning_rate": 0.00025573770491803277,
      "loss": 0.1827,
      "step": 994
    },
    {
      "epoch": 2.327485380116959,
      "grad_norm": 0.19945421814918518,
      "learning_rate": 0.00025565964090554254,
      "loss": 0.1116,
      "step": 995
    },
    {
      "epoch": 2.329824561403509,
      "grad_norm": 0.2765143811702728,
      "learning_rate": 0.00025558157689305227,
      "loss": 0.1392,
      "step": 996
    },
    {
      "epoch": 2.3321637426900583,
      "grad_norm": 0.3042948842048645,
      "learning_rate": 0.00025550351288056205,
      "loss": 0.1507,
      "step": 997
    },
    {
      "epoch": 2.334502923976608,
      "grad_norm": 0.48042139410972595,
      "learning_rate": 0.0002554254488680718,
      "loss": 0.2972,
      "step": 998
    },
    {
      "epoch": 2.336842105263158,
      "grad_norm": 0.38823646306991577,
      "learning_rate": 0.00025534738485558155,
      "loss": 0.1798,
      "step": 999
    },
    {
      "epoch": 2.3391812865497075,
      "grad_norm": 0.42640992999076843,
      "learning_rate": 0.00025526932084309127,
      "loss": 0.1888,
      "step": 1000
    },
    {
      "epoch": 2.3415204678362573,
      "grad_norm": 0.3707853853702545,
      "learning_rate": 0.0002551912568306011,
      "loss": 0.1935,
      "step": 1001
    },
    {
      "epoch": 2.343859649122807,
      "grad_norm": 0.3650893568992615,
      "learning_rate": 0.0002551131928181108,
      "loss": 0.1815,
      "step": 1002
    },
    {
      "epoch": 2.3461988304093566,
      "grad_norm": 0.3633582890033722,
      "learning_rate": 0.00025503512880562055,
      "loss": 0.2398,
      "step": 1003
    },
    {
      "epoch": 2.3485380116959065,
      "grad_norm": 0.31006988883018494,
      "learning_rate": 0.0002549570647931303,
      "loss": 0.2082,
      "step": 1004
    },
    {
      "epoch": 2.3508771929824563,
      "grad_norm": 0.27023282647132874,
      "learning_rate": 0.0002548790007806401,
      "loss": 0.1839,
      "step": 1005
    },
    {
      "epoch": 2.3532163742690058,
      "grad_norm": 0.292520672082901,
      "learning_rate": 0.0002548009367681499,
      "loss": 0.1503,
      "step": 1006
    },
    {
      "epoch": 2.3555555555555556,
      "grad_norm": 0.2527144253253937,
      "learning_rate": 0.0002547228727556596,
      "loss": 0.15,
      "step": 1007
    },
    {
      "epoch": 2.3578947368421055,
      "grad_norm": 0.3649445176124573,
      "learning_rate": 0.0002546448087431694,
      "loss": 0.2018,
      "step": 1008
    },
    {
      "epoch": 2.360233918128655,
      "grad_norm": 0.3104303479194641,
      "learning_rate": 0.00025456674473067916,
      "loss": 0.2016,
      "step": 1009
    },
    {
      "epoch": 2.3625730994152048,
      "grad_norm": 0.30978846549987793,
      "learning_rate": 0.0002544886807181889,
      "loss": 0.1766,
      "step": 1010
    },
    {
      "epoch": 2.3649122807017546,
      "grad_norm": 0.405731737613678,
      "learning_rate": 0.00025441061670569866,
      "loss": 0.1716,
      "step": 1011
    },
    {
      "epoch": 2.367251461988304,
      "grad_norm": 0.26174473762512207,
      "learning_rate": 0.00025433255269320844,
      "loss": 0.1495,
      "step": 1012
    },
    {
      "epoch": 2.369590643274854,
      "grad_norm": 0.2973193824291229,
      "learning_rate": 0.00025425448868071816,
      "loss": 0.1462,
      "step": 1013
    },
    {
      "epoch": 2.3719298245614033,
      "grad_norm": 0.2925716042518616,
      "learning_rate": 0.00025417642466822794,
      "loss": 0.1592,
      "step": 1014
    },
    {
      "epoch": 2.374269005847953,
      "grad_norm": 0.38939130306243896,
      "learning_rate": 0.0002540983606557377,
      "loss": 0.1508,
      "step": 1015
    },
    {
      "epoch": 2.376608187134503,
      "grad_norm": 0.3389122188091278,
      "learning_rate": 0.00025402029664324744,
      "loss": 0.1562,
      "step": 1016
    },
    {
      "epoch": 2.3789473684210525,
      "grad_norm": 0.34226104617118835,
      "learning_rate": 0.0002539422326307572,
      "loss": 0.143,
      "step": 1017
    },
    {
      "epoch": 2.3812865497076023,
      "grad_norm": 0.3208199739456177,
      "learning_rate": 0.000253864168618267,
      "loss": 0.1618,
      "step": 1018
    },
    {
      "epoch": 2.383625730994152,
      "grad_norm": 0.41262203454971313,
      "learning_rate": 0.0002537861046057767,
      "loss": 0.2322,
      "step": 1019
    },
    {
      "epoch": 2.3859649122807016,
      "grad_norm": 0.36857888102531433,
      "learning_rate": 0.00025370804059328644,
      "loss": 0.2054,
      "step": 1020
    },
    {
      "epoch": 2.3883040935672515,
      "grad_norm": 0.32359135150909424,
      "learning_rate": 0.00025362997658079627,
      "loss": 0.1768,
      "step": 1021
    },
    {
      "epoch": 2.3906432748538013,
      "grad_norm": 0.28496846556663513,
      "learning_rate": 0.000253551912568306,
      "loss": 0.1451,
      "step": 1022
    },
    {
      "epoch": 2.3929824561403508,
      "grad_norm": 0.2639343738555908,
      "learning_rate": 0.0002534738485558157,
      "loss": 0.1787,
      "step": 1023
    },
    {
      "epoch": 2.3953216374269006,
      "grad_norm": 0.2955261170864105,
      "learning_rate": 0.0002533957845433255,
      "loss": 0.1638,
      "step": 1024
    },
    {
      "epoch": 2.39766081871345,
      "grad_norm": 0.2773796021938324,
      "learning_rate": 0.0002533177205308353,
      "loss": 0.1319,
      "step": 1025
    },
    {
      "epoch": 2.4,
      "grad_norm": 0.33262550830841064,
      "learning_rate": 0.000253239656518345,
      "loss": 0.1786,
      "step": 1026
    },
    {
      "epoch": 2.4023391812865498,
      "grad_norm": 0.2764081656932831,
      "learning_rate": 0.0002531615925058548,
      "loss": 0.1632,
      "step": 1027
    },
    {
      "epoch": 2.404678362573099,
      "grad_norm": 0.25431138277053833,
      "learning_rate": 0.00025308352849336455,
      "loss": 0.1481,
      "step": 1028
    },
    {
      "epoch": 2.407017543859649,
      "grad_norm": 0.24644246697425842,
      "learning_rate": 0.0002530054644808743,
      "loss": 0.1641,
      "step": 1029
    },
    {
      "epoch": 2.409356725146199,
      "grad_norm": 0.3272852897644043,
      "learning_rate": 0.00025292740046838405,
      "loss": 0.2109,
      "step": 1030
    },
    {
      "epoch": 2.4116959064327483,
      "grad_norm": 0.3617566227912903,
      "learning_rate": 0.00025284933645589383,
      "loss": 0.1879,
      "step": 1031
    },
    {
      "epoch": 2.414035087719298,
      "grad_norm": 0.36391371488571167,
      "learning_rate": 0.00025277127244340355,
      "loss": 0.193,
      "step": 1032
    },
    {
      "epoch": 2.416374269005848,
      "grad_norm": 0.23115897178649902,
      "learning_rate": 0.00025269320843091333,
      "loss": 0.1587,
      "step": 1033
    },
    {
      "epoch": 2.4187134502923975,
      "grad_norm": 0.3114392161369324,
      "learning_rate": 0.0002526151444184231,
      "loss": 0.1705,
      "step": 1034
    },
    {
      "epoch": 2.4210526315789473,
      "grad_norm": 0.2744448184967041,
      "learning_rate": 0.00025253708040593283,
      "loss": 0.1212,
      "step": 1035
    },
    {
      "epoch": 2.423391812865497,
      "grad_norm": 0.3495420515537262,
      "learning_rate": 0.0002524590163934426,
      "loss": 0.1894,
      "step": 1036
    },
    {
      "epoch": 2.4257309941520466,
      "grad_norm": 0.32965531945228577,
      "learning_rate": 0.0002523809523809524,
      "loss": 0.1499,
      "step": 1037
    },
    {
      "epoch": 2.4280701754385965,
      "grad_norm": 0.3858906626701355,
      "learning_rate": 0.0002523028883684621,
      "loss": 0.2074,
      "step": 1038
    },
    {
      "epoch": 2.4304093567251464,
      "grad_norm": 0.3432232439517975,
      "learning_rate": 0.0002522248243559719,
      "loss": 0.2036,
      "step": 1039
    },
    {
      "epoch": 2.4327485380116958,
      "grad_norm": 0.2972969114780426,
      "learning_rate": 0.0002521467603434816,
      "loss": 0.1549,
      "step": 1040
    },
    {
      "epoch": 2.4350877192982456,
      "grad_norm": 0.409025639295578,
      "learning_rate": 0.0002520686963309914,
      "loss": 0.2148,
      "step": 1041
    },
    {
      "epoch": 2.4374269005847955,
      "grad_norm": 0.2674979567527771,
      "learning_rate": 0.00025199063231850117,
      "loss": 0.1413,
      "step": 1042
    },
    {
      "epoch": 2.439766081871345,
      "grad_norm": 0.26939550042152405,
      "learning_rate": 0.0002519125683060109,
      "loss": 0.1264,
      "step": 1043
    },
    {
      "epoch": 2.442105263157895,
      "grad_norm": 0.26005661487579346,
      "learning_rate": 0.00025183450429352067,
      "loss": 0.1325,
      "step": 1044
    },
    {
      "epoch": 2.4444444444444446,
      "grad_norm": 0.2302132397890091,
      "learning_rate": 0.00025175644028103044,
      "loss": 0.1208,
      "step": 1045
    },
    {
      "epoch": 2.446783625730994,
      "grad_norm": 0.43378108739852905,
      "learning_rate": 0.00025167837626854017,
      "loss": 0.2061,
      "step": 1046
    },
    {
      "epoch": 2.449122807017544,
      "grad_norm": 0.3548430800437927,
      "learning_rate": 0.00025160031225604995,
      "loss": 0.1689,
      "step": 1047
    },
    {
      "epoch": 2.451461988304094,
      "grad_norm": 0.28113895654678345,
      "learning_rate": 0.0002515222482435597,
      "loss": 0.1461,
      "step": 1048
    },
    {
      "epoch": 2.453801169590643,
      "grad_norm": 0.23577356338500977,
      "learning_rate": 0.00025144418423106945,
      "loss": 0.1298,
      "step": 1049
    },
    {
      "epoch": 2.456140350877193,
      "grad_norm": 0.3060316741466522,
      "learning_rate": 0.0002513661202185792,
      "loss": 0.1665,
      "step": 1050
    },
    {
      "epoch": 2.458479532163743,
      "grad_norm": 0.24780455231666565,
      "learning_rate": 0.000251288056206089,
      "loss": 0.1549,
      "step": 1051
    },
    {
      "epoch": 2.4608187134502923,
      "grad_norm": 0.2593565285205841,
      "learning_rate": 0.0002512099921935987,
      "loss": 0.1602,
      "step": 1052
    },
    {
      "epoch": 2.463157894736842,
      "grad_norm": 0.36489513516426086,
      "learning_rate": 0.0002511319281811085,
      "loss": 0.1883,
      "step": 1053
    },
    {
      "epoch": 2.4654970760233916,
      "grad_norm": 0.6368582248687744,
      "learning_rate": 0.0002510538641686183,
      "loss": 0.1701,
      "step": 1054
    },
    {
      "epoch": 2.4678362573099415,
      "grad_norm": 0.3530091345310211,
      "learning_rate": 0.000250975800156128,
      "loss": 0.169,
      "step": 1055
    },
    {
      "epoch": 2.4701754385964914,
      "grad_norm": 0.24837873876094818,
      "learning_rate": 0.0002508977361436377,
      "loss": 0.112,
      "step": 1056
    },
    {
      "epoch": 2.4725146198830408,
      "grad_norm": 0.31140750646591187,
      "learning_rate": 0.00025081967213114756,
      "loss": 0.1303,
      "step": 1057
    },
    {
      "epoch": 2.4748538011695906,
      "grad_norm": 0.2643057703971863,
      "learning_rate": 0.0002507416081186573,
      "loss": 0.1598,
      "step": 1058
    },
    {
      "epoch": 2.4771929824561405,
      "grad_norm": 0.31781646609306335,
      "learning_rate": 0.000250663544106167,
      "loss": 0.1802,
      "step": 1059
    },
    {
      "epoch": 2.47953216374269,
      "grad_norm": 0.25593528151512146,
      "learning_rate": 0.0002505854800936768,
      "loss": 0.1203,
      "step": 1060
    },
    {
      "epoch": 2.48187134502924,
      "grad_norm": 0.41201016306877136,
      "learning_rate": 0.00025050741608118656,
      "loss": 0.1895,
      "step": 1061
    },
    {
      "epoch": 2.4842105263157896,
      "grad_norm": 0.364581435918808,
      "learning_rate": 0.0002504293520686963,
      "loss": 0.1212,
      "step": 1062
    },
    {
      "epoch": 2.486549707602339,
      "grad_norm": 0.3576708137989044,
      "learning_rate": 0.00025035128805620606,
      "loss": 0.1995,
      "step": 1063
    },
    {
      "epoch": 2.488888888888889,
      "grad_norm": 0.4248863756656647,
      "learning_rate": 0.00025027322404371584,
      "loss": 0.227,
      "step": 1064
    },
    {
      "epoch": 2.4912280701754383,
      "grad_norm": 0.3053382337093353,
      "learning_rate": 0.00025019516003122556,
      "loss": 0.1611,
      "step": 1065
    },
    {
      "epoch": 2.493567251461988,
      "grad_norm": 0.3653790056705475,
      "learning_rate": 0.00025011709601873534,
      "loss": 0.1992,
      "step": 1066
    },
    {
      "epoch": 2.495906432748538,
      "grad_norm": 0.30656835436820984,
      "learning_rate": 0.0002500390320062451,
      "loss": 0.1846,
      "step": 1067
    },
    {
      "epoch": 2.4982456140350875,
      "grad_norm": 0.352790504693985,
      "learning_rate": 0.00024996096799375484,
      "loss": 0.2003,
      "step": 1068
    },
    {
      "epoch": 2.5005847953216374,
      "grad_norm": 0.3707987666130066,
      "learning_rate": 0.0002498829039812646,
      "loss": 0.1478,
      "step": 1069
    },
    {
      "epoch": 2.502923976608187,
      "grad_norm": 0.32857561111450195,
      "learning_rate": 0.0002498048399687744,
      "loss": 0.1767,
      "step": 1070
    },
    {
      "epoch": 2.5052631578947366,
      "grad_norm": 0.400378555059433,
      "learning_rate": 0.0002497267759562841,
      "loss": 0.2324,
      "step": 1071
    },
    {
      "epoch": 2.5076023391812865,
      "grad_norm": 0.35475102066993713,
      "learning_rate": 0.0002496487119437939,
      "loss": 0.1661,
      "step": 1072
    },
    {
      "epoch": 2.5099415204678364,
      "grad_norm": 0.45489442348480225,
      "learning_rate": 0.0002495706479313037,
      "loss": 0.222,
      "step": 1073
    },
    {
      "epoch": 2.512280701754386,
      "grad_norm": 0.46235138177871704,
      "learning_rate": 0.0002494925839188134,
      "loss": 0.2159,
      "step": 1074
    },
    {
      "epoch": 2.5146198830409356,
      "grad_norm": 0.23581932485103607,
      "learning_rate": 0.0002494145199063232,
      "loss": 0.1441,
      "step": 1075
    },
    {
      "epoch": 2.5169590643274855,
      "grad_norm": 0.3078501522541046,
      "learning_rate": 0.0002493364558938329,
      "loss": 0.2215,
      "step": 1076
    },
    {
      "epoch": 2.519298245614035,
      "grad_norm": 0.2778492867946625,
      "learning_rate": 0.0002492583918813427,
      "loss": 0.1533,
      "step": 1077
    },
    {
      "epoch": 2.521637426900585,
      "grad_norm": 0.31545475125312805,
      "learning_rate": 0.00024918032786885245,
      "loss": 0.175,
      "step": 1078
    },
    {
      "epoch": 2.5239766081871347,
      "grad_norm": 0.3257194757461548,
      "learning_rate": 0.0002491022638563622,
      "loss": 0.2233,
      "step": 1079
    },
    {
      "epoch": 2.526315789473684,
      "grad_norm": 0.3465562164783478,
      "learning_rate": 0.00024902419984387195,
      "loss": 0.1907,
      "step": 1080
    },
    {
      "epoch": 2.528654970760234,
      "grad_norm": 0.30364349484443665,
      "learning_rate": 0.00024894613583138173,
      "loss": 0.1832,
      "step": 1081
    },
    {
      "epoch": 2.530994152046784,
      "grad_norm": 0.32712414860725403,
      "learning_rate": 0.00024886807181889145,
      "loss": 0.2293,
      "step": 1082
    },
    {
      "epoch": 2.533333333333333,
      "grad_norm": 0.23840035498142242,
      "learning_rate": 0.00024879000780640123,
      "loss": 0.127,
      "step": 1083
    },
    {
      "epoch": 2.535672514619883,
      "grad_norm": 0.30697619915008545,
      "learning_rate": 0.000248711943793911,
      "loss": 0.168,
      "step": 1084
    },
    {
      "epoch": 2.538011695906433,
      "grad_norm": 0.3223157525062561,
      "learning_rate": 0.00024863387978142073,
      "loss": 0.1695,
      "step": 1085
    },
    {
      "epoch": 2.5403508771929824,
      "grad_norm": 0.4350162148475647,
      "learning_rate": 0.0002485558157689305,
      "loss": 0.1503,
      "step": 1086
    },
    {
      "epoch": 2.5426900584795322,
      "grad_norm": 0.2609867751598358,
      "learning_rate": 0.0002484777517564403,
      "loss": 0.1416,
      "step": 1087
    },
    {
      "epoch": 2.545029239766082,
      "grad_norm": 0.2581785023212433,
      "learning_rate": 0.00024839968774395,
      "loss": 0.1333,
      "step": 1088
    },
    {
      "epoch": 2.5473684210526315,
      "grad_norm": 0.26943325996398926,
      "learning_rate": 0.0002483216237314598,
      "loss": 0.1618,
      "step": 1089
    },
    {
      "epoch": 2.5497076023391814,
      "grad_norm": 0.2438916116952896,
      "learning_rate": 0.00024824355971896957,
      "loss": 0.168,
      "step": 1090
    },
    {
      "epoch": 2.5520467836257312,
      "grad_norm": 0.23720310628414154,
      "learning_rate": 0.0002481654957064793,
      "loss": 0.1346,
      "step": 1091
    },
    {
      "epoch": 2.5543859649122806,
      "grad_norm": 0.30767571926116943,
      "learning_rate": 0.000248087431693989,
      "loss": 0.2003,
      "step": 1092
    },
    {
      "epoch": 2.5567251461988305,
      "grad_norm": 0.3059462308883667,
      "learning_rate": 0.0002480093676814988,
      "loss": 0.1538,
      "step": 1093
    },
    {
      "epoch": 2.5590643274853804,
      "grad_norm": 0.25742489099502563,
      "learning_rate": 0.00024793130366900857,
      "loss": 0.169,
      "step": 1094
    },
    {
      "epoch": 2.56140350877193,
      "grad_norm": 0.2763276994228363,
      "learning_rate": 0.0002478532396565183,
      "loss": 0.1483,
      "step": 1095
    },
    {
      "epoch": 2.5637426900584797,
      "grad_norm": 0.20651403069496155,
      "learning_rate": 0.00024777517564402807,
      "loss": 0.1034,
      "step": 1096
    },
    {
      "epoch": 2.566081871345029,
      "grad_norm": 0.4237036108970642,
      "learning_rate": 0.00024769711163153785,
      "loss": 0.2137,
      "step": 1097
    },
    {
      "epoch": 2.568421052631579,
      "grad_norm": 0.3491637706756592,
      "learning_rate": 0.00024761904761904757,
      "loss": 0.1674,
      "step": 1098
    },
    {
      "epoch": 2.570760233918129,
      "grad_norm": 0.35413262248039246,
      "learning_rate": 0.00024754098360655735,
      "loss": 0.1936,
      "step": 1099
    },
    {
      "epoch": 2.573099415204678,
      "grad_norm": 0.31919366121292114,
      "learning_rate": 0.0002474629195940671,
      "loss": 0.1435,
      "step": 1100
    },
    {
      "epoch": 2.575438596491228,
      "grad_norm": 0.2734561562538147,
      "learning_rate": 0.00024738485558157685,
      "loss": 0.1362,
      "step": 1101
    },
    {
      "epoch": 2.5777777777777775,
      "grad_norm": 0.34951719641685486,
      "learning_rate": 0.0002473067915690866,
      "loss": 0.1658,
      "step": 1102
    },
    {
      "epoch": 2.5801169590643274,
      "grad_norm": 0.3725554645061493,
      "learning_rate": 0.0002472287275565964,
      "loss": 0.1524,
      "step": 1103
    },
    {
      "epoch": 2.5824561403508772,
      "grad_norm": 0.31048116087913513,
      "learning_rate": 0.0002471506635441061,
      "loss": 0.13,
      "step": 1104
    },
    {
      "epoch": 2.5847953216374266,
      "grad_norm": 0.3577478229999542,
      "learning_rate": 0.0002470725995316159,
      "loss": 0.1644,
      "step": 1105
    },
    {
      "epoch": 2.5871345029239765,
      "grad_norm": 0.3869287669658661,
      "learning_rate": 0.0002469945355191257,
      "loss": 0.2061,
      "step": 1106
    },
    {
      "epoch": 2.5894736842105264,
      "grad_norm": 0.43095898628234863,
      "learning_rate": 0.0002469164715066354,
      "loss": 0.1806,
      "step": 1107
    },
    {
      "epoch": 2.591812865497076,
      "grad_norm": 0.24267347157001495,
      "learning_rate": 0.0002468384074941452,
      "loss": 0.0898,
      "step": 1108
    },
    {
      "epoch": 2.5941520467836257,
      "grad_norm": 0.45106279850006104,
      "learning_rate": 0.0002467603434816549,
      "loss": 0.2397,
      "step": 1109
    },
    {
      "epoch": 2.5964912280701755,
      "grad_norm": 0.31007057428359985,
      "learning_rate": 0.0002466822794691647,
      "loss": 0.1278,
      "step": 1110
    },
    {
      "epoch": 2.598830409356725,
      "grad_norm": 0.3116055130958557,
      "learning_rate": 0.00024660421545667446,
      "loss": 0.1659,
      "step": 1111
    },
    {
      "epoch": 2.601169590643275,
      "grad_norm": 0.437614381313324,
      "learning_rate": 0.0002465261514441842,
      "loss": 0.1926,
      "step": 1112
    },
    {
      "epoch": 2.6035087719298247,
      "grad_norm": 0.240346759557724,
      "learning_rate": 0.00024644808743169396,
      "loss": 0.1142,
      "step": 1113
    },
    {
      "epoch": 2.605847953216374,
      "grad_norm": 0.3005375862121582,
      "learning_rate": 0.00024637002341920374,
      "loss": 0.1563,
      "step": 1114
    },
    {
      "epoch": 2.608187134502924,
      "grad_norm": 0.2451174557209015,
      "learning_rate": 0.00024629195940671346,
      "loss": 0.1226,
      "step": 1115
    },
    {
      "epoch": 2.610526315789474,
      "grad_norm": 0.26080840826034546,
      "learning_rate": 0.00024621389539422324,
      "loss": 0.1231,
      "step": 1116
    },
    {
      "epoch": 2.6128654970760232,
      "grad_norm": 0.3302234709262848,
      "learning_rate": 0.000246135831381733,
      "loss": 0.1865,
      "step": 1117
    },
    {
      "epoch": 2.615204678362573,
      "grad_norm": 0.3053692877292633,
      "learning_rate": 0.00024605776736924274,
      "loss": 0.1407,
      "step": 1118
    },
    {
      "epoch": 2.617543859649123,
      "grad_norm": 0.33853399753570557,
      "learning_rate": 0.0002459797033567525,
      "loss": 0.1977,
      "step": 1119
    },
    {
      "epoch": 2.6198830409356724,
      "grad_norm": 0.3249965012073517,
      "learning_rate": 0.0002459016393442623,
      "loss": 0.1921,
      "step": 1120
    },
    {
      "epoch": 2.6222222222222222,
      "grad_norm": 0.3160991072654724,
      "learning_rate": 0.000245823575331772,
      "loss": 0.1448,
      "step": 1121
    },
    {
      "epoch": 2.624561403508772,
      "grad_norm": 0.41875073313713074,
      "learning_rate": 0.0002457455113192818,
      "loss": 0.2158,
      "step": 1122
    },
    {
      "epoch": 2.6269005847953215,
      "grad_norm": 0.28463518619537354,
      "learning_rate": 0.0002456674473067916,
      "loss": 0.1274,
      "step": 1123
    },
    {
      "epoch": 2.6292397660818714,
      "grad_norm": 0.42520949244499207,
      "learning_rate": 0.0002455893832943013,
      "loss": 0.2109,
      "step": 1124
    },
    {
      "epoch": 2.6315789473684212,
      "grad_norm": 0.31331726908683777,
      "learning_rate": 0.0002455113192818111,
      "loss": 0.1676,
      "step": 1125
    },
    {
      "epoch": 2.6339181286549707,
      "grad_norm": 0.31840991973876953,
      "learning_rate": 0.00024543325526932085,
      "loss": 0.2028,
      "step": 1126
    },
    {
      "epoch": 2.6362573099415205,
      "grad_norm": 0.24395544826984406,
      "learning_rate": 0.0002453551912568306,
      "loss": 0.1561,
      "step": 1127
    },
    {
      "epoch": 2.6385964912280704,
      "grad_norm": 0.23776258528232574,
      "learning_rate": 0.0002452771272443403,
      "loss": 0.1241,
      "step": 1128
    },
    {
      "epoch": 2.64093567251462,
      "grad_norm": 0.28653645515441895,
      "learning_rate": 0.0002451990632318501,
      "loss": 0.172,
      "step": 1129
    },
    {
      "epoch": 2.6432748538011697,
      "grad_norm": 0.2614392936229706,
      "learning_rate": 0.00024512099921935985,
      "loss": 0.1411,
      "step": 1130
    },
    {
      "epoch": 2.6456140350877195,
      "grad_norm": 0.24439328908920288,
      "learning_rate": 0.0002450429352068696,
      "loss": 0.1176,
      "step": 1131
    },
    {
      "epoch": 2.647953216374269,
      "grad_norm": 0.3471844494342804,
      "learning_rate": 0.00024496487119437935,
      "loss": 0.2024,
      "step": 1132
    },
    {
      "epoch": 2.650292397660819,
      "grad_norm": 0.44974496960639954,
      "learning_rate": 0.00024488680718188913,
      "loss": 0.2456,
      "step": 1133
    },
    {
      "epoch": 2.6526315789473687,
      "grad_norm": 0.25144830346107483,
      "learning_rate": 0.0002448087431693989,
      "loss": 0.1393,
      "step": 1134
    },
    {
      "epoch": 2.654970760233918,
      "grad_norm": 0.3027925193309784,
      "learning_rate": 0.00024473067915690863,
      "loss": 0.1636,
      "step": 1135
    },
    {
      "epoch": 2.657309941520468,
      "grad_norm": 0.5114389657974243,
      "learning_rate": 0.0002446526151444184,
      "loss": 0.1639,
      "step": 1136
    },
    {
      "epoch": 2.659649122807018,
      "grad_norm": 0.39411741495132446,
      "learning_rate": 0.0002445745511319282,
      "loss": 0.194,
      "step": 1137
    },
    {
      "epoch": 2.6619883040935672,
      "grad_norm": 0.33194229006767273,
      "learning_rate": 0.0002444964871194379,
      "loss": 0.2322,
      "step": 1138
    },
    {
      "epoch": 2.664327485380117,
      "grad_norm": 0.2863258123397827,
      "learning_rate": 0.0002444184231069477,
      "loss": 0.1534,
      "step": 1139
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 0.36559197306632996,
      "learning_rate": 0.00024434035909445747,
      "loss": 0.2052,
      "step": 1140
    },
    {
      "epoch": 2.6690058479532164,
      "grad_norm": 0.29123082756996155,
      "learning_rate": 0.0002442622950819672,
      "loss": 0.1586,
      "step": 1141
    },
    {
      "epoch": 2.6713450292397662,
      "grad_norm": 0.24891632795333862,
      "learning_rate": 0.00024418423106947697,
      "loss": 0.1418,
      "step": 1142
    },
    {
      "epoch": 2.6736842105263157,
      "grad_norm": 0.25629177689552307,
      "learning_rate": 0.00024410616705698672,
      "loss": 0.1791,
      "step": 1143
    },
    {
      "epoch": 2.6760233918128655,
      "grad_norm": 0.27251148223876953,
      "learning_rate": 0.00024402810304449647,
      "loss": 0.1629,
      "step": 1144
    },
    {
      "epoch": 2.678362573099415,
      "grad_norm": 0.2266109585762024,
      "learning_rate": 0.00024395003903200622,
      "loss": 0.1231,
      "step": 1145
    },
    {
      "epoch": 2.680701754385965,
      "grad_norm": 0.24009864032268524,
      "learning_rate": 0.000243871975019516,
      "loss": 0.1672,
      "step": 1146
    },
    {
      "epoch": 2.6830409356725147,
      "grad_norm": 0.23302869498729706,
      "learning_rate": 0.00024379391100702575,
      "loss": 0.1102,
      "step": 1147
    },
    {
      "epoch": 2.685380116959064,
      "grad_norm": 0.44572681188583374,
      "learning_rate": 0.0002437158469945355,
      "loss": 0.26,
      "step": 1148
    },
    {
      "epoch": 2.687719298245614,
      "grad_norm": 0.28504595160484314,
      "learning_rate": 0.00024363778298204527,
      "loss": 0.1883,
      "step": 1149
    },
    {
      "epoch": 2.690058479532164,
      "grad_norm": 0.3514469563961029,
      "learning_rate": 0.00024355971896955502,
      "loss": 0.2059,
      "step": 1150
    },
    {
      "epoch": 2.6923976608187132,
      "grad_norm": 0.2129957377910614,
      "learning_rate": 0.00024348165495706477,
      "loss": 0.121,
      "step": 1151
    },
    {
      "epoch": 2.694736842105263,
      "grad_norm": 0.3676757514476776,
      "learning_rate": 0.00024340359094457455,
      "loss": 0.1798,
      "step": 1152
    },
    {
      "epoch": 2.697076023391813,
      "grad_norm": 0.21657997369766235,
      "learning_rate": 0.0002433255269320843,
      "loss": 0.1226,
      "step": 1153
    },
    {
      "epoch": 2.6994152046783624,
      "grad_norm": 0.33458346128463745,
      "learning_rate": 0.00024324746291959403,
      "loss": 0.2124,
      "step": 1154
    },
    {
      "epoch": 2.7017543859649122,
      "grad_norm": 0.35368287563323975,
      "learning_rate": 0.00024316939890710383,
      "loss": 0.2283,
      "step": 1155
    },
    {
      "epoch": 2.704093567251462,
      "grad_norm": 0.3792692720890045,
      "learning_rate": 0.00024309133489461355,
      "loss": 0.2317,
      "step": 1156
    },
    {
      "epoch": 2.7064327485380115,
      "grad_norm": 0.3526763916015625,
      "learning_rate": 0.0002430132708821233,
      "loss": 0.2096,
      "step": 1157
    },
    {
      "epoch": 2.7087719298245614,
      "grad_norm": 0.22975090146064758,
      "learning_rate": 0.00024293520686963308,
      "loss": 0.1692,
      "step": 1158
    },
    {
      "epoch": 2.7111111111111112,
      "grad_norm": 0.42146846652030945,
      "learning_rate": 0.00024285714285714283,
      "loss": 0.2461,
      "step": 1159
    },
    {
      "epoch": 2.7134502923976607,
      "grad_norm": 0.41570642590522766,
      "learning_rate": 0.00024277907884465258,
      "loss": 0.1974,
      "step": 1160
    },
    {
      "epoch": 2.7157894736842105,
      "grad_norm": 0.4563601315021515,
      "learning_rate": 0.00024270101483216236,
      "loss": 0.2583,
      "step": 1161
    },
    {
      "epoch": 2.7181286549707604,
      "grad_norm": 0.2860966622829437,
      "learning_rate": 0.0002426229508196721,
      "loss": 0.1408,
      "step": 1162
    },
    {
      "epoch": 2.72046783625731,
      "grad_norm": 0.29793301224708557,
      "learning_rate": 0.00024254488680718186,
      "loss": 0.1945,
      "step": 1163
    },
    {
      "epoch": 2.7228070175438597,
      "grad_norm": 0.2772233486175537,
      "learning_rate": 0.00024246682279469164,
      "loss": 0.147,
      "step": 1164
    },
    {
      "epoch": 2.7251461988304095,
      "grad_norm": 0.3076883554458618,
      "learning_rate": 0.0002423887587822014,
      "loss": 0.1659,
      "step": 1165
    },
    {
      "epoch": 2.727485380116959,
      "grad_norm": 0.26978054642677307,
      "learning_rate": 0.00024231069476971114,
      "loss": 0.1517,
      "step": 1166
    },
    {
      "epoch": 2.729824561403509,
      "grad_norm": 0.36564281582832336,
      "learning_rate": 0.00024223263075722092,
      "loss": 0.1507,
      "step": 1167
    },
    {
      "epoch": 2.7321637426900587,
      "grad_norm": 0.27101877331733704,
      "learning_rate": 0.00024215456674473067,
      "loss": 0.133,
      "step": 1168
    },
    {
      "epoch": 2.734502923976608,
      "grad_norm": 0.4729599058628082,
      "learning_rate": 0.00024207650273224042,
      "loss": 0.2703,
      "step": 1169
    },
    {
      "epoch": 2.736842105263158,
      "grad_norm": 0.3345654010772705,
      "learning_rate": 0.0002419984387197502,
      "loss": 0.1784,
      "step": 1170
    },
    {
      "epoch": 2.739181286549708,
      "grad_norm": 0.33564892411231995,
      "learning_rate": 0.00024192037470725995,
      "loss": 0.1462,
      "step": 1171
    },
    {
      "epoch": 2.7415204678362572,
      "grad_norm": 0.39931949973106384,
      "learning_rate": 0.00024184231069476967,
      "loss": 0.2034,
      "step": 1172
    },
    {
      "epoch": 2.743859649122807,
      "grad_norm": 0.295835018157959,
      "learning_rate": 0.00024176424668227947,
      "loss": 0.1658,
      "step": 1173
    },
    {
      "epoch": 2.746198830409357,
      "grad_norm": 0.2525967061519623,
      "learning_rate": 0.0002416861826697892,
      "loss": 0.1154,
      "step": 1174
    },
    {
      "epoch": 2.7485380116959064,
      "grad_norm": 0.44506871700286865,
      "learning_rate": 0.00024160811865729895,
      "loss": 0.278,
      "step": 1175
    },
    {
      "epoch": 2.7508771929824563,
      "grad_norm": 0.3519385755062103,
      "learning_rate": 0.00024153005464480872,
      "loss": 0.2196,
      "step": 1176
    },
    {
      "epoch": 2.753216374269006,
      "grad_norm": 0.3367045819759369,
      "learning_rate": 0.00024145199063231847,
      "loss": 0.1994,
      "step": 1177
    },
    {
      "epoch": 2.7555555555555555,
      "grad_norm": 0.29244327545166016,
      "learning_rate": 0.00024137392661982823,
      "loss": 0.1733,
      "step": 1178
    },
    {
      "epoch": 2.7578947368421054,
      "grad_norm": 0.2932978868484497,
      "learning_rate": 0.000241295862607338,
      "loss": 0.1933,
      "step": 1179
    },
    {
      "epoch": 2.760233918128655,
      "grad_norm": 0.2860317826271057,
      "learning_rate": 0.00024121779859484775,
      "loss": 0.1507,
      "step": 1180
    },
    {
      "epoch": 2.7625730994152047,
      "grad_norm": 0.4701542556285858,
      "learning_rate": 0.0002411397345823575,
      "loss": 0.2246,
      "step": 1181
    },
    {
      "epoch": 2.7649122807017545,
      "grad_norm": 0.27946436405181885,
      "learning_rate": 0.00024106167056986728,
      "loss": 0.1706,
      "step": 1182
    },
    {
      "epoch": 2.767251461988304,
      "grad_norm": 0.2621441185474396,
      "learning_rate": 0.00024098360655737703,
      "loss": 0.1463,
      "step": 1183
    },
    {
      "epoch": 2.769590643274854,
      "grad_norm": 0.3153962194919586,
      "learning_rate": 0.00024090554254488678,
      "loss": 0.1814,
      "step": 1184
    },
    {
      "epoch": 2.7719298245614032,
      "grad_norm": 0.2508370876312256,
      "learning_rate": 0.00024082747853239656,
      "loss": 0.1403,
      "step": 1185
    },
    {
      "epoch": 2.774269005847953,
      "grad_norm": 0.3628239333629608,
      "learning_rate": 0.0002407494145199063,
      "loss": 0.2155,
      "step": 1186
    },
    {
      "epoch": 2.776608187134503,
      "grad_norm": 0.3727502226829529,
      "learning_rate": 0.00024067135050741606,
      "loss": 0.1933,
      "step": 1187
    },
    {
      "epoch": 2.7789473684210524,
      "grad_norm": 0.37835243344306946,
      "learning_rate": 0.00024059328649492584,
      "loss": 0.1506,
      "step": 1188
    },
    {
      "epoch": 2.7812865497076023,
      "grad_norm": 0.2604568600654602,
      "learning_rate": 0.0002405152224824356,
      "loss": 0.0986,
      "step": 1189
    },
    {
      "epoch": 2.783625730994152,
      "grad_norm": 0.28236114978790283,
      "learning_rate": 0.0002404371584699453,
      "loss": 0.1385,
      "step": 1190
    },
    {
      "epoch": 2.7859649122807015,
      "grad_norm": 0.3909977972507477,
      "learning_rate": 0.00024035909445745512,
      "loss": 0.2256,
      "step": 1191
    },
    {
      "epoch": 2.7883040935672514,
      "grad_norm": 0.21019583940505981,
      "learning_rate": 0.00024028103044496484,
      "loss": 0.1213,
      "step": 1192
    },
    {
      "epoch": 2.7906432748538013,
      "grad_norm": 0.30457794666290283,
      "learning_rate": 0.0002402029664324746,
      "loss": 0.2025,
      "step": 1193
    },
    {
      "epoch": 2.7929824561403507,
      "grad_norm": 0.29399004578590393,
      "learning_rate": 0.00024012490241998437,
      "loss": 0.177,
      "step": 1194
    },
    {
      "epoch": 2.7953216374269005,
      "grad_norm": 0.37325191497802734,
      "learning_rate": 0.00024004683840749412,
      "loss": 0.1664,
      "step": 1195
    },
    {
      "epoch": 2.7976608187134504,
      "grad_norm": 0.28642621636390686,
      "learning_rate": 0.00023996877439500387,
      "loss": 0.1657,
      "step": 1196
    },
    {
      "epoch": 2.8,
      "grad_norm": 0.33063068985939026,
      "learning_rate": 0.00023989071038251365,
      "loss": 0.1445,
      "step": 1197
    },
    {
      "epoch": 2.8023391812865497,
      "grad_norm": 0.31062161922454834,
      "learning_rate": 0.0002398126463700234,
      "loss": 0.1692,
      "step": 1198
    },
    {
      "epoch": 2.8046783625730995,
      "grad_norm": 0.3241235613822937,
      "learning_rate": 0.00023973458235753315,
      "loss": 0.1527,
      "step": 1199
    },
    {
      "epoch": 2.807017543859649,
      "grad_norm": 0.4154420495033264,
      "learning_rate": 0.00023965651834504292,
      "loss": 0.2707,
      "step": 1200
    },
    {
      "epoch": 2.809356725146199,
      "grad_norm": 0.28775516152381897,
      "learning_rate": 0.00023957845433255267,
      "loss": 0.1917,
      "step": 1201
    },
    {
      "epoch": 2.8116959064327487,
      "grad_norm": 0.30999162793159485,
      "learning_rate": 0.00023950039032006242,
      "loss": 0.194,
      "step": 1202
    },
    {
      "epoch": 2.814035087719298,
      "grad_norm": 0.20967520773410797,
      "learning_rate": 0.0002394223263075722,
      "loss": 0.108,
      "step": 1203
    },
    {
      "epoch": 2.816374269005848,
      "grad_norm": 0.3083066940307617,
      "learning_rate": 0.00023934426229508195,
      "loss": 0.1987,
      "step": 1204
    },
    {
      "epoch": 2.818713450292398,
      "grad_norm": 0.30757081508636475,
      "learning_rate": 0.0002392661982825917,
      "loss": 0.1736,
      "step": 1205
    },
    {
      "epoch": 2.8210526315789473,
      "grad_norm": 0.3133728802204132,
      "learning_rate": 0.00023918813427010148,
      "loss": 0.1858,
      "step": 1206
    },
    {
      "epoch": 2.823391812865497,
      "grad_norm": 0.24401050806045532,
      "learning_rate": 0.00023911007025761123,
      "loss": 0.1431,
      "step": 1207
    },
    {
      "epoch": 2.825730994152047,
      "grad_norm": 0.20833483338356018,
      "learning_rate": 0.00023903200624512095,
      "loss": 0.1164,
      "step": 1208
    },
    {
      "epoch": 2.8280701754385964,
      "grad_norm": 0.36001425981521606,
      "learning_rate": 0.00023895394223263076,
      "loss": 0.1791,
      "step": 1209
    },
    {
      "epoch": 2.8304093567251463,
      "grad_norm": 0.29511338472366333,
      "learning_rate": 0.00023887587822014048,
      "loss": 0.1608,
      "step": 1210
    },
    {
      "epoch": 2.832748538011696,
      "grad_norm": 0.28051966428756714,
      "learning_rate": 0.00023879781420765023,
      "loss": 0.1366,
      "step": 1211
    },
    {
      "epoch": 2.8350877192982455,
      "grad_norm": 0.32069477438926697,
      "learning_rate": 0.00023871975019516,
      "loss": 0.19,
      "step": 1212
    },
    {
      "epoch": 2.8374269005847954,
      "grad_norm": 0.19394764304161072,
      "learning_rate": 0.00023864168618266976,
      "loss": 0.0893,
      "step": 1213
    },
    {
      "epoch": 2.8397660818713453,
      "grad_norm": 0.29061993956565857,
      "learning_rate": 0.0002385636221701795,
      "loss": 0.1963,
      "step": 1214
    },
    {
      "epoch": 2.8421052631578947,
      "grad_norm": 0.374860018491745,
      "learning_rate": 0.0002384855581576893,
      "loss": 0.1838,
      "step": 1215
    },
    {
      "epoch": 2.8444444444444446,
      "grad_norm": 0.33700254559516907,
      "learning_rate": 0.00023840749414519904,
      "loss": 0.1338,
      "step": 1216
    },
    {
      "epoch": 2.8467836257309944,
      "grad_norm": 0.29610517621040344,
      "learning_rate": 0.0002383294301327088,
      "loss": 0.1757,
      "step": 1217
    },
    {
      "epoch": 2.849122807017544,
      "grad_norm": 0.35018303990364075,
      "learning_rate": 0.00023825136612021857,
      "loss": 0.1874,
      "step": 1218
    },
    {
      "epoch": 2.8514619883040937,
      "grad_norm": 0.33124637603759766,
      "learning_rate": 0.00023817330210772832,
      "loss": 0.1556,
      "step": 1219
    },
    {
      "epoch": 2.853801169590643,
      "grad_norm": 0.42165789008140564,
      "learning_rate": 0.00023809523809523807,
      "loss": 0.1844,
      "step": 1220
    },
    {
      "epoch": 2.856140350877193,
      "grad_norm": 0.3111468255519867,
      "learning_rate": 0.00023801717408274785,
      "loss": 0.1623,
      "step": 1221
    },
    {
      "epoch": 2.858479532163743,
      "grad_norm": 0.38493162393569946,
      "learning_rate": 0.0002379391100702576,
      "loss": 0.1769,
      "step": 1222
    },
    {
      "epoch": 2.8608187134502923,
      "grad_norm": 0.29797324538230896,
      "learning_rate": 0.00023786104605776735,
      "loss": 0.127,
      "step": 1223
    },
    {
      "epoch": 2.863157894736842,
      "grad_norm": 0.48834332823753357,
      "learning_rate": 0.00023778298204527712,
      "loss": 0.2051,
      "step": 1224
    },
    {
      "epoch": 2.8654970760233915,
      "grad_norm": 0.3923748731613159,
      "learning_rate": 0.00023770491803278687,
      "loss": 0.2193,
      "step": 1225
    },
    {
      "epoch": 2.8678362573099414,
      "grad_norm": 0.40618211030960083,
      "learning_rate": 0.0002376268540202966,
      "loss": 0.1979,
      "step": 1226
    },
    {
      "epoch": 2.8701754385964913,
      "grad_norm": 0.2253505140542984,
      "learning_rate": 0.0002375487900078064,
      "loss": 0.159,
      "step": 1227
    },
    {
      "epoch": 2.8725146198830407,
      "grad_norm": 0.2928408086299896,
      "learning_rate": 0.00023747072599531613,
      "loss": 0.1302,
      "step": 1228
    },
    {
      "epoch": 2.8748538011695906,
      "grad_norm": 0.37249428033828735,
      "learning_rate": 0.00023739266198282588,
      "loss": 0.2101,
      "step": 1229
    },
    {
      "epoch": 2.8771929824561404,
      "grad_norm": 0.33474427461624146,
      "learning_rate": 0.00023731459797033565,
      "loss": 0.235,
      "step": 1230
    },
    {
      "epoch": 2.87953216374269,
      "grad_norm": 0.27318766713142395,
      "learning_rate": 0.0002372365339578454,
      "loss": 0.1882,
      "step": 1231
    },
    {
      "epoch": 2.8818713450292397,
      "grad_norm": 0.3976548910140991,
      "learning_rate": 0.00023715846994535515,
      "loss": 0.1823,
      "step": 1232
    },
    {
      "epoch": 2.8842105263157896,
      "grad_norm": 0.32102635502815247,
      "learning_rate": 0.00023708040593286493,
      "loss": 0.1779,
      "step": 1233
    },
    {
      "epoch": 2.886549707602339,
      "grad_norm": 0.3885665833950043,
      "learning_rate": 0.00023700234192037468,
      "loss": 0.2149,
      "step": 1234
    },
    {
      "epoch": 2.888888888888889,
      "grad_norm": 0.3036309778690338,
      "learning_rate": 0.00023692427790788443,
      "loss": 0.1551,
      "step": 1235
    },
    {
      "epoch": 2.8912280701754387,
      "grad_norm": 0.3316785991191864,
      "learning_rate": 0.0002368462138953942,
      "loss": 0.1865,
      "step": 1236
    },
    {
      "epoch": 2.893567251461988,
      "grad_norm": 0.39201897382736206,
      "learning_rate": 0.00023676814988290396,
      "loss": 0.2231,
      "step": 1237
    },
    {
      "epoch": 2.895906432748538,
      "grad_norm": 0.25590118765830994,
      "learning_rate": 0.0002366900858704137,
      "loss": 0.1265,
      "step": 1238
    },
    {
      "epoch": 2.898245614035088,
      "grad_norm": 0.3703541159629822,
      "learning_rate": 0.0002366120218579235,
      "loss": 0.1649,
      "step": 1239
    },
    {
      "epoch": 2.9005847953216373,
      "grad_norm": 0.34672364592552185,
      "learning_rate": 0.00023653395784543324,
      "loss": 0.2139,
      "step": 1240
    },
    {
      "epoch": 2.902923976608187,
      "grad_norm": 0.33485424518585205,
      "learning_rate": 0.000236455893832943,
      "loss": 0.2111,
      "step": 1241
    },
    {
      "epoch": 2.905263157894737,
      "grad_norm": 0.27526456117630005,
      "learning_rate": 0.00023637782982045277,
      "loss": 0.1586,
      "step": 1242
    },
    {
      "epoch": 2.9076023391812864,
      "grad_norm": 0.23747853934764862,
      "learning_rate": 0.00023629976580796252,
      "loss": 0.144,
      "step": 1243
    },
    {
      "epoch": 2.9099415204678363,
      "grad_norm": 0.299761027097702,
      "learning_rate": 0.00023622170179547224,
      "loss": 0.1818,
      "step": 1244
    },
    {
      "epoch": 2.912280701754386,
      "grad_norm": 0.36899200081825256,
      "learning_rate": 0.00023614363778298202,
      "loss": 0.1739,
      "step": 1245
    },
    {
      "epoch": 2.9146198830409356,
      "grad_norm": 0.3956530690193176,
      "learning_rate": 0.00023606557377049177,
      "loss": 0.2068,
      "step": 1246
    },
    {
      "epoch": 2.9169590643274854,
      "grad_norm": 0.38693493604660034,
      "learning_rate": 0.00023598750975800152,
      "loss": 0.2063,
      "step": 1247
    },
    {
      "epoch": 2.9192982456140353,
      "grad_norm": 0.34423089027404785,
      "learning_rate": 0.0002359094457455113,
      "loss": 0.165,
      "step": 1248
    },
    {
      "epoch": 2.9216374269005847,
      "grad_norm": 0.2975827157497406,
      "learning_rate": 0.00023583138173302105,
      "loss": 0.1828,
      "step": 1249
    },
    {
      "epoch": 2.9239766081871346,
      "grad_norm": 0.3319690525531769,
      "learning_rate": 0.0002357533177205308,
      "loss": 0.1666,
      "step": 1250
    },
    {
      "epoch": 2.9263157894736844,
      "grad_norm": 0.35155433416366577,
      "learning_rate": 0.00023567525370804057,
      "loss": 0.1643,
      "step": 1251
    },
    {
      "epoch": 2.928654970760234,
      "grad_norm": 0.3384537398815155,
      "learning_rate": 0.00023559718969555033,
      "loss": 0.1556,
      "step": 1252
    },
    {
      "epoch": 2.9309941520467837,
      "grad_norm": 0.3779353201389313,
      "learning_rate": 0.00023551912568306008,
      "loss": 0.2364,
      "step": 1253
    },
    {
      "epoch": 2.9333333333333336,
      "grad_norm": 0.3859921097755432,
      "learning_rate": 0.00023544106167056985,
      "loss": 0.2272,
      "step": 1254
    },
    {
      "epoch": 2.935672514619883,
      "grad_norm": 0.4024488627910614,
      "learning_rate": 0.0002353629976580796,
      "loss": 0.2365,
      "step": 1255
    },
    {
      "epoch": 2.938011695906433,
      "grad_norm": 0.33455976843833923,
      "learning_rate": 0.00023528493364558935,
      "loss": 0.1965,
      "step": 1256
    },
    {
      "epoch": 2.9403508771929827,
      "grad_norm": 0.332183301448822,
      "learning_rate": 0.00023520686963309913,
      "loss": 0.1761,
      "step": 1257
    },
    {
      "epoch": 2.942690058479532,
      "grad_norm": 0.19923612475395203,
      "learning_rate": 0.00023512880562060888,
      "loss": 0.0981,
      "step": 1258
    },
    {
      "epoch": 2.945029239766082,
      "grad_norm": 0.22912873327732086,
      "learning_rate": 0.00023505074160811863,
      "loss": 0.137,
      "step": 1259
    },
    {
      "epoch": 2.9473684210526314,
      "grad_norm": 0.2663649320602417,
      "learning_rate": 0.0002349726775956284,
      "loss": 0.1426,
      "step": 1260
    },
    {
      "epoch": 2.9497076023391813,
      "grad_norm": 0.2842768132686615,
      "learning_rate": 0.00023489461358313816,
      "loss": 0.1859,
      "step": 1261
    },
    {
      "epoch": 2.952046783625731,
      "grad_norm": 0.2800484895706177,
      "learning_rate": 0.00023481654957064794,
      "loss": 0.1682,
      "step": 1262
    },
    {
      "epoch": 2.9543859649122806,
      "grad_norm": 0.2769303321838379,
      "learning_rate": 0.00023473848555815766,
      "loss": 0.1727,
      "step": 1263
    },
    {
      "epoch": 2.9567251461988304,
      "grad_norm": 0.3026522696018219,
      "learning_rate": 0.0002346604215456674,
      "loss": 0.1618,
      "step": 1264
    },
    {
      "epoch": 2.95906432748538,
      "grad_norm": 0.3584998548030853,
      "learning_rate": 0.0002345823575331772,
      "loss": 0.2572,
      "step": 1265
    },
    {
      "epoch": 2.9614035087719297,
      "grad_norm": 0.25952792167663574,
      "learning_rate": 0.00023450429352068694,
      "loss": 0.1496,
      "step": 1266
    },
    {
      "epoch": 2.9637426900584796,
      "grad_norm": 0.24891741573810577,
      "learning_rate": 0.0002344262295081967,
      "loss": 0.1268,
      "step": 1267
    },
    {
      "epoch": 2.966081871345029,
      "grad_norm": 0.25009363889694214,
      "learning_rate": 0.00023434816549570647,
      "loss": 0.1454,
      "step": 1268
    },
    {
      "epoch": 2.968421052631579,
      "grad_norm": 0.2760692536830902,
      "learning_rate": 0.00023427010148321622,
      "loss": 0.1659,
      "step": 1269
    },
    {
      "epoch": 2.9707602339181287,
      "grad_norm": 0.3576957583427429,
      "learning_rate": 0.00023419203747072597,
      "loss": 0.1935,
      "step": 1270
    },
    {
      "epoch": 2.973099415204678,
      "grad_norm": 0.30464282631874084,
      "learning_rate": 0.00023411397345823575,
      "loss": 0.13,
      "step": 1271
    },
    {
      "epoch": 2.975438596491228,
      "grad_norm": 0.3117501735687256,
      "learning_rate": 0.0002340359094457455,
      "loss": 0.1728,
      "step": 1272
    },
    {
      "epoch": 2.977777777777778,
      "grad_norm": 0.33824583888053894,
      "learning_rate": 0.00023395784543325525,
      "loss": 0.1799,
      "step": 1273
    },
    {
      "epoch": 2.9801169590643273,
      "grad_norm": 0.29063960909843445,
      "learning_rate": 0.00023387978142076502,
      "loss": 0.1649,
      "step": 1274
    },
    {
      "epoch": 2.982456140350877,
      "grad_norm": 0.27298039197921753,
      "learning_rate": 0.00023380171740827477,
      "loss": 0.1342,
      "step": 1275
    },
    {
      "epoch": 2.984795321637427,
      "grad_norm": 0.3243846893310547,
      "learning_rate": 0.00023372365339578452,
      "loss": 0.1771,
      "step": 1276
    },
    {
      "epoch": 2.9871345029239764,
      "grad_norm": 0.28715765476226807,
      "learning_rate": 0.0002336455893832943,
      "loss": 0.1675,
      "step": 1277
    },
    {
      "epoch": 2.9894736842105263,
      "grad_norm": 0.23637332022190094,
      "learning_rate": 0.00023356752537080405,
      "loss": 0.1538,
      "step": 1278
    },
    {
      "epoch": 2.991812865497076,
      "grad_norm": 0.2842686176300049,
      "learning_rate": 0.0002334894613583138,
      "loss": 0.1712,
      "step": 1279
    },
    {
      "epoch": 2.9941520467836256,
      "grad_norm": 0.3031802475452423,
      "learning_rate": 0.00023341139734582358,
      "loss": 0.1697,
      "step": 1280
    },
    {
      "epoch": 2.9964912280701754,
      "grad_norm": 0.31973469257354736,
      "learning_rate": 0.0002333333333333333,
      "loss": 0.1703,
      "step": 1281
    },
    {
      "epoch": 2.9988304093567253,
      "grad_norm": 0.3401114344596863,
      "learning_rate": 0.00023325526932084305,
      "loss": 0.1826,
      "step": 1282
    },
    {
      "epoch": 2.9988304093567253,
      "eval_loss": 0.2274738848209381,
      "eval_runtime": 125.3122,
      "eval_samples_per_second": 4.405,
      "eval_steps_per_second": 0.551,
      "step": 1282
    },
    {
      "epoch": 3.0011695906432747,
      "grad_norm": 0.3128340244293213,
      "learning_rate": 0.00023317720530835283,
      "loss": 0.1205,
      "step": 1283
    },
    {
      "epoch": 3.0035087719298246,
      "grad_norm": 0.3466627299785614,
      "learning_rate": 0.00023309914129586258,
      "loss": 0.1725,
      "step": 1284
    },
    {
      "epoch": 3.0058479532163744,
      "grad_norm": 0.2936493754386902,
      "learning_rate": 0.00023302107728337233,
      "loss": 0.1247,
      "step": 1285
    },
    {
      "epoch": 3.008187134502924,
      "grad_norm": 0.29733189940452576,
      "learning_rate": 0.0002329430132708821,
      "loss": 0.1733,
      "step": 1286
    },
    {
      "epoch": 3.0105263157894737,
      "grad_norm": 0.30696791410446167,
      "learning_rate": 0.00023286494925839186,
      "loss": 0.1717,
      "step": 1287
    },
    {
      "epoch": 3.0128654970760236,
      "grad_norm": 0.27115556597709656,
      "learning_rate": 0.0002327868852459016,
      "loss": 0.1363,
      "step": 1288
    },
    {
      "epoch": 3.015204678362573,
      "grad_norm": 0.25459063053131104,
      "learning_rate": 0.0002327088212334114,
      "loss": 0.1306,
      "step": 1289
    },
    {
      "epoch": 3.017543859649123,
      "grad_norm": 0.26876914501190186,
      "learning_rate": 0.00023263075722092114,
      "loss": 0.1354,
      "step": 1290
    },
    {
      "epoch": 3.0198830409356727,
      "grad_norm": 0.3325898051261902,
      "learning_rate": 0.0002325526932084309,
      "loss": 0.1678,
      "step": 1291
    },
    {
      "epoch": 3.022222222222222,
      "grad_norm": 0.26020458340644836,
      "learning_rate": 0.00023247462919594067,
      "loss": 0.1371,
      "step": 1292
    },
    {
      "epoch": 3.024561403508772,
      "grad_norm": 0.21862880885601044,
      "learning_rate": 0.00023239656518345042,
      "loss": 0.1391,
      "step": 1293
    },
    {
      "epoch": 3.0269005847953214,
      "grad_norm": 0.256069540977478,
      "learning_rate": 0.00023231850117096017,
      "loss": 0.1273,
      "step": 1294
    },
    {
      "epoch": 3.0292397660818713,
      "grad_norm": 0.2915889024734497,
      "learning_rate": 0.00023224043715846995,
      "loss": 0.1484,
      "step": 1295
    },
    {
      "epoch": 3.031578947368421,
      "grad_norm": 0.294159859418869,
      "learning_rate": 0.0002321623731459797,
      "loss": 0.1333,
      "step": 1296
    },
    {
      "epoch": 3.0339181286549706,
      "grad_norm": 0.28211551904678345,
      "learning_rate": 0.00023208430913348942,
      "loss": 0.1331,
      "step": 1297
    },
    {
      "epoch": 3.0362573099415204,
      "grad_norm": 0.2911798357963562,
      "learning_rate": 0.00023200624512099922,
      "loss": 0.1395,
      "step": 1298
    },
    {
      "epoch": 3.0385964912280703,
      "grad_norm": 0.3544800579547882,
      "learning_rate": 0.00023192818110850895,
      "loss": 0.1461,
      "step": 1299
    },
    {
      "epoch": 3.0409356725146197,
      "grad_norm": 0.2672157883644104,
      "learning_rate": 0.0002318501170960187,
      "loss": 0.1666,
      "step": 1300
    },
    {
      "epoch": 3.0432748538011696,
      "grad_norm": 0.3245377540588379,
      "learning_rate": 0.00023177205308352847,
      "loss": 0.19,
      "step": 1301
    },
    {
      "epoch": 3.0456140350877194,
      "grad_norm": 0.4511847198009491,
      "learning_rate": 0.00023169398907103823,
      "loss": 0.1693,
      "step": 1302
    },
    {
      "epoch": 3.047953216374269,
      "grad_norm": 0.2831798791885376,
      "learning_rate": 0.00023161592505854798,
      "loss": 0.1311,
      "step": 1303
    },
    {
      "epoch": 3.0502923976608187,
      "grad_norm": 0.3274931013584137,
      "learning_rate": 0.00023153786104605775,
      "loss": 0.1573,
      "step": 1304
    },
    {
      "epoch": 3.0526315789473686,
      "grad_norm": 0.38476189970970154,
      "learning_rate": 0.0002314597970335675,
      "loss": 0.177,
      "step": 1305
    },
    {
      "epoch": 3.054970760233918,
      "grad_norm": 0.36408212780952454,
      "learning_rate": 0.00023138173302107725,
      "loss": 0.1502,
      "step": 1306
    },
    {
      "epoch": 3.057309941520468,
      "grad_norm": 0.34429872035980225,
      "learning_rate": 0.00023130366900858703,
      "loss": 0.134,
      "step": 1307
    },
    {
      "epoch": 3.0596491228070177,
      "grad_norm": 0.33821070194244385,
      "learning_rate": 0.00023122560499609678,
      "loss": 0.1501,
      "step": 1308
    },
    {
      "epoch": 3.061988304093567,
      "grad_norm": 0.38072308897972107,
      "learning_rate": 0.00023114754098360653,
      "loss": 0.1405,
      "step": 1309
    },
    {
      "epoch": 3.064327485380117,
      "grad_norm": 0.4436589181423187,
      "learning_rate": 0.0002310694769711163,
      "loss": 0.2146,
      "step": 1310
    },
    {
      "epoch": 3.066666666666667,
      "grad_norm": 0.33174315094947815,
      "learning_rate": 0.00023099141295862606,
      "loss": 0.1879,
      "step": 1311
    },
    {
      "epoch": 3.0690058479532163,
      "grad_norm": 0.3791716396808624,
      "learning_rate": 0.0002309133489461358,
      "loss": 0.1954,
      "step": 1312
    },
    {
      "epoch": 3.071345029239766,
      "grad_norm": 0.33086052536964417,
      "learning_rate": 0.0002308352849336456,
      "loss": 0.1434,
      "step": 1313
    },
    {
      "epoch": 3.0736842105263156,
      "grad_norm": 0.2529614567756653,
      "learning_rate": 0.00023075722092115534,
      "loss": 0.1533,
      "step": 1314
    },
    {
      "epoch": 3.0760233918128654,
      "grad_norm": 0.3998104929924011,
      "learning_rate": 0.00023067915690866506,
      "loss": 0.1728,
      "step": 1315
    },
    {
      "epoch": 3.0783625730994153,
      "grad_norm": 0.29550793766975403,
      "learning_rate": 0.00023060109289617487,
      "loss": 0.1453,
      "step": 1316
    },
    {
      "epoch": 3.0807017543859647,
      "grad_norm": 0.3173713684082031,
      "learning_rate": 0.0002305230288836846,
      "loss": 0.1561,
      "step": 1317
    },
    {
      "epoch": 3.0830409356725146,
      "grad_norm": 0.32212889194488525,
      "learning_rate": 0.00023044496487119434,
      "loss": 0.1657,
      "step": 1318
    },
    {
      "epoch": 3.0853801169590644,
      "grad_norm": 0.33790668845176697,
      "learning_rate": 0.00023036690085870412,
      "loss": 0.1637,
      "step": 1319
    },
    {
      "epoch": 3.087719298245614,
      "grad_norm": 0.2942577004432678,
      "learning_rate": 0.00023028883684621387,
      "loss": 0.1334,
      "step": 1320
    },
    {
      "epoch": 3.0900584795321637,
      "grad_norm": 0.3045758306980133,
      "learning_rate": 0.00023021077283372362,
      "loss": 0.1496,
      "step": 1321
    },
    {
      "epoch": 3.0923976608187136,
      "grad_norm": 0.2909312844276428,
      "learning_rate": 0.0002301327088212334,
      "loss": 0.1515,
      "step": 1322
    },
    {
      "epoch": 3.094736842105263,
      "grad_norm": 0.33825427293777466,
      "learning_rate": 0.00023005464480874315,
      "loss": 0.1972,
      "step": 1323
    },
    {
      "epoch": 3.097076023391813,
      "grad_norm": 0.4209724962711334,
      "learning_rate": 0.0002299765807962529,
      "loss": 0.2043,
      "step": 1324
    },
    {
      "epoch": 3.0994152046783627,
      "grad_norm": 0.34883901476860046,
      "learning_rate": 0.00022989851678376267,
      "loss": 0.1551,
      "step": 1325
    },
    {
      "epoch": 3.101754385964912,
      "grad_norm": 0.33457258343696594,
      "learning_rate": 0.00022982045277127242,
      "loss": 0.1495,
      "step": 1326
    },
    {
      "epoch": 3.104093567251462,
      "grad_norm": 0.5076102614402771,
      "learning_rate": 0.00022974238875878218,
      "loss": 0.2157,
      "step": 1327
    },
    {
      "epoch": 3.106432748538012,
      "grad_norm": 0.33938708901405334,
      "learning_rate": 0.00022966432474629195,
      "loss": 0.142,
      "step": 1328
    },
    {
      "epoch": 3.1087719298245613,
      "grad_norm": 0.3277318775653839,
      "learning_rate": 0.0002295862607338017,
      "loss": 0.1438,
      "step": 1329
    },
    {
      "epoch": 3.111111111111111,
      "grad_norm": 0.4116004705429077,
      "learning_rate": 0.00022950819672131145,
      "loss": 0.154,
      "step": 1330
    },
    {
      "epoch": 3.113450292397661,
      "grad_norm": 0.3777068257331848,
      "learning_rate": 0.00022943013270882123,
      "loss": 0.1247,
      "step": 1331
    },
    {
      "epoch": 3.1157894736842104,
      "grad_norm": 0.28198355436325073,
      "learning_rate": 0.00022935206869633098,
      "loss": 0.0967,
      "step": 1332
    },
    {
      "epoch": 3.1181286549707603,
      "grad_norm": 0.2447945773601532,
      "learning_rate": 0.0002292740046838407,
      "loss": 0.0904,
      "step": 1333
    },
    {
      "epoch": 3.12046783625731,
      "grad_norm": 0.39327698945999146,
      "learning_rate": 0.0002291959406713505,
      "loss": 0.1701,
      "step": 1334
    },
    {
      "epoch": 3.1228070175438596,
      "grad_norm": 0.3203401565551758,
      "learning_rate": 0.00022911787665886023,
      "loss": 0.1445,
      "step": 1335
    },
    {
      "epoch": 3.1251461988304094,
      "grad_norm": 0.41498374938964844,
      "learning_rate": 0.00022903981264636998,
      "loss": 0.185,
      "step": 1336
    },
    {
      "epoch": 3.127485380116959,
      "grad_norm": 0.4223026633262634,
      "learning_rate": 0.00022896174863387976,
      "loss": 0.1573,
      "step": 1337
    },
    {
      "epoch": 3.1298245614035087,
      "grad_norm": 0.36815106868743896,
      "learning_rate": 0.0002288836846213895,
      "loss": 0.1338,
      "step": 1338
    },
    {
      "epoch": 3.1321637426900586,
      "grad_norm": 0.27713727951049805,
      "learning_rate": 0.00022880562060889926,
      "loss": 0.1173,
      "step": 1339
    },
    {
      "epoch": 3.134502923976608,
      "grad_norm": 0.4121900796890259,
      "learning_rate": 0.00022872755659640904,
      "loss": 0.1912,
      "step": 1340
    },
    {
      "epoch": 3.136842105263158,
      "grad_norm": 0.25155743956565857,
      "learning_rate": 0.0002286494925839188,
      "loss": 0.1046,
      "step": 1341
    },
    {
      "epoch": 3.1391812865497077,
      "grad_norm": 0.3084454834461212,
      "learning_rate": 0.00022857142857142854,
      "loss": 0.1578,
      "step": 1342
    },
    {
      "epoch": 3.141520467836257,
      "grad_norm": 0.33336469531059265,
      "learning_rate": 0.00022849336455893832,
      "loss": 0.1842,
      "step": 1343
    },
    {
      "epoch": 3.143859649122807,
      "grad_norm": 0.38580310344696045,
      "learning_rate": 0.00022841530054644807,
      "loss": 0.1636,
      "step": 1344
    },
    {
      "epoch": 3.146198830409357,
      "grad_norm": 0.3810915946960449,
      "learning_rate": 0.00022833723653395782,
      "loss": 0.171,
      "step": 1345
    },
    {
      "epoch": 3.1485380116959063,
      "grad_norm": 0.29788336157798767,
      "learning_rate": 0.0002282591725214676,
      "loss": 0.1397,
      "step": 1346
    },
    {
      "epoch": 3.150877192982456,
      "grad_norm": 0.33006346225738525,
      "learning_rate": 0.00022818110850897735,
      "loss": 0.155,
      "step": 1347
    },
    {
      "epoch": 3.153216374269006,
      "grad_norm": 0.3023974299430847,
      "learning_rate": 0.0002281030444964871,
      "loss": 0.1315,
      "step": 1348
    },
    {
      "epoch": 3.1555555555555554,
      "grad_norm": 0.26117292046546936,
      "learning_rate": 0.00022802498048399687,
      "loss": 0.1358,
      "step": 1349
    },
    {
      "epoch": 3.1578947368421053,
      "grad_norm": 0.3279888331890106,
      "learning_rate": 0.00022794691647150662,
      "loss": 0.1917,
      "step": 1350
    },
    {
      "epoch": 3.160233918128655,
      "grad_norm": 0.30495646595954895,
      "learning_rate": 0.00022786885245901635,
      "loss": 0.1679,
      "step": 1351
    },
    {
      "epoch": 3.1625730994152046,
      "grad_norm": 0.315141886472702,
      "learning_rate": 0.00022779078844652615,
      "loss": 0.1459,
      "step": 1352
    },
    {
      "epoch": 3.1649122807017545,
      "grad_norm": 0.35544875264167786,
      "learning_rate": 0.00022771272443403588,
      "loss": 0.1784,
      "step": 1353
    },
    {
      "epoch": 3.167251461988304,
      "grad_norm": 0.49660494923591614,
      "learning_rate": 0.00022763466042154563,
      "loss": 0.2213,
      "step": 1354
    },
    {
      "epoch": 3.1695906432748537,
      "grad_norm": 0.32077720761299133,
      "learning_rate": 0.0002275565964090554,
      "loss": 0.151,
      "step": 1355
    },
    {
      "epoch": 3.1719298245614036,
      "grad_norm": 0.27150917053222656,
      "learning_rate": 0.00022747853239656515,
      "loss": 0.1011,
      "step": 1356
    },
    {
      "epoch": 3.174269005847953,
      "grad_norm": 0.2839632034301758,
      "learning_rate": 0.0002274004683840749,
      "loss": 0.117,
      "step": 1357
    },
    {
      "epoch": 3.176608187134503,
      "grad_norm": 0.38503116369247437,
      "learning_rate": 0.00022732240437158468,
      "loss": 0.1446,
      "step": 1358
    },
    {
      "epoch": 3.1789473684210527,
      "grad_norm": 0.26213690638542175,
      "learning_rate": 0.00022724434035909443,
      "loss": 0.1314,
      "step": 1359
    },
    {
      "epoch": 3.181286549707602,
      "grad_norm": 0.28550106287002563,
      "learning_rate": 0.00022716627634660418,
      "loss": 0.1062,
      "step": 1360
    },
    {
      "epoch": 3.183625730994152,
      "grad_norm": 0.3510790467262268,
      "learning_rate": 0.00022708821233411396,
      "loss": 0.1731,
      "step": 1361
    },
    {
      "epoch": 3.185964912280702,
      "grad_norm": 0.2927636504173279,
      "learning_rate": 0.0002270101483216237,
      "loss": 0.1583,
      "step": 1362
    },
    {
      "epoch": 3.1883040935672513,
      "grad_norm": 0.33886370062828064,
      "learning_rate": 0.00022693208430913346,
      "loss": 0.1439,
      "step": 1363
    },
    {
      "epoch": 3.190643274853801,
      "grad_norm": 0.32360079884529114,
      "learning_rate": 0.00022685402029664324,
      "loss": 0.1132,
      "step": 1364
    },
    {
      "epoch": 3.192982456140351,
      "grad_norm": 0.3463292419910431,
      "learning_rate": 0.000226775956284153,
      "loss": 0.1597,
      "step": 1365
    },
    {
      "epoch": 3.1953216374269005,
      "grad_norm": 0.40107202529907227,
      "learning_rate": 0.00022669789227166274,
      "loss": 0.184,
      "step": 1366
    },
    {
      "epoch": 3.1976608187134503,
      "grad_norm": 0.31258484721183777,
      "learning_rate": 0.00022661982825917252,
      "loss": 0.1654,
      "step": 1367
    },
    {
      "epoch": 3.2,
      "grad_norm": 0.4064609706401825,
      "learning_rate": 0.00022654176424668227,
      "loss": 0.1583,
      "step": 1368
    },
    {
      "epoch": 3.2023391812865496,
      "grad_norm": 0.23746062815189362,
      "learning_rate": 0.000226463700234192,
      "loss": 0.1096,
      "step": 1369
    },
    {
      "epoch": 3.2046783625730995,
      "grad_norm": 0.384366899728775,
      "learning_rate": 0.0002263856362217018,
      "loss": 0.1566,
      "step": 1370
    },
    {
      "epoch": 3.2070175438596493,
      "grad_norm": 0.4477408528327942,
      "learning_rate": 0.00022630757220921152,
      "loss": 0.1723,
      "step": 1371
    },
    {
      "epoch": 3.2093567251461987,
      "grad_norm": 0.3106464743614197,
      "learning_rate": 0.00022622950819672127,
      "loss": 0.1416,
      "step": 1372
    },
    {
      "epoch": 3.2116959064327486,
      "grad_norm": 0.31090685725212097,
      "learning_rate": 0.00022615144418423105,
      "loss": 0.1361,
      "step": 1373
    },
    {
      "epoch": 3.2140350877192985,
      "grad_norm": 0.29230549931526184,
      "learning_rate": 0.0002260733801717408,
      "loss": 0.1422,
      "step": 1374
    },
    {
      "epoch": 3.216374269005848,
      "grad_norm": 0.25768834352493286,
      "learning_rate": 0.00022599531615925055,
      "loss": 0.1117,
      "step": 1375
    },
    {
      "epoch": 3.2187134502923977,
      "grad_norm": 0.390760600566864,
      "learning_rate": 0.00022591725214676032,
      "loss": 0.1241,
      "step": 1376
    },
    {
      "epoch": 3.221052631578947,
      "grad_norm": 0.25444936752319336,
      "learning_rate": 0.00022583918813427008,
      "loss": 0.1282,
      "step": 1377
    },
    {
      "epoch": 3.223391812865497,
      "grad_norm": 0.31104227900505066,
      "learning_rate": 0.00022576112412177983,
      "loss": 0.145,
      "step": 1378
    },
    {
      "epoch": 3.225730994152047,
      "grad_norm": 0.3104715645313263,
      "learning_rate": 0.0002256830601092896,
      "loss": 0.1389,
      "step": 1379
    },
    {
      "epoch": 3.2280701754385963,
      "grad_norm": 0.26499709486961365,
      "learning_rate": 0.00022560499609679935,
      "loss": 0.1411,
      "step": 1380
    },
    {
      "epoch": 3.230409356725146,
      "grad_norm": 0.2528896629810333,
      "learning_rate": 0.0002255269320843091,
      "loss": 0.123,
      "step": 1381
    },
    {
      "epoch": 3.232748538011696,
      "grad_norm": 0.2722682058811188,
      "learning_rate": 0.00022544886807181888,
      "loss": 0.1074,
      "step": 1382
    },
    {
      "epoch": 3.2350877192982455,
      "grad_norm": 0.4353894293308258,
      "learning_rate": 0.00022537080405932863,
      "loss": 0.1262,
      "step": 1383
    },
    {
      "epoch": 3.2374269005847953,
      "grad_norm": 0.3401600122451782,
      "learning_rate": 0.00022529274004683838,
      "loss": 0.1271,
      "step": 1384
    },
    {
      "epoch": 3.239766081871345,
      "grad_norm": 0.5337190628051758,
      "learning_rate": 0.00022521467603434816,
      "loss": 0.1663,
      "step": 1385
    },
    {
      "epoch": 3.2421052631578946,
      "grad_norm": 0.24147221446037292,
      "learning_rate": 0.0002251366120218579,
      "loss": 0.108,
      "step": 1386
    },
    {
      "epoch": 3.2444444444444445,
      "grad_norm": 0.43228891491889954,
      "learning_rate": 0.00022505854800936763,
      "loss": 0.1882,
      "step": 1387
    },
    {
      "epoch": 3.2467836257309943,
      "grad_norm": 0.34210798144340515,
      "learning_rate": 0.00022498048399687744,
      "loss": 0.1497,
      "step": 1388
    },
    {
      "epoch": 3.2491228070175437,
      "grad_norm": 0.2976556718349457,
      "learning_rate": 0.00022490241998438716,
      "loss": 0.1456,
      "step": 1389
    },
    {
      "epoch": 3.2514619883040936,
      "grad_norm": 0.2275930643081665,
      "learning_rate": 0.00022482435597189694,
      "loss": 0.13,
      "step": 1390
    },
    {
      "epoch": 3.253801169590643,
      "grad_norm": 0.3731426000595093,
      "learning_rate": 0.0002247462919594067,
      "loss": 0.1558,
      "step": 1391
    },
    {
      "epoch": 3.256140350877193,
      "grad_norm": 0.3665125370025635,
      "learning_rate": 0.00022466822794691644,
      "loss": 0.1758,
      "step": 1392
    },
    {
      "epoch": 3.2584795321637428,
      "grad_norm": 0.28296178579330444,
      "learning_rate": 0.00022459016393442622,
      "loss": 0.1294,
      "step": 1393
    },
    {
      "epoch": 3.260818713450292,
      "grad_norm": 0.3140498697757721,
      "learning_rate": 0.00022451209992193597,
      "loss": 0.1534,
      "step": 1394
    },
    {
      "epoch": 3.263157894736842,
      "grad_norm": 0.3372822701931,
      "learning_rate": 0.00022443403590944572,
      "loss": 0.1322,
      "step": 1395
    },
    {
      "epoch": 3.265497076023392,
      "grad_norm": 0.3428294360637665,
      "learning_rate": 0.0002243559718969555,
      "loss": 0.1352,
      "step": 1396
    },
    {
      "epoch": 3.2678362573099413,
      "grad_norm": 0.27770936489105225,
      "learning_rate": 0.00022427790788446525,
      "loss": 0.1433,
      "step": 1397
    },
    {
      "epoch": 3.270175438596491,
      "grad_norm": 0.3259495496749878,
      "learning_rate": 0.000224199843871975,
      "loss": 0.1505,
      "step": 1398
    },
    {
      "epoch": 3.272514619883041,
      "grad_norm": 0.37363865971565247,
      "learning_rate": 0.00022412177985948477,
      "loss": 0.1512,
      "step": 1399
    },
    {
      "epoch": 3.2748538011695905,
      "grad_norm": 0.363741010427475,
      "learning_rate": 0.00022404371584699452,
      "loss": 0.1778,
      "step": 1400
    },
    {
      "epoch": 3.2771929824561403,
      "grad_norm": 0.35508060455322266,
      "learning_rate": 0.00022396565183450427,
      "loss": 0.1719,
      "step": 1401
    },
    {
      "epoch": 3.27953216374269,
      "grad_norm": 0.25540122389793396,
      "learning_rate": 0.00022388758782201405,
      "loss": 0.1314,
      "step": 1402
    },
    {
      "epoch": 3.2818713450292396,
      "grad_norm": 0.24025402963161469,
      "learning_rate": 0.0002238095238095238,
      "loss": 0.1304,
      "step": 1403
    },
    {
      "epoch": 3.2842105263157895,
      "grad_norm": 0.3300989866256714,
      "learning_rate": 0.00022373145979703355,
      "loss": 0.1705,
      "step": 1404
    },
    {
      "epoch": 3.2865497076023393,
      "grad_norm": 0.2665095925331116,
      "learning_rate": 0.00022365339578454333,
      "loss": 0.1433,
      "step": 1405
    },
    {
      "epoch": 3.2888888888888888,
      "grad_norm": 0.32426097989082336,
      "learning_rate": 0.00022357533177205308,
      "loss": 0.1411,
      "step": 1406
    },
    {
      "epoch": 3.2912280701754386,
      "grad_norm": 0.3268362283706665,
      "learning_rate": 0.0002234972677595628,
      "loss": 0.1365,
      "step": 1407
    },
    {
      "epoch": 3.2935672514619885,
      "grad_norm": 0.35920238494873047,
      "learning_rate": 0.00022341920374707258,
      "loss": 0.1889,
      "step": 1408
    },
    {
      "epoch": 3.295906432748538,
      "grad_norm": 0.3222962021827698,
      "learning_rate": 0.00022334113973458233,
      "loss": 0.1328,
      "step": 1409
    },
    {
      "epoch": 3.2982456140350878,
      "grad_norm": 0.45512136816978455,
      "learning_rate": 0.00022326307572209208,
      "loss": 0.1407,
      "step": 1410
    },
    {
      "epoch": 3.3005847953216376,
      "grad_norm": 0.34356042742729187,
      "learning_rate": 0.00022318501170960186,
      "loss": 0.1507,
      "step": 1411
    },
    {
      "epoch": 3.302923976608187,
      "grad_norm": 0.30993956327438354,
      "learning_rate": 0.0002231069476971116,
      "loss": 0.1674,
      "step": 1412
    },
    {
      "epoch": 3.305263157894737,
      "grad_norm": 0.391564279794693,
      "learning_rate": 0.00022302888368462136,
      "loss": 0.1246,
      "step": 1413
    },
    {
      "epoch": 3.3076023391812868,
      "grad_norm": 0.35122600197792053,
      "learning_rate": 0.00022295081967213114,
      "loss": 0.1556,
      "step": 1414
    },
    {
      "epoch": 3.309941520467836,
      "grad_norm": 0.3433949649333954,
      "learning_rate": 0.0002228727556596409,
      "loss": 0.1334,
      "step": 1415
    },
    {
      "epoch": 3.312280701754386,
      "grad_norm": 0.3939467668533325,
      "learning_rate": 0.00022279469164715064,
      "loss": 0.2118,
      "step": 1416
    },
    {
      "epoch": 3.314619883040936,
      "grad_norm": 0.309080570936203,
      "learning_rate": 0.00022271662763466042,
      "loss": 0.1042,
      "step": 1417
    },
    {
      "epoch": 3.3169590643274853,
      "grad_norm": 0.31879690289497375,
      "learning_rate": 0.00022263856362217017,
      "loss": 0.1684,
      "step": 1418
    },
    {
      "epoch": 3.319298245614035,
      "grad_norm": 0.3212248682975769,
      "learning_rate": 0.00022256049960967992,
      "loss": 0.1161,
      "step": 1419
    },
    {
      "epoch": 3.3216374269005846,
      "grad_norm": 0.2798587381839752,
      "learning_rate": 0.0002224824355971897,
      "loss": 0.1711,
      "step": 1420
    },
    {
      "epoch": 3.3239766081871345,
      "grad_norm": 0.29727938771247864,
      "learning_rate": 0.00022240437158469945,
      "loss": 0.1742,
      "step": 1421
    },
    {
      "epoch": 3.3263157894736843,
      "grad_norm": 0.29589688777923584,
      "learning_rate": 0.0002223263075722092,
      "loss": 0.1467,
      "step": 1422
    },
    {
      "epoch": 3.3286549707602338,
      "grad_norm": 0.5194247961044312,
      "learning_rate": 0.00022224824355971897,
      "loss": 0.2448,
      "step": 1423
    },
    {
      "epoch": 3.3309941520467836,
      "grad_norm": 0.36842358112335205,
      "learning_rate": 0.0002221701795472287,
      "loss": 0.1723,
      "step": 1424
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 0.34266528487205505,
      "learning_rate": 0.00022209211553473845,
      "loss": 0.1918,
      "step": 1425
    },
    {
      "epoch": 3.335672514619883,
      "grad_norm": 0.31988224387168884,
      "learning_rate": 0.00022201405152224822,
      "loss": 0.0917,
      "step": 1426
    },
    {
      "epoch": 3.3380116959064328,
      "grad_norm": 0.3027092218399048,
      "learning_rate": 0.00022193598750975798,
      "loss": 0.1482,
      "step": 1427
    },
    {
      "epoch": 3.3403508771929826,
      "grad_norm": 0.22544443607330322,
      "learning_rate": 0.00022185792349726773,
      "loss": 0.0901,
      "step": 1428
    },
    {
      "epoch": 3.342690058479532,
      "grad_norm": 0.2812950611114502,
      "learning_rate": 0.0002217798594847775,
      "loss": 0.1188,
      "step": 1429
    },
    {
      "epoch": 3.345029239766082,
      "grad_norm": 0.2390459179878235,
      "learning_rate": 0.00022170179547228725,
      "loss": 0.1264,
      "step": 1430
    },
    {
      "epoch": 3.3473684210526318,
      "grad_norm": 0.3743996024131775,
      "learning_rate": 0.000221623731459797,
      "loss": 0.1502,
      "step": 1431
    },
    {
      "epoch": 3.349707602339181,
      "grad_norm": 0.29078415036201477,
      "learning_rate": 0.00022154566744730678,
      "loss": 0.1152,
      "step": 1432
    },
    {
      "epoch": 3.352046783625731,
      "grad_norm": 0.3355250954627991,
      "learning_rate": 0.00022146760343481653,
      "loss": 0.1704,
      "step": 1433
    },
    {
      "epoch": 3.3543859649122805,
      "grad_norm": 0.37078261375427246,
      "learning_rate": 0.00022138953942232628,
      "loss": 0.1384,
      "step": 1434
    },
    {
      "epoch": 3.3567251461988303,
      "grad_norm": 0.3645659387111664,
      "learning_rate": 0.00022131147540983606,
      "loss": 0.1541,
      "step": 1435
    },
    {
      "epoch": 3.35906432748538,
      "grad_norm": 0.3831096589565277,
      "learning_rate": 0.0002212334113973458,
      "loss": 0.1496,
      "step": 1436
    },
    {
      "epoch": 3.3614035087719296,
      "grad_norm": 0.3534286320209503,
      "learning_rate": 0.00022115534738485556,
      "loss": 0.1932,
      "step": 1437
    },
    {
      "epoch": 3.3637426900584795,
      "grad_norm": 0.298666775226593,
      "learning_rate": 0.00022107728337236534,
      "loss": 0.1395,
      "step": 1438
    },
    {
      "epoch": 3.3660818713450293,
      "grad_norm": 0.29874610900878906,
      "learning_rate": 0.0002209992193598751,
      "loss": 0.1175,
      "step": 1439
    },
    {
      "epoch": 3.3684210526315788,
      "grad_norm": 0.5143707990646362,
      "learning_rate": 0.00022092115534738484,
      "loss": 0.1391,
      "step": 1440
    },
    {
      "epoch": 3.3707602339181286,
      "grad_norm": 0.3862958252429962,
      "learning_rate": 0.00022084309133489462,
      "loss": 0.1874,
      "step": 1441
    },
    {
      "epoch": 3.3730994152046785,
      "grad_norm": 0.23562416434288025,
      "learning_rate": 0.00022076502732240434,
      "loss": 0.1127,
      "step": 1442
    },
    {
      "epoch": 3.375438596491228,
      "grad_norm": 0.2782258093357086,
      "learning_rate": 0.0002206869633099141,
      "loss": 0.1674,
      "step": 1443
    },
    {
      "epoch": 3.3777777777777778,
      "grad_norm": 0.34119072556495667,
      "learning_rate": 0.00022060889929742387,
      "loss": 0.168,
      "step": 1444
    },
    {
      "epoch": 3.3801169590643276,
      "grad_norm": 0.41891366243362427,
      "learning_rate": 0.00022053083528493362,
      "loss": 0.2099,
      "step": 1445
    },
    {
      "epoch": 3.382456140350877,
      "grad_norm": 0.39219093322753906,
      "learning_rate": 0.00022045277127244337,
      "loss": 0.1719,
      "step": 1446
    },
    {
      "epoch": 3.384795321637427,
      "grad_norm": 0.3120711147785187,
      "learning_rate": 0.00022037470725995315,
      "loss": 0.1486,
      "step": 1447
    },
    {
      "epoch": 3.3871345029239768,
      "grad_norm": 0.47782382369041443,
      "learning_rate": 0.0002202966432474629,
      "loss": 0.1605,
      "step": 1448
    },
    {
      "epoch": 3.389473684210526,
      "grad_norm": 0.41106730699539185,
      "learning_rate": 0.00022021857923497265,
      "loss": 0.1969,
      "step": 1449
    },
    {
      "epoch": 3.391812865497076,
      "grad_norm": 0.4128488004207611,
      "learning_rate": 0.00022014051522248242,
      "loss": 0.1736,
      "step": 1450
    },
    {
      "epoch": 3.394152046783626,
      "grad_norm": 0.3340233266353607,
      "learning_rate": 0.00022006245120999217,
      "loss": 0.167,
      "step": 1451
    },
    {
      "epoch": 3.3964912280701753,
      "grad_norm": 0.29877129197120667,
      "learning_rate": 0.00021998438719750193,
      "loss": 0.1257,
      "step": 1452
    },
    {
      "epoch": 3.398830409356725,
      "grad_norm": 0.31450486183166504,
      "learning_rate": 0.0002199063231850117,
      "loss": 0.1436,
      "step": 1453
    },
    {
      "epoch": 3.401169590643275,
      "grad_norm": 0.3213183879852295,
      "learning_rate": 0.00021982825917252145,
      "loss": 0.1738,
      "step": 1454
    },
    {
      "epoch": 3.4035087719298245,
      "grad_norm": 0.39121368527412415,
      "learning_rate": 0.0002197501951600312,
      "loss": 0.1169,
      "step": 1455
    },
    {
      "epoch": 3.4058479532163743,
      "grad_norm": 0.3506922423839569,
      "learning_rate": 0.00021967213114754098,
      "loss": 0.137,
      "step": 1456
    },
    {
      "epoch": 3.408187134502924,
      "grad_norm": 0.5109611749649048,
      "learning_rate": 0.00021959406713505073,
      "loss": 0.2384,
      "step": 1457
    },
    {
      "epoch": 3.4105263157894736,
      "grad_norm": 0.2795100510120392,
      "learning_rate": 0.00021951600312256048,
      "loss": 0.12,
      "step": 1458
    },
    {
      "epoch": 3.4128654970760235,
      "grad_norm": 0.32243669033050537,
      "learning_rate": 0.00021943793911007026,
      "loss": 0.1421,
      "step": 1459
    },
    {
      "epoch": 3.415204678362573,
      "grad_norm": 0.3535406291484833,
      "learning_rate": 0.00021935987509757998,
      "loss": 0.1416,
      "step": 1460
    },
    {
      "epoch": 3.4175438596491228,
      "grad_norm": 0.3948327898979187,
      "learning_rate": 0.00021928181108508973,
      "loss": 0.1371,
      "step": 1461
    },
    {
      "epoch": 3.4198830409356726,
      "grad_norm": 0.43044382333755493,
      "learning_rate": 0.0002192037470725995,
      "loss": 0.1405,
      "step": 1462
    },
    {
      "epoch": 3.422222222222222,
      "grad_norm": 0.35493722558021545,
      "learning_rate": 0.00021912568306010926,
      "loss": 0.1614,
      "step": 1463
    },
    {
      "epoch": 3.424561403508772,
      "grad_norm": 0.3303990066051483,
      "learning_rate": 0.000219047619047619,
      "loss": 0.1098,
      "step": 1464
    },
    {
      "epoch": 3.426900584795322,
      "grad_norm": 0.4360145330429077,
      "learning_rate": 0.0002189695550351288,
      "loss": 0.1491,
      "step": 1465
    },
    {
      "epoch": 3.429239766081871,
      "grad_norm": 0.3536205291748047,
      "learning_rate": 0.00021889149102263854,
      "loss": 0.1678,
      "step": 1466
    },
    {
      "epoch": 3.431578947368421,
      "grad_norm": 0.42056187987327576,
      "learning_rate": 0.0002188134270101483,
      "loss": 0.2056,
      "step": 1467
    },
    {
      "epoch": 3.433918128654971,
      "grad_norm": 0.3910064101219177,
      "learning_rate": 0.00021873536299765807,
      "loss": 0.1436,
      "step": 1468
    },
    {
      "epoch": 3.4362573099415203,
      "grad_norm": 0.3321482241153717,
      "learning_rate": 0.00021865729898516782,
      "loss": 0.1463,
      "step": 1469
    },
    {
      "epoch": 3.43859649122807,
      "grad_norm": 0.38637402653694153,
      "learning_rate": 0.00021857923497267757,
      "loss": 0.2327,
      "step": 1470
    },
    {
      "epoch": 3.44093567251462,
      "grad_norm": 0.3601485788822174,
      "learning_rate": 0.00021850117096018735,
      "loss": 0.1978,
      "step": 1471
    },
    {
      "epoch": 3.4432748538011695,
      "grad_norm": 0.3045212924480438,
      "learning_rate": 0.0002184231069476971,
      "loss": 0.1393,
      "step": 1472
    },
    {
      "epoch": 3.4456140350877194,
      "grad_norm": 0.24974894523620605,
      "learning_rate": 0.00021834504293520685,
      "loss": 0.1176,
      "step": 1473
    },
    {
      "epoch": 3.4479532163742688,
      "grad_norm": 0.4224400222301483,
      "learning_rate": 0.00021826697892271662,
      "loss": 0.1743,
      "step": 1474
    },
    {
      "epoch": 3.4502923976608186,
      "grad_norm": 0.3085203766822815,
      "learning_rate": 0.00021818891491022637,
      "loss": 0.1524,
      "step": 1475
    },
    {
      "epoch": 3.4526315789473685,
      "grad_norm": 0.2596988081932068,
      "learning_rate": 0.0002181108508977361,
      "loss": 0.1222,
      "step": 1476
    },
    {
      "epoch": 3.454970760233918,
      "grad_norm": 0.3957137167453766,
      "learning_rate": 0.0002180327868852459,
      "loss": 0.1881,
      "step": 1477
    },
    {
      "epoch": 3.4573099415204678,
      "grad_norm": 0.3641105890274048,
      "learning_rate": 0.00021795472287275563,
      "loss": 0.1973,
      "step": 1478
    },
    {
      "epoch": 3.4596491228070176,
      "grad_norm": 0.3930398225784302,
      "learning_rate": 0.00021787665886026538,
      "loss": 0.205,
      "step": 1479
    },
    {
      "epoch": 3.461988304093567,
      "grad_norm": 0.34533941745758057,
      "learning_rate": 0.00021779859484777515,
      "loss": 0.1812,
      "step": 1480
    },
    {
      "epoch": 3.464327485380117,
      "grad_norm": 0.2780124545097351,
      "learning_rate": 0.0002177205308352849,
      "loss": 0.1378,
      "step": 1481
    },
    {
      "epoch": 3.466666666666667,
      "grad_norm": 0.24450547993183136,
      "learning_rate": 0.00021764246682279465,
      "loss": 0.1265,
      "step": 1482
    },
    {
      "epoch": 3.469005847953216,
      "grad_norm": 0.29647544026374817,
      "learning_rate": 0.00021756440281030443,
      "loss": 0.1343,
      "step": 1483
    },
    {
      "epoch": 3.471345029239766,
      "grad_norm": 0.34878894686698914,
      "learning_rate": 0.00021748633879781418,
      "loss": 0.1661,
      "step": 1484
    },
    {
      "epoch": 3.473684210526316,
      "grad_norm": 0.2921658158302307,
      "learning_rate": 0.00021740827478532393,
      "loss": 0.1458,
      "step": 1485
    },
    {
      "epoch": 3.4760233918128653,
      "grad_norm": 0.3370106518268585,
      "learning_rate": 0.0002173302107728337,
      "loss": 0.1365,
      "step": 1486
    },
    {
      "epoch": 3.478362573099415,
      "grad_norm": 0.3692304193973541,
      "learning_rate": 0.00021725214676034346,
      "loss": 0.1216,
      "step": 1487
    },
    {
      "epoch": 3.480701754385965,
      "grad_norm": 0.3232305347919464,
      "learning_rate": 0.0002171740827478532,
      "loss": 0.1575,
      "step": 1488
    },
    {
      "epoch": 3.4830409356725145,
      "grad_norm": 0.44996482133865356,
      "learning_rate": 0.000217096018735363,
      "loss": 0.1239,
      "step": 1489
    },
    {
      "epoch": 3.4853801169590644,
      "grad_norm": 0.34981098771095276,
      "learning_rate": 0.00021701795472287274,
      "loss": 0.1305,
      "step": 1490
    },
    {
      "epoch": 3.487719298245614,
      "grad_norm": 0.2781054377555847,
      "learning_rate": 0.0002169398907103825,
      "loss": 0.1208,
      "step": 1491
    },
    {
      "epoch": 3.4900584795321636,
      "grad_norm": 0.32297608256340027,
      "learning_rate": 0.00021686182669789227,
      "loss": 0.118,
      "step": 1492
    },
    {
      "epoch": 3.4923976608187135,
      "grad_norm": 0.3499961793422699,
      "learning_rate": 0.00021678376268540202,
      "loss": 0.1678,
      "step": 1493
    },
    {
      "epoch": 3.4947368421052634,
      "grad_norm": 0.4696419835090637,
      "learning_rate": 0.00021670569867291174,
      "loss": 0.1956,
      "step": 1494
    },
    {
      "epoch": 3.497076023391813,
      "grad_norm": 0.39955174922943115,
      "learning_rate": 0.00021662763466042155,
      "loss": 0.1028,
      "step": 1495
    },
    {
      "epoch": 3.4994152046783626,
      "grad_norm": 0.23345626890659332,
      "learning_rate": 0.00021654957064793127,
      "loss": 0.1064,
      "step": 1496
    },
    {
      "epoch": 3.5017543859649125,
      "grad_norm": 0.42180076241493225,
      "learning_rate": 0.00021647150663544102,
      "loss": 0.1658,
      "step": 1497
    },
    {
      "epoch": 3.504093567251462,
      "grad_norm": 0.5436196327209473,
      "learning_rate": 0.0002163934426229508,
      "loss": 0.1348,
      "step": 1498
    },
    {
      "epoch": 3.506432748538012,
      "grad_norm": 0.24921180307865143,
      "learning_rate": 0.00021631537861046055,
      "loss": 0.1342,
      "step": 1499
    },
    {
      "epoch": 3.5087719298245617,
      "grad_norm": 0.26267901062965393,
      "learning_rate": 0.0002162373145979703,
      "loss": 0.1198,
      "step": 1500
    },
    {
      "epoch": 3.511111111111111,
      "grad_norm": 0.4293108880519867,
      "learning_rate": 0.00021615925058548008,
      "loss": 0.1621,
      "step": 1501
    },
    {
      "epoch": 3.513450292397661,
      "grad_norm": 0.522749662399292,
      "learning_rate": 0.00021608118657298983,
      "loss": 0.1928,
      "step": 1502
    },
    {
      "epoch": 3.515789473684211,
      "grad_norm": 0.2799654006958008,
      "learning_rate": 0.00021600312256049958,
      "loss": 0.1107,
      "step": 1503
    },
    {
      "epoch": 3.51812865497076,
      "grad_norm": 0.36422520875930786,
      "learning_rate": 0.00021592505854800935,
      "loss": 0.1753,
      "step": 1504
    },
    {
      "epoch": 3.52046783625731,
      "grad_norm": 0.26045045256614685,
      "learning_rate": 0.0002158469945355191,
      "loss": 0.1494,
      "step": 1505
    },
    {
      "epoch": 3.5228070175438595,
      "grad_norm": 0.3429511785507202,
      "learning_rate": 0.00021576893052302885,
      "loss": 0.1659,
      "step": 1506
    },
    {
      "epoch": 3.5251461988304094,
      "grad_norm": 0.28575119376182556,
      "learning_rate": 0.00021569086651053863,
      "loss": 0.1599,
      "step": 1507
    },
    {
      "epoch": 3.5274853801169592,
      "grad_norm": 0.22946974635124207,
      "learning_rate": 0.00021561280249804838,
      "loss": 0.1028,
      "step": 1508
    },
    {
      "epoch": 3.5298245614035086,
      "grad_norm": 0.38398614525794983,
      "learning_rate": 0.00021553473848555813,
      "loss": 0.1066,
      "step": 1509
    },
    {
      "epoch": 3.5321637426900585,
      "grad_norm": 0.32329481840133667,
      "learning_rate": 0.0002154566744730679,
      "loss": 0.1637,
      "step": 1510
    },
    {
      "epoch": 3.534502923976608,
      "grad_norm": 0.3464691936969757,
      "learning_rate": 0.00021537861046057766,
      "loss": 0.1641,
      "step": 1511
    },
    {
      "epoch": 3.536842105263158,
      "grad_norm": 0.4838687479496002,
      "learning_rate": 0.00021530054644808738,
      "loss": 0.1578,
      "step": 1512
    },
    {
      "epoch": 3.5391812865497077,
      "grad_norm": 0.35648226737976074,
      "learning_rate": 0.0002152224824355972,
      "loss": 0.1518,
      "step": 1513
    },
    {
      "epoch": 3.541520467836257,
      "grad_norm": 0.3343130350112915,
      "learning_rate": 0.0002151444184231069,
      "loss": 0.1461,
      "step": 1514
    },
    {
      "epoch": 3.543859649122807,
      "grad_norm": 0.5028850436210632,
      "learning_rate": 0.00021506635441061666,
      "loss": 0.1775,
      "step": 1515
    },
    {
      "epoch": 3.546198830409357,
      "grad_norm": 0.30629414319992065,
      "learning_rate": 0.00021498829039812644,
      "loss": 0.1434,
      "step": 1516
    },
    {
      "epoch": 3.548538011695906,
      "grad_norm": 0.2729150354862213,
      "learning_rate": 0.0002149102263856362,
      "loss": 0.1154,
      "step": 1517
    },
    {
      "epoch": 3.550877192982456,
      "grad_norm": 0.35165610909461975,
      "learning_rate": 0.00021483216237314597,
      "loss": 0.135,
      "step": 1518
    },
    {
      "epoch": 3.553216374269006,
      "grad_norm": 0.42813989520072937,
      "learning_rate": 0.00021475409836065572,
      "loss": 0.19,
      "step": 1519
    },
    {
      "epoch": 3.5555555555555554,
      "grad_norm": 0.37304723262786865,
      "learning_rate": 0.00021467603434816547,
      "loss": 0.1442,
      "step": 1520
    },
    {
      "epoch": 3.557894736842105,
      "grad_norm": 0.3354918360710144,
      "learning_rate": 0.00021459797033567525,
      "loss": 0.1461,
      "step": 1521
    },
    {
      "epoch": 3.560233918128655,
      "grad_norm": 0.37262001633644104,
      "learning_rate": 0.000214519906323185,
      "loss": 0.1196,
      "step": 1522
    },
    {
      "epoch": 3.5625730994152045,
      "grad_norm": 0.2656013071537018,
      "learning_rate": 0.00021444184231069475,
      "loss": 0.1154,
      "step": 1523
    },
    {
      "epoch": 3.5649122807017544,
      "grad_norm": 0.35974133014678955,
      "learning_rate": 0.00021436377829820452,
      "loss": 0.1291,
      "step": 1524
    },
    {
      "epoch": 3.5672514619883042,
      "grad_norm": 0.3590090274810791,
      "learning_rate": 0.00021428571428571427,
      "loss": 0.1689,
      "step": 1525
    },
    {
      "epoch": 3.5695906432748536,
      "grad_norm": 0.2560919225215912,
      "learning_rate": 0.00021420765027322403,
      "loss": 0.0961,
      "step": 1526
    },
    {
      "epoch": 3.5719298245614035,
      "grad_norm": 0.30837973952293396,
      "learning_rate": 0.0002141295862607338,
      "loss": 0.1756,
      "step": 1527
    },
    {
      "epoch": 3.5742690058479534,
      "grad_norm": 0.3339907228946686,
      "learning_rate": 0.00021405152224824355,
      "loss": 0.1298,
      "step": 1528
    },
    {
      "epoch": 3.576608187134503,
      "grad_norm": 0.4303111433982849,
      "learning_rate": 0.0002139734582357533,
      "loss": 0.1678,
      "step": 1529
    },
    {
      "epoch": 3.5789473684210527,
      "grad_norm": 0.39381203055381775,
      "learning_rate": 0.00021389539422326308,
      "loss": 0.1765,
      "step": 1530
    },
    {
      "epoch": 3.5812865497076025,
      "grad_norm": 0.32741254568099976,
      "learning_rate": 0.00021381733021077283,
      "loss": 0.1742,
      "step": 1531
    },
    {
      "epoch": 3.583625730994152,
      "grad_norm": 0.36834126710891724,
      "learning_rate": 0.00021373926619828255,
      "loss": 0.1554,
      "step": 1532
    },
    {
      "epoch": 3.585964912280702,
      "grad_norm": 0.733065664768219,
      "learning_rate": 0.00021366120218579236,
      "loss": 0.1294,
      "step": 1533
    },
    {
      "epoch": 3.5883040935672517,
      "grad_norm": 0.2930053174495697,
      "learning_rate": 0.00021358313817330208,
      "loss": 0.1402,
      "step": 1534
    },
    {
      "epoch": 3.590643274853801,
      "grad_norm": 0.3839908540248871,
      "learning_rate": 0.00021350507416081183,
      "loss": 0.1623,
      "step": 1535
    },
    {
      "epoch": 3.592982456140351,
      "grad_norm": 0.2928210198879242,
      "learning_rate": 0.0002134270101483216,
      "loss": 0.1288,
      "step": 1536
    },
    {
      "epoch": 3.595321637426901,
      "grad_norm": 0.27417224645614624,
      "learning_rate": 0.00021334894613583136,
      "loss": 0.1436,
      "step": 1537
    },
    {
      "epoch": 3.5976608187134502,
      "grad_norm": 0.32583707571029663,
      "learning_rate": 0.0002132708821233411,
      "loss": 0.1608,
      "step": 1538
    },
    {
      "epoch": 3.6,
      "grad_norm": 0.5717341899871826,
      "learning_rate": 0.0002131928181108509,
      "loss": 0.1181,
      "step": 1539
    },
    {
      "epoch": 3.60233918128655,
      "grad_norm": 0.39313608407974243,
      "learning_rate": 0.00021311475409836064,
      "loss": 0.1837,
      "step": 1540
    },
    {
      "epoch": 3.6046783625730994,
      "grad_norm": 0.3236202299594879,
      "learning_rate": 0.0002130366900858704,
      "loss": 0.1331,
      "step": 1541
    },
    {
      "epoch": 3.6070175438596492,
      "grad_norm": 0.3326026499271393,
      "learning_rate": 0.00021295862607338017,
      "loss": 0.1124,
      "step": 1542
    },
    {
      "epoch": 3.609356725146199,
      "grad_norm": 0.3571290373802185,
      "learning_rate": 0.00021288056206088992,
      "loss": 0.1451,
      "step": 1543
    },
    {
      "epoch": 3.6116959064327485,
      "grad_norm": 0.4198104441165924,
      "learning_rate": 0.00021280249804839967,
      "loss": 0.186,
      "step": 1544
    },
    {
      "epoch": 3.6140350877192984,
      "grad_norm": 0.30773916840553284,
      "learning_rate": 0.00021272443403590945,
      "loss": 0.1147,
      "step": 1545
    },
    {
      "epoch": 3.616374269005848,
      "grad_norm": 0.3571411967277527,
      "learning_rate": 0.0002126463700234192,
      "loss": 0.1533,
      "step": 1546
    },
    {
      "epoch": 3.6187134502923977,
      "grad_norm": 0.445927232503891,
      "learning_rate": 0.00021256830601092895,
      "loss": 0.1371,
      "step": 1547
    },
    {
      "epoch": 3.6210526315789475,
      "grad_norm": 0.3827291429042816,
      "learning_rate": 0.00021249024199843872,
      "loss": 0.1587,
      "step": 1548
    },
    {
      "epoch": 3.623391812865497,
      "grad_norm": 0.34038645029067993,
      "learning_rate": 0.00021241217798594847,
      "loss": 0.1533,
      "step": 1549
    },
    {
      "epoch": 3.625730994152047,
      "grad_norm": 0.34216752648353577,
      "learning_rate": 0.0002123341139734582,
      "loss": 0.1277,
      "step": 1550
    },
    {
      "epoch": 3.6280701754385962,
      "grad_norm": 0.33465176820755005,
      "learning_rate": 0.000212256049960968,
      "loss": 0.1523,
      "step": 1551
    },
    {
      "epoch": 3.630409356725146,
      "grad_norm": 0.2636202573776245,
      "learning_rate": 0.00021217798594847773,
      "loss": 0.1216,
      "step": 1552
    },
    {
      "epoch": 3.632748538011696,
      "grad_norm": 0.48712608218193054,
      "learning_rate": 0.00021209992193598748,
      "loss": 0.1933,
      "step": 1553
    },
    {
      "epoch": 3.6350877192982454,
      "grad_norm": 0.31669747829437256,
      "learning_rate": 0.00021202185792349725,
      "loss": 0.1351,
      "step": 1554
    },
    {
      "epoch": 3.6374269005847952,
      "grad_norm": 0.5321541428565979,
      "learning_rate": 0.000211943793911007,
      "loss": 0.2108,
      "step": 1555
    },
    {
      "epoch": 3.639766081871345,
      "grad_norm": 0.24761034548282623,
      "learning_rate": 0.00021186572989851675,
      "loss": 0.1093,
      "step": 1556
    },
    {
      "epoch": 3.6421052631578945,
      "grad_norm": 0.37569311261177063,
      "learning_rate": 0.00021178766588602653,
      "loss": 0.1568,
      "step": 1557
    },
    {
      "epoch": 3.6444444444444444,
      "grad_norm": 0.25595563650131226,
      "learning_rate": 0.00021170960187353628,
      "loss": 0.1143,
      "step": 1558
    },
    {
      "epoch": 3.6467836257309942,
      "grad_norm": 0.3611985445022583,
      "learning_rate": 0.00021163153786104603,
      "loss": 0.1492,
      "step": 1559
    },
    {
      "epoch": 3.6491228070175437,
      "grad_norm": 0.3365902304649353,
      "learning_rate": 0.0002115534738485558,
      "loss": 0.1452,
      "step": 1560
    },
    {
      "epoch": 3.6514619883040935,
      "grad_norm": 0.30010363459587097,
      "learning_rate": 0.00021147540983606556,
      "loss": 0.1161,
      "step": 1561
    },
    {
      "epoch": 3.6538011695906434,
      "grad_norm": 0.56517094373703,
      "learning_rate": 0.0002113973458235753,
      "loss": 0.1869,
      "step": 1562
    },
    {
      "epoch": 3.656140350877193,
      "grad_norm": 0.47080472111701965,
      "learning_rate": 0.0002113192818110851,
      "loss": 0.1815,
      "step": 1563
    },
    {
      "epoch": 3.6584795321637427,
      "grad_norm": 0.38796699047088623,
      "learning_rate": 0.00021124121779859484,
      "loss": 0.1471,
      "step": 1564
    },
    {
      "epoch": 3.6608187134502925,
      "grad_norm": 0.39308226108551025,
      "learning_rate": 0.0002111631537861046,
      "loss": 0.1791,
      "step": 1565
    },
    {
      "epoch": 3.663157894736842,
      "grad_norm": 0.34577280282974243,
      "learning_rate": 0.00021108508977361437,
      "loss": 0.15,
      "step": 1566
    },
    {
      "epoch": 3.665497076023392,
      "grad_norm": 0.3759143650531769,
      "learning_rate": 0.00021100702576112412,
      "loss": 0.1828,
      "step": 1567
    },
    {
      "epoch": 3.6678362573099417,
      "grad_norm": 0.33577561378479004,
      "learning_rate": 0.00021092896174863384,
      "loss": 0.1306,
      "step": 1568
    },
    {
      "epoch": 3.670175438596491,
      "grad_norm": 0.366059273481369,
      "learning_rate": 0.00021085089773614362,
      "loss": 0.1566,
      "step": 1569
    },
    {
      "epoch": 3.672514619883041,
      "grad_norm": 0.36863476037979126,
      "learning_rate": 0.00021077283372365337,
      "loss": 0.1729,
      "step": 1570
    },
    {
      "epoch": 3.674853801169591,
      "grad_norm": 0.29134923219680786,
      "learning_rate": 0.00021069476971116312,
      "loss": 0.0962,
      "step": 1571
    },
    {
      "epoch": 3.6771929824561402,
      "grad_norm": 0.48540109395980835,
      "learning_rate": 0.0002106167056986729,
      "loss": 0.149,
      "step": 1572
    },
    {
      "epoch": 3.67953216374269,
      "grad_norm": 0.348389595746994,
      "learning_rate": 0.00021053864168618265,
      "loss": 0.2005,
      "step": 1573
    },
    {
      "epoch": 3.68187134502924,
      "grad_norm": 0.2501417398452759,
      "learning_rate": 0.0002104605776736924,
      "loss": 0.1343,
      "step": 1574
    },
    {
      "epoch": 3.6842105263157894,
      "grad_norm": 0.3006580173969269,
      "learning_rate": 0.00021038251366120217,
      "loss": 0.1248,
      "step": 1575
    },
    {
      "epoch": 3.6865497076023392,
      "grad_norm": 0.2926579713821411,
      "learning_rate": 0.00021030444964871193,
      "loss": 0.1581,
      "step": 1576
    },
    {
      "epoch": 3.688888888888889,
      "grad_norm": 0.35572579503059387,
      "learning_rate": 0.00021022638563622168,
      "loss": 0.1532,
      "step": 1577
    },
    {
      "epoch": 3.6912280701754385,
      "grad_norm": 0.19286316633224487,
      "learning_rate": 0.00021014832162373145,
      "loss": 0.0992,
      "step": 1578
    },
    {
      "epoch": 3.6935672514619884,
      "grad_norm": 0.2815271317958832,
      "learning_rate": 0.0002100702576112412,
      "loss": 0.1146,
      "step": 1579
    },
    {
      "epoch": 3.6959064327485383,
      "grad_norm": 0.5012610554695129,
      "learning_rate": 0.00020999219359875095,
      "loss": 0.1522,
      "step": 1580
    },
    {
      "epoch": 3.6982456140350877,
      "grad_norm": 0.37013664841651917,
      "learning_rate": 0.00020991412958626073,
      "loss": 0.1615,
      "step": 1581
    },
    {
      "epoch": 3.7005847953216375,
      "grad_norm": 0.31733447313308716,
      "learning_rate": 0.00020983606557377048,
      "loss": 0.1261,
      "step": 1582
    },
    {
      "epoch": 3.7029239766081874,
      "grad_norm": 0.34149929881095886,
      "learning_rate": 0.00020975800156128023,
      "loss": 0.1539,
      "step": 1583
    },
    {
      "epoch": 3.705263157894737,
      "grad_norm": 0.34099122881889343,
      "learning_rate": 0.00020967993754879,
      "loss": 0.1363,
      "step": 1584
    },
    {
      "epoch": 3.7076023391812867,
      "grad_norm": 0.3482613265514374,
      "learning_rate": 0.00020960187353629976,
      "loss": 0.1374,
      "step": 1585
    },
    {
      "epoch": 3.709941520467836,
      "grad_norm": 0.38117629289627075,
      "learning_rate": 0.00020952380952380948,
      "loss": 0.1525,
      "step": 1586
    },
    {
      "epoch": 3.712280701754386,
      "grad_norm": 0.2726593017578125,
      "learning_rate": 0.00020944574551131926,
      "loss": 0.1223,
      "step": 1587
    },
    {
      "epoch": 3.714619883040936,
      "grad_norm": 0.28732797503471375,
      "learning_rate": 0.000209367681498829,
      "loss": 0.1564,
      "step": 1588
    },
    {
      "epoch": 3.7169590643274852,
      "grad_norm": 0.3137120306491852,
      "learning_rate": 0.00020928961748633876,
      "loss": 0.1233,
      "step": 1589
    },
    {
      "epoch": 3.719298245614035,
      "grad_norm": 0.36927464604377747,
      "learning_rate": 0.00020921155347384854,
      "loss": 0.1489,
      "step": 1590
    },
    {
      "epoch": 3.7216374269005845,
      "grad_norm": 0.4038289189338684,
      "learning_rate": 0.0002091334894613583,
      "loss": 0.1752,
      "step": 1591
    },
    {
      "epoch": 3.7239766081871344,
      "grad_norm": 0.3618132770061493,
      "learning_rate": 0.00020905542544886804,
      "loss": 0.1559,
      "step": 1592
    },
    {
      "epoch": 3.7263157894736842,
      "grad_norm": 0.4824864864349365,
      "learning_rate": 0.00020897736143637782,
      "loss": 0.1585,
      "step": 1593
    },
    {
      "epoch": 3.7286549707602337,
      "grad_norm": 0.23393011093139648,
      "learning_rate": 0.00020889929742388757,
      "loss": 0.117,
      "step": 1594
    },
    {
      "epoch": 3.7309941520467835,
      "grad_norm": 0.24712923169136047,
      "learning_rate": 0.00020882123341139732,
      "loss": 0.1223,
      "step": 1595
    },
    {
      "epoch": 3.7333333333333334,
      "grad_norm": 0.2831277847290039,
      "learning_rate": 0.0002087431693989071,
      "loss": 0.1028,
      "step": 1596
    },
    {
      "epoch": 3.735672514619883,
      "grad_norm": 0.2447805255651474,
      "learning_rate": 0.00020866510538641685,
      "loss": 0.1054,
      "step": 1597
    },
    {
      "epoch": 3.7380116959064327,
      "grad_norm": 0.3288244307041168,
      "learning_rate": 0.0002085870413739266,
      "loss": 0.1482,
      "step": 1598
    },
    {
      "epoch": 3.7403508771929825,
      "grad_norm": 0.45869842171669006,
      "learning_rate": 0.00020850897736143637,
      "loss": 0.1926,
      "step": 1599
    },
    {
      "epoch": 3.742690058479532,
      "grad_norm": 0.38821086287498474,
      "learning_rate": 0.00020843091334894612,
      "loss": 0.1592,
      "step": 1600
    },
    {
      "epoch": 3.745029239766082,
      "grad_norm": 0.3776046633720398,
      "learning_rate": 0.00020835284933645588,
      "loss": 0.1945,
      "step": 1601
    },
    {
      "epoch": 3.7473684210526317,
      "grad_norm": 0.345802903175354,
      "learning_rate": 0.00020827478532396565,
      "loss": 0.1413,
      "step": 1602
    },
    {
      "epoch": 3.749707602339181,
      "grad_norm": 0.2708752453327179,
      "learning_rate": 0.00020819672131147538,
      "loss": 0.1526,
      "step": 1603
    },
    {
      "epoch": 3.752046783625731,
      "grad_norm": 0.2686411738395691,
      "learning_rate": 0.00020811865729898513,
      "loss": 0.1071,
      "step": 1604
    },
    {
      "epoch": 3.754385964912281,
      "grad_norm": 0.3492981493473053,
      "learning_rate": 0.0002080405932864949,
      "loss": 0.1796,
      "step": 1605
    },
    {
      "epoch": 3.7567251461988302,
      "grad_norm": 0.33277615904808044,
      "learning_rate": 0.00020796252927400465,
      "loss": 0.1352,
      "step": 1606
    },
    {
      "epoch": 3.75906432748538,
      "grad_norm": 0.2761223018169403,
      "learning_rate": 0.0002078844652615144,
      "loss": 0.154,
      "step": 1607
    },
    {
      "epoch": 3.76140350877193,
      "grad_norm": 0.3225981891155243,
      "learning_rate": 0.00020780640124902418,
      "loss": 0.1458,
      "step": 1608
    },
    {
      "epoch": 3.7637426900584794,
      "grad_norm": 0.41979965567588806,
      "learning_rate": 0.00020772833723653393,
      "loss": 0.1438,
      "step": 1609
    },
    {
      "epoch": 3.7660818713450293,
      "grad_norm": 0.42004895210266113,
      "learning_rate": 0.00020765027322404368,
      "loss": 0.1518,
      "step": 1610
    },
    {
      "epoch": 3.768421052631579,
      "grad_norm": 0.6137215495109558,
      "learning_rate": 0.00020757220921155346,
      "loss": 0.1818,
      "step": 1611
    },
    {
      "epoch": 3.7707602339181285,
      "grad_norm": 0.4295116364955902,
      "learning_rate": 0.0002074941451990632,
      "loss": 0.1945,
      "step": 1612
    },
    {
      "epoch": 3.7730994152046784,
      "grad_norm": 0.3271504044532776,
      "learning_rate": 0.00020741608118657296,
      "loss": 0.1421,
      "step": 1613
    },
    {
      "epoch": 3.7754385964912283,
      "grad_norm": 0.2765434980392456,
      "learning_rate": 0.00020733801717408274,
      "loss": 0.1316,
      "step": 1614
    },
    {
      "epoch": 3.7777777777777777,
      "grad_norm": 0.32199421525001526,
      "learning_rate": 0.0002072599531615925,
      "loss": 0.1279,
      "step": 1615
    },
    {
      "epoch": 3.7801169590643275,
      "grad_norm": 0.38015496730804443,
      "learning_rate": 0.00020718188914910224,
      "loss": 0.1564,
      "step": 1616
    },
    {
      "epoch": 3.7824561403508774,
      "grad_norm": 0.31016722321510315,
      "learning_rate": 0.00020710382513661202,
      "loss": 0.1377,
      "step": 1617
    },
    {
      "epoch": 3.784795321637427,
      "grad_norm": 0.5303143858909607,
      "learning_rate": 0.00020702576112412177,
      "loss": 0.1648,
      "step": 1618
    },
    {
      "epoch": 3.7871345029239767,
      "grad_norm": 0.3979972302913666,
      "learning_rate": 0.00020694769711163152,
      "loss": 0.1328,
      "step": 1619
    },
    {
      "epoch": 3.7894736842105265,
      "grad_norm": 0.4349346458911896,
      "learning_rate": 0.0002068696330991413,
      "loss": 0.1806,
      "step": 1620
    },
    {
      "epoch": 3.791812865497076,
      "grad_norm": 0.3116273880004883,
      "learning_rate": 0.00020679156908665102,
      "loss": 0.1061,
      "step": 1621
    },
    {
      "epoch": 3.794152046783626,
      "grad_norm": 0.43244266510009766,
      "learning_rate": 0.00020671350507416077,
      "loss": 0.1955,
      "step": 1622
    },
    {
      "epoch": 3.7964912280701757,
      "grad_norm": 0.3690956234931946,
      "learning_rate": 0.00020663544106167055,
      "loss": 0.1383,
      "step": 1623
    },
    {
      "epoch": 3.798830409356725,
      "grad_norm": 0.2958112955093384,
      "learning_rate": 0.0002065573770491803,
      "loss": 0.1314,
      "step": 1624
    },
    {
      "epoch": 3.801169590643275,
      "grad_norm": 0.2160971462726593,
      "learning_rate": 0.00020647931303669005,
      "loss": 0.0935,
      "step": 1625
    },
    {
      "epoch": 3.803508771929825,
      "grad_norm": 0.3996082842350006,
      "learning_rate": 0.00020640124902419983,
      "loss": 0.1616,
      "step": 1626
    },
    {
      "epoch": 3.8058479532163743,
      "grad_norm": 0.38101616501808167,
      "learning_rate": 0.00020632318501170958,
      "loss": 0.1725,
      "step": 1627
    },
    {
      "epoch": 3.808187134502924,
      "grad_norm": 0.4066626727581024,
      "learning_rate": 0.00020624512099921933,
      "loss": 0.1651,
      "step": 1628
    },
    {
      "epoch": 3.8105263157894735,
      "grad_norm": 0.4065924286842346,
      "learning_rate": 0.0002061670569867291,
      "loss": 0.1835,
      "step": 1629
    },
    {
      "epoch": 3.8128654970760234,
      "grad_norm": 0.38451236486434937,
      "learning_rate": 0.00020608899297423885,
      "loss": 0.1787,
      "step": 1630
    },
    {
      "epoch": 3.8152046783625733,
      "grad_norm": 0.374227911233902,
      "learning_rate": 0.0002060109289617486,
      "loss": 0.1546,
      "step": 1631
    },
    {
      "epoch": 3.8175438596491227,
      "grad_norm": 0.3874533176422119,
      "learning_rate": 0.00020593286494925838,
      "loss": 0.1877,
      "step": 1632
    },
    {
      "epoch": 3.8198830409356725,
      "grad_norm": 0.38174372911453247,
      "learning_rate": 0.00020585480093676813,
      "loss": 0.202,
      "step": 1633
    },
    {
      "epoch": 3.822222222222222,
      "grad_norm": 0.34682607650756836,
      "learning_rate": 0.00020577673692427788,
      "loss": 0.1582,
      "step": 1634
    },
    {
      "epoch": 3.824561403508772,
      "grad_norm": 0.40621069073677063,
      "learning_rate": 0.00020569867291178766,
      "loss": 0.1772,
      "step": 1635
    },
    {
      "epoch": 3.8269005847953217,
      "grad_norm": 0.24282699823379517,
      "learning_rate": 0.0002056206088992974,
      "loss": 0.1168,
      "step": 1636
    },
    {
      "epoch": 3.829239766081871,
      "grad_norm": 0.40921634435653687,
      "learning_rate": 0.00020554254488680716,
      "loss": 0.2037,
      "step": 1637
    },
    {
      "epoch": 3.831578947368421,
      "grad_norm": 0.36530882120132446,
      "learning_rate": 0.00020546448087431694,
      "loss": 0.2154,
      "step": 1638
    },
    {
      "epoch": 3.833918128654971,
      "grad_norm": 0.336317777633667,
      "learning_rate": 0.00020538641686182666,
      "loss": 0.1532,
      "step": 1639
    },
    {
      "epoch": 3.8362573099415203,
      "grad_norm": 0.3767513930797577,
      "learning_rate": 0.0002053083528493364,
      "loss": 0.1825,
      "step": 1640
    },
    {
      "epoch": 3.83859649122807,
      "grad_norm": 0.381536066532135,
      "learning_rate": 0.0002052302888368462,
      "loss": 0.1623,
      "step": 1641
    },
    {
      "epoch": 3.84093567251462,
      "grad_norm": 0.3081648349761963,
      "learning_rate": 0.00020515222482435594,
      "loss": 0.1344,
      "step": 1642
    },
    {
      "epoch": 3.8432748538011694,
      "grad_norm": 0.2561911940574646,
      "learning_rate": 0.0002050741608118657,
      "loss": 0.1279,
      "step": 1643
    },
    {
      "epoch": 3.8456140350877193,
      "grad_norm": 0.30208665132522583,
      "learning_rate": 0.00020499609679937547,
      "loss": 0.1435,
      "step": 1644
    },
    {
      "epoch": 3.847953216374269,
      "grad_norm": 0.4136791229248047,
      "learning_rate": 0.00020491803278688522,
      "loss": 0.2149,
      "step": 1645
    },
    {
      "epoch": 3.8502923976608185,
      "grad_norm": 0.24728387594223022,
      "learning_rate": 0.000204839968774395,
      "loss": 0.1151,
      "step": 1646
    },
    {
      "epoch": 3.8526315789473684,
      "grad_norm": 0.47959282994270325,
      "learning_rate": 0.00020476190476190475,
      "loss": 0.2002,
      "step": 1647
    },
    {
      "epoch": 3.8549707602339183,
      "grad_norm": 0.3285366892814636,
      "learning_rate": 0.0002046838407494145,
      "loss": 0.1693,
      "step": 1648
    },
    {
      "epoch": 3.8573099415204677,
      "grad_norm": 0.42189931869506836,
      "learning_rate": 0.00020460577673692427,
      "loss": 0.1775,
      "step": 1649
    },
    {
      "epoch": 3.8596491228070176,
      "grad_norm": 0.37399059534072876,
      "learning_rate": 0.00020452771272443402,
      "loss": 0.1103,
      "step": 1650
    },
    {
      "epoch": 3.8619883040935674,
      "grad_norm": 0.2427695393562317,
      "learning_rate": 0.00020444964871194378,
      "loss": 0.0991,
      "step": 1651
    },
    {
      "epoch": 3.864327485380117,
      "grad_norm": 0.398868590593338,
      "learning_rate": 0.00020437158469945355,
      "loss": 0.1205,
      "step": 1652
    },
    {
      "epoch": 3.8666666666666667,
      "grad_norm": 0.34147679805755615,
      "learning_rate": 0.0002042935206869633,
      "loss": 0.1652,
      "step": 1653
    },
    {
      "epoch": 3.8690058479532166,
      "grad_norm": 0.46099919080734253,
      "learning_rate": 0.00020421545667447305,
      "loss": 0.2001,
      "step": 1654
    },
    {
      "epoch": 3.871345029239766,
      "grad_norm": 0.5229901671409607,
      "learning_rate": 0.00020413739266198283,
      "loss": 0.1442,
      "step": 1655
    },
    {
      "epoch": 3.873684210526316,
      "grad_norm": 0.39232903718948364,
      "learning_rate": 0.00020405932864949258,
      "loss": 0.1713,
      "step": 1656
    },
    {
      "epoch": 3.8760233918128657,
      "grad_norm": 0.3516796827316284,
      "learning_rate": 0.0002039812646370023,
      "loss": 0.157,
      "step": 1657
    },
    {
      "epoch": 3.878362573099415,
      "grad_norm": 0.47665929794311523,
      "learning_rate": 0.0002039032006245121,
      "loss": 0.2058,
      "step": 1658
    },
    {
      "epoch": 3.880701754385965,
      "grad_norm": 0.326427161693573,
      "learning_rate": 0.00020382513661202183,
      "loss": 0.1296,
      "step": 1659
    },
    {
      "epoch": 3.883040935672515,
      "grad_norm": 0.2508130371570587,
      "learning_rate": 0.00020374707259953158,
      "loss": 0.1369,
      "step": 1660
    },
    {
      "epoch": 3.8853801169590643,
      "grad_norm": 0.28651800751686096,
      "learning_rate": 0.00020366900858704136,
      "loss": 0.1272,
      "step": 1661
    },
    {
      "epoch": 3.887719298245614,
      "grad_norm": 0.3455033600330353,
      "learning_rate": 0.0002035909445745511,
      "loss": 0.115,
      "step": 1662
    },
    {
      "epoch": 3.890058479532164,
      "grad_norm": 0.3197375237941742,
      "learning_rate": 0.00020351288056206086,
      "loss": 0.1274,
      "step": 1663
    },
    {
      "epoch": 3.8923976608187134,
      "grad_norm": 0.3916495144367218,
      "learning_rate": 0.00020343481654957064,
      "loss": 0.1706,
      "step": 1664
    },
    {
      "epoch": 3.8947368421052633,
      "grad_norm": 0.449307918548584,
      "learning_rate": 0.0002033567525370804,
      "loss": 0.1778,
      "step": 1665
    },
    {
      "epoch": 3.897076023391813,
      "grad_norm": 0.3767433166503906,
      "learning_rate": 0.00020327868852459014,
      "loss": 0.213,
      "step": 1666
    },
    {
      "epoch": 3.8994152046783626,
      "grad_norm": 0.28130149841308594,
      "learning_rate": 0.00020320062451209992,
      "loss": 0.1272,
      "step": 1667
    },
    {
      "epoch": 3.9017543859649124,
      "grad_norm": 0.2953956127166748,
      "learning_rate": 0.00020312256049960967,
      "loss": 0.1531,
      "step": 1668
    },
    {
      "epoch": 3.904093567251462,
      "grad_norm": 0.33475595712661743,
      "learning_rate": 0.00020304449648711942,
      "loss": 0.167,
      "step": 1669
    },
    {
      "epoch": 3.9064327485380117,
      "grad_norm": 0.2152986079454422,
      "learning_rate": 0.0002029664324746292,
      "loss": 0.0952,
      "step": 1670
    },
    {
      "epoch": 3.9087719298245616,
      "grad_norm": 0.33379021286964417,
      "learning_rate": 0.00020288836846213895,
      "loss": 0.148,
      "step": 1671
    },
    {
      "epoch": 3.911111111111111,
      "grad_norm": 0.26537632942199707,
      "learning_rate": 0.0002028103044496487,
      "loss": 0.1428,
      "step": 1672
    },
    {
      "epoch": 3.913450292397661,
      "grad_norm": 0.32739904522895813,
      "learning_rate": 0.00020273224043715847,
      "loss": 0.1402,
      "step": 1673
    },
    {
      "epoch": 3.9157894736842103,
      "grad_norm": 0.25834864377975464,
      "learning_rate": 0.00020265417642466822,
      "loss": 0.1247,
      "step": 1674
    },
    {
      "epoch": 3.91812865497076,
      "grad_norm": 0.2831713855266571,
      "learning_rate": 0.00020257611241217795,
      "loss": 0.1407,
      "step": 1675
    },
    {
      "epoch": 3.92046783625731,
      "grad_norm": 0.37227925658226013,
      "learning_rate": 0.00020249804839968775,
      "loss": 0.16,
      "step": 1676
    },
    {
      "epoch": 3.9228070175438594,
      "grad_norm": 0.41328948736190796,
      "learning_rate": 0.00020241998438719748,
      "loss": 0.179,
      "step": 1677
    },
    {
      "epoch": 3.9251461988304093,
      "grad_norm": 0.36713704466819763,
      "learning_rate": 0.00020234192037470723,
      "loss": 0.1701,
      "step": 1678
    },
    {
      "epoch": 3.927485380116959,
      "grad_norm": 0.43110790848731995,
      "learning_rate": 0.000202263856362217,
      "loss": 0.1532,
      "step": 1679
    },
    {
      "epoch": 3.9298245614035086,
      "grad_norm": 0.3156019449234009,
      "learning_rate": 0.00020218579234972675,
      "loss": 0.1577,
      "step": 1680
    },
    {
      "epoch": 3.9321637426900584,
      "grad_norm": 0.29379022121429443,
      "learning_rate": 0.0002021077283372365,
      "loss": 0.1675,
      "step": 1681
    },
    {
      "epoch": 3.9345029239766083,
      "grad_norm": 0.3317720890045166,
      "learning_rate": 0.00020202966432474628,
      "loss": 0.1403,
      "step": 1682
    },
    {
      "epoch": 3.9368421052631577,
      "grad_norm": 0.3154236078262329,
      "learning_rate": 0.00020195160031225603,
      "loss": 0.1836,
      "step": 1683
    },
    {
      "epoch": 3.9391812865497076,
      "grad_norm": 0.20546279847621918,
      "learning_rate": 0.00020187353629976578,
      "loss": 0.1058,
      "step": 1684
    },
    {
      "epoch": 3.9415204678362574,
      "grad_norm": 0.3229931890964508,
      "learning_rate": 0.00020179547228727556,
      "loss": 0.1618,
      "step": 1685
    },
    {
      "epoch": 3.943859649122807,
      "grad_norm": 0.3256067931652069,
      "learning_rate": 0.0002017174082747853,
      "loss": 0.1607,
      "step": 1686
    },
    {
      "epoch": 3.9461988304093567,
      "grad_norm": 0.3203842043876648,
      "learning_rate": 0.00020163934426229506,
      "loss": 0.1499,
      "step": 1687
    },
    {
      "epoch": 3.9485380116959066,
      "grad_norm": 0.4271097481250763,
      "learning_rate": 0.00020156128024980484,
      "loss": 0.1822,
      "step": 1688
    },
    {
      "epoch": 3.950877192982456,
      "grad_norm": 0.24525004625320435,
      "learning_rate": 0.0002014832162373146,
      "loss": 0.1383,
      "step": 1689
    },
    {
      "epoch": 3.953216374269006,
      "grad_norm": 0.22797046601772308,
      "learning_rate": 0.00020140515222482434,
      "loss": 0.0955,
      "step": 1690
    },
    {
      "epoch": 3.9555555555555557,
      "grad_norm": 0.26499617099761963,
      "learning_rate": 0.00020132708821233412,
      "loss": 0.1504,
      "step": 1691
    },
    {
      "epoch": 3.957894736842105,
      "grad_norm": 0.3678635358810425,
      "learning_rate": 0.00020124902419984387,
      "loss": 0.1616,
      "step": 1692
    },
    {
      "epoch": 3.960233918128655,
      "grad_norm": 0.3063100278377533,
      "learning_rate": 0.0002011709601873536,
      "loss": 0.185,
      "step": 1693
    },
    {
      "epoch": 3.962573099415205,
      "grad_norm": 0.28225764632225037,
      "learning_rate": 0.0002010928961748634,
      "loss": 0.1291,
      "step": 1694
    },
    {
      "epoch": 3.9649122807017543,
      "grad_norm": 0.4373652935028076,
      "learning_rate": 0.00020101483216237312,
      "loss": 0.1968,
      "step": 1695
    },
    {
      "epoch": 3.967251461988304,
      "grad_norm": 0.3423842787742615,
      "learning_rate": 0.00020093676814988287,
      "loss": 0.1525,
      "step": 1696
    },
    {
      "epoch": 3.969590643274854,
      "grad_norm": 0.29260870814323425,
      "learning_rate": 0.00020085870413739265,
      "loss": 0.1351,
      "step": 1697
    },
    {
      "epoch": 3.9719298245614034,
      "grad_norm": 0.43885305523872375,
      "learning_rate": 0.0002007806401249024,
      "loss": 0.1726,
      "step": 1698
    },
    {
      "epoch": 3.9742690058479533,
      "grad_norm": 0.2996557652950287,
      "learning_rate": 0.00020070257611241215,
      "loss": 0.1282,
      "step": 1699
    },
    {
      "epoch": 3.976608187134503,
      "grad_norm": 0.2758041322231293,
      "learning_rate": 0.00020062451209992192,
      "loss": 0.1252,
      "step": 1700
    },
    {
      "epoch": 3.9789473684210526,
      "grad_norm": 0.37038666009902954,
      "learning_rate": 0.00020054644808743168,
      "loss": 0.1355,
      "step": 1701
    },
    {
      "epoch": 3.9812865497076024,
      "grad_norm": 0.5237747430801392,
      "learning_rate": 0.00020046838407494143,
      "loss": 0.1349,
      "step": 1702
    },
    {
      "epoch": 3.9836257309941523,
      "grad_norm": 0.32316768169403076,
      "learning_rate": 0.0002003903200624512,
      "loss": 0.1377,
      "step": 1703
    },
    {
      "epoch": 3.9859649122807017,
      "grad_norm": 0.3672725558280945,
      "learning_rate": 0.00020031225604996095,
      "loss": 0.1368,
      "step": 1704
    },
    {
      "epoch": 3.9883040935672516,
      "grad_norm": 0.3117210865020752,
      "learning_rate": 0.0002002341920374707,
      "loss": 0.1416,
      "step": 1705
    },
    {
      "epoch": 3.9906432748538014,
      "grad_norm": 0.48461487889289856,
      "learning_rate": 0.00020015612802498048,
      "loss": 0.1437,
      "step": 1706
    },
    {
      "epoch": 3.992982456140351,
      "grad_norm": 0.3624936640262604,
      "learning_rate": 0.00020007806401249023,
      "loss": 0.1266,
      "step": 1707
    },
    {
      "epoch": 3.9953216374269007,
      "grad_norm": 0.3880232870578766,
      "learning_rate": 0.00019999999999999998,
      "loss": 0.1401,
      "step": 1708
    },
    {
      "epoch": 3.99766081871345,
      "grad_norm": 0.4664638340473175,
      "learning_rate": 0.00019992193598750976,
      "loss": 0.2571,
      "step": 1709
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.562856912612915,
      "learning_rate": 0.0001998438719750195,
      "loss": 0.1626,
      "step": 1710
    },
    {
      "epoch": 4.0,
      "eval_loss": 0.22524040937423706,
      "eval_runtime": 125.4565,
      "eval_samples_per_second": 4.4,
      "eval_steps_per_second": 0.55,
      "step": 1710
    }
  ],
  "logging_steps": 1,
  "max_steps": 4270,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 4.990820532861665e+17,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
