{
  "best_metric": 0.0431111678481102,
  "best_model_checkpoint": "LLaMinerva/GeoLingIt-mod/LLaMA/checkpoint-1281",
  "epoch": 2.9982445874780574,
  "eval_steps": 500,
  "global_step": 1281,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0023405500292568754,
      "grad_norm": 1.4386879205703735,
      "learning_rate": 7.025761124121779e-07,
      "loss": 2.2938,
      "step": 1
    },
    {
      "epoch": 0.004681100058513751,
      "grad_norm": 1.3960613012313843,
      "learning_rate": 1.4051522248243558e-06,
      "loss": 2.2321,
      "step": 2
    },
    {
      "epoch": 0.007021650087770626,
      "grad_norm": 1.4666733741760254,
      "learning_rate": 2.107728337236534e-06,
      "loss": 2.2602,
      "step": 3
    },
    {
      "epoch": 0.009362200117027502,
      "grad_norm": 1.433728575706482,
      "learning_rate": 2.8103044496487116e-06,
      "loss": 2.2734,
      "step": 4
    },
    {
      "epoch": 0.011702750146284377,
      "grad_norm": 1.4815378189086914,
      "learning_rate": 3.5128805620608897e-06,
      "loss": 2.3047,
      "step": 5
    },
    {
      "epoch": 0.014043300175541252,
      "grad_norm": 1.504467248916626,
      "learning_rate": 4.215456674473068e-06,
      "loss": 2.3024,
      "step": 6
    },
    {
      "epoch": 0.016383850204798128,
      "grad_norm": 1.4432237148284912,
      "learning_rate": 4.9180327868852455e-06,
      "loss": 2.2987,
      "step": 7
    },
    {
      "epoch": 0.018724400234055003,
      "grad_norm": 1.5072933435440063,
      "learning_rate": 5.620608899297423e-06,
      "loss": 2.2591,
      "step": 8
    },
    {
      "epoch": 0.021064950263311878,
      "grad_norm": 1.5276622772216797,
      "learning_rate": 6.323185011709601e-06,
      "loss": 2.2457,
      "step": 9
    },
    {
      "epoch": 0.023405500292568753,
      "grad_norm": 1.4815077781677246,
      "learning_rate": 7.025761124121779e-06,
      "loss": 2.2506,
      "step": 10
    },
    {
      "epoch": 0.025746050321825628,
      "grad_norm": 1.5633798837661743,
      "learning_rate": 7.728337236533957e-06,
      "loss": 2.249,
      "step": 11
    },
    {
      "epoch": 0.028086600351082503,
      "grad_norm": 1.570768117904663,
      "learning_rate": 8.430913348946136e-06,
      "loss": 2.1942,
      "step": 12
    },
    {
      "epoch": 0.030427150380339378,
      "grad_norm": 1.469356894493103,
      "learning_rate": 9.133489461358312e-06,
      "loss": 2.2398,
      "step": 13
    },
    {
      "epoch": 0.032767700409596257,
      "grad_norm": 1.5417308807373047,
      "learning_rate": 9.836065573770491e-06,
      "loss": 2.2358,
      "step": 14
    },
    {
      "epoch": 0.03510825043885313,
      "grad_norm": 1.5349199771881104,
      "learning_rate": 1.0538641686182668e-05,
      "loss": 2.2099,
      "step": 15
    },
    {
      "epoch": 0.037448800468110006,
      "grad_norm": 1.4904340505599976,
      "learning_rate": 1.1241217798594846e-05,
      "loss": 2.1917,
      "step": 16
    },
    {
      "epoch": 0.03978935049736688,
      "grad_norm": 1.518115758895874,
      "learning_rate": 1.1943793911007025e-05,
      "loss": 2.1515,
      "step": 17
    },
    {
      "epoch": 0.042129900526623756,
      "grad_norm": 1.599041223526001,
      "learning_rate": 1.2646370023419202e-05,
      "loss": 2.1681,
      "step": 18
    },
    {
      "epoch": 0.044470450555880635,
      "grad_norm": 1.611703634262085,
      "learning_rate": 1.334894613583138e-05,
      "loss": 2.1418,
      "step": 19
    },
    {
      "epoch": 0.046811000585137506,
      "grad_norm": 1.5856472253799438,
      "learning_rate": 1.4051522248243559e-05,
      "loss": 2.0911,
      "step": 20
    },
    {
      "epoch": 0.049151550614394385,
      "grad_norm": 1.5196318626403809,
      "learning_rate": 1.4754098360655736e-05,
      "loss": 2.0897,
      "step": 21
    },
    {
      "epoch": 0.051492100643651256,
      "grad_norm": 1.5267635583877563,
      "learning_rate": 1.5456674473067914e-05,
      "loss": 2.0796,
      "step": 22
    },
    {
      "epoch": 0.053832650672908135,
      "grad_norm": 1.5144039392471313,
      "learning_rate": 1.6159250585480093e-05,
      "loss": 2.0052,
      "step": 23
    },
    {
      "epoch": 0.056173200702165006,
      "grad_norm": 1.5810461044311523,
      "learning_rate": 1.686182669789227e-05,
      "loss": 2.0004,
      "step": 24
    },
    {
      "epoch": 0.058513750731421885,
      "grad_norm": 1.5540616512298584,
      "learning_rate": 1.756440281030445e-05,
      "loss": 1.9334,
      "step": 25
    },
    {
      "epoch": 0.060854300760678756,
      "grad_norm": 1.5544313192367554,
      "learning_rate": 1.8266978922716625e-05,
      "loss": 1.9401,
      "step": 26
    },
    {
      "epoch": 0.06319485078993564,
      "grad_norm": 1.656162977218628,
      "learning_rate": 1.8969555035128803e-05,
      "loss": 1.9192,
      "step": 27
    },
    {
      "epoch": 0.06553540081919251,
      "grad_norm": 1.5680745840072632,
      "learning_rate": 1.9672131147540982e-05,
      "loss": 1.8368,
      "step": 28
    },
    {
      "epoch": 0.06787595084844938,
      "grad_norm": 1.6254264116287231,
      "learning_rate": 2.037470725995316e-05,
      "loss": 1.8505,
      "step": 29
    },
    {
      "epoch": 0.07021650087770626,
      "grad_norm": 1.70123291015625,
      "learning_rate": 2.1077283372365335e-05,
      "loss": 1.7625,
      "step": 30
    },
    {
      "epoch": 0.07255705090696314,
      "grad_norm": 1.7648051977157593,
      "learning_rate": 2.1779859484777514e-05,
      "loss": 1.741,
      "step": 31
    },
    {
      "epoch": 0.07489760093622001,
      "grad_norm": 1.7142890691757202,
      "learning_rate": 2.2482435597189693e-05,
      "loss": 1.6991,
      "step": 32
    },
    {
      "epoch": 0.07723815096547688,
      "grad_norm": 1.7960137128829956,
      "learning_rate": 2.318501170960187e-05,
      "loss": 1.6618,
      "step": 33
    },
    {
      "epoch": 0.07957870099473376,
      "grad_norm": 1.8352220058441162,
      "learning_rate": 2.388758782201405e-05,
      "loss": 1.6073,
      "step": 34
    },
    {
      "epoch": 0.08191925102399064,
      "grad_norm": 1.8350716829299927,
      "learning_rate": 2.4590163934426225e-05,
      "loss": 1.532,
      "step": 35
    },
    {
      "epoch": 0.08425980105324751,
      "grad_norm": 1.9048328399658203,
      "learning_rate": 2.5292740046838403e-05,
      "loss": 1.4093,
      "step": 36
    },
    {
      "epoch": 0.08660035108250438,
      "grad_norm": 1.9854762554168701,
      "learning_rate": 2.5995316159250582e-05,
      "loss": 1.3321,
      "step": 37
    },
    {
      "epoch": 0.08894090111176127,
      "grad_norm": 1.9742672443389893,
      "learning_rate": 2.669789227166276e-05,
      "loss": 1.3433,
      "step": 38
    },
    {
      "epoch": 0.09128145114101814,
      "grad_norm": 2.0442981719970703,
      "learning_rate": 2.740046838407494e-05,
      "loss": 1.2606,
      "step": 39
    },
    {
      "epoch": 0.09362200117027501,
      "grad_norm": 2.0376453399658203,
      "learning_rate": 2.8103044496487117e-05,
      "loss": 1.1797,
      "step": 40
    },
    {
      "epoch": 0.09596255119953188,
      "grad_norm": 1.9991830587387085,
      "learning_rate": 2.8805620608899293e-05,
      "loss": 1.0977,
      "step": 41
    },
    {
      "epoch": 0.09830310122878877,
      "grad_norm": 2.0080995559692383,
      "learning_rate": 2.950819672131147e-05,
      "loss": 0.9966,
      "step": 42
    },
    {
      "epoch": 0.10064365125804564,
      "grad_norm": 1.9351564645767212,
      "learning_rate": 3.021077283372365e-05,
      "loss": 0.9562,
      "step": 43
    },
    {
      "epoch": 0.10298420128730251,
      "grad_norm": 1.941870927810669,
      "learning_rate": 3.091334894613583e-05,
      "loss": 0.8393,
      "step": 44
    },
    {
      "epoch": 0.1053247513165594,
      "grad_norm": 1.8528642654418945,
      "learning_rate": 3.161592505854801e-05,
      "loss": 0.7831,
      "step": 45
    },
    {
      "epoch": 0.10766530134581627,
      "grad_norm": 1.6571756601333618,
      "learning_rate": 3.2318501170960185e-05,
      "loss": 0.7566,
      "step": 46
    },
    {
      "epoch": 0.11000585137507314,
      "grad_norm": 1.7153589725494385,
      "learning_rate": 3.3021077283372364e-05,
      "loss": 0.6204,
      "step": 47
    },
    {
      "epoch": 0.11234640140433001,
      "grad_norm": 1.6097487211227417,
      "learning_rate": 3.372365339578454e-05,
      "loss": 0.5619,
      "step": 48
    },
    {
      "epoch": 0.1146869514335869,
      "grad_norm": 1.3878653049468994,
      "learning_rate": 3.442622950819672e-05,
      "loss": 0.5061,
      "step": 49
    },
    {
      "epoch": 0.11702750146284377,
      "grad_norm": 1.0812417268753052,
      "learning_rate": 3.51288056206089e-05,
      "loss": 0.504,
      "step": 50
    },
    {
      "epoch": 0.11936805149210064,
      "grad_norm": 0.9796047210693359,
      "learning_rate": 3.583138173302107e-05,
      "loss": 0.3999,
      "step": 51
    },
    {
      "epoch": 0.12170860152135751,
      "grad_norm": 0.9367277026176453,
      "learning_rate": 3.653395784543325e-05,
      "loss": 0.3584,
      "step": 52
    },
    {
      "epoch": 0.1240491515506144,
      "grad_norm": 0.8259384632110596,
      "learning_rate": 3.723653395784543e-05,
      "loss": 0.3701,
      "step": 53
    },
    {
      "epoch": 0.12638970157987128,
      "grad_norm": 0.8386067152023315,
      "learning_rate": 3.7939110070257607e-05,
      "loss": 0.3639,
      "step": 54
    },
    {
      "epoch": 0.12873025160912815,
      "grad_norm": 0.787015974521637,
      "learning_rate": 3.8641686182669785e-05,
      "loss": 0.32,
      "step": 55
    },
    {
      "epoch": 0.13107080163838503,
      "grad_norm": 0.654330849647522,
      "learning_rate": 3.9344262295081964e-05,
      "loss": 0.3024,
      "step": 56
    },
    {
      "epoch": 0.1334113516676419,
      "grad_norm": 0.7069533467292786,
      "learning_rate": 4.004683840749414e-05,
      "loss": 0.281,
      "step": 57
    },
    {
      "epoch": 0.13575190169689877,
      "grad_norm": 0.8037585020065308,
      "learning_rate": 4.074941451990632e-05,
      "loss": 0.2717,
      "step": 58
    },
    {
      "epoch": 0.13809245172615564,
      "grad_norm": 0.7044102549552917,
      "learning_rate": 4.145199063231849e-05,
      "loss": 0.2173,
      "step": 59
    },
    {
      "epoch": 0.1404330017554125,
      "grad_norm": 0.6403013467788696,
      "learning_rate": 4.215456674473067e-05,
      "loss": 0.2057,
      "step": 60
    },
    {
      "epoch": 0.14277355178466938,
      "grad_norm": 0.5501474738121033,
      "learning_rate": 4.285714285714285e-05,
      "loss": 0.2439,
      "step": 61
    },
    {
      "epoch": 0.14511410181392628,
      "grad_norm": 0.5609679818153381,
      "learning_rate": 4.355971896955503e-05,
      "loss": 0.2181,
      "step": 62
    },
    {
      "epoch": 0.14745465184318315,
      "grad_norm": 0.8103487491607666,
      "learning_rate": 4.4262295081967207e-05,
      "loss": 0.3035,
      "step": 63
    },
    {
      "epoch": 0.14979520187244003,
      "grad_norm": 0.48474717140197754,
      "learning_rate": 4.4964871194379385e-05,
      "loss": 0.2194,
      "step": 64
    },
    {
      "epoch": 0.1521357519016969,
      "grad_norm": 0.47984907031059265,
      "learning_rate": 4.5667447306791564e-05,
      "loss": 0.1586,
      "step": 65
    },
    {
      "epoch": 0.15447630193095377,
      "grad_norm": 0.7764332294464111,
      "learning_rate": 4.637002341920374e-05,
      "loss": 0.2337,
      "step": 66
    },
    {
      "epoch": 0.15681685196021064,
      "grad_norm": 0.6899380087852478,
      "learning_rate": 4.707259953161592e-05,
      "loss": 0.2247,
      "step": 67
    },
    {
      "epoch": 0.1591574019894675,
      "grad_norm": 0.43097320199012756,
      "learning_rate": 4.77751756440281e-05,
      "loss": 0.2053,
      "step": 68
    },
    {
      "epoch": 0.1614979520187244,
      "grad_norm": 0.6267377734184265,
      "learning_rate": 4.847775175644028e-05,
      "loss": 0.1672,
      "step": 69
    },
    {
      "epoch": 0.16383850204798128,
      "grad_norm": 0.4738757610321045,
      "learning_rate": 4.918032786885245e-05,
      "loss": 0.1321,
      "step": 70
    },
    {
      "epoch": 0.16617905207723815,
      "grad_norm": 0.42365261912345886,
      "learning_rate": 4.988290398126463e-05,
      "loss": 0.1729,
      "step": 71
    },
    {
      "epoch": 0.16851960210649503,
      "grad_norm": 0.528808057308197,
      "learning_rate": 5.0585480093676807e-05,
      "loss": 0.1647,
      "step": 72
    },
    {
      "epoch": 0.1708601521357519,
      "grad_norm": 0.48317185044288635,
      "learning_rate": 5.1288056206088985e-05,
      "loss": 0.199,
      "step": 73
    },
    {
      "epoch": 0.17320070216500877,
      "grad_norm": 0.5465429425239563,
      "learning_rate": 5.1990632318501164e-05,
      "loss": 0.171,
      "step": 74
    },
    {
      "epoch": 0.17554125219426564,
      "grad_norm": 0.5429360866546631,
      "learning_rate": 5.269320843091334e-05,
      "loss": 0.1856,
      "step": 75
    },
    {
      "epoch": 0.17788180222352254,
      "grad_norm": 0.5805875658988953,
      "learning_rate": 5.339578454332552e-05,
      "loss": 0.1152,
      "step": 76
    },
    {
      "epoch": 0.1802223522527794,
      "grad_norm": 0.6931231021881104,
      "learning_rate": 5.40983606557377e-05,
      "loss": 0.1812,
      "step": 77
    },
    {
      "epoch": 0.18256290228203628,
      "grad_norm": 0.7001753449440002,
      "learning_rate": 5.480093676814988e-05,
      "loss": 0.1885,
      "step": 78
    },
    {
      "epoch": 0.18490345231129315,
      "grad_norm": 0.3040752708911896,
      "learning_rate": 5.5503512880562056e-05,
      "loss": 0.1381,
      "step": 79
    },
    {
      "epoch": 0.18724400234055003,
      "grad_norm": 0.5345640182495117,
      "learning_rate": 5.6206088992974235e-05,
      "loss": 0.2054,
      "step": 80
    },
    {
      "epoch": 0.1895845523698069,
      "grad_norm": 0.3600844442844391,
      "learning_rate": 5.6908665105386407e-05,
      "loss": 0.146,
      "step": 81
    },
    {
      "epoch": 0.19192510239906377,
      "grad_norm": 0.5008469223976135,
      "learning_rate": 5.7611241217798585e-05,
      "loss": 0.1537,
      "step": 82
    },
    {
      "epoch": 0.19426565242832067,
      "grad_norm": 0.3625437617301941,
      "learning_rate": 5.8313817330210764e-05,
      "loss": 0.1377,
      "step": 83
    },
    {
      "epoch": 0.19660620245757754,
      "grad_norm": 0.352911114692688,
      "learning_rate": 5.901639344262294e-05,
      "loss": 0.0937,
      "step": 84
    },
    {
      "epoch": 0.1989467524868344,
      "grad_norm": 0.31406646966934204,
      "learning_rate": 5.971896955503512e-05,
      "loss": 0.1282,
      "step": 85
    },
    {
      "epoch": 0.20128730251609128,
      "grad_norm": 0.6330552101135254,
      "learning_rate": 6.04215456674473e-05,
      "loss": 0.1509,
      "step": 86
    },
    {
      "epoch": 0.20362785254534815,
      "grad_norm": 0.3801358640193939,
      "learning_rate": 6.112412177985948e-05,
      "loss": 0.1142,
      "step": 87
    },
    {
      "epoch": 0.20596840257460503,
      "grad_norm": 0.3311139941215515,
      "learning_rate": 6.182669789227166e-05,
      "loss": 0.1002,
      "step": 88
    },
    {
      "epoch": 0.2083089526038619,
      "grad_norm": 0.5925964117050171,
      "learning_rate": 6.252927400468383e-05,
      "loss": 0.2169,
      "step": 89
    },
    {
      "epoch": 0.2106495026331188,
      "grad_norm": 0.4133622646331787,
      "learning_rate": 6.323185011709601e-05,
      "loss": 0.089,
      "step": 90
    },
    {
      "epoch": 0.21299005266237567,
      "grad_norm": 0.2863861322402954,
      "learning_rate": 6.393442622950819e-05,
      "loss": 0.1125,
      "step": 91
    },
    {
      "epoch": 0.21533060269163254,
      "grad_norm": 0.2728464603424072,
      "learning_rate": 6.463700234192037e-05,
      "loss": 0.0841,
      "step": 92
    },
    {
      "epoch": 0.2176711527208894,
      "grad_norm": 0.34999775886535645,
      "learning_rate": 6.533957845433255e-05,
      "loss": 0.0908,
      "step": 93
    },
    {
      "epoch": 0.22001170275014628,
      "grad_norm": 0.3566703498363495,
      "learning_rate": 6.604215456674473e-05,
      "loss": 0.1058,
      "step": 94
    },
    {
      "epoch": 0.22235225277940315,
      "grad_norm": 0.47213098406791687,
      "learning_rate": 6.67447306791569e-05,
      "loss": 0.1502,
      "step": 95
    },
    {
      "epoch": 0.22469280280866002,
      "grad_norm": 0.3267618715763092,
      "learning_rate": 6.744730679156908e-05,
      "loss": 0.1079,
      "step": 96
    },
    {
      "epoch": 0.22703335283791692,
      "grad_norm": 0.44412410259246826,
      "learning_rate": 6.814988290398126e-05,
      "loss": 0.1204,
      "step": 97
    },
    {
      "epoch": 0.2293739028671738,
      "grad_norm": 0.38484975695610046,
      "learning_rate": 6.885245901639344e-05,
      "loss": 0.0866,
      "step": 98
    },
    {
      "epoch": 0.23171445289643067,
      "grad_norm": 0.2533586919307709,
      "learning_rate": 6.955503512880562e-05,
      "loss": 0.0901,
      "step": 99
    },
    {
      "epoch": 0.23405500292568754,
      "grad_norm": 0.319011390209198,
      "learning_rate": 7.02576112412178e-05,
      "loss": 0.1,
      "step": 100
    },
    {
      "epoch": 0.2363955529549444,
      "grad_norm": 0.3179778754711151,
      "learning_rate": 7.096018735362998e-05,
      "loss": 0.0827,
      "step": 101
    },
    {
      "epoch": 0.23873610298420128,
      "grad_norm": 0.28516703844070435,
      "learning_rate": 7.166276346604214e-05,
      "loss": 0.115,
      "step": 102
    },
    {
      "epoch": 0.24107665301345815,
      "grad_norm": 0.48119232058525085,
      "learning_rate": 7.236533957845432e-05,
      "loss": 0.1134,
      "step": 103
    },
    {
      "epoch": 0.24341720304271502,
      "grad_norm": 0.27381017804145813,
      "learning_rate": 7.30679156908665e-05,
      "loss": 0.0842,
      "step": 104
    },
    {
      "epoch": 0.24575775307197192,
      "grad_norm": 0.1980096697807312,
      "learning_rate": 7.377049180327868e-05,
      "loss": 0.0803,
      "step": 105
    },
    {
      "epoch": 0.2480983031012288,
      "grad_norm": 0.3699685037136078,
      "learning_rate": 7.447306791569086e-05,
      "loss": 0.1366,
      "step": 106
    },
    {
      "epoch": 0.25043885313048564,
      "grad_norm": 0.32814502716064453,
      "learning_rate": 7.517564402810303e-05,
      "loss": 0.1019,
      "step": 107
    },
    {
      "epoch": 0.25277940315974257,
      "grad_norm": 0.35988423228263855,
      "learning_rate": 7.587822014051521e-05,
      "loss": 0.091,
      "step": 108
    },
    {
      "epoch": 0.25511995318899944,
      "grad_norm": 0.6146625876426697,
      "learning_rate": 7.658079625292739e-05,
      "loss": 0.1279,
      "step": 109
    },
    {
      "epoch": 0.2574605032182563,
      "grad_norm": 0.3878476321697235,
      "learning_rate": 7.728337236533957e-05,
      "loss": 0.0762,
      "step": 110
    },
    {
      "epoch": 0.2598010532475132,
      "grad_norm": 0.24983437359333038,
      "learning_rate": 7.798594847775175e-05,
      "loss": 0.077,
      "step": 111
    },
    {
      "epoch": 0.26214160327677005,
      "grad_norm": 0.3886657655239105,
      "learning_rate": 7.868852459016393e-05,
      "loss": 0.0967,
      "step": 112
    },
    {
      "epoch": 0.2644821533060269,
      "grad_norm": 0.3602314293384552,
      "learning_rate": 7.93911007025761e-05,
      "loss": 0.1079,
      "step": 113
    },
    {
      "epoch": 0.2668227033352838,
      "grad_norm": 0.251139372587204,
      "learning_rate": 8.009367681498828e-05,
      "loss": 0.075,
      "step": 114
    },
    {
      "epoch": 0.26916325336454067,
      "grad_norm": 0.4059337377548218,
      "learning_rate": 8.079625292740046e-05,
      "loss": 0.0918,
      "step": 115
    },
    {
      "epoch": 0.27150380339379754,
      "grad_norm": 0.42691197991371155,
      "learning_rate": 8.149882903981264e-05,
      "loss": 0.1107,
      "step": 116
    },
    {
      "epoch": 0.2738443534230544,
      "grad_norm": 0.32998594641685486,
      "learning_rate": 8.220140515222482e-05,
      "loss": 0.1011,
      "step": 117
    },
    {
      "epoch": 0.2761849034523113,
      "grad_norm": 0.29636016488075256,
      "learning_rate": 8.290398126463698e-05,
      "loss": 0.0776,
      "step": 118
    },
    {
      "epoch": 0.27852545348156815,
      "grad_norm": 0.4008658528327942,
      "learning_rate": 8.360655737704916e-05,
      "loss": 0.0699,
      "step": 119
    },
    {
      "epoch": 0.280866003510825,
      "grad_norm": 0.18341483175754547,
      "learning_rate": 8.430913348946134e-05,
      "loss": 0.0793,
      "step": 120
    },
    {
      "epoch": 0.2832065535400819,
      "grad_norm": 0.27980518341064453,
      "learning_rate": 8.501170960187352e-05,
      "loss": 0.0572,
      "step": 121
    },
    {
      "epoch": 0.28554710356933877,
      "grad_norm": 0.34867483377456665,
      "learning_rate": 8.57142857142857e-05,
      "loss": 0.0976,
      "step": 122
    },
    {
      "epoch": 0.2878876535985957,
      "grad_norm": 0.33403703570365906,
      "learning_rate": 8.641686182669788e-05,
      "loss": 0.0841,
      "step": 123
    },
    {
      "epoch": 0.29022820362785257,
      "grad_norm": 0.2822224199771881,
      "learning_rate": 8.711943793911006e-05,
      "loss": 0.0693,
      "step": 124
    },
    {
      "epoch": 0.29256875365710944,
      "grad_norm": 0.29887306690216064,
      "learning_rate": 8.782201405152223e-05,
      "loss": 0.084,
      "step": 125
    },
    {
      "epoch": 0.2949093036863663,
      "grad_norm": 0.2903980016708374,
      "learning_rate": 8.852459016393441e-05,
      "loss": 0.0945,
      "step": 126
    },
    {
      "epoch": 0.2972498537156232,
      "grad_norm": 0.3849150538444519,
      "learning_rate": 8.922716627634659e-05,
      "loss": 0.0909,
      "step": 127
    },
    {
      "epoch": 0.29959040374488005,
      "grad_norm": 0.5351554751396179,
      "learning_rate": 8.992974238875877e-05,
      "loss": 0.1081,
      "step": 128
    },
    {
      "epoch": 0.3019309537741369,
      "grad_norm": 0.38454899191856384,
      "learning_rate": 9.063231850117095e-05,
      "loss": 0.0727,
      "step": 129
    },
    {
      "epoch": 0.3042715038033938,
      "grad_norm": 0.2441501021385193,
      "learning_rate": 9.133489461358313e-05,
      "loss": 0.083,
      "step": 130
    },
    {
      "epoch": 0.30661205383265067,
      "grad_norm": 0.2656939923763275,
      "learning_rate": 9.20374707259953e-05,
      "loss": 0.071,
      "step": 131
    },
    {
      "epoch": 0.30895260386190754,
      "grad_norm": 0.5120667219161987,
      "learning_rate": 9.274004683840748e-05,
      "loss": 0.1031,
      "step": 132
    },
    {
      "epoch": 0.3112931538911644,
      "grad_norm": 0.2987515330314636,
      "learning_rate": 9.344262295081966e-05,
      "loss": 0.0864,
      "step": 133
    },
    {
      "epoch": 0.3136337039204213,
      "grad_norm": 0.2838038504123688,
      "learning_rate": 9.414519906323184e-05,
      "loss": 0.0888,
      "step": 134
    },
    {
      "epoch": 0.31597425394967815,
      "grad_norm": 0.25136855244636536,
      "learning_rate": 9.484777517564402e-05,
      "loss": 0.0962,
      "step": 135
    },
    {
      "epoch": 0.318314803978935,
      "grad_norm": 0.4961239695549011,
      "learning_rate": 9.55503512880562e-05,
      "loss": 0.0964,
      "step": 136
    },
    {
      "epoch": 0.32065535400819195,
      "grad_norm": 0.283364862203598,
      "learning_rate": 9.625292740046838e-05,
      "loss": 0.0723,
      "step": 137
    },
    {
      "epoch": 0.3229959040374488,
      "grad_norm": 0.20299039781093597,
      "learning_rate": 9.695550351288056e-05,
      "loss": 0.071,
      "step": 138
    },
    {
      "epoch": 0.3253364540667057,
      "grad_norm": 0.4019401967525482,
      "learning_rate": 9.765807962529272e-05,
      "loss": 0.1027,
      "step": 139
    },
    {
      "epoch": 0.32767700409596257,
      "grad_norm": 0.46868786215782166,
      "learning_rate": 9.83606557377049e-05,
      "loss": 0.1215,
      "step": 140
    },
    {
      "epoch": 0.33001755412521944,
      "grad_norm": 0.15559057891368866,
      "learning_rate": 9.906323185011708e-05,
      "loss": 0.0633,
      "step": 141
    },
    {
      "epoch": 0.3323581041544763,
      "grad_norm": 0.19746778905391693,
      "learning_rate": 9.976580796252926e-05,
      "loss": 0.0617,
      "step": 142
    },
    {
      "epoch": 0.3346986541837332,
      "grad_norm": 0.2529307007789612,
      "learning_rate": 0.00010046838407494143,
      "loss": 0.0953,
      "step": 143
    },
    {
      "epoch": 0.33703920421299005,
      "grad_norm": 0.1324387490749359,
      "learning_rate": 0.00010117096018735361,
      "loss": 0.0744,
      "step": 144
    },
    {
      "epoch": 0.3393797542422469,
      "grad_norm": 0.2316831350326538,
      "learning_rate": 0.00010187353629976579,
      "loss": 0.0931,
      "step": 145
    },
    {
      "epoch": 0.3417203042715038,
      "grad_norm": 0.24604591727256775,
      "learning_rate": 0.00010257611241217797,
      "loss": 0.0771,
      "step": 146
    },
    {
      "epoch": 0.34406085430076067,
      "grad_norm": 0.19399812817573547,
      "learning_rate": 0.00010327868852459015,
      "loss": 0.0724,
      "step": 147
    },
    {
      "epoch": 0.34640140433001754,
      "grad_norm": 0.20025157928466797,
      "learning_rate": 0.00010398126463700233,
      "loss": 0.0761,
      "step": 148
    },
    {
      "epoch": 0.3487419543592744,
      "grad_norm": 0.3183451294898987,
      "learning_rate": 0.0001046838407494145,
      "loss": 0.0797,
      "step": 149
    },
    {
      "epoch": 0.3510825043885313,
      "grad_norm": 0.20719680190086365,
      "learning_rate": 0.00010538641686182668,
      "loss": 0.0754,
      "step": 150
    },
    {
      "epoch": 0.3534230544177882,
      "grad_norm": 0.1753503382205963,
      "learning_rate": 0.00010608899297423886,
      "loss": 0.0701,
      "step": 151
    },
    {
      "epoch": 0.3557636044470451,
      "grad_norm": 0.21561960875988007,
      "learning_rate": 0.00010679156908665104,
      "loss": 0.082,
      "step": 152
    },
    {
      "epoch": 0.35810415447630195,
      "grad_norm": 0.19424204528331757,
      "learning_rate": 0.00010749414519906322,
      "loss": 0.0796,
      "step": 153
    },
    {
      "epoch": 0.3604447045055588,
      "grad_norm": 0.252142071723938,
      "learning_rate": 0.0001081967213114754,
      "loss": 0.0948,
      "step": 154
    },
    {
      "epoch": 0.3627852545348157,
      "grad_norm": 0.29374071955680847,
      "learning_rate": 0.00010889929742388758,
      "loss": 0.0614,
      "step": 155
    },
    {
      "epoch": 0.36512580456407256,
      "grad_norm": 0.20252026617527008,
      "learning_rate": 0.00010960187353629976,
      "loss": 0.0886,
      "step": 156
    },
    {
      "epoch": 0.36746635459332944,
      "grad_norm": 0.16509486734867096,
      "learning_rate": 0.00011030444964871193,
      "loss": 0.0781,
      "step": 157
    },
    {
      "epoch": 0.3698069046225863,
      "grad_norm": 0.3144805133342743,
      "learning_rate": 0.00011100702576112411,
      "loss": 0.0998,
      "step": 158
    },
    {
      "epoch": 0.3721474546518432,
      "grad_norm": 0.2728705406188965,
      "learning_rate": 0.00011170960187353629,
      "loss": 0.0791,
      "step": 159
    },
    {
      "epoch": 0.37448800468110005,
      "grad_norm": 0.21588601171970367,
      "learning_rate": 0.00011241217798594847,
      "loss": 0.078,
      "step": 160
    },
    {
      "epoch": 0.3768285547103569,
      "grad_norm": 0.2361045479774475,
      "learning_rate": 0.00011311475409836063,
      "loss": 0.0627,
      "step": 161
    },
    {
      "epoch": 0.3791691047396138,
      "grad_norm": 0.20064666867256165,
      "learning_rate": 0.00011381733021077281,
      "loss": 0.0758,
      "step": 162
    },
    {
      "epoch": 0.38150965476887067,
      "grad_norm": 0.154964879155159,
      "learning_rate": 0.00011451990632318499,
      "loss": 0.0689,
      "step": 163
    },
    {
      "epoch": 0.38385020479812754,
      "grad_norm": 0.18385547399520874,
      "learning_rate": 0.00011522248243559717,
      "loss": 0.0747,
      "step": 164
    },
    {
      "epoch": 0.3861907548273844,
      "grad_norm": 0.2233102023601532,
      "learning_rate": 0.00011592505854800935,
      "loss": 0.0704,
      "step": 165
    },
    {
      "epoch": 0.38853130485664134,
      "grad_norm": 0.28615516424179077,
      "learning_rate": 0.00011662763466042153,
      "loss": 0.08,
      "step": 166
    },
    {
      "epoch": 0.3908718548858982,
      "grad_norm": 0.1823260337114334,
      "learning_rate": 0.0001173302107728337,
      "loss": 0.0676,
      "step": 167
    },
    {
      "epoch": 0.3932124049151551,
      "grad_norm": 0.21503931283950806,
      "learning_rate": 0.00011803278688524588,
      "loss": 0.062,
      "step": 168
    },
    {
      "epoch": 0.39555295494441195,
      "grad_norm": 0.18485671281814575,
      "learning_rate": 0.00011873536299765806,
      "loss": 0.0747,
      "step": 169
    },
    {
      "epoch": 0.3978935049736688,
      "grad_norm": 0.19762471318244934,
      "learning_rate": 0.00011943793911007024,
      "loss": 0.076,
      "step": 170
    },
    {
      "epoch": 0.4002340550029257,
      "grad_norm": 0.19211551547050476,
      "learning_rate": 0.00012014051522248242,
      "loss": 0.0641,
      "step": 171
    },
    {
      "epoch": 0.40257460503218256,
      "grad_norm": 0.4115981161594391,
      "learning_rate": 0.0001208430913348946,
      "loss": 0.0966,
      "step": 172
    },
    {
      "epoch": 0.40491515506143944,
      "grad_norm": 0.2455805093050003,
      "learning_rate": 0.00012154566744730678,
      "loss": 0.0769,
      "step": 173
    },
    {
      "epoch": 0.4072557050906963,
      "grad_norm": 0.18538784980773926,
      "learning_rate": 0.00012224824355971896,
      "loss": 0.0775,
      "step": 174
    },
    {
      "epoch": 0.4095962551199532,
      "grad_norm": 0.1927618533372879,
      "learning_rate": 0.00012295081967213115,
      "loss": 0.0806,
      "step": 175
    },
    {
      "epoch": 0.41193680514921005,
      "grad_norm": 0.2224532961845398,
      "learning_rate": 0.0001236533957845433,
      "loss": 0.0777,
      "step": 176
    },
    {
      "epoch": 0.4142773551784669,
      "grad_norm": 0.2520523965358734,
      "learning_rate": 0.0001243559718969555,
      "loss": 0.0504,
      "step": 177
    },
    {
      "epoch": 0.4166179052077238,
      "grad_norm": 0.17727448046207428,
      "learning_rate": 0.00012505854800936767,
      "loss": 0.085,
      "step": 178
    },
    {
      "epoch": 0.41895845523698066,
      "grad_norm": 0.16878554224967957,
      "learning_rate": 0.00012576112412177986,
      "loss": 0.0642,
      "step": 179
    },
    {
      "epoch": 0.4212990052662376,
      "grad_norm": 0.1806192547082901,
      "learning_rate": 0.00012646370023419203,
      "loss": 0.0661,
      "step": 180
    },
    {
      "epoch": 0.42363955529549446,
      "grad_norm": 0.21009641885757446,
      "learning_rate": 0.00012716627634660422,
      "loss": 0.0586,
      "step": 181
    },
    {
      "epoch": 0.42598010532475133,
      "grad_norm": 0.25836601853370667,
      "learning_rate": 0.00012786885245901638,
      "loss": 0.0841,
      "step": 182
    },
    {
      "epoch": 0.4283206553540082,
      "grad_norm": 0.30804330110549927,
      "learning_rate": 0.00012857142857142855,
      "loss": 0.0844,
      "step": 183
    },
    {
      "epoch": 0.4306612053832651,
      "grad_norm": 0.24137790501117706,
      "learning_rate": 0.00012927400468384074,
      "loss": 0.0825,
      "step": 184
    },
    {
      "epoch": 0.43300175541252195,
      "grad_norm": 0.26291191577911377,
      "learning_rate": 0.0001299765807962529,
      "loss": 0.0706,
      "step": 185
    },
    {
      "epoch": 0.4353423054417788,
      "grad_norm": 0.3730803430080414,
      "learning_rate": 0.0001306791569086651,
      "loss": 0.0959,
      "step": 186
    },
    {
      "epoch": 0.4376828554710357,
      "grad_norm": 0.321720689535141,
      "learning_rate": 0.00013138173302107726,
      "loss": 0.0773,
      "step": 187
    },
    {
      "epoch": 0.44002340550029256,
      "grad_norm": 0.18197616934776306,
      "learning_rate": 0.00013208430913348945,
      "loss": 0.0781,
      "step": 188
    },
    {
      "epoch": 0.44236395552954944,
      "grad_norm": 0.2036827802658081,
      "learning_rate": 0.00013278688524590162,
      "loss": 0.0663,
      "step": 189
    },
    {
      "epoch": 0.4447045055588063,
      "grad_norm": 0.17174863815307617,
      "learning_rate": 0.0001334894613583138,
      "loss": 0.0645,
      "step": 190
    },
    {
      "epoch": 0.4470450555880632,
      "grad_norm": 0.1785440742969513,
      "learning_rate": 0.00013419203747072598,
      "loss": 0.0535,
      "step": 191
    },
    {
      "epoch": 0.44938560561732005,
      "grad_norm": 0.24990825355052948,
      "learning_rate": 0.00013489461358313817,
      "loss": 0.0763,
      "step": 192
    },
    {
      "epoch": 0.4517261556465769,
      "grad_norm": 0.28782564401626587,
      "learning_rate": 0.00013559718969555033,
      "loss": 0.0786,
      "step": 193
    },
    {
      "epoch": 0.45406670567583385,
      "grad_norm": 0.19877685606479645,
      "learning_rate": 0.00013629976580796253,
      "loss": 0.0626,
      "step": 194
    },
    {
      "epoch": 0.4564072557050907,
      "grad_norm": 0.3738671839237213,
      "learning_rate": 0.0001370023419203747,
      "loss": 0.0906,
      "step": 195
    },
    {
      "epoch": 0.4587478057343476,
      "grad_norm": 0.3066193461418152,
      "learning_rate": 0.00013770491803278688,
      "loss": 0.061,
      "step": 196
    },
    {
      "epoch": 0.46108835576360446,
      "grad_norm": 0.3424912691116333,
      "learning_rate": 0.00013840749414519905,
      "loss": 0.1095,
      "step": 197
    },
    {
      "epoch": 0.46342890579286133,
      "grad_norm": 0.19527220726013184,
      "learning_rate": 0.00013911007025761124,
      "loss": 0.0728,
      "step": 198
    },
    {
      "epoch": 0.4657694558221182,
      "grad_norm": 0.19364385306835175,
      "learning_rate": 0.0001398126463700234,
      "loss": 0.0712,
      "step": 199
    },
    {
      "epoch": 0.4681100058513751,
      "grad_norm": 0.1860208362340927,
      "learning_rate": 0.0001405152224824356,
      "loss": 0.0828,
      "step": 200
    },
    {
      "epoch": 0.47045055588063195,
      "grad_norm": 0.31784841418266296,
      "learning_rate": 0.00014121779859484776,
      "loss": 0.0636,
      "step": 201
    },
    {
      "epoch": 0.4727911059098888,
      "grad_norm": 0.2579590380191803,
      "learning_rate": 0.00014192037470725995,
      "loss": 0.0759,
      "step": 202
    },
    {
      "epoch": 0.4751316559391457,
      "grad_norm": 0.14848017692565918,
      "learning_rate": 0.00014262295081967212,
      "loss": 0.0764,
      "step": 203
    },
    {
      "epoch": 0.47747220596840256,
      "grad_norm": 0.3108172118663788,
      "learning_rate": 0.00014332552693208428,
      "loss": 0.0919,
      "step": 204
    },
    {
      "epoch": 0.47981275599765943,
      "grad_norm": 0.1689160168170929,
      "learning_rate": 0.00014402810304449648,
      "loss": 0.0558,
      "step": 205
    },
    {
      "epoch": 0.4821533060269163,
      "grad_norm": 0.2060498744249344,
      "learning_rate": 0.00014473067915690864,
      "loss": 0.0823,
      "step": 206
    },
    {
      "epoch": 0.4844938560561732,
      "grad_norm": 0.2838783264160156,
      "learning_rate": 0.00014543325526932083,
      "loss": 0.0895,
      "step": 207
    },
    {
      "epoch": 0.48683440608543005,
      "grad_norm": 0.30478182435035706,
      "learning_rate": 0.000146135831381733,
      "loss": 0.0696,
      "step": 208
    },
    {
      "epoch": 0.489174956114687,
      "grad_norm": 0.3146803379058838,
      "learning_rate": 0.0001468384074941452,
      "loss": 0.0677,
      "step": 209
    },
    {
      "epoch": 0.49151550614394385,
      "grad_norm": 0.13045716285705566,
      "learning_rate": 0.00014754098360655736,
      "loss": 0.0692,
      "step": 210
    },
    {
      "epoch": 0.4938560561732007,
      "grad_norm": 0.21953783929347992,
      "learning_rate": 0.00014824355971896955,
      "loss": 0.08,
      "step": 211
    },
    {
      "epoch": 0.4961966062024576,
      "grad_norm": 0.2485659271478653,
      "learning_rate": 0.0001489461358313817,
      "loss": 0.0618,
      "step": 212
    },
    {
      "epoch": 0.49853715623171446,
      "grad_norm": 0.26901891827583313,
      "learning_rate": 0.0001496487119437939,
      "loss": 0.0726,
      "step": 213
    },
    {
      "epoch": 0.5008777062609713,
      "grad_norm": 0.281956285238266,
      "learning_rate": 0.00015035128805620607,
      "loss": 0.0737,
      "step": 214
    },
    {
      "epoch": 0.5032182562902282,
      "grad_norm": 0.36899837851524353,
      "learning_rate": 0.00015105386416861826,
      "loss": 0.1069,
      "step": 215
    },
    {
      "epoch": 0.5055588063194851,
      "grad_norm": 0.2139657586812973,
      "learning_rate": 0.00015175644028103043,
      "loss": 0.0671,
      "step": 216
    },
    {
      "epoch": 0.507899356348742,
      "grad_norm": 0.1449689120054245,
      "learning_rate": 0.00015245901639344262,
      "loss": 0.0627,
      "step": 217
    },
    {
      "epoch": 0.5102399063779989,
      "grad_norm": 0.1750330626964569,
      "learning_rate": 0.00015316159250585478,
      "loss": 0.0748,
      "step": 218
    },
    {
      "epoch": 0.5125804564072557,
      "grad_norm": 0.22593972086906433,
      "learning_rate": 0.00015386416861826698,
      "loss": 0.0801,
      "step": 219
    },
    {
      "epoch": 0.5149210064365126,
      "grad_norm": 0.2595554292201996,
      "learning_rate": 0.00015456674473067914,
      "loss": 0.0859,
      "step": 220
    },
    {
      "epoch": 0.5172615564657694,
      "grad_norm": 0.16140402853488922,
      "learning_rate": 0.00015526932084309133,
      "loss": 0.0706,
      "step": 221
    },
    {
      "epoch": 0.5196021064950264,
      "grad_norm": 0.1990547925233841,
      "learning_rate": 0.0001559718969555035,
      "loss": 0.0811,
      "step": 222
    },
    {
      "epoch": 0.5219426565242832,
      "grad_norm": 0.20919346809387207,
      "learning_rate": 0.0001566744730679157,
      "loss": 0.0652,
      "step": 223
    },
    {
      "epoch": 0.5242832065535401,
      "grad_norm": 0.23766092956066132,
      "learning_rate": 0.00015737704918032785,
      "loss": 0.074,
      "step": 224
    },
    {
      "epoch": 0.5266237565827969,
      "grad_norm": 0.22473961114883423,
      "learning_rate": 0.00015807962529274005,
      "loss": 0.1002,
      "step": 225
    },
    {
      "epoch": 0.5289643066120538,
      "grad_norm": 0.15511323511600494,
      "learning_rate": 0.0001587822014051522,
      "loss": 0.0713,
      "step": 226
    },
    {
      "epoch": 0.5313048566413107,
      "grad_norm": 0.21672052145004272,
      "learning_rate": 0.0001594847775175644,
      "loss": 0.0669,
      "step": 227
    },
    {
      "epoch": 0.5336454066705676,
      "grad_norm": 0.3708781898021698,
      "learning_rate": 0.00016018735362997657,
      "loss": 0.1022,
      "step": 228
    },
    {
      "epoch": 0.5359859566998244,
      "grad_norm": 0.1727592945098877,
      "learning_rate": 0.00016088992974238876,
      "loss": 0.0754,
      "step": 229
    },
    {
      "epoch": 0.5383265067290813,
      "grad_norm": 0.2390577495098114,
      "learning_rate": 0.00016159250585480093,
      "loss": 0.0685,
      "step": 230
    },
    {
      "epoch": 0.5406670567583383,
      "grad_norm": 0.18726381659507751,
      "learning_rate": 0.00016229508196721312,
      "loss": 0.0887,
      "step": 231
    },
    {
      "epoch": 0.5430076067875951,
      "grad_norm": 0.21425588428974152,
      "learning_rate": 0.00016299765807962528,
      "loss": 0.0605,
      "step": 232
    },
    {
      "epoch": 0.545348156816852,
      "grad_norm": 0.19504137337207794,
      "learning_rate": 0.00016370023419203747,
      "loss": 0.0707,
      "step": 233
    },
    {
      "epoch": 0.5476887068461088,
      "grad_norm": 0.17225967347621918,
      "learning_rate": 0.00016440281030444964,
      "loss": 0.0605,
      "step": 234
    },
    {
      "epoch": 0.5500292568753657,
      "grad_norm": 0.21790747344493866,
      "learning_rate": 0.0001651053864168618,
      "loss": 0.0711,
      "step": 235
    },
    {
      "epoch": 0.5523698069046226,
      "grad_norm": 0.26764345169067383,
      "learning_rate": 0.00016580796252927397,
      "loss": 0.1023,
      "step": 236
    },
    {
      "epoch": 0.5547103569338795,
      "grad_norm": 0.2107781618833542,
      "learning_rate": 0.00016651053864168616,
      "loss": 0.0705,
      "step": 237
    },
    {
      "epoch": 0.5570509069631363,
      "grad_norm": 0.2053374946117401,
      "learning_rate": 0.00016721311475409833,
      "loss": 0.0794,
      "step": 238
    },
    {
      "epoch": 0.5593914569923932,
      "grad_norm": 0.2233268767595291,
      "learning_rate": 0.00016791569086651052,
      "loss": 0.0797,
      "step": 239
    },
    {
      "epoch": 0.56173200702165,
      "grad_norm": 0.22051338851451874,
      "learning_rate": 0.00016861826697892268,
      "loss": 0.0467,
      "step": 240
    },
    {
      "epoch": 0.564072557050907,
      "grad_norm": 0.23496823012828827,
      "learning_rate": 0.00016932084309133488,
      "loss": 0.065,
      "step": 241
    },
    {
      "epoch": 0.5664131070801638,
      "grad_norm": 0.2410179078578949,
      "learning_rate": 0.00017002341920374704,
      "loss": 0.0536,
      "step": 242
    },
    {
      "epoch": 0.5687536571094207,
      "grad_norm": 0.19473768770694733,
      "learning_rate": 0.00017072599531615923,
      "loss": 0.0653,
      "step": 243
    },
    {
      "epoch": 0.5710942071386775,
      "grad_norm": 0.2748405635356903,
      "learning_rate": 0.0001714285714285714,
      "loss": 0.0738,
      "step": 244
    },
    {
      "epoch": 0.5734347571679345,
      "grad_norm": 0.21368643641471863,
      "learning_rate": 0.0001721311475409836,
      "loss": 0.071,
      "step": 245
    },
    {
      "epoch": 0.5757753071971914,
      "grad_norm": 0.18811295926570892,
      "learning_rate": 0.00017283372365339576,
      "loss": 0.0624,
      "step": 246
    },
    {
      "epoch": 0.5781158572264482,
      "grad_norm": 0.23751351237297058,
      "learning_rate": 0.00017353629976580795,
      "loss": 0.0533,
      "step": 247
    },
    {
      "epoch": 0.5804564072557051,
      "grad_norm": 0.21942152082920074,
      "learning_rate": 0.0001742388758782201,
      "loss": 0.0739,
      "step": 248
    },
    {
      "epoch": 0.582796957284962,
      "grad_norm": 0.2765601575374603,
      "learning_rate": 0.0001749414519906323,
      "loss": 0.1023,
      "step": 249
    },
    {
      "epoch": 0.5851375073142189,
      "grad_norm": 0.22082635760307312,
      "learning_rate": 0.00017564402810304447,
      "loss": 0.0677,
      "step": 250
    },
    {
      "epoch": 0.5874780573434757,
      "grad_norm": 0.2716074287891388,
      "learning_rate": 0.00017634660421545666,
      "loss": 0.0805,
      "step": 251
    },
    {
      "epoch": 0.5898186073727326,
      "grad_norm": 0.24191153049468994,
      "learning_rate": 0.00017704918032786883,
      "loss": 0.0713,
      "step": 252
    },
    {
      "epoch": 0.5921591574019894,
      "grad_norm": 0.32269537448883057,
      "learning_rate": 0.00017775175644028102,
      "loss": 0.0726,
      "step": 253
    },
    {
      "epoch": 0.5944997074312464,
      "grad_norm": 0.23759518563747406,
      "learning_rate": 0.00017845433255269318,
      "loss": 0.0673,
      "step": 254
    },
    {
      "epoch": 0.5968402574605032,
      "grad_norm": 0.3744170367717743,
      "learning_rate": 0.00017915690866510538,
      "loss": 0.0443,
      "step": 255
    },
    {
      "epoch": 0.5991808074897601,
      "grad_norm": 0.2493986338376999,
      "learning_rate": 0.00017985948477751754,
      "loss": 0.0727,
      "step": 256
    },
    {
      "epoch": 0.6015213575190169,
      "grad_norm": 0.21430931985378265,
      "learning_rate": 0.00018056206088992973,
      "loss": 0.0698,
      "step": 257
    },
    {
      "epoch": 0.6038619075482738,
      "grad_norm": 0.3089439272880554,
      "learning_rate": 0.0001812646370023419,
      "loss": 0.0767,
      "step": 258
    },
    {
      "epoch": 0.6062024575775308,
      "grad_norm": 0.23547092080116272,
      "learning_rate": 0.0001819672131147541,
      "loss": 0.0678,
      "step": 259
    },
    {
      "epoch": 0.6085430076067876,
      "grad_norm": 0.2941837012767792,
      "learning_rate": 0.00018266978922716625,
      "loss": 0.084,
      "step": 260
    },
    {
      "epoch": 0.6108835576360445,
      "grad_norm": 0.3240150809288025,
      "learning_rate": 0.00018337236533957845,
      "loss": 0.0759,
      "step": 261
    },
    {
      "epoch": 0.6132241076653013,
      "grad_norm": 0.3140295445919037,
      "learning_rate": 0.0001840749414519906,
      "loss": 0.0971,
      "step": 262
    },
    {
      "epoch": 0.6155646576945583,
      "grad_norm": 0.2554670572280884,
      "learning_rate": 0.0001847775175644028,
      "loss": 0.0882,
      "step": 263
    },
    {
      "epoch": 0.6179052077238151,
      "grad_norm": 0.21980534493923187,
      "learning_rate": 0.00018548009367681497,
      "loss": 0.0608,
      "step": 264
    },
    {
      "epoch": 0.620245757753072,
      "grad_norm": 0.22742238640785217,
      "learning_rate": 0.00018618266978922716,
      "loss": 0.0667,
      "step": 265
    },
    {
      "epoch": 0.6225863077823288,
      "grad_norm": 0.32373377680778503,
      "learning_rate": 0.00018688524590163933,
      "loss": 0.0706,
      "step": 266
    },
    {
      "epoch": 0.6249268578115857,
      "grad_norm": 0.2636866867542267,
      "learning_rate": 0.00018758782201405152,
      "loss": 0.0663,
      "step": 267
    },
    {
      "epoch": 0.6272674078408426,
      "grad_norm": 0.1987038254737854,
      "learning_rate": 0.00018829039812646368,
      "loss": 0.0873,
      "step": 268
    },
    {
      "epoch": 0.6296079578700995,
      "grad_norm": 0.2958182692527771,
      "learning_rate": 0.00018899297423887587,
      "loss": 0.0879,
      "step": 269
    },
    {
      "epoch": 0.6319485078993563,
      "grad_norm": 0.32855695486068726,
      "learning_rate": 0.00018969555035128804,
      "loss": 0.0858,
      "step": 270
    },
    {
      "epoch": 0.6342890579286132,
      "grad_norm": 0.6748530268669128,
      "learning_rate": 0.00019039812646370023,
      "loss": 0.0667,
      "step": 271
    },
    {
      "epoch": 0.63662960795787,
      "grad_norm": 0.20581403374671936,
      "learning_rate": 0.0001911007025761124,
      "loss": 0.0624,
      "step": 272
    },
    {
      "epoch": 0.638970157987127,
      "grad_norm": 0.27415838837623596,
      "learning_rate": 0.0001918032786885246,
      "loss": 0.0825,
      "step": 273
    },
    {
      "epoch": 0.6413107080163839,
      "grad_norm": 0.1837451457977295,
      "learning_rate": 0.00019250585480093675,
      "loss": 0.0639,
      "step": 274
    },
    {
      "epoch": 0.6436512580456407,
      "grad_norm": 0.22457842528820038,
      "learning_rate": 0.00019320843091334895,
      "loss": 0.0868,
      "step": 275
    },
    {
      "epoch": 0.6459918080748976,
      "grad_norm": 0.22788214683532715,
      "learning_rate": 0.0001939110070257611,
      "loss": 0.0772,
      "step": 276
    },
    {
      "epoch": 0.6483323581041545,
      "grad_norm": 0.2117481380701065,
      "learning_rate": 0.0001946135831381733,
      "loss": 0.0678,
      "step": 277
    },
    {
      "epoch": 0.6506729081334114,
      "grad_norm": 0.3039746880531311,
      "learning_rate": 0.00019531615925058544,
      "loss": 0.0792,
      "step": 278
    },
    {
      "epoch": 0.6530134581626682,
      "grad_norm": 0.2437116801738739,
      "learning_rate": 0.00019601873536299763,
      "loss": 0.0849,
      "step": 279
    },
    {
      "epoch": 0.6553540081919251,
      "grad_norm": 0.21204185485839844,
      "learning_rate": 0.0001967213114754098,
      "loss": 0.0638,
      "step": 280
    },
    {
      "epoch": 0.657694558221182,
      "grad_norm": 0.24784302711486816,
      "learning_rate": 0.000197423887587822,
      "loss": 0.0677,
      "step": 281
    },
    {
      "epoch": 0.6600351082504389,
      "grad_norm": 0.16380243003368378,
      "learning_rate": 0.00019812646370023416,
      "loss": 0.0639,
      "step": 282
    },
    {
      "epoch": 0.6623756582796957,
      "grad_norm": 0.1933191418647766,
      "learning_rate": 0.00019882903981264635,
      "loss": 0.0485,
      "step": 283
    },
    {
      "epoch": 0.6647162083089526,
      "grad_norm": 0.23233751952648163,
      "learning_rate": 0.0001995316159250585,
      "loss": 0.074,
      "step": 284
    },
    {
      "epoch": 0.6670567583382094,
      "grad_norm": 0.232991561293602,
      "learning_rate": 0.0002002341920374707,
      "loss": 0.0805,
      "step": 285
    },
    {
      "epoch": 0.6693973083674664,
      "grad_norm": 0.23193806409835815,
      "learning_rate": 0.00020093676814988287,
      "loss": 0.0828,
      "step": 286
    },
    {
      "epoch": 0.6717378583967232,
      "grad_norm": 0.19885744154453278,
      "learning_rate": 0.00020163934426229506,
      "loss": 0.0466,
      "step": 287
    },
    {
      "epoch": 0.6740784084259801,
      "grad_norm": 0.20519854128360748,
      "learning_rate": 0.00020234192037470723,
      "loss": 0.0871,
      "step": 288
    },
    {
      "epoch": 0.676418958455237,
      "grad_norm": 0.22914551198482513,
      "learning_rate": 0.00020304449648711942,
      "loss": 0.0653,
      "step": 289
    },
    {
      "epoch": 0.6787595084844938,
      "grad_norm": 0.213272362947464,
      "learning_rate": 0.00020374707259953158,
      "loss": 0.0669,
      "step": 290
    },
    {
      "epoch": 0.6811000585137508,
      "grad_norm": 0.560653030872345,
      "learning_rate": 0.00020444964871194378,
      "loss": 0.0897,
      "step": 291
    },
    {
      "epoch": 0.6834406085430076,
      "grad_norm": 0.2130347341299057,
      "learning_rate": 0.00020515222482435594,
      "loss": 0.0552,
      "step": 292
    },
    {
      "epoch": 0.6857811585722645,
      "grad_norm": 0.17549578845500946,
      "learning_rate": 0.00020585480093676813,
      "loss": 0.0383,
      "step": 293
    },
    {
      "epoch": 0.6881217086015213,
      "grad_norm": 0.582098126411438,
      "learning_rate": 0.0002065573770491803,
      "loss": 0.0652,
      "step": 294
    },
    {
      "epoch": 0.6904622586307783,
      "grad_norm": 0.4167969822883606,
      "learning_rate": 0.0002072599531615925,
      "loss": 0.0771,
      "step": 295
    },
    {
      "epoch": 0.6928028086600351,
      "grad_norm": 0.6171346306800842,
      "learning_rate": 0.00020796252927400465,
      "loss": 0.0599,
      "step": 296
    },
    {
      "epoch": 0.695143358689292,
      "grad_norm": 0.21384520828723907,
      "learning_rate": 0.00020866510538641685,
      "loss": 0.062,
      "step": 297
    },
    {
      "epoch": 0.6974839087185488,
      "grad_norm": 0.3257471024990082,
      "learning_rate": 0.000209367681498829,
      "loss": 0.0908,
      "step": 298
    },
    {
      "epoch": 0.6998244587478057,
      "grad_norm": 0.22626179456710815,
      "learning_rate": 0.0002100702576112412,
      "loss": 0.0716,
      "step": 299
    },
    {
      "epoch": 0.7021650087770626,
      "grad_norm": 0.14025384187698364,
      "learning_rate": 0.00021077283372365337,
      "loss": 0.0652,
      "step": 300
    },
    {
      "epoch": 0.7045055588063195,
      "grad_norm": 0.2956196069717407,
      "learning_rate": 0.00021147540983606556,
      "loss": 0.0646,
      "step": 301
    },
    {
      "epoch": 0.7068461088355764,
      "grad_norm": 0.3854401409626007,
      "learning_rate": 0.00021217798594847773,
      "loss": 0.0896,
      "step": 302
    },
    {
      "epoch": 0.7091866588648332,
      "grad_norm": 0.3025522232055664,
      "learning_rate": 0.00021288056206088992,
      "loss": 0.0723,
      "step": 303
    },
    {
      "epoch": 0.7115272088940902,
      "grad_norm": 0.5821725130081177,
      "learning_rate": 0.00021358313817330208,
      "loss": 0.0748,
      "step": 304
    },
    {
      "epoch": 0.713867758923347,
      "grad_norm": 0.16930797696113586,
      "learning_rate": 0.00021428571428571427,
      "loss": 0.0552,
      "step": 305
    },
    {
      "epoch": 0.7162083089526039,
      "grad_norm": 0.7655242681503296,
      "learning_rate": 0.00021498829039812644,
      "loss": 0.0797,
      "step": 306
    },
    {
      "epoch": 0.7185488589818607,
      "grad_norm": 0.1744079440832138,
      "learning_rate": 0.00021569086651053863,
      "loss": 0.0513,
      "step": 307
    },
    {
      "epoch": 0.7208894090111176,
      "grad_norm": 0.20807258784770966,
      "learning_rate": 0.0002163934426229508,
      "loss": 0.0577,
      "step": 308
    },
    {
      "epoch": 0.7232299590403745,
      "grad_norm": 0.1893397569656372,
      "learning_rate": 0.000217096018735363,
      "loss": 0.0613,
      "step": 309
    },
    {
      "epoch": 0.7255705090696314,
      "grad_norm": 0.2590862810611725,
      "learning_rate": 0.00021779859484777515,
      "loss": 0.064,
      "step": 310
    },
    {
      "epoch": 0.7279110590988882,
      "grad_norm": 0.25303399562835693,
      "learning_rate": 0.00021850117096018735,
      "loss": 0.0756,
      "step": 311
    },
    {
      "epoch": 0.7302516091281451,
      "grad_norm": 0.3389957547187805,
      "learning_rate": 0.0002192037470725995,
      "loss": 0.0659,
      "step": 312
    },
    {
      "epoch": 0.732592159157402,
      "grad_norm": 0.1869984269142151,
      "learning_rate": 0.0002199063231850117,
      "loss": 0.0386,
      "step": 313
    },
    {
      "epoch": 0.7349327091866589,
      "grad_norm": 0.20390556752681732,
      "learning_rate": 0.00022060889929742387,
      "loss": 0.0682,
      "step": 314
    },
    {
      "epoch": 0.7372732592159157,
      "grad_norm": 0.26977789402008057,
      "learning_rate": 0.00022131147540983606,
      "loss": 0.0637,
      "step": 315
    },
    {
      "epoch": 0.7396138092451726,
      "grad_norm": 0.21662698686122894,
      "learning_rate": 0.00022201405152224822,
      "loss": 0.0457,
      "step": 316
    },
    {
      "epoch": 0.7419543592744295,
      "grad_norm": 0.24263028800487518,
      "learning_rate": 0.00022271662763466042,
      "loss": 0.0596,
      "step": 317
    },
    {
      "epoch": 0.7442949093036864,
      "grad_norm": 0.2350638210773468,
      "learning_rate": 0.00022341920374707258,
      "loss": 0.0619,
      "step": 318
    },
    {
      "epoch": 0.7466354593329433,
      "grad_norm": 0.2805100083351135,
      "learning_rate": 0.00022412177985948477,
      "loss": 0.0741,
      "step": 319
    },
    {
      "epoch": 0.7489760093622001,
      "grad_norm": 0.31344282627105713,
      "learning_rate": 0.00022482435597189694,
      "loss": 0.0647,
      "step": 320
    },
    {
      "epoch": 0.751316559391457,
      "grad_norm": 0.188757061958313,
      "learning_rate": 0.0002255269320843091,
      "loss": 0.0729,
      "step": 321
    },
    {
      "epoch": 0.7536571094207138,
      "grad_norm": 0.21561269462108612,
      "learning_rate": 0.00022622950819672127,
      "loss": 0.0597,
      "step": 322
    },
    {
      "epoch": 0.7559976594499708,
      "grad_norm": 0.1844445914030075,
      "learning_rate": 0.00022693208430913346,
      "loss": 0.0692,
      "step": 323
    },
    {
      "epoch": 0.7583382094792276,
      "grad_norm": 1.4257322549819946,
      "learning_rate": 0.00022763466042154563,
      "loss": 0.0962,
      "step": 324
    },
    {
      "epoch": 0.7606787595084845,
      "grad_norm": 0.4255724847316742,
      "learning_rate": 0.00022833723653395782,
      "loss": 0.0691,
      "step": 325
    },
    {
      "epoch": 0.7630193095377413,
      "grad_norm": 0.2605849802494049,
      "learning_rate": 0.00022903981264636998,
      "loss": 0.0662,
      "step": 326
    },
    {
      "epoch": 0.7653598595669983,
      "grad_norm": 0.9783930778503418,
      "learning_rate": 0.00022974238875878218,
      "loss": 0.0762,
      "step": 327
    },
    {
      "epoch": 0.7677004095962551,
      "grad_norm": 0.1930849850177765,
      "learning_rate": 0.00023044496487119434,
      "loss": 0.0453,
      "step": 328
    },
    {
      "epoch": 0.770040959625512,
      "grad_norm": 0.44910112023353577,
      "learning_rate": 0.00023114754098360653,
      "loss": 0.0735,
      "step": 329
    },
    {
      "epoch": 0.7723815096547688,
      "grad_norm": 0.25639790296554565,
      "learning_rate": 0.0002318501170960187,
      "loss": 0.0821,
      "step": 330
    },
    {
      "epoch": 0.7747220596840257,
      "grad_norm": 0.43869125843048096,
      "learning_rate": 0.0002325526932084309,
      "loss": 0.0852,
      "step": 331
    },
    {
      "epoch": 0.7770626097132827,
      "grad_norm": 0.3334386348724365,
      "learning_rate": 0.00023325526932084305,
      "loss": 0.072,
      "step": 332
    },
    {
      "epoch": 0.7794031597425395,
      "grad_norm": 0.37717607617378235,
      "learning_rate": 0.00023395784543325525,
      "loss": 0.079,
      "step": 333
    },
    {
      "epoch": 0.7817437097717964,
      "grad_norm": 0.16471698880195618,
      "learning_rate": 0.0002346604215456674,
      "loss": 0.068,
      "step": 334
    },
    {
      "epoch": 0.7840842598010532,
      "grad_norm": 0.16776888072490692,
      "learning_rate": 0.0002353629976580796,
      "loss": 0.0513,
      "step": 335
    },
    {
      "epoch": 0.7864248098303102,
      "grad_norm": 0.1927589327096939,
      "learning_rate": 0.00023606557377049177,
      "loss": 0.0554,
      "step": 336
    },
    {
      "epoch": 0.788765359859567,
      "grad_norm": 0.1969420164823532,
      "learning_rate": 0.00023676814988290396,
      "loss": 0.0652,
      "step": 337
    },
    {
      "epoch": 0.7911059098888239,
      "grad_norm": 0.16639165580272675,
      "learning_rate": 0.00023747072599531613,
      "loss": 0.0689,
      "step": 338
    },
    {
      "epoch": 0.7934464599180807,
      "grad_norm": 0.21252906322479248,
      "learning_rate": 0.00023817330210772832,
      "loss": 0.0528,
      "step": 339
    },
    {
      "epoch": 0.7957870099473376,
      "grad_norm": 0.16237708926200867,
      "learning_rate": 0.00023887587822014048,
      "loss": 0.0469,
      "step": 340
    },
    {
      "epoch": 0.7981275599765945,
      "grad_norm": 0.19586141407489777,
      "learning_rate": 0.00023957845433255267,
      "loss": 0.0472,
      "step": 341
    },
    {
      "epoch": 0.8004681100058514,
      "grad_norm": 0.24335479736328125,
      "learning_rate": 0.00024028103044496484,
      "loss": 0.0727,
      "step": 342
    },
    {
      "epoch": 0.8028086600351082,
      "grad_norm": 0.2388763278722763,
      "learning_rate": 0.00024098360655737703,
      "loss": 0.0725,
      "step": 343
    },
    {
      "epoch": 0.8051492100643651,
      "grad_norm": 0.22003667056560516,
      "learning_rate": 0.0002416861826697892,
      "loss": 0.0651,
      "step": 344
    },
    {
      "epoch": 0.807489760093622,
      "grad_norm": 0.36706647276878357,
      "learning_rate": 0.0002423887587822014,
      "loss": 0.0875,
      "step": 345
    },
    {
      "epoch": 0.8098303101228789,
      "grad_norm": 0.367848664522171,
      "learning_rate": 0.00024309133489461355,
      "loss": 0.0616,
      "step": 346
    },
    {
      "epoch": 0.8121708601521358,
      "grad_norm": 0.2263612598180771,
      "learning_rate": 0.00024379391100702575,
      "loss": 0.0459,
      "step": 347
    },
    {
      "epoch": 0.8145114101813926,
      "grad_norm": 0.25099191069602966,
      "learning_rate": 0.0002444964871194379,
      "loss": 0.0515,
      "step": 348
    },
    {
      "epoch": 0.8168519602106495,
      "grad_norm": 0.19903573393821716,
      "learning_rate": 0.0002451990632318501,
      "loss": 0.0518,
      "step": 349
    },
    {
      "epoch": 0.8191925102399064,
      "grad_norm": 0.21911898255348206,
      "learning_rate": 0.0002459016393442623,
      "loss": 0.0559,
      "step": 350
    },
    {
      "epoch": 0.8215330602691633,
      "grad_norm": 0.4175863564014435,
      "learning_rate": 0.00024660421545667446,
      "loss": 0.0359,
      "step": 351
    },
    {
      "epoch": 0.8238736102984201,
      "grad_norm": 0.4428240954875946,
      "learning_rate": 0.0002473067915690866,
      "loss": 0.0874,
      "step": 352
    },
    {
      "epoch": 0.826214160327677,
      "grad_norm": 0.17041397094726562,
      "learning_rate": 0.0002480093676814988,
      "loss": 0.0497,
      "step": 353
    },
    {
      "epoch": 0.8285547103569338,
      "grad_norm": 0.23627832531929016,
      "learning_rate": 0.000248711943793911,
      "loss": 0.0818,
      "step": 354
    },
    {
      "epoch": 0.8308952603861908,
      "grad_norm": 0.18898212909698486,
      "learning_rate": 0.0002494145199063232,
      "loss": 0.0657,
      "step": 355
    },
    {
      "epoch": 0.8332358104154476,
      "grad_norm": 0.1738288253545761,
      "learning_rate": 0.00025011709601873534,
      "loss": 0.061,
      "step": 356
    },
    {
      "epoch": 0.8355763604447045,
      "grad_norm": 0.14756959676742554,
      "learning_rate": 0.00025081967213114756,
      "loss": 0.0389,
      "step": 357
    },
    {
      "epoch": 0.8379169104739613,
      "grad_norm": 0.1323087066411972,
      "learning_rate": 0.0002515222482435597,
      "loss": 0.0471,
      "step": 358
    },
    {
      "epoch": 0.8402574605032183,
      "grad_norm": 0.14647893607616425,
      "learning_rate": 0.0002522248243559719,
      "loss": 0.0649,
      "step": 359
    },
    {
      "epoch": 0.8425980105324752,
      "grad_norm": 0.16163772344589233,
      "learning_rate": 0.00025292740046838405,
      "loss": 0.0538,
      "step": 360
    },
    {
      "epoch": 0.844938560561732,
      "grad_norm": 0.19905145466327667,
      "learning_rate": 0.00025362997658079627,
      "loss": 0.0601,
      "step": 361
    },
    {
      "epoch": 0.8472791105909889,
      "grad_norm": 0.17620912194252014,
      "learning_rate": 0.00025433255269320844,
      "loss": 0.0523,
      "step": 362
    },
    {
      "epoch": 0.8496196606202457,
      "grad_norm": 0.17878329753875732,
      "learning_rate": 0.00025503512880562055,
      "loss": 0.0426,
      "step": 363
    },
    {
      "epoch": 0.8519602106495027,
      "grad_norm": 0.24285969138145447,
      "learning_rate": 0.00025573770491803277,
      "loss": 0.0721,
      "step": 364
    },
    {
      "epoch": 0.8543007606787595,
      "grad_norm": 0.33081358671188354,
      "learning_rate": 0.00025644028103044493,
      "loss": 0.0948,
      "step": 365
    },
    {
      "epoch": 0.8566413107080164,
      "grad_norm": 0.24232664704322815,
      "learning_rate": 0.0002571428571428571,
      "loss": 0.0641,
      "step": 366
    },
    {
      "epoch": 0.8589818607372732,
      "grad_norm": 0.32038605213165283,
      "learning_rate": 0.00025784543325526926,
      "loss": 0.0863,
      "step": 367
    },
    {
      "epoch": 0.8613224107665302,
      "grad_norm": 0.1793597936630249,
      "learning_rate": 0.0002585480093676815,
      "loss": 0.0504,
      "step": 368
    },
    {
      "epoch": 0.863662960795787,
      "grad_norm": 0.1952175348997116,
      "learning_rate": 0.00025925058548009365,
      "loss": 0.0543,
      "step": 369
    },
    {
      "epoch": 0.8660035108250439,
      "grad_norm": 0.3039569556713104,
      "learning_rate": 0.0002599531615925058,
      "loss": 0.0565,
      "step": 370
    },
    {
      "epoch": 0.8683440608543007,
      "grad_norm": 0.21828269958496094,
      "learning_rate": 0.000260655737704918,
      "loss": 0.059,
      "step": 371
    },
    {
      "epoch": 0.8706846108835576,
      "grad_norm": 0.21422646939754486,
      "learning_rate": 0.0002613583138173302,
      "loss": 0.0788,
      "step": 372
    },
    {
      "epoch": 0.8730251609128145,
      "grad_norm": 0.7427377104759216,
      "learning_rate": 0.00026206088992974236,
      "loss": 0.0471,
      "step": 373
    },
    {
      "epoch": 0.8753657109420714,
      "grad_norm": 0.7765772342681885,
      "learning_rate": 0.0002627634660421545,
      "loss": 0.0527,
      "step": 374
    },
    {
      "epoch": 0.8777062609713283,
      "grad_norm": 0.1810406744480133,
      "learning_rate": 0.0002634660421545667,
      "loss": 0.0447,
      "step": 375
    },
    {
      "epoch": 0.8800468110005851,
      "grad_norm": 1.4134942293167114,
      "learning_rate": 0.0002641686182669789,
      "loss": 0.0709,
      "step": 376
    },
    {
      "epoch": 0.882387361029842,
      "grad_norm": 0.33341026306152344,
      "learning_rate": 0.0002648711943793911,
      "loss": 0.0596,
      "step": 377
    },
    {
      "epoch": 0.8847279110590989,
      "grad_norm": 0.1390356570482254,
      "learning_rate": 0.00026557377049180324,
      "loss": 0.0156,
      "step": 378
    },
    {
      "epoch": 0.8870684610883558,
      "grad_norm": 0.2614056169986725,
      "learning_rate": 0.0002662763466042154,
      "loss": 0.0373,
      "step": 379
    },
    {
      "epoch": 0.8894090111176126,
      "grad_norm": 0.41955089569091797,
      "learning_rate": 0.0002669789227166276,
      "loss": 0.0335,
      "step": 380
    },
    {
      "epoch": 0.8917495611468695,
      "grad_norm": 0.2809726893901825,
      "learning_rate": 0.0002676814988290398,
      "loss": 0.0519,
      "step": 381
    },
    {
      "epoch": 0.8940901111761264,
      "grad_norm": 0.44381028413772583,
      "learning_rate": 0.00026838407494145195,
      "loss": 0.0888,
      "step": 382
    },
    {
      "epoch": 0.8964306612053833,
      "grad_norm": 0.3120572865009308,
      "learning_rate": 0.0002690866510538641,
      "loss": 0.0669,
      "step": 383
    },
    {
      "epoch": 0.8987712112346401,
      "grad_norm": 0.2518059313297272,
      "learning_rate": 0.00026978922716627634,
      "loss": 0.065,
      "step": 384
    },
    {
      "epoch": 0.901111761263897,
      "grad_norm": 0.303812175989151,
      "learning_rate": 0.0002704918032786885,
      "loss": 0.0784,
      "step": 385
    },
    {
      "epoch": 0.9034523112931538,
      "grad_norm": 0.2597064971923828,
      "learning_rate": 0.00027119437939110067,
      "loss": 0.0547,
      "step": 386
    },
    {
      "epoch": 0.9057928613224108,
      "grad_norm": 0.1569831818342209,
      "learning_rate": 0.00027189695550351283,
      "loss": 0.0538,
      "step": 387
    },
    {
      "epoch": 0.9081334113516677,
      "grad_norm": 0.5316851139068604,
      "learning_rate": 0.00027259953161592505,
      "loss": 0.08,
      "step": 388
    },
    {
      "epoch": 0.9104739613809245,
      "grad_norm": 0.21028228104114532,
      "learning_rate": 0.0002733021077283372,
      "loss": 0.037,
      "step": 389
    },
    {
      "epoch": 0.9128145114101814,
      "grad_norm": 0.17034035921096802,
      "learning_rate": 0.0002740046838407494,
      "loss": 0.0566,
      "step": 390
    },
    {
      "epoch": 0.9151550614394383,
      "grad_norm": 0.20780299603939056,
      "learning_rate": 0.00027470725995316155,
      "loss": 0.065,
      "step": 391
    },
    {
      "epoch": 0.9174956114686952,
      "grad_norm": 0.14059719443321228,
      "learning_rate": 0.00027540983606557377,
      "loss": 0.0527,
      "step": 392
    },
    {
      "epoch": 0.919836161497952,
      "grad_norm": 0.402035653591156,
      "learning_rate": 0.00027611241217798593,
      "loss": 0.0541,
      "step": 393
    },
    {
      "epoch": 0.9221767115272089,
      "grad_norm": 0.14397574961185455,
      "learning_rate": 0.0002768149882903981,
      "loss": 0.0416,
      "step": 394
    },
    {
      "epoch": 0.9245172615564657,
      "grad_norm": 0.20297399163246155,
      "learning_rate": 0.0002775175644028103,
      "loss": 0.0629,
      "step": 395
    },
    {
      "epoch": 0.9268578115857227,
      "grad_norm": 0.17974838614463806,
      "learning_rate": 0.0002782201405152225,
      "loss": 0.0531,
      "step": 396
    },
    {
      "epoch": 0.9291983616149795,
      "grad_norm": 0.1743568331003189,
      "learning_rate": 0.00027892271662763465,
      "loss": 0.0466,
      "step": 397
    },
    {
      "epoch": 0.9315389116442364,
      "grad_norm": 0.15419995784759521,
      "learning_rate": 0.0002796252927400468,
      "loss": 0.0624,
      "step": 398
    },
    {
      "epoch": 0.9338794616734932,
      "grad_norm": 0.13516239821910858,
      "learning_rate": 0.00028032786885245903,
      "loss": 0.045,
      "step": 399
    },
    {
      "epoch": 0.9362200117027502,
      "grad_norm": 0.2177482545375824,
      "learning_rate": 0.0002810304449648712,
      "loss": 0.049,
      "step": 400
    },
    {
      "epoch": 0.938560561732007,
      "grad_norm": 0.23287124931812286,
      "learning_rate": 0.00028173302107728336,
      "loss": 0.0515,
      "step": 401
    },
    {
      "epoch": 0.9409011117612639,
      "grad_norm": 0.3030524253845215,
      "learning_rate": 0.0002824355971896955,
      "loss": 0.0694,
      "step": 402
    },
    {
      "epoch": 0.9432416617905208,
      "grad_norm": 0.1405664086341858,
      "learning_rate": 0.00028313817330210774,
      "loss": 0.0551,
      "step": 403
    },
    {
      "epoch": 0.9455822118197776,
      "grad_norm": 0.2665010094642639,
      "learning_rate": 0.0002838407494145199,
      "loss": 0.0852,
      "step": 404
    },
    {
      "epoch": 0.9479227618490346,
      "grad_norm": 0.2118506133556366,
      "learning_rate": 0.0002845433255269321,
      "loss": 0.0685,
      "step": 405
    },
    {
      "epoch": 0.9502633118782914,
      "grad_norm": 0.3704584836959839,
      "learning_rate": 0.00028524590163934424,
      "loss": 0.0715,
      "step": 406
    },
    {
      "epoch": 0.9526038619075483,
      "grad_norm": 0.2252623587846756,
      "learning_rate": 0.0002859484777517564,
      "loss": 0.0578,
      "step": 407
    },
    {
      "epoch": 0.9549444119368051,
      "grad_norm": 0.2523285746574402,
      "learning_rate": 0.00028665105386416857,
      "loss": 0.0686,
      "step": 408
    },
    {
      "epoch": 0.957284961966062,
      "grad_norm": 0.14561998844146729,
      "learning_rate": 0.00028735362997658073,
      "loss": 0.0492,
      "step": 409
    },
    {
      "epoch": 0.9596255119953189,
      "grad_norm": 0.14285434782505035,
      "learning_rate": 0.00028805620608899295,
      "loss": 0.0551,
      "step": 410
    },
    {
      "epoch": 0.9619660620245758,
      "grad_norm": 0.17985229194164276,
      "learning_rate": 0.0002887587822014051,
      "loss": 0.0838,
      "step": 411
    },
    {
      "epoch": 0.9643066120538326,
      "grad_norm": 0.11179563403129578,
      "learning_rate": 0.0002894613583138173,
      "loss": 0.0588,
      "step": 412
    },
    {
      "epoch": 0.9666471620830895,
      "grad_norm": 0.12872596085071564,
      "learning_rate": 0.00029016393442622945,
      "loss": 0.0537,
      "step": 413
    },
    {
      "epoch": 0.9689877121123464,
      "grad_norm": 0.15696561336517334,
      "learning_rate": 0.00029086651053864167,
      "loss": 0.0602,
      "step": 414
    },
    {
      "epoch": 0.9713282621416033,
      "grad_norm": 0.16175009310245514,
      "learning_rate": 0.00029156908665105383,
      "loss": 0.0755,
      "step": 415
    },
    {
      "epoch": 0.9736688121708601,
      "grad_norm": 0.1214589849114418,
      "learning_rate": 0.000292271662763466,
      "loss": 0.0443,
      "step": 416
    },
    {
      "epoch": 0.976009362200117,
      "grad_norm": 0.16425654292106628,
      "learning_rate": 0.00029297423887587816,
      "loss": 0.0561,
      "step": 417
    },
    {
      "epoch": 0.978349912229374,
      "grad_norm": 0.17815281450748444,
      "learning_rate": 0.0002936768149882904,
      "loss": 0.0668,
      "step": 418
    },
    {
      "epoch": 0.9806904622586308,
      "grad_norm": 0.14347293972969055,
      "learning_rate": 0.00029437939110070255,
      "loss": 0.0686,
      "step": 419
    },
    {
      "epoch": 0.9830310122878877,
      "grad_norm": 0.1291062831878662,
      "learning_rate": 0.0002950819672131147,
      "loss": 0.052,
      "step": 420
    },
    {
      "epoch": 0.9853715623171445,
      "grad_norm": 0.17884881794452667,
      "learning_rate": 0.0002957845433255269,
      "loss": 0.0577,
      "step": 421
    },
    {
      "epoch": 0.9877121123464014,
      "grad_norm": 0.1483738124370575,
      "learning_rate": 0.0002964871194379391,
      "loss": 0.0546,
      "step": 422
    },
    {
      "epoch": 0.9900526623756583,
      "grad_norm": 0.23003150522708893,
      "learning_rate": 0.00029718969555035126,
      "loss": 0.0473,
      "step": 423
    },
    {
      "epoch": 0.9923932124049152,
      "grad_norm": 0.15187926590442657,
      "learning_rate": 0.0002978922716627634,
      "loss": 0.0533,
      "step": 424
    },
    {
      "epoch": 0.994733762434172,
      "grad_norm": 0.11729928106069565,
      "learning_rate": 0.0002985948477751756,
      "loss": 0.0438,
      "step": 425
    },
    {
      "epoch": 0.9970743124634289,
      "grad_norm": 0.1724518984556198,
      "learning_rate": 0.0002992974238875878,
      "loss": 0.065,
      "step": 426
    },
    {
      "epoch": 0.9994148624926857,
      "grad_norm": 0.2532108426094055,
      "learning_rate": 0.0003,
      "loss": 0.0711,
      "step": 427
    },
    {
      "epoch": 0.9994148624926857,
      "eval_loss": 0.08032073080539703,
      "eval_runtime": 127.2101,
      "eval_samples_per_second": 4.339,
      "eval_steps_per_second": 0.542,
      "step": 427
    },
    {
      "epoch": 1.0017554125219426,
      "grad_norm": 0.21804934740066528,
      "learning_rate": 0.0002999219359875097,
      "loss": 0.0385,
      "step": 428
    },
    {
      "epoch": 1.0040959625511996,
      "grad_norm": 0.16634200513362885,
      "learning_rate": 0.0002998438719750195,
      "loss": 0.0405,
      "step": 429
    },
    {
      "epoch": 1.0064365125804564,
      "grad_norm": 0.2449033558368683,
      "learning_rate": 0.00029976580796252925,
      "loss": 0.0539,
      "step": 430
    },
    {
      "epoch": 1.0087770626097132,
      "grad_norm": 0.15714433789253235,
      "learning_rate": 0.000299687743950039,
      "loss": 0.0382,
      "step": 431
    },
    {
      "epoch": 1.0111176126389703,
      "grad_norm": 0.2897557020187378,
      "learning_rate": 0.00029960967993754875,
      "loss": 0.0866,
      "step": 432
    },
    {
      "epoch": 1.013458162668227,
      "grad_norm": 0.17755873501300812,
      "learning_rate": 0.00029953161592505853,
      "loss": 0.0554,
      "step": 433
    },
    {
      "epoch": 1.015798712697484,
      "grad_norm": 0.11810938268899918,
      "learning_rate": 0.00029945355191256825,
      "loss": 0.0433,
      "step": 434
    },
    {
      "epoch": 1.0181392627267407,
      "grad_norm": 0.17790302634239197,
      "learning_rate": 0.00029937548790007803,
      "loss": 0.083,
      "step": 435
    },
    {
      "epoch": 1.0204798127559978,
      "grad_norm": 0.14791257679462433,
      "learning_rate": 0.0002992974238875878,
      "loss": 0.0651,
      "step": 436
    },
    {
      "epoch": 1.0228203627852546,
      "grad_norm": 0.1586376577615738,
      "learning_rate": 0.00029921935987509753,
      "loss": 0.0523,
      "step": 437
    },
    {
      "epoch": 1.0251609128145114,
      "grad_norm": 0.15402698516845703,
      "learning_rate": 0.0002991412958626073,
      "loss": 0.0632,
      "step": 438
    },
    {
      "epoch": 1.0275014628437682,
      "grad_norm": 0.20708905160427094,
      "learning_rate": 0.0002990632318501171,
      "loss": 0.0563,
      "step": 439
    },
    {
      "epoch": 1.0298420128730252,
      "grad_norm": 0.1887229084968567,
      "learning_rate": 0.0002989851678376268,
      "loss": 0.0533,
      "step": 440
    },
    {
      "epoch": 1.032182562902282,
      "grad_norm": 0.20384429395198822,
      "learning_rate": 0.0002989071038251366,
      "loss": 0.0697,
      "step": 441
    },
    {
      "epoch": 1.0345231129315389,
      "grad_norm": 0.18686267733573914,
      "learning_rate": 0.00029882903981264637,
      "loss": 0.0574,
      "step": 442
    },
    {
      "epoch": 1.0368636629607957,
      "grad_norm": 0.17002713680267334,
      "learning_rate": 0.0002987509758001561,
      "loss": 0.0474,
      "step": 443
    },
    {
      "epoch": 1.0392042129900527,
      "grad_norm": 0.11678843945264816,
      "learning_rate": 0.00029867291178766587,
      "loss": 0.0405,
      "step": 444
    },
    {
      "epoch": 1.0415447630193095,
      "grad_norm": 0.20232237875461578,
      "learning_rate": 0.0002985948477751756,
      "loss": 0.0546,
      "step": 445
    },
    {
      "epoch": 1.0438853130485664,
      "grad_norm": 0.23916654288768768,
      "learning_rate": 0.00029851678376268537,
      "loss": 0.0724,
      "step": 446
    },
    {
      "epoch": 1.0462258630778234,
      "grad_norm": 0.26329293847084045,
      "learning_rate": 0.00029843871975019514,
      "loss": 0.0563,
      "step": 447
    },
    {
      "epoch": 1.0485664131070802,
      "grad_norm": 0.12684880197048187,
      "learning_rate": 0.00029836065573770487,
      "loss": 0.0537,
      "step": 448
    },
    {
      "epoch": 1.050906963136337,
      "grad_norm": 0.1141497939825058,
      "learning_rate": 0.00029828259172521465,
      "loss": 0.0403,
      "step": 449
    },
    {
      "epoch": 1.0532475131655938,
      "grad_norm": 0.1440262794494629,
      "learning_rate": 0.0002982045277127244,
      "loss": 0.0535,
      "step": 450
    },
    {
      "epoch": 1.0555880631948509,
      "grad_norm": 0.14205345511436462,
      "learning_rate": 0.00029812646370023415,
      "loss": 0.0449,
      "step": 451
    },
    {
      "epoch": 1.0579286132241077,
      "grad_norm": 0.13380925357341766,
      "learning_rate": 0.0002980483996877439,
      "loss": 0.0567,
      "step": 452
    },
    {
      "epoch": 1.0602691632533645,
      "grad_norm": 0.16497372090816498,
      "learning_rate": 0.0002979703356752537,
      "loss": 0.0444,
      "step": 453
    },
    {
      "epoch": 1.0626097132826213,
      "grad_norm": 0.11785455048084259,
      "learning_rate": 0.0002978922716627634,
      "loss": 0.0377,
      "step": 454
    },
    {
      "epoch": 1.0649502633118784,
      "grad_norm": 0.16982175409793854,
      "learning_rate": 0.0002978142076502732,
      "loss": 0.0574,
      "step": 455
    },
    {
      "epoch": 1.0672908133411352,
      "grad_norm": 0.14660529792308807,
      "learning_rate": 0.000297736143637783,
      "loss": 0.057,
      "step": 456
    },
    {
      "epoch": 1.069631363370392,
      "grad_norm": 0.2356836050748825,
      "learning_rate": 0.0002976580796252927,
      "loss": 0.0706,
      "step": 457
    },
    {
      "epoch": 1.0719719133996488,
      "grad_norm": 0.17826057970523834,
      "learning_rate": 0.0002975800156128025,
      "loss": 0.0547,
      "step": 458
    },
    {
      "epoch": 1.0743124634289059,
      "grad_norm": 0.13650791347026825,
      "learning_rate": 0.00029750195160031226,
      "loss": 0.0462,
      "step": 459
    },
    {
      "epoch": 1.0766530134581627,
      "grad_norm": 0.1759009212255478,
      "learning_rate": 0.000297423887587822,
      "loss": 0.0589,
      "step": 460
    },
    {
      "epoch": 1.0789935634874195,
      "grad_norm": 0.17905405163764954,
      "learning_rate": 0.00029734582357533176,
      "loss": 0.0716,
      "step": 461
    },
    {
      "epoch": 1.0813341135166765,
      "grad_norm": 0.1180078536272049,
      "learning_rate": 0.00029726775956284154,
      "loss": 0.05,
      "step": 462
    },
    {
      "epoch": 1.0836746635459333,
      "grad_norm": 0.1613827645778656,
      "learning_rate": 0.00029718969555035126,
      "loss": 0.0589,
      "step": 463
    },
    {
      "epoch": 1.0860152135751902,
      "grad_norm": 0.1553131341934204,
      "learning_rate": 0.000297111631537861,
      "loss": 0.0501,
      "step": 464
    },
    {
      "epoch": 1.088355763604447,
      "grad_norm": 0.15572965145111084,
      "learning_rate": 0.00029703356752537076,
      "loss": 0.0705,
      "step": 465
    },
    {
      "epoch": 1.090696313633704,
      "grad_norm": 0.12649482488632202,
      "learning_rate": 0.00029695550351288054,
      "loss": 0.0429,
      "step": 466
    },
    {
      "epoch": 1.0930368636629608,
      "grad_norm": 0.10627232491970062,
      "learning_rate": 0.00029687743950039026,
      "loss": 0.0403,
      "step": 467
    },
    {
      "epoch": 1.0953774136922176,
      "grad_norm": 0.15932543575763702,
      "learning_rate": 0.00029679937548790004,
      "loss": 0.0463,
      "step": 468
    },
    {
      "epoch": 1.0977179637214745,
      "grad_norm": 0.14206640422344208,
      "learning_rate": 0.0002967213114754098,
      "loss": 0.0454,
      "step": 469
    },
    {
      "epoch": 1.1000585137507315,
      "grad_norm": 0.10508661717176437,
      "learning_rate": 0.00029664324746291954,
      "loss": 0.0366,
      "step": 470
    },
    {
      "epoch": 1.1023990637799883,
      "grad_norm": 0.2968927025794983,
      "learning_rate": 0.0002965651834504293,
      "loss": 0.0704,
      "step": 471
    },
    {
      "epoch": 1.1047396138092451,
      "grad_norm": 0.15427662432193756,
      "learning_rate": 0.0002964871194379391,
      "loss": 0.0431,
      "step": 472
    },
    {
      "epoch": 1.1070801638385022,
      "grad_norm": 0.1306522637605667,
      "learning_rate": 0.0002964090554254488,
      "loss": 0.0358,
      "step": 473
    },
    {
      "epoch": 1.109420713867759,
      "grad_norm": 0.14456208050251007,
      "learning_rate": 0.0002963309914129586,
      "loss": 0.0492,
      "step": 474
    },
    {
      "epoch": 1.1117612638970158,
      "grad_norm": 0.1285320669412613,
      "learning_rate": 0.0002962529274004684,
      "loss": 0.0454,
      "step": 475
    },
    {
      "epoch": 1.1141018139262726,
      "grad_norm": 0.30106738209724426,
      "learning_rate": 0.0002961748633879781,
      "loss": 0.0866,
      "step": 476
    },
    {
      "epoch": 1.1164423639555296,
      "grad_norm": 0.19616180658340454,
      "learning_rate": 0.0002960967993754879,
      "loss": 0.0411,
      "step": 477
    },
    {
      "epoch": 1.1187829139847865,
      "grad_norm": 0.18097974359989166,
      "learning_rate": 0.00029601873536299765,
      "loss": 0.0422,
      "step": 478
    },
    {
      "epoch": 1.1211234640140433,
      "grad_norm": 0.14123156666755676,
      "learning_rate": 0.0002959406713505074,
      "loss": 0.0415,
      "step": 479
    },
    {
      "epoch": 1.1234640140433,
      "grad_norm": 0.17400358617305756,
      "learning_rate": 0.00029586260733801715,
      "loss": 0.044,
      "step": 480
    },
    {
      "epoch": 1.1258045640725571,
      "grad_norm": 0.2280646115541458,
      "learning_rate": 0.0002957845433255269,
      "loss": 0.0595,
      "step": 481
    },
    {
      "epoch": 1.128145114101814,
      "grad_norm": 0.2446908950805664,
      "learning_rate": 0.00029570647931303665,
      "loss": 0.0591,
      "step": 482
    },
    {
      "epoch": 1.1304856641310708,
      "grad_norm": 0.20579630136489868,
      "learning_rate": 0.00029562841530054643,
      "loss": 0.0519,
      "step": 483
    },
    {
      "epoch": 1.1328262141603276,
      "grad_norm": 0.1777038872241974,
      "learning_rate": 0.00029555035128805615,
      "loss": 0.0418,
      "step": 484
    },
    {
      "epoch": 1.1351667641895846,
      "grad_norm": 0.18625999987125397,
      "learning_rate": 0.00029547228727556593,
      "loss": 0.0442,
      "step": 485
    },
    {
      "epoch": 1.1375073142188414,
      "grad_norm": 0.21433965861797333,
      "learning_rate": 0.0002953942232630757,
      "loss": 0.0589,
      "step": 486
    },
    {
      "epoch": 1.1398478642480983,
      "grad_norm": 0.1979682296514511,
      "learning_rate": 0.00029531615925058543,
      "loss": 0.0631,
      "step": 487
    },
    {
      "epoch": 1.142188414277355,
      "grad_norm": 0.13394859433174133,
      "learning_rate": 0.0002952380952380952,
      "loss": 0.0447,
      "step": 488
    },
    {
      "epoch": 1.144528964306612,
      "grad_norm": 0.17869256436824799,
      "learning_rate": 0.000295160031225605,
      "loss": 0.0552,
      "step": 489
    },
    {
      "epoch": 1.146869514335869,
      "grad_norm": 0.1470206379890442,
      "learning_rate": 0.0002950819672131147,
      "loss": 0.05,
      "step": 490
    },
    {
      "epoch": 1.1492100643651257,
      "grad_norm": 0.1719900667667389,
      "learning_rate": 0.0002950039032006245,
      "loss": 0.055,
      "step": 491
    },
    {
      "epoch": 1.1515506143943828,
      "grad_norm": 0.17024275660514832,
      "learning_rate": 0.00029492583918813427,
      "loss": 0.0494,
      "step": 492
    },
    {
      "epoch": 1.1538911644236396,
      "grad_norm": 0.14480070769786835,
      "learning_rate": 0.000294847775175644,
      "loss": 0.0518,
      "step": 493
    },
    {
      "epoch": 1.1562317144528964,
      "grad_norm": 0.10582520067691803,
      "learning_rate": 0.00029476971116315377,
      "loss": 0.0411,
      "step": 494
    },
    {
      "epoch": 1.1585722644821532,
      "grad_norm": 0.1254405379295349,
      "learning_rate": 0.00029469164715066354,
      "loss": 0.0501,
      "step": 495
    },
    {
      "epoch": 1.1609128145114103,
      "grad_norm": 0.13431349396705627,
      "learning_rate": 0.00029461358313817327,
      "loss": 0.0556,
      "step": 496
    },
    {
      "epoch": 1.163253364540667,
      "grad_norm": 0.155593603849411,
      "learning_rate": 0.00029453551912568304,
      "loss": 0.0531,
      "step": 497
    },
    {
      "epoch": 1.165593914569924,
      "grad_norm": 0.13992400467395782,
      "learning_rate": 0.0002944574551131928,
      "loss": 0.055,
      "step": 498
    },
    {
      "epoch": 1.1679344645991807,
      "grad_norm": 0.12626564502716064,
      "learning_rate": 0.00029437939110070255,
      "loss": 0.0463,
      "step": 499
    },
    {
      "epoch": 1.1702750146284377,
      "grad_norm": 0.1688164919614792,
      "learning_rate": 0.0002943013270882123,
      "loss": 0.059,
      "step": 500
    },
    {
      "epoch": 1.1726155646576946,
      "grad_norm": 0.1213286966085434,
      "learning_rate": 0.00029422326307572205,
      "loss": 0.0445,
      "step": 501
    },
    {
      "epoch": 1.1749561146869514,
      "grad_norm": 0.11606194823980331,
      "learning_rate": 0.0002941451990632318,
      "loss": 0.046,
      "step": 502
    },
    {
      "epoch": 1.1772966647162084,
      "grad_norm": 0.10733884572982788,
      "learning_rate": 0.0002940671350507416,
      "loss": 0.0303,
      "step": 503
    },
    {
      "epoch": 1.1796372147454652,
      "grad_norm": 0.1761365830898285,
      "learning_rate": 0.0002939890710382513,
      "loss": 0.0644,
      "step": 504
    },
    {
      "epoch": 1.181977764774722,
      "grad_norm": 0.2580004930496216,
      "learning_rate": 0.0002939110070257611,
      "loss": 0.0684,
      "step": 505
    },
    {
      "epoch": 1.1843183148039789,
      "grad_norm": 0.17614279687404633,
      "learning_rate": 0.0002938329430132709,
      "loss": 0.0432,
      "step": 506
    },
    {
      "epoch": 1.186658864833236,
      "grad_norm": 0.16726650297641754,
      "learning_rate": 0.0002937548790007806,
      "loss": 0.0525,
      "step": 507
    },
    {
      "epoch": 1.1889994148624927,
      "grad_norm": 0.11618917435407639,
      "learning_rate": 0.0002936768149882904,
      "loss": 0.0361,
      "step": 508
    },
    {
      "epoch": 1.1913399648917495,
      "grad_norm": 0.1634228676557541,
      "learning_rate": 0.00029359875097580016,
      "loss": 0.0458,
      "step": 509
    },
    {
      "epoch": 1.1936805149210064,
      "grad_norm": 0.15456527471542358,
      "learning_rate": 0.0002935206869633099,
      "loss": 0.0352,
      "step": 510
    },
    {
      "epoch": 1.1960210649502634,
      "grad_norm": 0.1459311991930008,
      "learning_rate": 0.00029344262295081966,
      "loss": 0.0443,
      "step": 511
    },
    {
      "epoch": 1.1983616149795202,
      "grad_norm": 0.15609011054039001,
      "learning_rate": 0.00029336455893832944,
      "loss": 0.0374,
      "step": 512
    },
    {
      "epoch": 1.200702165008777,
      "grad_norm": 0.17539839446544647,
      "learning_rate": 0.00029328649492583916,
      "loss": 0.0559,
      "step": 513
    },
    {
      "epoch": 1.203042715038034,
      "grad_norm": 0.12956301867961884,
      "learning_rate": 0.00029320843091334894,
      "loss": 0.0294,
      "step": 514
    },
    {
      "epoch": 1.2053832650672909,
      "grad_norm": 0.13898171484470367,
      "learning_rate": 0.0002931303669008587,
      "loss": 0.0398,
      "step": 515
    },
    {
      "epoch": 1.2077238150965477,
      "grad_norm": 0.1973174810409546,
      "learning_rate": 0.00029305230288836844,
      "loss": 0.0554,
      "step": 516
    },
    {
      "epoch": 1.2100643651258045,
      "grad_norm": 0.22550798952579498,
      "learning_rate": 0.00029297423887587816,
      "loss": 0.0527,
      "step": 517
    },
    {
      "epoch": 1.2124049151550613,
      "grad_norm": 0.13336661458015442,
      "learning_rate": 0.000292896174863388,
      "loss": 0.0348,
      "step": 518
    },
    {
      "epoch": 1.2147454651843184,
      "grad_norm": 0.19066794216632843,
      "learning_rate": 0.0002928181108508977,
      "loss": 0.0534,
      "step": 519
    },
    {
      "epoch": 1.2170860152135752,
      "grad_norm": 0.1423959732055664,
      "learning_rate": 0.00029274004683840744,
      "loss": 0.0451,
      "step": 520
    },
    {
      "epoch": 1.219426565242832,
      "grad_norm": 0.2002698928117752,
      "learning_rate": 0.0002926619828259172,
      "loss": 0.0627,
      "step": 521
    },
    {
      "epoch": 1.221767115272089,
      "grad_norm": 0.21176917850971222,
      "learning_rate": 0.000292583918813427,
      "loss": 0.066,
      "step": 522
    },
    {
      "epoch": 1.2241076653013458,
      "grad_norm": 0.1548503190279007,
      "learning_rate": 0.0002925058548009367,
      "loss": 0.0594,
      "step": 523
    },
    {
      "epoch": 1.2264482153306027,
      "grad_norm": 0.14327244460582733,
      "learning_rate": 0.0002924277907884465,
      "loss": 0.0609,
      "step": 524
    },
    {
      "epoch": 1.2287887653598595,
      "grad_norm": 0.1559297740459442,
      "learning_rate": 0.0002923497267759563,
      "loss": 0.0632,
      "step": 525
    },
    {
      "epoch": 1.2311293153891165,
      "grad_norm": 0.1128629520535469,
      "learning_rate": 0.000292271662763466,
      "loss": 0.038,
      "step": 526
    },
    {
      "epoch": 1.2334698654183733,
      "grad_norm": 0.14978130161762238,
      "learning_rate": 0.0002921935987509758,
      "loss": 0.069,
      "step": 527
    },
    {
      "epoch": 1.2358104154476302,
      "grad_norm": 0.08288449794054031,
      "learning_rate": 0.00029211553473848555,
      "loss": 0.0455,
      "step": 528
    },
    {
      "epoch": 1.238150965476887,
      "grad_norm": 0.11577577143907547,
      "learning_rate": 0.0002920374707259953,
      "loss": 0.0288,
      "step": 529
    },
    {
      "epoch": 1.240491515506144,
      "grad_norm": 0.13335351645946503,
      "learning_rate": 0.00029195940671350505,
      "loss": 0.0473,
      "step": 530
    },
    {
      "epoch": 1.2428320655354008,
      "grad_norm": 0.08360744267702103,
      "learning_rate": 0.00029188134270101483,
      "loss": 0.0476,
      "step": 531
    },
    {
      "epoch": 1.2451726155646576,
      "grad_norm": 0.11149913817644119,
      "learning_rate": 0.00029180327868852455,
      "loss": 0.0397,
      "step": 532
    },
    {
      "epoch": 1.2475131655939147,
      "grad_norm": 0.1534763127565384,
      "learning_rate": 0.00029172521467603433,
      "loss": 0.0542,
      "step": 533
    },
    {
      "epoch": 1.2498537156231715,
      "grad_norm": 0.11154048889875412,
      "learning_rate": 0.0002916471506635441,
      "loss": 0.0549,
      "step": 534
    },
    {
      "epoch": 1.2521942656524283,
      "grad_norm": 0.15836918354034424,
      "learning_rate": 0.00029156908665105383,
      "loss": 0.0699,
      "step": 535
    },
    {
      "epoch": 1.2545348156816851,
      "grad_norm": 0.10850701481103897,
      "learning_rate": 0.0002914910226385636,
      "loss": 0.0411,
      "step": 536
    },
    {
      "epoch": 1.256875365710942,
      "grad_norm": 0.09109733998775482,
      "learning_rate": 0.00029141295862607333,
      "loss": 0.0362,
      "step": 537
    },
    {
      "epoch": 1.259215915740199,
      "grad_norm": 0.1678888350725174,
      "learning_rate": 0.0002913348946135831,
      "loss": 0.0427,
      "step": 538
    },
    {
      "epoch": 1.2615564657694558,
      "grad_norm": 0.12920117378234863,
      "learning_rate": 0.0002912568306010929,
      "loss": 0.0386,
      "step": 539
    },
    {
      "epoch": 1.2638970157987126,
      "grad_norm": 0.17634360492229462,
      "learning_rate": 0.0002911787665886026,
      "loss": 0.0428,
      "step": 540
    },
    {
      "epoch": 1.2662375658279696,
      "grad_norm": 0.11761943250894547,
      "learning_rate": 0.0002911007025761124,
      "loss": 0.0439,
      "step": 541
    },
    {
      "epoch": 1.2685781158572265,
      "grad_norm": 0.11782462894916534,
      "learning_rate": 0.00029102263856362217,
      "loss": 0.0372,
      "step": 542
    },
    {
      "epoch": 1.2709186658864833,
      "grad_norm": 0.16122779250144958,
      "learning_rate": 0.0002909445745511319,
      "loss": 0.0553,
      "step": 543
    },
    {
      "epoch": 1.2732592159157403,
      "grad_norm": 0.17862075567245483,
      "learning_rate": 0.00029086651053864167,
      "loss": 0.0419,
      "step": 544
    },
    {
      "epoch": 1.2755997659449971,
      "grad_norm": 0.1539636254310608,
      "learning_rate": 0.00029078844652615144,
      "loss": 0.0638,
      "step": 545
    },
    {
      "epoch": 1.277940315974254,
      "grad_norm": 0.1554494947195053,
      "learning_rate": 0.00029071038251366117,
      "loss": 0.0467,
      "step": 546
    },
    {
      "epoch": 1.2802808660035108,
      "grad_norm": 0.12083878368139267,
      "learning_rate": 0.00029063231850117094,
      "loss": 0.0323,
      "step": 547
    },
    {
      "epoch": 1.2826214160327676,
      "grad_norm": 0.15218687057495117,
      "learning_rate": 0.0002905542544886807,
      "loss": 0.0506,
      "step": 548
    },
    {
      "epoch": 1.2849619660620246,
      "grad_norm": 0.12590396404266357,
      "learning_rate": 0.00029047619047619045,
      "loss": 0.0461,
      "step": 549
    },
    {
      "epoch": 1.2873025160912814,
      "grad_norm": 0.16233107447624207,
      "learning_rate": 0.0002903981264637002,
      "loss": 0.0434,
      "step": 550
    },
    {
      "epoch": 1.2896430661205383,
      "grad_norm": 0.15540815889835358,
      "learning_rate": 0.00029032006245121,
      "loss": 0.0608,
      "step": 551
    },
    {
      "epoch": 1.2919836161497953,
      "grad_norm": 0.14522112905979156,
      "learning_rate": 0.0002902419984387197,
      "loss": 0.0495,
      "step": 552
    },
    {
      "epoch": 1.294324166179052,
      "grad_norm": 0.08996362239122391,
      "learning_rate": 0.00029016393442622945,
      "loss": 0.0315,
      "step": 553
    },
    {
      "epoch": 1.296664716208309,
      "grad_norm": 0.15849269926548004,
      "learning_rate": 0.0002900858704137393,
      "loss": 0.0466,
      "step": 554
    },
    {
      "epoch": 1.299005266237566,
      "grad_norm": 0.16602911055088043,
      "learning_rate": 0.000290007806401249,
      "loss": 0.0506,
      "step": 555
    },
    {
      "epoch": 1.3013458162668228,
      "grad_norm": 0.14604802429676056,
      "learning_rate": 0.0002899297423887587,
      "loss": 0.0354,
      "step": 556
    },
    {
      "epoch": 1.3036863662960796,
      "grad_norm": 0.14058664441108704,
      "learning_rate": 0.0002898516783762685,
      "loss": 0.0402,
      "step": 557
    },
    {
      "epoch": 1.3060269163253364,
      "grad_norm": 0.1610068380832672,
      "learning_rate": 0.0002897736143637783,
      "loss": 0.0602,
      "step": 558
    },
    {
      "epoch": 1.3083674663545932,
      "grad_norm": 0.1212761327624321,
      "learning_rate": 0.000289695550351288,
      "loss": 0.05,
      "step": 559
    },
    {
      "epoch": 1.3107080163838503,
      "grad_norm": 0.20430107414722443,
      "learning_rate": 0.0002896174863387978,
      "loss": 0.0553,
      "step": 560
    },
    {
      "epoch": 1.313048566413107,
      "grad_norm": 0.13993321359157562,
      "learning_rate": 0.00028953942232630756,
      "loss": 0.0392,
      "step": 561
    },
    {
      "epoch": 1.315389116442364,
      "grad_norm": 0.1544693559408188,
      "learning_rate": 0.0002894613583138173,
      "loss": 0.0476,
      "step": 562
    },
    {
      "epoch": 1.317729666471621,
      "grad_norm": 0.18119747936725616,
      "learning_rate": 0.00028938329430132706,
      "loss": 0.0674,
      "step": 563
    },
    {
      "epoch": 1.3200702165008777,
      "grad_norm": 0.1529349535703659,
      "learning_rate": 0.00028930523028883684,
      "loss": 0.0508,
      "step": 564
    },
    {
      "epoch": 1.3224107665301346,
      "grad_norm": 0.13439767062664032,
      "learning_rate": 0.00028922716627634656,
      "loss": 0.0506,
      "step": 565
    },
    {
      "epoch": 1.3247513165593914,
      "grad_norm": 0.16861048340797424,
      "learning_rate": 0.00028914910226385634,
      "loss": 0.0494,
      "step": 566
    },
    {
      "epoch": 1.3270918665886482,
      "grad_norm": 0.16817843914031982,
      "learning_rate": 0.0002890710382513661,
      "loss": 0.0618,
      "step": 567
    },
    {
      "epoch": 1.3294324166179052,
      "grad_norm": 0.11680976301431656,
      "learning_rate": 0.00028899297423887584,
      "loss": 0.0317,
      "step": 568
    },
    {
      "epoch": 1.331772966647162,
      "grad_norm": 0.14342568814754486,
      "learning_rate": 0.0002889149102263856,
      "loss": 0.0317,
      "step": 569
    },
    {
      "epoch": 1.3341135166764189,
      "grad_norm": 0.11046776920557022,
      "learning_rate": 0.0002888368462138954,
      "loss": 0.0388,
      "step": 570
    },
    {
      "epoch": 1.336454066705676,
      "grad_norm": 0.16333989799022675,
      "learning_rate": 0.0002887587822014051,
      "loss": 0.0567,
      "step": 571
    },
    {
      "epoch": 1.3387946167349327,
      "grad_norm": 0.21077187359333038,
      "learning_rate": 0.0002886807181889149,
      "loss": 0.0455,
      "step": 572
    },
    {
      "epoch": 1.3411351667641895,
      "grad_norm": 0.14799396693706512,
      "learning_rate": 0.0002886026541764246,
      "loss": 0.0464,
      "step": 573
    },
    {
      "epoch": 1.3434757167934466,
      "grad_norm": 0.1870274692773819,
      "learning_rate": 0.0002885245901639344,
      "loss": 0.0544,
      "step": 574
    },
    {
      "epoch": 1.3458162668227034,
      "grad_norm": 0.17919951677322388,
      "learning_rate": 0.0002884465261514442,
      "loss": 0.0776,
      "step": 575
    },
    {
      "epoch": 1.3481568168519602,
      "grad_norm": 0.11561151593923569,
      "learning_rate": 0.0002883684621389539,
      "loss": 0.0302,
      "step": 576
    },
    {
      "epoch": 1.350497366881217,
      "grad_norm": 0.1237419918179512,
      "learning_rate": 0.0002882903981264637,
      "loss": 0.0585,
      "step": 577
    },
    {
      "epoch": 1.3528379169104738,
      "grad_norm": 0.11777094751596451,
      "learning_rate": 0.00028821233411397345,
      "loss": 0.0346,
      "step": 578
    },
    {
      "epoch": 1.3551784669397309,
      "grad_norm": 0.14149929583072662,
      "learning_rate": 0.0002881342701014832,
      "loss": 0.0532,
      "step": 579
    },
    {
      "epoch": 1.3575190169689877,
      "grad_norm": 0.17482583224773407,
      "learning_rate": 0.00028805620608899295,
      "loss": 0.0488,
      "step": 580
    },
    {
      "epoch": 1.3598595669982445,
      "grad_norm": 0.14647789299488068,
      "learning_rate": 0.00028797814207650273,
      "loss": 0.0403,
      "step": 581
    },
    {
      "epoch": 1.3622001170275015,
      "grad_norm": 0.14386054873466492,
      "learning_rate": 0.00028790007806401245,
      "loss": 0.0524,
      "step": 582
    },
    {
      "epoch": 1.3645406670567584,
      "grad_norm": 0.14283950626850128,
      "learning_rate": 0.00028782201405152223,
      "loss": 0.048,
      "step": 583
    },
    {
      "epoch": 1.3668812170860152,
      "grad_norm": 0.14075607061386108,
      "learning_rate": 0.000287743950039032,
      "loss": 0.0627,
      "step": 584
    },
    {
      "epoch": 1.3692217671152722,
      "grad_norm": 0.10598918795585632,
      "learning_rate": 0.00028766588602654173,
      "loss": 0.0338,
      "step": 585
    },
    {
      "epoch": 1.371562317144529,
      "grad_norm": 0.13270047307014465,
      "learning_rate": 0.0002875878220140515,
      "loss": 0.0372,
      "step": 586
    },
    {
      "epoch": 1.3739028671737858,
      "grad_norm": 0.15726783871650696,
      "learning_rate": 0.0002875097580015613,
      "loss": 0.0489,
      "step": 587
    },
    {
      "epoch": 1.3762434172030427,
      "grad_norm": 0.12609831988811493,
      "learning_rate": 0.000287431693989071,
      "loss": 0.0397,
      "step": 588
    },
    {
      "epoch": 1.3785839672322995,
      "grad_norm": 0.1399606615304947,
      "learning_rate": 0.00028735362997658073,
      "loss": 0.0406,
      "step": 589
    },
    {
      "epoch": 1.3809245172615565,
      "grad_norm": 0.19059830904006958,
      "learning_rate": 0.0002872755659640905,
      "loss": 0.0528,
      "step": 590
    },
    {
      "epoch": 1.3832650672908133,
      "grad_norm": 0.15072596073150635,
      "learning_rate": 0.0002871975019516003,
      "loss": 0.0495,
      "step": 591
    },
    {
      "epoch": 1.3856056173200701,
      "grad_norm": 0.15330609679222107,
      "learning_rate": 0.00028711943793911,
      "loss": 0.0263,
      "step": 592
    },
    {
      "epoch": 1.3879461673493272,
      "grad_norm": 0.13342411816120148,
      "learning_rate": 0.0002870413739266198,
      "loss": 0.0449,
      "step": 593
    },
    {
      "epoch": 1.390286717378584,
      "grad_norm": 0.20266355574131012,
      "learning_rate": 0.00028696330991412957,
      "loss": 0.0465,
      "step": 594
    },
    {
      "epoch": 1.3926272674078408,
      "grad_norm": 0.15538877248764038,
      "learning_rate": 0.0002868852459016393,
      "loss": 0.0633,
      "step": 595
    },
    {
      "epoch": 1.3949678174370979,
      "grad_norm": 0.17042121291160583,
      "learning_rate": 0.00028680718188914907,
      "loss": 0.0474,
      "step": 596
    },
    {
      "epoch": 1.3973083674663547,
      "grad_norm": 0.18082723021507263,
      "learning_rate": 0.00028672911787665884,
      "loss": 0.0448,
      "step": 597
    },
    {
      "epoch": 1.3996489174956115,
      "grad_norm": 0.14876340329647064,
      "learning_rate": 0.00028665105386416857,
      "loss": 0.0587,
      "step": 598
    },
    {
      "epoch": 1.4019894675248683,
      "grad_norm": 0.1473539173603058,
      "learning_rate": 0.00028657298985167835,
      "loss": 0.0476,
      "step": 599
    },
    {
      "epoch": 1.4043300175541251,
      "grad_norm": 0.1577076017856598,
      "learning_rate": 0.0002864949258391881,
      "loss": 0.0336,
      "step": 600
    },
    {
      "epoch": 1.4066705675833822,
      "grad_norm": 0.166984423995018,
      "learning_rate": 0.00028641686182669785,
      "loss": 0.0598,
      "step": 601
    },
    {
      "epoch": 1.409011117612639,
      "grad_norm": 0.1482168436050415,
      "learning_rate": 0.0002863387978142076,
      "loss": 0.0425,
      "step": 602
    },
    {
      "epoch": 1.4113516676418958,
      "grad_norm": 0.17350085079669952,
      "learning_rate": 0.0002862607338017174,
      "loss": 0.0469,
      "step": 603
    },
    {
      "epoch": 1.4136922176711528,
      "grad_norm": 0.13300028443336487,
      "learning_rate": 0.0002861826697892271,
      "loss": 0.0572,
      "step": 604
    },
    {
      "epoch": 1.4160327677004096,
      "grad_norm": 0.16137517988681793,
      "learning_rate": 0.0002861046057767369,
      "loss": 0.0559,
      "step": 605
    },
    {
      "epoch": 1.4183733177296665,
      "grad_norm": 0.1624729037284851,
      "learning_rate": 0.0002860265417642467,
      "loss": 0.0674,
      "step": 606
    },
    {
      "epoch": 1.4207138677589233,
      "grad_norm": 0.18148818612098694,
      "learning_rate": 0.0002859484777517564,
      "loss": 0.0452,
      "step": 607
    },
    {
      "epoch": 1.42305441778818,
      "grad_norm": 0.10931342095136642,
      "learning_rate": 0.0002858704137392662,
      "loss": 0.0403,
      "step": 608
    },
    {
      "epoch": 1.4253949678174371,
      "grad_norm": 0.11990415304899216,
      "learning_rate": 0.0002857923497267759,
      "loss": 0.0435,
      "step": 609
    },
    {
      "epoch": 1.427735517846694,
      "grad_norm": 0.12027093768119812,
      "learning_rate": 0.0002857142857142857,
      "loss": 0.0498,
      "step": 610
    },
    {
      "epoch": 1.4300760678759508,
      "grad_norm": 0.13788656890392303,
      "learning_rate": 0.00028563622170179546,
      "loss": 0.0578,
      "step": 611
    },
    {
      "epoch": 1.4324166179052078,
      "grad_norm": 0.16981807351112366,
      "learning_rate": 0.0002855581576893052,
      "loss": 0.0373,
      "step": 612
    },
    {
      "epoch": 1.4347571679344646,
      "grad_norm": 0.14921648800373077,
      "learning_rate": 0.00028548009367681496,
      "loss": 0.042,
      "step": 613
    },
    {
      "epoch": 1.4370977179637214,
      "grad_norm": 0.14703193306922913,
      "learning_rate": 0.00028540202966432474,
      "loss": 0.0496,
      "step": 614
    },
    {
      "epoch": 1.4394382679929785,
      "grad_norm": 0.139468714594841,
      "learning_rate": 0.00028532396565183446,
      "loss": 0.0384,
      "step": 615
    },
    {
      "epoch": 1.4417788180222353,
      "grad_norm": 0.23802874982357025,
      "learning_rate": 0.00028524590163934424,
      "loss": 0.0743,
      "step": 616
    },
    {
      "epoch": 1.444119368051492,
      "grad_norm": 0.12422025948762894,
      "learning_rate": 0.000285167837626854,
      "loss": 0.0347,
      "step": 617
    },
    {
      "epoch": 1.446459918080749,
      "grad_norm": 0.25104567408561707,
      "learning_rate": 0.00028508977361436374,
      "loss": 0.0797,
      "step": 618
    },
    {
      "epoch": 1.4488004681100057,
      "grad_norm": 0.16617223620414734,
      "learning_rate": 0.0002850117096018735,
      "loss": 0.0373,
      "step": 619
    },
    {
      "epoch": 1.4511410181392628,
      "grad_norm": 0.13582280278205872,
      "learning_rate": 0.0002849336455893833,
      "loss": 0.0472,
      "step": 620
    },
    {
      "epoch": 1.4534815681685196,
      "grad_norm": 0.11525869369506836,
      "learning_rate": 0.000284855581576893,
      "loss": 0.0256,
      "step": 621
    },
    {
      "epoch": 1.4558221181977764,
      "grad_norm": 0.14013658463954926,
      "learning_rate": 0.0002847775175644028,
      "loss": 0.0559,
      "step": 622
    },
    {
      "epoch": 1.4581626682270334,
      "grad_norm": 0.1652929186820984,
      "learning_rate": 0.00028469945355191257,
      "loss": 0.0427,
      "step": 623
    },
    {
      "epoch": 1.4605032182562903,
      "grad_norm": 0.1410921812057495,
      "learning_rate": 0.0002846213895394223,
      "loss": 0.0497,
      "step": 624
    },
    {
      "epoch": 1.462843768285547,
      "grad_norm": 0.14703801274299622,
      "learning_rate": 0.0002845433255269321,
      "loss": 0.0426,
      "step": 625
    },
    {
      "epoch": 1.4651843183148041,
      "grad_norm": 0.19639943540096283,
      "learning_rate": 0.0002844652615144418,
      "loss": 0.0567,
      "step": 626
    },
    {
      "epoch": 1.467524868344061,
      "grad_norm": 0.09754244983196259,
      "learning_rate": 0.0002843871975019516,
      "loss": 0.0227,
      "step": 627
    },
    {
      "epoch": 1.4698654183733177,
      "grad_norm": 0.13475388288497925,
      "learning_rate": 0.00028430913348946135,
      "loss": 0.0254,
      "step": 628
    },
    {
      "epoch": 1.4722059684025746,
      "grad_norm": 0.17084737122058868,
      "learning_rate": 0.0002842310694769711,
      "loss": 0.054,
      "step": 629
    },
    {
      "epoch": 1.4745465184318314,
      "grad_norm": 0.12813310325145721,
      "learning_rate": 0.00028415300546448085,
      "loss": 0.0444,
      "step": 630
    },
    {
      "epoch": 1.4768870684610884,
      "grad_norm": 0.15119831264019012,
      "learning_rate": 0.00028407494145199063,
      "loss": 0.0403,
      "step": 631
    },
    {
      "epoch": 1.4792276184903452,
      "grad_norm": 0.14408650994300842,
      "learning_rate": 0.00028399687743950035,
      "loss": 0.0403,
      "step": 632
    },
    {
      "epoch": 1.481568168519602,
      "grad_norm": 0.16942507028579712,
      "learning_rate": 0.00028391881342701013,
      "loss": 0.0501,
      "step": 633
    },
    {
      "epoch": 1.483908718548859,
      "grad_norm": 0.17771652340888977,
      "learning_rate": 0.0002838407494145199,
      "loss": 0.0485,
      "step": 634
    },
    {
      "epoch": 1.486249268578116,
      "grad_norm": 0.1756303757429123,
      "learning_rate": 0.00028376268540202963,
      "loss": 0.064,
      "step": 635
    },
    {
      "epoch": 1.4885898186073727,
      "grad_norm": 0.1714443862438202,
      "learning_rate": 0.0002836846213895394,
      "loss": 0.0632,
      "step": 636
    },
    {
      "epoch": 1.4909303686366295,
      "grad_norm": 0.13853757083415985,
      "learning_rate": 0.0002836065573770492,
      "loss": 0.0408,
      "step": 637
    },
    {
      "epoch": 1.4932709186658863,
      "grad_norm": 0.1273476481437683,
      "learning_rate": 0.0002835284933645589,
      "loss": 0.0422,
      "step": 638
    },
    {
      "epoch": 1.4956114686951434,
      "grad_norm": 0.13357651233673096,
      "learning_rate": 0.0002834504293520687,
      "loss": 0.0482,
      "step": 639
    },
    {
      "epoch": 1.4979520187244002,
      "grad_norm": 0.14576220512390137,
      "learning_rate": 0.00028337236533957846,
      "loss": 0.0476,
      "step": 640
    },
    {
      "epoch": 1.500292568753657,
      "grad_norm": 0.21695896983146667,
      "learning_rate": 0.0002832943013270882,
      "loss": 0.0463,
      "step": 641
    },
    {
      "epoch": 1.502633118782914,
      "grad_norm": 0.09828992933034897,
      "learning_rate": 0.0002832162373145979,
      "loss": 0.0355,
      "step": 642
    },
    {
      "epoch": 1.5049736688121709,
      "grad_norm": 0.23431329429149628,
      "learning_rate": 0.00028313817330210774,
      "loss": 0.0571,
      "step": 643
    },
    {
      "epoch": 1.5073142188414277,
      "grad_norm": 0.16922973096370697,
      "learning_rate": 0.00028306010928961747,
      "loss": 0.0678,
      "step": 644
    },
    {
      "epoch": 1.5096547688706847,
      "grad_norm": 0.14698928594589233,
      "learning_rate": 0.0002829820452771272,
      "loss": 0.0466,
      "step": 645
    },
    {
      "epoch": 1.5119953188999413,
      "grad_norm": 0.1664460003376007,
      "learning_rate": 0.00028290398126463697,
      "loss": 0.0517,
      "step": 646
    },
    {
      "epoch": 1.5143358689291984,
      "grad_norm": 0.13238629698753357,
      "learning_rate": 0.00028282591725214674,
      "loss": 0.0413,
      "step": 647
    },
    {
      "epoch": 1.5166764189584554,
      "grad_norm": 0.16496500372886658,
      "learning_rate": 0.00028274785323965647,
      "loss": 0.0451,
      "step": 648
    },
    {
      "epoch": 1.519016968987712,
      "grad_norm": 0.14589735865592957,
      "learning_rate": 0.00028266978922716625,
      "loss": 0.0553,
      "step": 649
    },
    {
      "epoch": 1.521357519016969,
      "grad_norm": 0.14261016249656677,
      "learning_rate": 0.000282591725214676,
      "loss": 0.0722,
      "step": 650
    },
    {
      "epoch": 1.5236980690462258,
      "grad_norm": 0.1642467975616455,
      "learning_rate": 0.00028251366120218575,
      "loss": 0.0657,
      "step": 651
    },
    {
      "epoch": 1.5260386190754827,
      "grad_norm": 0.19031624495983124,
      "learning_rate": 0.0002824355971896955,
      "loss": 0.0746,
      "step": 652
    },
    {
      "epoch": 1.5283791691047397,
      "grad_norm": 0.1642458736896515,
      "learning_rate": 0.0002823575331772053,
      "loss": 0.0351,
      "step": 653
    },
    {
      "epoch": 1.5307197191339965,
      "grad_norm": 0.18984144926071167,
      "learning_rate": 0.000282279469164715,
      "loss": 0.0598,
      "step": 654
    },
    {
      "epoch": 1.5330602691632533,
      "grad_norm": 0.17405718564987183,
      "learning_rate": 0.0002822014051522248,
      "loss": 0.0498,
      "step": 655
    },
    {
      "epoch": 1.5354008191925104,
      "grad_norm": 0.18893088400363922,
      "learning_rate": 0.0002821233411397346,
      "loss": 0.0688,
      "step": 656
    },
    {
      "epoch": 1.537741369221767,
      "grad_norm": 0.09700368344783783,
      "learning_rate": 0.0002820452771272443,
      "loss": 0.0471,
      "step": 657
    },
    {
      "epoch": 1.540081919251024,
      "grad_norm": 0.17297841608524323,
      "learning_rate": 0.0002819672131147541,
      "loss": 0.0614,
      "step": 658
    },
    {
      "epoch": 1.5424224692802808,
      "grad_norm": 0.13750424981117249,
      "learning_rate": 0.00028188914910226386,
      "loss": 0.0323,
      "step": 659
    },
    {
      "epoch": 1.5447630193095376,
      "grad_norm": 0.14229856431484222,
      "learning_rate": 0.0002818110850897736,
      "loss": 0.0537,
      "step": 660
    },
    {
      "epoch": 1.5471035693387947,
      "grad_norm": 0.15897709131240845,
      "learning_rate": 0.00028173302107728336,
      "loss": 0.0452,
      "step": 661
    },
    {
      "epoch": 1.5494441193680515,
      "grad_norm": 0.13842298090457916,
      "learning_rate": 0.0002816549570647931,
      "loss": 0.0471,
      "step": 662
    },
    {
      "epoch": 1.5517846693973083,
      "grad_norm": 0.14539821445941925,
      "learning_rate": 0.00028157689305230286,
      "loss": 0.0398,
      "step": 663
    },
    {
      "epoch": 1.5541252194265653,
      "grad_norm": 0.10296301543712616,
      "learning_rate": 0.00028149882903981264,
      "loss": 0.0335,
      "step": 664
    },
    {
      "epoch": 1.5564657694558222,
      "grad_norm": 0.15192481875419617,
      "learning_rate": 0.00028142076502732236,
      "loss": 0.0638,
      "step": 665
    },
    {
      "epoch": 1.558806319485079,
      "grad_norm": 0.2030075192451477,
      "learning_rate": 0.00028134270101483214,
      "loss": 0.0627,
      "step": 666
    },
    {
      "epoch": 1.561146869514336,
      "grad_norm": 0.13270990550518036,
      "learning_rate": 0.0002812646370023419,
      "loss": 0.0409,
      "step": 667
    },
    {
      "epoch": 1.5634874195435926,
      "grad_norm": 0.12450022995471954,
      "learning_rate": 0.00028118657298985164,
      "loss": 0.0364,
      "step": 668
    },
    {
      "epoch": 1.5658279695728496,
      "grad_norm": 0.20377109944820404,
      "learning_rate": 0.0002811085089773614,
      "loss": 0.0557,
      "step": 669
    },
    {
      "epoch": 1.5681685196021065,
      "grad_norm": 0.1399642527103424,
      "learning_rate": 0.0002810304449648712,
      "loss": 0.0393,
      "step": 670
    },
    {
      "epoch": 1.5705090696313633,
      "grad_norm": 0.21811312437057495,
      "learning_rate": 0.0002809523809523809,
      "loss": 0.0478,
      "step": 671
    },
    {
      "epoch": 1.5728496196606203,
      "grad_norm": 0.14154978096485138,
      "learning_rate": 0.0002808743169398907,
      "loss": 0.0543,
      "step": 672
    },
    {
      "epoch": 1.5751901696898771,
      "grad_norm": 0.14617066085338593,
      "learning_rate": 0.00028079625292740047,
      "loss": 0.0351,
      "step": 673
    },
    {
      "epoch": 1.577530719719134,
      "grad_norm": 0.18541856110095978,
      "learning_rate": 0.0002807181889149102,
      "loss": 0.0428,
      "step": 674
    },
    {
      "epoch": 1.579871269748391,
      "grad_norm": 0.1719909906387329,
      "learning_rate": 0.00028064012490242,
      "loss": 0.0533,
      "step": 675
    },
    {
      "epoch": 1.5822118197776478,
      "grad_norm": 0.15591248869895935,
      "learning_rate": 0.00028056206088992975,
      "loss": 0.0559,
      "step": 676
    },
    {
      "epoch": 1.5845523698069046,
      "grad_norm": 0.1888672113418579,
      "learning_rate": 0.0002804839968774395,
      "loss": 0.0609,
      "step": 677
    },
    {
      "epoch": 1.5868929198361617,
      "grad_norm": 0.13629354536533356,
      "learning_rate": 0.0002804059328649492,
      "loss": 0.061,
      "step": 678
    },
    {
      "epoch": 1.5892334698654182,
      "grad_norm": 0.1891973465681076,
      "learning_rate": 0.00028032786885245903,
      "loss": 0.0621,
      "step": 679
    },
    {
      "epoch": 1.5915740198946753,
      "grad_norm": 0.20009629428386688,
      "learning_rate": 0.00028024980483996875,
      "loss": 0.0542,
      "step": 680
    },
    {
      "epoch": 1.593914569923932,
      "grad_norm": 0.1523730456829071,
      "learning_rate": 0.0002801717408274785,
      "loss": 0.0594,
      "step": 681
    },
    {
      "epoch": 1.596255119953189,
      "grad_norm": 0.10827874392271042,
      "learning_rate": 0.00028009367681498825,
      "loss": 0.0331,
      "step": 682
    },
    {
      "epoch": 1.598595669982446,
      "grad_norm": 0.20174172520637512,
      "learning_rate": 0.00028001561280249803,
      "loss": 0.0612,
      "step": 683
    },
    {
      "epoch": 1.6009362200117028,
      "grad_norm": 0.13016435503959656,
      "learning_rate": 0.00027993754879000775,
      "loss": 0.0425,
      "step": 684
    },
    {
      "epoch": 1.6032767700409596,
      "grad_norm": 0.10535770654678345,
      "learning_rate": 0.00027985948477751753,
      "loss": 0.0235,
      "step": 685
    },
    {
      "epoch": 1.6056173200702166,
      "grad_norm": 0.13293199241161346,
      "learning_rate": 0.0002797814207650273,
      "loss": 0.0382,
      "step": 686
    },
    {
      "epoch": 1.6079578700994732,
      "grad_norm": 0.18920838832855225,
      "learning_rate": 0.00027970335675253703,
      "loss": 0.0651,
      "step": 687
    },
    {
      "epoch": 1.6102984201287303,
      "grad_norm": 0.1275138407945633,
      "learning_rate": 0.0002796252927400468,
      "loss": 0.0465,
      "step": 688
    },
    {
      "epoch": 1.612638970157987,
      "grad_norm": 0.17549341917037964,
      "learning_rate": 0.0002795472287275566,
      "loss": 0.0498,
      "step": 689
    },
    {
      "epoch": 1.6149795201872439,
      "grad_norm": 0.16874071955680847,
      "learning_rate": 0.0002794691647150663,
      "loss": 0.0576,
      "step": 690
    },
    {
      "epoch": 1.617320070216501,
      "grad_norm": 0.1837470382452011,
      "learning_rate": 0.0002793911007025761,
      "loss": 0.056,
      "step": 691
    },
    {
      "epoch": 1.6196606202457577,
      "grad_norm": 0.13519492745399475,
      "learning_rate": 0.00027931303669008587,
      "loss": 0.0478,
      "step": 692
    },
    {
      "epoch": 1.6220011702750146,
      "grad_norm": 0.17921575903892517,
      "learning_rate": 0.0002792349726775956,
      "loss": 0.0625,
      "step": 693
    },
    {
      "epoch": 1.6243417203042716,
      "grad_norm": 0.15359003841876984,
      "learning_rate": 0.00027915690866510537,
      "loss": 0.0442,
      "step": 694
    },
    {
      "epoch": 1.6266822703335284,
      "grad_norm": 0.10983893275260925,
      "learning_rate": 0.00027907884465261514,
      "loss": 0.0508,
      "step": 695
    },
    {
      "epoch": 1.6290228203627852,
      "grad_norm": 0.16676579415798187,
      "learning_rate": 0.00027900078064012487,
      "loss": 0.0496,
      "step": 696
    },
    {
      "epoch": 1.6313633703920423,
      "grad_norm": 0.1649477630853653,
      "learning_rate": 0.00027892271662763465,
      "loss": 0.0541,
      "step": 697
    },
    {
      "epoch": 1.6337039204212989,
      "grad_norm": 0.15396137535572052,
      "learning_rate": 0.00027884465261514437,
      "loss": 0.0536,
      "step": 698
    },
    {
      "epoch": 1.636044470450556,
      "grad_norm": 0.16484256088733673,
      "learning_rate": 0.00027876658860265415,
      "loss": 0.0527,
      "step": 699
    },
    {
      "epoch": 1.6383850204798127,
      "grad_norm": 0.15350857377052307,
      "learning_rate": 0.0002786885245901639,
      "loss": 0.0412,
      "step": 700
    },
    {
      "epoch": 1.6407255705090695,
      "grad_norm": 0.13477429747581482,
      "learning_rate": 0.00027861046057767365,
      "loss": 0.0505,
      "step": 701
    },
    {
      "epoch": 1.6430661205383266,
      "grad_norm": 0.1377808302640915,
      "learning_rate": 0.0002785323965651834,
      "loss": 0.0272,
      "step": 702
    },
    {
      "epoch": 1.6454066705675834,
      "grad_norm": 0.11579301208257675,
      "learning_rate": 0.0002784543325526932,
      "loss": 0.0415,
      "step": 703
    },
    {
      "epoch": 1.6477472205968402,
      "grad_norm": 0.17399922013282776,
      "learning_rate": 0.0002783762685402029,
      "loss": 0.0524,
      "step": 704
    },
    {
      "epoch": 1.6500877706260972,
      "grad_norm": 0.18455682694911957,
      "learning_rate": 0.0002782982045277127,
      "loss": 0.0484,
      "step": 705
    },
    {
      "epoch": 1.652428320655354,
      "grad_norm": 0.11977031826972961,
      "learning_rate": 0.0002782201405152225,
      "loss": 0.0378,
      "step": 706
    },
    {
      "epoch": 1.6547688706846109,
      "grad_norm": 0.17436723411083221,
      "learning_rate": 0.0002781420765027322,
      "loss": 0.0485,
      "step": 707
    },
    {
      "epoch": 1.657109420713868,
      "grad_norm": 0.18152333796024323,
      "learning_rate": 0.000278064012490242,
      "loss": 0.0565,
      "step": 708
    },
    {
      "epoch": 1.6594499707431245,
      "grad_norm": 0.19136445224285126,
      "learning_rate": 0.00027798594847775176,
      "loss": 0.0695,
      "step": 709
    },
    {
      "epoch": 1.6617905207723815,
      "grad_norm": 0.14144369959831238,
      "learning_rate": 0.0002779078844652615,
      "loss": 0.0378,
      "step": 710
    },
    {
      "epoch": 1.6641310708016384,
      "grad_norm": 0.14639751613140106,
      "learning_rate": 0.00027782982045277126,
      "loss": 0.0442,
      "step": 711
    },
    {
      "epoch": 1.6664716208308952,
      "grad_norm": 0.15747658908367157,
      "learning_rate": 0.00027775175644028104,
      "loss": 0.0368,
      "step": 712
    },
    {
      "epoch": 1.6688121708601522,
      "grad_norm": 0.14835284650325775,
      "learning_rate": 0.00027767369242779076,
      "loss": 0.0419,
      "step": 713
    },
    {
      "epoch": 1.671152720889409,
      "grad_norm": 0.10997939109802246,
      "learning_rate": 0.0002775956284153005,
      "loss": 0.0427,
      "step": 714
    },
    {
      "epoch": 1.6734932709186658,
      "grad_norm": 0.19150270521640778,
      "learning_rate": 0.0002775175644028103,
      "loss": 0.024,
      "step": 715
    },
    {
      "epoch": 1.6758338209479229,
      "grad_norm": 0.18824660778045654,
      "learning_rate": 0.00027743950039032004,
      "loss": 0.0277,
      "step": 716
    },
    {
      "epoch": 1.6781743709771795,
      "grad_norm": 0.20899300277233124,
      "learning_rate": 0.00027736143637782976,
      "loss": 0.06,
      "step": 717
    },
    {
      "epoch": 1.6805149210064365,
      "grad_norm": 0.077788345515728,
      "learning_rate": 0.00027728337236533954,
      "loss": 0.0247,
      "step": 718
    },
    {
      "epoch": 1.6828554710356936,
      "grad_norm": 0.14428076148033142,
      "learning_rate": 0.0002772053083528493,
      "loss": 0.0561,
      "step": 719
    },
    {
      "epoch": 1.6851960210649501,
      "grad_norm": 0.06641039997339249,
      "learning_rate": 0.00027712724434035904,
      "loss": 0.0248,
      "step": 720
    },
    {
      "epoch": 1.6875365710942072,
      "grad_norm": 0.16417334973812103,
      "learning_rate": 0.0002770491803278688,
      "loss": 0.0515,
      "step": 721
    },
    {
      "epoch": 1.689877121123464,
      "grad_norm": 0.17783351242542267,
      "learning_rate": 0.0002769711163153786,
      "loss": 0.0501,
      "step": 722
    },
    {
      "epoch": 1.6922176711527208,
      "grad_norm": 0.1252652108669281,
      "learning_rate": 0.0002768930523028883,
      "loss": 0.0505,
      "step": 723
    },
    {
      "epoch": 1.6945582211819779,
      "grad_norm": 0.14660735428333282,
      "learning_rate": 0.0002768149882903981,
      "loss": 0.0561,
      "step": 724
    },
    {
      "epoch": 1.6968987712112347,
      "grad_norm": 0.14866164326667786,
      "learning_rate": 0.0002767369242779079,
      "loss": 0.0456,
      "step": 725
    },
    {
      "epoch": 1.6992393212404915,
      "grad_norm": 0.15502141416072845,
      "learning_rate": 0.0002766588602654176,
      "loss": 0.0445,
      "step": 726
    },
    {
      "epoch": 1.7015798712697485,
      "grad_norm": 0.13911060988903046,
      "learning_rate": 0.0002765807962529274,
      "loss": 0.0611,
      "step": 727
    },
    {
      "epoch": 1.7039204212990051,
      "grad_norm": 0.13983874022960663,
      "learning_rate": 0.00027650273224043715,
      "loss": 0.0417,
      "step": 728
    },
    {
      "epoch": 1.7062609713282622,
      "grad_norm": 0.12274470925331116,
      "learning_rate": 0.0002764246682279469,
      "loss": 0.0727,
      "step": 729
    },
    {
      "epoch": 1.708601521357519,
      "grad_norm": 0.12695223093032837,
      "learning_rate": 0.00027634660421545665,
      "loss": 0.0435,
      "step": 730
    },
    {
      "epoch": 1.7109420713867758,
      "grad_norm": 0.13381604850292206,
      "learning_rate": 0.00027626854020296643,
      "loss": 0.0535,
      "step": 731
    },
    {
      "epoch": 1.7132826214160328,
      "grad_norm": 0.10630623996257782,
      "learning_rate": 0.00027619047619047615,
      "loss": 0.0421,
      "step": 732
    },
    {
      "epoch": 1.7156231714452896,
      "grad_norm": 0.11505663394927979,
      "learning_rate": 0.00027611241217798593,
      "loss": 0.0608,
      "step": 733
    },
    {
      "epoch": 1.7179637214745465,
      "grad_norm": 0.14952825009822845,
      "learning_rate": 0.00027603434816549565,
      "loss": 0.0504,
      "step": 734
    },
    {
      "epoch": 1.7203042715038035,
      "grad_norm": 0.1421717256307602,
      "learning_rate": 0.00027595628415300543,
      "loss": 0.0529,
      "step": 735
    },
    {
      "epoch": 1.7226448215330603,
      "grad_norm": 0.09814899414777756,
      "learning_rate": 0.0002758782201405152,
      "loss": 0.0444,
      "step": 736
    },
    {
      "epoch": 1.7249853715623171,
      "grad_norm": 0.1165069043636322,
      "learning_rate": 0.00027580015612802493,
      "loss": 0.0491,
      "step": 737
    },
    {
      "epoch": 1.7273259215915742,
      "grad_norm": 0.13242179155349731,
      "learning_rate": 0.0002757220921155347,
      "loss": 0.0484,
      "step": 738
    },
    {
      "epoch": 1.7296664716208308,
      "grad_norm": 0.13584955036640167,
      "learning_rate": 0.0002756440281030445,
      "loss": 0.0458,
      "step": 739
    },
    {
      "epoch": 1.7320070216500878,
      "grad_norm": 0.13142573833465576,
      "learning_rate": 0.0002755659640905542,
      "loss": 0.045,
      "step": 740
    },
    {
      "epoch": 1.7343475716793446,
      "grad_norm": 0.12565329670906067,
      "learning_rate": 0.000275487900078064,
      "loss": 0.0498,
      "step": 741
    },
    {
      "epoch": 1.7366881217086014,
      "grad_norm": 0.15411405265331268,
      "learning_rate": 0.00027540983606557377,
      "loss": 0.052,
      "step": 742
    },
    {
      "epoch": 1.7390286717378585,
      "grad_norm": 0.18758566677570343,
      "learning_rate": 0.0002753317720530835,
      "loss": 0.0503,
      "step": 743
    },
    {
      "epoch": 1.7413692217671153,
      "grad_norm": 0.10440848767757416,
      "learning_rate": 0.00027525370804059327,
      "loss": 0.0218,
      "step": 744
    },
    {
      "epoch": 1.743709771796372,
      "grad_norm": 0.12183352559804916,
      "learning_rate": 0.00027517564402810304,
      "loss": 0.0429,
      "step": 745
    },
    {
      "epoch": 1.7460503218256291,
      "grad_norm": 0.191852867603302,
      "learning_rate": 0.00027509758001561277,
      "loss": 0.0449,
      "step": 746
    },
    {
      "epoch": 1.748390871854886,
      "grad_norm": 0.11775554716587067,
      "learning_rate": 0.00027501951600312255,
      "loss": 0.0387,
      "step": 747
    },
    {
      "epoch": 1.7507314218841428,
      "grad_norm": 0.15812534093856812,
      "learning_rate": 0.0002749414519906323,
      "loss": 0.0487,
      "step": 748
    },
    {
      "epoch": 1.7530719719133998,
      "grad_norm": 0.12483052909374237,
      "learning_rate": 0.00027486338797814205,
      "loss": 0.0242,
      "step": 749
    },
    {
      "epoch": 1.7554125219426564,
      "grad_norm": 0.14269472658634186,
      "learning_rate": 0.0002747853239656518,
      "loss": 0.0481,
      "step": 750
    },
    {
      "epoch": 1.7577530719719134,
      "grad_norm": 0.1556752622127533,
      "learning_rate": 0.00027470725995316155,
      "loss": 0.0516,
      "step": 751
    },
    {
      "epoch": 1.7600936220011703,
      "grad_norm": 0.23031745851039886,
      "learning_rate": 0.0002746291959406713,
      "loss": 0.0598,
      "step": 752
    },
    {
      "epoch": 1.762434172030427,
      "grad_norm": 0.1393490582704544,
      "learning_rate": 0.0002745511319281811,
      "loss": 0.051,
      "step": 753
    },
    {
      "epoch": 1.764774722059684,
      "grad_norm": 0.13713759183883667,
      "learning_rate": 0.0002744730679156908,
      "loss": 0.0406,
      "step": 754
    },
    {
      "epoch": 1.767115272088941,
      "grad_norm": 0.28021982312202454,
      "learning_rate": 0.0002743950039032006,
      "loss": 0.0461,
      "step": 755
    },
    {
      "epoch": 1.7694558221181977,
      "grad_norm": 0.1678316742181778,
      "learning_rate": 0.0002743169398907104,
      "loss": 0.0372,
      "step": 756
    },
    {
      "epoch": 1.7717963721474548,
      "grad_norm": 0.17203551530838013,
      "learning_rate": 0.0002742388758782201,
      "loss": 0.0435,
      "step": 757
    },
    {
      "epoch": 1.7741369221767114,
      "grad_norm": 0.11407439410686493,
      "learning_rate": 0.0002741608118657299,
      "loss": 0.0501,
      "step": 758
    },
    {
      "epoch": 1.7764774722059684,
      "grad_norm": 0.16989827156066895,
      "learning_rate": 0.00027408274785323966,
      "loss": 0.0455,
      "step": 759
    },
    {
      "epoch": 1.7788180222352252,
      "grad_norm": 0.1409887969493866,
      "learning_rate": 0.0002740046838407494,
      "loss": 0.043,
      "step": 760
    },
    {
      "epoch": 1.781158572264482,
      "grad_norm": 0.150722473859787,
      "learning_rate": 0.00027392661982825916,
      "loss": 0.0531,
      "step": 761
    },
    {
      "epoch": 1.783499122293739,
      "grad_norm": 0.1204099953174591,
      "learning_rate": 0.00027384855581576894,
      "loss": 0.0271,
      "step": 762
    },
    {
      "epoch": 1.785839672322996,
      "grad_norm": 0.1549573540687561,
      "learning_rate": 0.00027377049180327866,
      "loss": 0.0575,
      "step": 763
    },
    {
      "epoch": 1.7881802223522527,
      "grad_norm": 0.14639721810817719,
      "learning_rate": 0.00027369242779078844,
      "loss": 0.039,
      "step": 764
    },
    {
      "epoch": 1.7905207723815098,
      "grad_norm": 0.09609652310609818,
      "learning_rate": 0.0002736143637782982,
      "loss": 0.0394,
      "step": 765
    },
    {
      "epoch": 1.7928613224107666,
      "grad_norm": 0.16313442587852478,
      "learning_rate": 0.00027353629976580794,
      "loss": 0.0495,
      "step": 766
    },
    {
      "epoch": 1.7952018724400234,
      "grad_norm": 0.21597382426261902,
      "learning_rate": 0.0002734582357533177,
      "loss": 0.0427,
      "step": 767
    },
    {
      "epoch": 1.7975424224692804,
      "grad_norm": 0.16540424525737762,
      "learning_rate": 0.0002733801717408275,
      "loss": 0.0462,
      "step": 768
    },
    {
      "epoch": 1.799882972498537,
      "grad_norm": 0.1930052936077118,
      "learning_rate": 0.0002733021077283372,
      "loss": 0.061,
      "step": 769
    },
    {
      "epoch": 1.802223522527794,
      "grad_norm": 0.1396678388118744,
      "learning_rate": 0.00027322404371584694,
      "loss": 0.0417,
      "step": 770
    },
    {
      "epoch": 1.8045640725570509,
      "grad_norm": 0.1316436231136322,
      "learning_rate": 0.0002731459797033567,
      "loss": 0.0338,
      "step": 771
    },
    {
      "epoch": 1.8069046225863077,
      "grad_norm": 0.16810603439807892,
      "learning_rate": 0.0002730679156908665,
      "loss": 0.0502,
      "step": 772
    },
    {
      "epoch": 1.8092451726155647,
      "grad_norm": 0.15747284889221191,
      "learning_rate": 0.0002729898516783762,
      "loss": 0.0363,
      "step": 773
    },
    {
      "epoch": 1.8115857226448215,
      "grad_norm": 0.15499374270439148,
      "learning_rate": 0.000272911787665886,
      "loss": 0.0585,
      "step": 774
    },
    {
      "epoch": 1.8139262726740784,
      "grad_norm": 0.18051603436470032,
      "learning_rate": 0.0002728337236533958,
      "loss": 0.0652,
      "step": 775
    },
    {
      "epoch": 1.8162668227033354,
      "grad_norm": 0.16155897080898285,
      "learning_rate": 0.0002727556596409055,
      "loss": 0.032,
      "step": 776
    },
    {
      "epoch": 1.8186073727325922,
      "grad_norm": 0.15362286567687988,
      "learning_rate": 0.0002726775956284153,
      "loss": 0.0644,
      "step": 777
    },
    {
      "epoch": 1.820947922761849,
      "grad_norm": 0.10404124855995178,
      "learning_rate": 0.00027259953161592505,
      "loss": 0.0334,
      "step": 778
    },
    {
      "epoch": 1.823288472791106,
      "grad_norm": 0.2329428791999817,
      "learning_rate": 0.0002725214676034348,
      "loss": 0.0611,
      "step": 779
    },
    {
      "epoch": 1.8256290228203627,
      "grad_norm": 0.1463456004858017,
      "learning_rate": 0.00027244340359094455,
      "loss": 0.0666,
      "step": 780
    },
    {
      "epoch": 1.8279695728496197,
      "grad_norm": 0.1995156705379486,
      "learning_rate": 0.00027236533957845433,
      "loss": 0.0555,
      "step": 781
    },
    {
      "epoch": 1.8303101228788765,
      "grad_norm": 0.10868395864963531,
      "learning_rate": 0.00027228727556596405,
      "loss": 0.044,
      "step": 782
    },
    {
      "epoch": 1.8326506729081333,
      "grad_norm": 0.1118946298956871,
      "learning_rate": 0.00027220921155347383,
      "loss": 0.0428,
      "step": 783
    },
    {
      "epoch": 1.8349912229373904,
      "grad_norm": 0.13608203828334808,
      "learning_rate": 0.0002721311475409836,
      "loss": 0.0515,
      "step": 784
    },
    {
      "epoch": 1.8373317729666472,
      "grad_norm": 0.11463767290115356,
      "learning_rate": 0.00027205308352849333,
      "loss": 0.0587,
      "step": 785
    },
    {
      "epoch": 1.839672322995904,
      "grad_norm": 0.13760975003242493,
      "learning_rate": 0.0002719750195160031,
      "loss": 0.0731,
      "step": 786
    },
    {
      "epoch": 1.842012873025161,
      "grad_norm": 0.16537153720855713,
      "learning_rate": 0.00027189695550351283,
      "loss": 0.0542,
      "step": 787
    },
    {
      "epoch": 1.8443534230544176,
      "grad_norm": 0.16000108420848846,
      "learning_rate": 0.0002718188914910226,
      "loss": 0.0355,
      "step": 788
    },
    {
      "epoch": 1.8466939730836747,
      "grad_norm": 0.13486221432685852,
      "learning_rate": 0.0002717408274785324,
      "loss": 0.0544,
      "step": 789
    },
    {
      "epoch": 1.8490345231129317,
      "grad_norm": 0.12919530272483826,
      "learning_rate": 0.0002716627634660421,
      "loss": 0.0325,
      "step": 790
    },
    {
      "epoch": 1.8513750731421883,
      "grad_norm": 0.12232176214456558,
      "learning_rate": 0.0002715846994535519,
      "loss": 0.0369,
      "step": 791
    },
    {
      "epoch": 1.8537156231714453,
      "grad_norm": 0.14404761791229248,
      "learning_rate": 0.00027150663544106167,
      "loss": 0.0566,
      "step": 792
    },
    {
      "epoch": 1.8560561732007022,
      "grad_norm": 0.11112670600414276,
      "learning_rate": 0.0002714285714285714,
      "loss": 0.0179,
      "step": 793
    },
    {
      "epoch": 1.858396723229959,
      "grad_norm": 0.12811897695064545,
      "learning_rate": 0.00027135050741608117,
      "loss": 0.0348,
      "step": 794
    },
    {
      "epoch": 1.860737273259216,
      "grad_norm": 0.09969937801361084,
      "learning_rate": 0.00027127244340359094,
      "loss": 0.0385,
      "step": 795
    },
    {
      "epoch": 1.8630778232884728,
      "grad_norm": 0.1263199895620346,
      "learning_rate": 0.00027119437939110067,
      "loss": 0.0429,
      "step": 796
    },
    {
      "epoch": 1.8654183733177296,
      "grad_norm": 0.16272258758544922,
      "learning_rate": 0.00027111631537861045,
      "loss": 0.0461,
      "step": 797
    },
    {
      "epoch": 1.8677589233469867,
      "grad_norm": 0.17512601613998413,
      "learning_rate": 0.0002710382513661202,
      "loss": 0.0374,
      "step": 798
    },
    {
      "epoch": 1.8700994733762433,
      "grad_norm": 0.10754399001598358,
      "learning_rate": 0.00027096018735362995,
      "loss": 0.0311,
      "step": 799
    },
    {
      "epoch": 1.8724400234055003,
      "grad_norm": 0.20508870482444763,
      "learning_rate": 0.0002708821233411397,
      "loss": 0.0882,
      "step": 800
    },
    {
      "epoch": 1.8747805734347571,
      "grad_norm": 0.1223282516002655,
      "learning_rate": 0.0002708040593286495,
      "loss": 0.0378,
      "step": 801
    },
    {
      "epoch": 1.877121123464014,
      "grad_norm": 0.17954754829406738,
      "learning_rate": 0.0002707259953161592,
      "loss": 0.0504,
      "step": 802
    },
    {
      "epoch": 1.879461673493271,
      "grad_norm": 0.15739049017429352,
      "learning_rate": 0.00027064793130366895,
      "loss": 0.031,
      "step": 803
    },
    {
      "epoch": 1.8818022235225278,
      "grad_norm": 0.16476289927959442,
      "learning_rate": 0.0002705698672911788,
      "loss": 0.0535,
      "step": 804
    },
    {
      "epoch": 1.8841427735517846,
      "grad_norm": 0.15142256021499634,
      "learning_rate": 0.0002704918032786885,
      "loss": 0.0567,
      "step": 805
    },
    {
      "epoch": 1.8864833235810416,
      "grad_norm": 0.11906732618808746,
      "learning_rate": 0.0002704137392661982,
      "loss": 0.0297,
      "step": 806
    },
    {
      "epoch": 1.8888238736102985,
      "grad_norm": 0.12400855869054794,
      "learning_rate": 0.000270335675253708,
      "loss": 0.0473,
      "step": 807
    },
    {
      "epoch": 1.8911644236395553,
      "grad_norm": 0.0760040357708931,
      "learning_rate": 0.0002702576112412178,
      "loss": 0.0221,
      "step": 808
    },
    {
      "epoch": 1.8935049736688123,
      "grad_norm": 0.09713473916053772,
      "learning_rate": 0.0002701795472287275,
      "loss": 0.035,
      "step": 809
    },
    {
      "epoch": 1.895845523698069,
      "grad_norm": 0.14270088076591492,
      "learning_rate": 0.0002701014832162373,
      "loss": 0.0542,
      "step": 810
    },
    {
      "epoch": 1.898186073727326,
      "grad_norm": 0.10253016650676727,
      "learning_rate": 0.00027002341920374706,
      "loss": 0.0336,
      "step": 811
    },
    {
      "epoch": 1.9005266237565828,
      "grad_norm": 0.12983042001724243,
      "learning_rate": 0.0002699453551912568,
      "loss": 0.0368,
      "step": 812
    },
    {
      "epoch": 1.9028671737858396,
      "grad_norm": 0.18920180201530457,
      "learning_rate": 0.00026986729117876656,
      "loss": 0.0447,
      "step": 813
    },
    {
      "epoch": 1.9052077238150966,
      "grad_norm": 0.13282257318496704,
      "learning_rate": 0.00026978922716627634,
      "loss": 0.0567,
      "step": 814
    },
    {
      "epoch": 1.9075482738443534,
      "grad_norm": 0.12674133479595184,
      "learning_rate": 0.00026971116315378606,
      "loss": 0.0287,
      "step": 815
    },
    {
      "epoch": 1.9098888238736103,
      "grad_norm": 0.14782926440238953,
      "learning_rate": 0.00026963309914129584,
      "loss": 0.0405,
      "step": 816
    },
    {
      "epoch": 1.9122293739028673,
      "grad_norm": 0.14597322046756744,
      "learning_rate": 0.0002695550351288056,
      "loss": 0.0507,
      "step": 817
    },
    {
      "epoch": 1.9145699239321239,
      "grad_norm": 0.18147309124469757,
      "learning_rate": 0.00026947697111631534,
      "loss": 0.0471,
      "step": 818
    },
    {
      "epoch": 1.916910473961381,
      "grad_norm": 0.17686273157596588,
      "learning_rate": 0.0002693989071038251,
      "loss": 0.0347,
      "step": 819
    },
    {
      "epoch": 1.919251023990638,
      "grad_norm": 0.13825352489948273,
      "learning_rate": 0.0002693208430913349,
      "loss": 0.0322,
      "step": 820
    },
    {
      "epoch": 1.9215915740198946,
      "grad_norm": 0.1697951704263687,
      "learning_rate": 0.0002692427790788446,
      "loss": 0.0503,
      "step": 821
    },
    {
      "epoch": 1.9239321240491516,
      "grad_norm": 0.16388030350208282,
      "learning_rate": 0.0002691647150663544,
      "loss": 0.0244,
      "step": 822
    },
    {
      "epoch": 1.9262726740784084,
      "grad_norm": 0.19129203259944916,
      "learning_rate": 0.0002690866510538641,
      "loss": 0.037,
      "step": 823
    },
    {
      "epoch": 1.9286132241076652,
      "grad_norm": 0.17284907400608063,
      "learning_rate": 0.0002690085870413739,
      "loss": 0.0654,
      "step": 824
    },
    {
      "epoch": 1.9309537741369223,
      "grad_norm": 0.18823884427547455,
      "learning_rate": 0.0002689305230288837,
      "loss": 0.0598,
      "step": 825
    },
    {
      "epoch": 1.933294324166179,
      "grad_norm": 0.1562025547027588,
      "learning_rate": 0.0002688524590163934,
      "loss": 0.0548,
      "step": 826
    },
    {
      "epoch": 1.935634874195436,
      "grad_norm": 0.1171034500002861,
      "learning_rate": 0.0002687743950039032,
      "loss": 0.0273,
      "step": 827
    },
    {
      "epoch": 1.937975424224693,
      "grad_norm": 0.1706034541130066,
      "learning_rate": 0.00026869633099141295,
      "loss": 0.0634,
      "step": 828
    },
    {
      "epoch": 1.9403159742539495,
      "grad_norm": 0.1151408851146698,
      "learning_rate": 0.0002686182669789227,
      "loss": 0.0411,
      "step": 829
    },
    {
      "epoch": 1.9426565242832066,
      "grad_norm": 0.12487045675516129,
      "learning_rate": 0.00026854020296643245,
      "loss": 0.0285,
      "step": 830
    },
    {
      "epoch": 1.9449970743124634,
      "grad_norm": 0.20212987065315247,
      "learning_rate": 0.00026846213895394223,
      "loss": 0.0465,
      "step": 831
    },
    {
      "epoch": 1.9473376243417202,
      "grad_norm": 0.15481151640415192,
      "learning_rate": 0.00026838407494145195,
      "loss": 0.0476,
      "step": 832
    },
    {
      "epoch": 1.9496781743709772,
      "grad_norm": 0.11156770586967468,
      "learning_rate": 0.00026830601092896173,
      "loss": 0.0354,
      "step": 833
    },
    {
      "epoch": 1.952018724400234,
      "grad_norm": 0.24496251344680786,
      "learning_rate": 0.0002682279469164715,
      "loss": 0.0353,
      "step": 834
    },
    {
      "epoch": 1.9543592744294909,
      "grad_norm": 0.17233003675937653,
      "learning_rate": 0.00026814988290398123,
      "loss": 0.0356,
      "step": 835
    },
    {
      "epoch": 1.956699824458748,
      "grad_norm": 0.16921602189540863,
      "learning_rate": 0.000268071818891491,
      "loss": 0.0598,
      "step": 836
    },
    {
      "epoch": 1.9590403744880047,
      "grad_norm": 0.15713487565517426,
      "learning_rate": 0.0002679937548790008,
      "loss": 0.0399,
      "step": 837
    },
    {
      "epoch": 1.9613809245172615,
      "grad_norm": 0.13845966756343842,
      "learning_rate": 0.0002679156908665105,
      "loss": 0.0325,
      "step": 838
    },
    {
      "epoch": 1.9637214745465186,
      "grad_norm": 0.07381529361009598,
      "learning_rate": 0.00026783762685402023,
      "loss": 0.0144,
      "step": 839
    },
    {
      "epoch": 1.9660620245757752,
      "grad_norm": 0.1737706959247589,
      "learning_rate": 0.00026775956284153007,
      "loss": 0.0358,
      "step": 840
    },
    {
      "epoch": 1.9684025746050322,
      "grad_norm": 0.12965373694896698,
      "learning_rate": 0.0002676814988290398,
      "loss": 0.0459,
      "step": 841
    },
    {
      "epoch": 1.970743124634289,
      "grad_norm": 0.163375124335289,
      "learning_rate": 0.0002676034348165495,
      "loss": 0.0506,
      "step": 842
    },
    {
      "epoch": 1.9730836746635458,
      "grad_norm": 0.1589745581150055,
      "learning_rate": 0.0002675253708040593,
      "loss": 0.0383,
      "step": 843
    },
    {
      "epoch": 1.9754242246928029,
      "grad_norm": 0.14150196313858032,
      "learning_rate": 0.00026744730679156907,
      "loss": 0.0508,
      "step": 844
    },
    {
      "epoch": 1.9777647747220597,
      "grad_norm": 0.18037760257720947,
      "learning_rate": 0.0002673692427790788,
      "loss": 0.0276,
      "step": 845
    },
    {
      "epoch": 1.9801053247513165,
      "grad_norm": 0.20320595800876617,
      "learning_rate": 0.00026729117876658857,
      "loss": 0.0657,
      "step": 846
    },
    {
      "epoch": 1.9824458747805735,
      "grad_norm": 0.16309143602848053,
      "learning_rate": 0.00026721311475409835,
      "loss": 0.0442,
      "step": 847
    },
    {
      "epoch": 1.9847864248098304,
      "grad_norm": 0.1387868970632553,
      "learning_rate": 0.00026713505074160807,
      "loss": 0.0277,
      "step": 848
    },
    {
      "epoch": 1.9871269748390872,
      "grad_norm": 0.15648144483566284,
      "learning_rate": 0.00026705698672911785,
      "loss": 0.0407,
      "step": 849
    },
    {
      "epoch": 1.9894675248683442,
      "grad_norm": 0.16725675761699677,
      "learning_rate": 0.0002669789227166276,
      "loss": 0.0397,
      "step": 850
    },
    {
      "epoch": 1.9918080748976008,
      "grad_norm": 0.1018831878900528,
      "learning_rate": 0.00026690085870413735,
      "loss": 0.0239,
      "step": 851
    },
    {
      "epoch": 1.9941486249268578,
      "grad_norm": 0.1863713413476944,
      "learning_rate": 0.0002668227946916471,
      "loss": 0.0557,
      "step": 852
    },
    {
      "epoch": 1.9964891749561147,
      "grad_norm": 0.13397611677646637,
      "learning_rate": 0.0002667447306791569,
      "loss": 0.0395,
      "step": 853
    },
    {
      "epoch": 1.9988297249853715,
      "grad_norm": 0.10631981492042542,
      "learning_rate": 0.0002666666666666666,
      "loss": 0.0268,
      "step": 854
    },
    {
      "epoch": 1.9988297249853715,
      "eval_loss": 0.055936265736818314,
      "eval_runtime": 127.1306,
      "eval_samples_per_second": 4.342,
      "eval_steps_per_second": 0.543,
      "step": 854
    },
    {
      "epoch": 2.0011702750146285,
      "grad_norm": 0.1139257624745369,
      "learning_rate": 0.0002665886026541764,
      "loss": 0.0416,
      "step": 855
    },
    {
      "epoch": 2.003510825043885,
      "grad_norm": 0.1214214339852333,
      "learning_rate": 0.0002665105386416862,
      "loss": 0.0388,
      "step": 856
    },
    {
      "epoch": 2.005851375073142,
      "grad_norm": 0.1407688856124878,
      "learning_rate": 0.0002664324746291959,
      "loss": 0.0423,
      "step": 857
    },
    {
      "epoch": 2.008191925102399,
      "grad_norm": 0.10358662903308868,
      "learning_rate": 0.0002663544106167057,
      "loss": 0.0363,
      "step": 858
    },
    {
      "epoch": 2.010532475131656,
      "grad_norm": 0.10888351500034332,
      "learning_rate": 0.0002662763466042154,
      "loss": 0.0424,
      "step": 859
    },
    {
      "epoch": 2.012873025160913,
      "grad_norm": 0.09287054091691971,
      "learning_rate": 0.0002661982825917252,
      "loss": 0.0244,
      "step": 860
    },
    {
      "epoch": 2.01521357519017,
      "grad_norm": 0.1115071177482605,
      "learning_rate": 0.00026612021857923496,
      "loss": 0.0403,
      "step": 861
    },
    {
      "epoch": 2.0175541252194265,
      "grad_norm": 0.1046963781118393,
      "learning_rate": 0.0002660421545667447,
      "loss": 0.0377,
      "step": 862
    },
    {
      "epoch": 2.0198946752486835,
      "grad_norm": 0.14649465680122375,
      "learning_rate": 0.00026596409055425446,
      "loss": 0.0413,
      "step": 863
    },
    {
      "epoch": 2.0222352252779405,
      "grad_norm": 0.1557951122522354,
      "learning_rate": 0.00026588602654176424,
      "loss": 0.0293,
      "step": 864
    },
    {
      "epoch": 2.024575775307197,
      "grad_norm": 0.1782291829586029,
      "learning_rate": 0.00026580796252927396,
      "loss": 0.047,
      "step": 865
    },
    {
      "epoch": 2.026916325336454,
      "grad_norm": 0.1051454097032547,
      "learning_rate": 0.00026572989851678374,
      "loss": 0.0216,
      "step": 866
    },
    {
      "epoch": 2.0292568753657108,
      "grad_norm": 0.15962235629558563,
      "learning_rate": 0.0002656518345042935,
      "loss": 0.0407,
      "step": 867
    },
    {
      "epoch": 2.031597425394968,
      "grad_norm": 0.1314273178577423,
      "learning_rate": 0.00026557377049180324,
      "loss": 0.0311,
      "step": 868
    },
    {
      "epoch": 2.033937975424225,
      "grad_norm": 0.17245030403137207,
      "learning_rate": 0.000265495706479313,
      "loss": 0.032,
      "step": 869
    },
    {
      "epoch": 2.0362785254534814,
      "grad_norm": 0.11847774684429169,
      "learning_rate": 0.0002654176424668228,
      "loss": 0.0265,
      "step": 870
    },
    {
      "epoch": 2.0386190754827385,
      "grad_norm": 0.12496565282344818,
      "learning_rate": 0.0002653395784543325,
      "loss": 0.0402,
      "step": 871
    },
    {
      "epoch": 2.0409596255119955,
      "grad_norm": 0.15883462131023407,
      "learning_rate": 0.0002652615144418423,
      "loss": 0.0267,
      "step": 872
    },
    {
      "epoch": 2.043300175541252,
      "grad_norm": 0.20705634355545044,
      "learning_rate": 0.00026518345042935207,
      "loss": 0.055,
      "step": 873
    },
    {
      "epoch": 2.045640725570509,
      "grad_norm": 0.19800153374671936,
      "learning_rate": 0.0002651053864168618,
      "loss": 0.038,
      "step": 874
    },
    {
      "epoch": 2.0479812755997657,
      "grad_norm": 0.20874929428100586,
      "learning_rate": 0.0002650273224043715,
      "loss": 0.0412,
      "step": 875
    },
    {
      "epoch": 2.0503218256290228,
      "grad_norm": 0.14918142557144165,
      "learning_rate": 0.00026494925839188135,
      "loss": 0.0241,
      "step": 876
    },
    {
      "epoch": 2.05266237565828,
      "grad_norm": 0.15044201910495758,
      "learning_rate": 0.0002648711943793911,
      "loss": 0.0284,
      "step": 877
    },
    {
      "epoch": 2.0550029256875364,
      "grad_norm": 0.1739206463098526,
      "learning_rate": 0.00026479313036690085,
      "loss": 0.0495,
      "step": 878
    },
    {
      "epoch": 2.0573434757167934,
      "grad_norm": 0.20161867141723633,
      "learning_rate": 0.0002647150663544106,
      "loss": 0.0425,
      "step": 879
    },
    {
      "epoch": 2.0596840257460505,
      "grad_norm": 0.15970782935619354,
      "learning_rate": 0.00026463700234192035,
      "loss": 0.0361,
      "step": 880
    },
    {
      "epoch": 2.062024575775307,
      "grad_norm": 0.1326289176940918,
      "learning_rate": 0.00026455893832943013,
      "loss": 0.0327,
      "step": 881
    },
    {
      "epoch": 2.064365125804564,
      "grad_norm": 0.15436860918998718,
      "learning_rate": 0.00026448087431693985,
      "loss": 0.0366,
      "step": 882
    },
    {
      "epoch": 2.066705675833821,
      "grad_norm": 0.15775349736213684,
      "learning_rate": 0.00026440281030444963,
      "loss": 0.0495,
      "step": 883
    },
    {
      "epoch": 2.0690462258630777,
      "grad_norm": 0.1204790472984314,
      "learning_rate": 0.0002643247462919594,
      "loss": 0.029,
      "step": 884
    },
    {
      "epoch": 2.0713867758923348,
      "grad_norm": 0.20152202248573303,
      "learning_rate": 0.00026424668227946913,
      "loss": 0.0429,
      "step": 885
    },
    {
      "epoch": 2.0737273259215914,
      "grad_norm": 0.10468950867652893,
      "learning_rate": 0.0002641686182669789,
      "loss": 0.0304,
      "step": 886
    },
    {
      "epoch": 2.0760678759508484,
      "grad_norm": 0.10816997289657593,
      "learning_rate": 0.0002640905542544887,
      "loss": 0.0223,
      "step": 887
    },
    {
      "epoch": 2.0784084259801054,
      "grad_norm": 0.20421989262104034,
      "learning_rate": 0.0002640124902419984,
      "loss": 0.0458,
      "step": 888
    },
    {
      "epoch": 2.080748976009362,
      "grad_norm": 0.2051886022090912,
      "learning_rate": 0.0002639344262295082,
      "loss": 0.0409,
      "step": 889
    },
    {
      "epoch": 2.083089526038619,
      "grad_norm": 0.15012392401695251,
      "learning_rate": 0.00026385636221701797,
      "loss": 0.048,
      "step": 890
    },
    {
      "epoch": 2.085430076067876,
      "grad_norm": 0.1595713347196579,
      "learning_rate": 0.0002637782982045277,
      "loss": 0.0304,
      "step": 891
    },
    {
      "epoch": 2.0877706260971327,
      "grad_norm": 0.11617547273635864,
      "learning_rate": 0.00026370023419203747,
      "loss": 0.027,
      "step": 892
    },
    {
      "epoch": 2.0901111761263897,
      "grad_norm": 0.19873046875,
      "learning_rate": 0.00026362217017954724,
      "loss": 0.0459,
      "step": 893
    },
    {
      "epoch": 2.092451726155647,
      "grad_norm": 0.17797593772411346,
      "learning_rate": 0.00026354410616705697,
      "loss": 0.0501,
      "step": 894
    },
    {
      "epoch": 2.0947922761849034,
      "grad_norm": 0.09213882684707642,
      "learning_rate": 0.0002634660421545667,
      "loss": 0.0256,
      "step": 895
    },
    {
      "epoch": 2.0971328262141604,
      "grad_norm": 0.11627226322889328,
      "learning_rate": 0.00026338797814207647,
      "loss": 0.0354,
      "step": 896
    },
    {
      "epoch": 2.099473376243417,
      "grad_norm": 0.14710316061973572,
      "learning_rate": 0.00026330991412958625,
      "loss": 0.039,
      "step": 897
    },
    {
      "epoch": 2.101813926272674,
      "grad_norm": 0.14853975176811218,
      "learning_rate": 0.00026323185011709597,
      "loss": 0.0423,
      "step": 898
    },
    {
      "epoch": 2.104154476301931,
      "grad_norm": 0.20537087321281433,
      "learning_rate": 0.00026315378610460575,
      "loss": 0.0274,
      "step": 899
    },
    {
      "epoch": 2.1064950263311877,
      "grad_norm": 0.18030035495758057,
      "learning_rate": 0.0002630757220921155,
      "loss": 0.0579,
      "step": 900
    },
    {
      "epoch": 2.1088355763604447,
      "grad_norm": 0.13684000074863434,
      "learning_rate": 0.00026299765807962525,
      "loss": 0.04,
      "step": 901
    },
    {
      "epoch": 2.1111761263897018,
      "grad_norm": 0.1465545892715454,
      "learning_rate": 0.000262919594067135,
      "loss": 0.0519,
      "step": 902
    },
    {
      "epoch": 2.1135166764189584,
      "grad_norm": 0.11472027003765106,
      "learning_rate": 0.0002628415300546448,
      "loss": 0.0331,
      "step": 903
    },
    {
      "epoch": 2.1158572264482154,
      "grad_norm": 0.13612525165081024,
      "learning_rate": 0.0002627634660421545,
      "loss": 0.0345,
      "step": 904
    },
    {
      "epoch": 2.118197776477472,
      "grad_norm": 0.1968427300453186,
      "learning_rate": 0.0002626854020296643,
      "loss": 0.0695,
      "step": 905
    },
    {
      "epoch": 2.120538326506729,
      "grad_norm": 0.15906350314617157,
      "learning_rate": 0.0002626073380171741,
      "loss": 0.0264,
      "step": 906
    },
    {
      "epoch": 2.122878876535986,
      "grad_norm": 0.19140248000621796,
      "learning_rate": 0.0002625292740046838,
      "loss": 0.0411,
      "step": 907
    },
    {
      "epoch": 2.1252194265652427,
      "grad_norm": 0.12845462560653687,
      "learning_rate": 0.0002624512099921936,
      "loss": 0.0261,
      "step": 908
    },
    {
      "epoch": 2.1275599765944997,
      "grad_norm": 0.17632132768630981,
      "learning_rate": 0.00026237314597970336,
      "loss": 0.0606,
      "step": 909
    },
    {
      "epoch": 2.1299005266237567,
      "grad_norm": 0.1352660208940506,
      "learning_rate": 0.0002622950819672131,
      "loss": 0.0301,
      "step": 910
    },
    {
      "epoch": 2.1322410766530133,
      "grad_norm": 0.2114516645669937,
      "learning_rate": 0.00026221701795472286,
      "loss": 0.0629,
      "step": 911
    },
    {
      "epoch": 2.1345816266822704,
      "grad_norm": 0.17041069269180298,
      "learning_rate": 0.00026213895394223264,
      "loss": 0.0377,
      "step": 912
    },
    {
      "epoch": 2.1369221767115274,
      "grad_norm": 0.12810617685317993,
      "learning_rate": 0.00026206088992974236,
      "loss": 0.0282,
      "step": 913
    },
    {
      "epoch": 2.139262726740784,
      "grad_norm": 0.18417100608348846,
      "learning_rate": 0.00026198282591725214,
      "loss": 0.0354,
      "step": 914
    },
    {
      "epoch": 2.141603276770041,
      "grad_norm": 0.13453714549541473,
      "learning_rate": 0.00026190476190476186,
      "loss": 0.0343,
      "step": 915
    },
    {
      "epoch": 2.1439438267992976,
      "grad_norm": 0.15160447359085083,
      "learning_rate": 0.00026182669789227164,
      "loss": 0.0342,
      "step": 916
    },
    {
      "epoch": 2.1462843768285547,
      "grad_norm": 0.13727889955043793,
      "learning_rate": 0.0002617486338797814,
      "loss": 0.0389,
      "step": 917
    },
    {
      "epoch": 2.1486249268578117,
      "grad_norm": 0.2205248326063156,
      "learning_rate": 0.00026167056986729114,
      "loss": 0.0481,
      "step": 918
    },
    {
      "epoch": 2.1509654768870683,
      "grad_norm": 0.13867519795894623,
      "learning_rate": 0.0002615925058548009,
      "loss": 0.0329,
      "step": 919
    },
    {
      "epoch": 2.1533060269163253,
      "grad_norm": 0.18872936069965363,
      "learning_rate": 0.0002615144418423107,
      "loss": 0.0416,
      "step": 920
    },
    {
      "epoch": 2.1556465769455824,
      "grad_norm": 0.14572711288928986,
      "learning_rate": 0.0002614363778298204,
      "loss": 0.0369,
      "step": 921
    },
    {
      "epoch": 2.157987126974839,
      "grad_norm": 0.15962514281272888,
      "learning_rate": 0.0002613583138173302,
      "loss": 0.0366,
      "step": 922
    },
    {
      "epoch": 2.160327677004096,
      "grad_norm": 0.15487602353096008,
      "learning_rate": 0.00026128024980483997,
      "loss": 0.0489,
      "step": 923
    },
    {
      "epoch": 2.162668227033353,
      "grad_norm": 0.15846112370491028,
      "learning_rate": 0.0002612021857923497,
      "loss": 0.0221,
      "step": 924
    },
    {
      "epoch": 2.1650087770626096,
      "grad_norm": 0.14122898876667023,
      "learning_rate": 0.0002611241217798595,
      "loss": 0.0285,
      "step": 925
    },
    {
      "epoch": 2.1673493270918667,
      "grad_norm": 0.14711183309555054,
      "learning_rate": 0.00026104605776736925,
      "loss": 0.0398,
      "step": 926
    },
    {
      "epoch": 2.1696898771211233,
      "grad_norm": 0.1549735963344574,
      "learning_rate": 0.000260967993754879,
      "loss": 0.0318,
      "step": 927
    },
    {
      "epoch": 2.1720304271503803,
      "grad_norm": 0.20308473706245422,
      "learning_rate": 0.00026088992974238875,
      "loss": 0.0351,
      "step": 928
    },
    {
      "epoch": 2.1743709771796373,
      "grad_norm": 0.17523130774497986,
      "learning_rate": 0.00026081186572989853,
      "loss": 0.0403,
      "step": 929
    },
    {
      "epoch": 2.176711527208894,
      "grad_norm": 0.23567605018615723,
      "learning_rate": 0.00026073380171740825,
      "loss": 0.0341,
      "step": 930
    },
    {
      "epoch": 2.179052077238151,
      "grad_norm": 0.14541253447532654,
      "learning_rate": 0.000260655737704918,
      "loss": 0.0228,
      "step": 931
    },
    {
      "epoch": 2.181392627267408,
      "grad_norm": 0.18832644820213318,
      "learning_rate": 0.00026057767369242775,
      "loss": 0.0522,
      "step": 932
    },
    {
      "epoch": 2.1837331772966646,
      "grad_norm": 0.15005071461200714,
      "learning_rate": 0.00026049960967993753,
      "loss": 0.028,
      "step": 933
    },
    {
      "epoch": 2.1860737273259216,
      "grad_norm": 0.14568661153316498,
      "learning_rate": 0.00026042154566744725,
      "loss": 0.0368,
      "step": 934
    },
    {
      "epoch": 2.1884142773551787,
      "grad_norm": 0.17831145226955414,
      "learning_rate": 0.00026034348165495703,
      "loss": 0.022,
      "step": 935
    },
    {
      "epoch": 2.1907548273844353,
      "grad_norm": 0.13973766565322876,
      "learning_rate": 0.0002602654176424668,
      "loss": 0.0458,
      "step": 936
    },
    {
      "epoch": 2.1930953774136923,
      "grad_norm": 0.16031259298324585,
      "learning_rate": 0.00026018735362997653,
      "loss": 0.0517,
      "step": 937
    },
    {
      "epoch": 2.195435927442949,
      "grad_norm": 0.16148655116558075,
      "learning_rate": 0.0002601092896174863,
      "loss": 0.052,
      "step": 938
    },
    {
      "epoch": 2.197776477472206,
      "grad_norm": 0.12075561285018921,
      "learning_rate": 0.0002600312256049961,
      "loss": 0.0396,
      "step": 939
    },
    {
      "epoch": 2.200117027501463,
      "grad_norm": 0.1663622409105301,
      "learning_rate": 0.0002599531615925058,
      "loss": 0.0411,
      "step": 940
    },
    {
      "epoch": 2.2024575775307196,
      "grad_norm": 0.14567120373249054,
      "learning_rate": 0.0002598750975800156,
      "loss": 0.0306,
      "step": 941
    },
    {
      "epoch": 2.2047981275599766,
      "grad_norm": 0.1183365136384964,
      "learning_rate": 0.00025979703356752537,
      "loss": 0.0237,
      "step": 942
    },
    {
      "epoch": 2.2071386775892337,
      "grad_norm": 0.1493445783853531,
      "learning_rate": 0.0002597189695550351,
      "loss": 0.0362,
      "step": 943
    },
    {
      "epoch": 2.2094792276184902,
      "grad_norm": 0.11989283561706543,
      "learning_rate": 0.00025964090554254487,
      "loss": 0.0291,
      "step": 944
    },
    {
      "epoch": 2.2118197776477473,
      "grad_norm": 0.12833911180496216,
      "learning_rate": 0.00025956284153005464,
      "loss": 0.0415,
      "step": 945
    },
    {
      "epoch": 2.2141603276770043,
      "grad_norm": 0.11747045814990997,
      "learning_rate": 0.00025948477751756437,
      "loss": 0.0342,
      "step": 946
    },
    {
      "epoch": 2.216500877706261,
      "grad_norm": 0.13982993364334106,
      "learning_rate": 0.00025940671350507415,
      "loss": 0.0337,
      "step": 947
    },
    {
      "epoch": 2.218841427735518,
      "grad_norm": 0.09400880336761475,
      "learning_rate": 0.00025932864949258387,
      "loss": 0.0144,
      "step": 948
    },
    {
      "epoch": 2.2211819777647746,
      "grad_norm": 0.09236586838960648,
      "learning_rate": 0.00025925058548009365,
      "loss": 0.0214,
      "step": 949
    },
    {
      "epoch": 2.2235225277940316,
      "grad_norm": 0.10809732973575592,
      "learning_rate": 0.0002591725214676034,
      "loss": 0.0282,
      "step": 950
    },
    {
      "epoch": 2.2258630778232886,
      "grad_norm": 0.2348693609237671,
      "learning_rate": 0.00025909445745511315,
      "loss": 0.0289,
      "step": 951
    },
    {
      "epoch": 2.228203627852545,
      "grad_norm": 0.16156262159347534,
      "learning_rate": 0.0002590163934426229,
      "loss": 0.0426,
      "step": 952
    },
    {
      "epoch": 2.2305441778818023,
      "grad_norm": 0.14170582592487335,
      "learning_rate": 0.0002589383294301327,
      "loss": 0.027,
      "step": 953
    },
    {
      "epoch": 2.2328847279110593,
      "grad_norm": 0.10725490748882294,
      "learning_rate": 0.0002588602654176424,
      "loss": 0.0139,
      "step": 954
    },
    {
      "epoch": 2.235225277940316,
      "grad_norm": 0.13107647001743317,
      "learning_rate": 0.0002587822014051522,
      "loss": 0.0209,
      "step": 955
    },
    {
      "epoch": 2.237565827969573,
      "grad_norm": 0.16367168724536896,
      "learning_rate": 0.000258704137392662,
      "loss": 0.0267,
      "step": 956
    },
    {
      "epoch": 2.2399063779988295,
      "grad_norm": 0.16560567915439606,
      "learning_rate": 0.0002586260733801717,
      "loss": 0.043,
      "step": 957
    },
    {
      "epoch": 2.2422469280280866,
      "grad_norm": 0.12341032922267914,
      "learning_rate": 0.0002585480093676815,
      "loss": 0.0282,
      "step": 958
    },
    {
      "epoch": 2.2445874780573436,
      "grad_norm": 0.1398986130952835,
      "learning_rate": 0.00025846994535519126,
      "loss": 0.0349,
      "step": 959
    },
    {
      "epoch": 2.2469280280866,
      "grad_norm": 0.7492942810058594,
      "learning_rate": 0.000258391881342701,
      "loss": 0.0426,
      "step": 960
    },
    {
      "epoch": 2.2492685781158572,
      "grad_norm": 0.07819529622793198,
      "learning_rate": 0.00025831381733021076,
      "loss": 0.0071,
      "step": 961
    },
    {
      "epoch": 2.2516091281451143,
      "grad_norm": 0.08555354177951813,
      "learning_rate": 0.00025823575331772054,
      "loss": 0.0164,
      "step": 962
    },
    {
      "epoch": 2.253949678174371,
      "grad_norm": 0.1402372270822525,
      "learning_rate": 0.00025815768930523026,
      "loss": 0.0349,
      "step": 963
    },
    {
      "epoch": 2.256290228203628,
      "grad_norm": 0.9433332085609436,
      "learning_rate": 0.00025807962529274004,
      "loss": 0.0389,
      "step": 964
    },
    {
      "epoch": 2.2586307782328845,
      "grad_norm": 0.19329489767551422,
      "learning_rate": 0.0002580015612802498,
      "loss": 0.0428,
      "step": 965
    },
    {
      "epoch": 2.2609713282621415,
      "grad_norm": 0.6594419479370117,
      "learning_rate": 0.00025792349726775954,
      "loss": 0.0416,
      "step": 966
    },
    {
      "epoch": 2.2633118782913986,
      "grad_norm": 0.784214973449707,
      "learning_rate": 0.00025784543325526926,
      "loss": 0.0368,
      "step": 967
    },
    {
      "epoch": 2.265652428320655,
      "grad_norm": 0.3774191737174988,
      "learning_rate": 0.00025776736924277904,
      "loss": 0.0423,
      "step": 968
    },
    {
      "epoch": 2.267992978349912,
      "grad_norm": 0.23871131241321564,
      "learning_rate": 0.0002576893052302888,
      "loss": 0.0312,
      "step": 969
    },
    {
      "epoch": 2.2703335283791692,
      "grad_norm": 0.44901174306869507,
      "learning_rate": 0.00025761124121779854,
      "loss": 0.0436,
      "step": 970
    },
    {
      "epoch": 2.272674078408426,
      "grad_norm": 0.29041895270347595,
      "learning_rate": 0.0002575331772053083,
      "loss": 0.0381,
      "step": 971
    },
    {
      "epoch": 2.275014628437683,
      "grad_norm": 0.2374659776687622,
      "learning_rate": 0.0002574551131928181,
      "loss": 0.0373,
      "step": 972
    },
    {
      "epoch": 2.27735517846694,
      "grad_norm": 0.5888580679893494,
      "learning_rate": 0.0002573770491803278,
      "loss": 0.0518,
      "step": 973
    },
    {
      "epoch": 2.2796957284961965,
      "grad_norm": 0.3550755977630615,
      "learning_rate": 0.0002572989851678376,
      "loss": 0.0732,
      "step": 974
    },
    {
      "epoch": 2.2820362785254535,
      "grad_norm": 0.2542547881603241,
      "learning_rate": 0.0002572209211553474,
      "loss": 0.0544,
      "step": 975
    },
    {
      "epoch": 2.28437682855471,
      "grad_norm": 0.14294812083244324,
      "learning_rate": 0.0002571428571428571,
      "loss": 0.0317,
      "step": 976
    },
    {
      "epoch": 2.286717378583967,
      "grad_norm": 0.17306140065193176,
      "learning_rate": 0.0002570647931303669,
      "loss": 0.0351,
      "step": 977
    },
    {
      "epoch": 2.289057928613224,
      "grad_norm": 0.17800617218017578,
      "learning_rate": 0.00025698672911787665,
      "loss": 0.0246,
      "step": 978
    },
    {
      "epoch": 2.291398478642481,
      "grad_norm": 0.22808308899402618,
      "learning_rate": 0.0002569086651053864,
      "loss": 0.0297,
      "step": 979
    },
    {
      "epoch": 2.293739028671738,
      "grad_norm": 0.2595137655735016,
      "learning_rate": 0.00025683060109289615,
      "loss": 0.0464,
      "step": 980
    },
    {
      "epoch": 2.296079578700995,
      "grad_norm": 0.4089643657207489,
      "learning_rate": 0.00025675253708040593,
      "loss": 0.0563,
      "step": 981
    },
    {
      "epoch": 2.2984201287302515,
      "grad_norm": 0.20226377248764038,
      "learning_rate": 0.00025667447306791565,
      "loss": 0.0347,
      "step": 982
    },
    {
      "epoch": 2.3007606787595085,
      "grad_norm": 0.3940850794315338,
      "learning_rate": 0.00025659640905542543,
      "loss": 0.0295,
      "step": 983
    },
    {
      "epoch": 2.3031012287887656,
      "grad_norm": 0.15354765951633453,
      "learning_rate": 0.00025651834504293515,
      "loss": 0.0321,
      "step": 984
    },
    {
      "epoch": 2.305441778818022,
      "grad_norm": 0.23761597275733948,
      "learning_rate": 0.00025644028103044493,
      "loss": 0.047,
      "step": 985
    },
    {
      "epoch": 2.307782328847279,
      "grad_norm": 0.1449744552373886,
      "learning_rate": 0.0002563622170179547,
      "loss": 0.0268,
      "step": 986
    },
    {
      "epoch": 2.310122878876536,
      "grad_norm": 0.22233553230762482,
      "learning_rate": 0.00025628415300546443,
      "loss": 0.0351,
      "step": 987
    },
    {
      "epoch": 2.312463428905793,
      "grad_norm": 0.13947615027427673,
      "learning_rate": 0.0002562060889929742,
      "loss": 0.0247,
      "step": 988
    },
    {
      "epoch": 2.31480397893505,
      "grad_norm": 0.17340993881225586,
      "learning_rate": 0.000256128024980484,
      "loss": 0.0363,
      "step": 989
    },
    {
      "epoch": 2.3171445289643064,
      "grad_norm": 0.3406277894973755,
      "learning_rate": 0.0002560499609679937,
      "loss": 0.0805,
      "step": 990
    },
    {
      "epoch": 2.3194850789935635,
      "grad_norm": 0.17246244847774506,
      "learning_rate": 0.0002559718969555035,
      "loss": 0.0367,
      "step": 991
    },
    {
      "epoch": 2.3218256290228205,
      "grad_norm": 0.16467759013175964,
      "learning_rate": 0.00025589383294301327,
      "loss": 0.0454,
      "step": 992
    },
    {
      "epoch": 2.324166179052077,
      "grad_norm": 0.13127630949020386,
      "learning_rate": 0.000255815768930523,
      "loss": 0.0285,
      "step": 993
    },
    {
      "epoch": 2.326506729081334,
      "grad_norm": 0.15154320001602173,
      "learning_rate": 0.00025573770491803277,
      "loss": 0.0283,
      "step": 994
    },
    {
      "epoch": 2.328847279110591,
      "grad_norm": 0.2725178599357605,
      "learning_rate": 0.00025565964090554254,
      "loss": 0.0508,
      "step": 995
    },
    {
      "epoch": 2.331187829139848,
      "grad_norm": 0.16042345762252808,
      "learning_rate": 0.00025558157689305227,
      "loss": 0.0344,
      "step": 996
    },
    {
      "epoch": 2.333528379169105,
      "grad_norm": 0.20337244868278503,
      "learning_rate": 0.00025550351288056205,
      "loss": 0.0463,
      "step": 997
    },
    {
      "epoch": 2.3358689291983614,
      "grad_norm": 0.18815718591213226,
      "learning_rate": 0.0002554254488680718,
      "loss": 0.0319,
      "step": 998
    },
    {
      "epoch": 2.3382094792276185,
      "grad_norm": 0.1314963847398758,
      "learning_rate": 0.00025534738485558155,
      "loss": 0.0287,
      "step": 999
    },
    {
      "epoch": 2.3405500292568755,
      "grad_norm": 0.18084347248077393,
      "learning_rate": 0.00025526932084309127,
      "loss": 0.0405,
      "step": 1000
    },
    {
      "epoch": 2.342890579286132,
      "grad_norm": 0.27811673283576965,
      "learning_rate": 0.0002551912568306011,
      "loss": 0.0752,
      "step": 1001
    },
    {
      "epoch": 2.345231129315389,
      "grad_norm": 0.14422950148582458,
      "learning_rate": 0.0002551131928181108,
      "loss": 0.0438,
      "step": 1002
    },
    {
      "epoch": 2.347571679344646,
      "grad_norm": 0.1787361204624176,
      "learning_rate": 0.00025503512880562055,
      "loss": 0.0527,
      "step": 1003
    },
    {
      "epoch": 2.3499122293739028,
      "grad_norm": 0.13556508719921112,
      "learning_rate": 0.0002549570647931303,
      "loss": 0.0156,
      "step": 1004
    },
    {
      "epoch": 2.35225277940316,
      "grad_norm": 0.19010113179683685,
      "learning_rate": 0.0002548790007806401,
      "loss": 0.0415,
      "step": 1005
    },
    {
      "epoch": 2.354593329432417,
      "grad_norm": 0.31308144330978394,
      "learning_rate": 0.0002548009367681499,
      "loss": 0.0473,
      "step": 1006
    },
    {
      "epoch": 2.3569338794616734,
      "grad_norm": 0.166979119181633,
      "learning_rate": 0.0002547228727556596,
      "loss": 0.0307,
      "step": 1007
    },
    {
      "epoch": 2.3592744294909305,
      "grad_norm": 0.17997042834758759,
      "learning_rate": 0.0002546448087431694,
      "loss": 0.0515,
      "step": 1008
    },
    {
      "epoch": 2.361614979520187,
      "grad_norm": 0.16808731853961945,
      "learning_rate": 0.00025456674473067916,
      "loss": 0.0438,
      "step": 1009
    },
    {
      "epoch": 2.363955529549444,
      "grad_norm": 0.2101154774427414,
      "learning_rate": 0.0002544886807181889,
      "loss": 0.0354,
      "step": 1010
    },
    {
      "epoch": 2.366296079578701,
      "grad_norm": 0.1712503880262375,
      "learning_rate": 0.00025441061670569866,
      "loss": 0.0379,
      "step": 1011
    },
    {
      "epoch": 2.3686366296079577,
      "grad_norm": 0.1375528872013092,
      "learning_rate": 0.00025433255269320844,
      "loss": 0.0444,
      "step": 1012
    },
    {
      "epoch": 2.3709771796372148,
      "grad_norm": 0.13618551194667816,
      "learning_rate": 0.00025425448868071816,
      "loss": 0.0321,
      "step": 1013
    },
    {
      "epoch": 2.373317729666472,
      "grad_norm": 0.10930109024047852,
      "learning_rate": 0.00025417642466822794,
      "loss": 0.032,
      "step": 1014
    },
    {
      "epoch": 2.3756582796957284,
      "grad_norm": 0.12827035784721375,
      "learning_rate": 0.0002540983606557377,
      "loss": 0.031,
      "step": 1015
    },
    {
      "epoch": 2.3779988297249854,
      "grad_norm": 0.14810983836650848,
      "learning_rate": 0.00025402029664324744,
      "loss": 0.0267,
      "step": 1016
    },
    {
      "epoch": 2.3803393797542425,
      "grad_norm": 0.16863110661506653,
      "learning_rate": 0.0002539422326307572,
      "loss": 0.0287,
      "step": 1017
    },
    {
      "epoch": 2.382679929783499,
      "grad_norm": 0.26088544726371765,
      "learning_rate": 0.000253864168618267,
      "loss": 0.0674,
      "step": 1018
    },
    {
      "epoch": 2.385020479812756,
      "grad_norm": 0.09378360956907272,
      "learning_rate": 0.0002537861046057767,
      "loss": 0.0206,
      "step": 1019
    },
    {
      "epoch": 2.3873610298420127,
      "grad_norm": 0.1890178620815277,
      "learning_rate": 0.00025370804059328644,
      "loss": 0.028,
      "step": 1020
    },
    {
      "epoch": 2.3897015798712697,
      "grad_norm": 0.19770456850528717,
      "learning_rate": 0.00025362997658079627,
      "loss": 0.0504,
      "step": 1021
    },
    {
      "epoch": 2.392042129900527,
      "grad_norm": 0.2004789561033249,
      "learning_rate": 0.000253551912568306,
      "loss": 0.0407,
      "step": 1022
    },
    {
      "epoch": 2.3943826799297834,
      "grad_norm": 0.1932273507118225,
      "learning_rate": 0.0002534738485558157,
      "loss": 0.0458,
      "step": 1023
    },
    {
      "epoch": 2.3967232299590404,
      "grad_norm": 0.21428032219409943,
      "learning_rate": 0.0002533957845433255,
      "loss": 0.0303,
      "step": 1024
    },
    {
      "epoch": 2.399063779988297,
      "grad_norm": 0.1372225433588028,
      "learning_rate": 0.0002533177205308353,
      "loss": 0.035,
      "step": 1025
    },
    {
      "epoch": 2.401404330017554,
      "grad_norm": 0.25220564007759094,
      "learning_rate": 0.000253239656518345,
      "loss": 0.0443,
      "step": 1026
    },
    {
      "epoch": 2.403744880046811,
      "grad_norm": 0.15477146208286285,
      "learning_rate": 0.0002531615925058548,
      "loss": 0.0412,
      "step": 1027
    },
    {
      "epoch": 2.406085430076068,
      "grad_norm": 0.09384671598672867,
      "learning_rate": 0.00025308352849336455,
      "loss": 0.0238,
      "step": 1028
    },
    {
      "epoch": 2.4084259801053247,
      "grad_norm": 0.2091345340013504,
      "learning_rate": 0.0002530054644808743,
      "loss": 0.0539,
      "step": 1029
    },
    {
      "epoch": 2.4107665301345818,
      "grad_norm": 0.17031419277191162,
      "learning_rate": 0.00025292740046838405,
      "loss": 0.0314,
      "step": 1030
    },
    {
      "epoch": 2.4131070801638383,
      "grad_norm": 0.18760062754154205,
      "learning_rate": 0.00025284933645589383,
      "loss": 0.0324,
      "step": 1031
    },
    {
      "epoch": 2.4154476301930954,
      "grad_norm": 0.1542009860277176,
      "learning_rate": 0.00025277127244340355,
      "loss": 0.0379,
      "step": 1032
    },
    {
      "epoch": 2.4177881802223524,
      "grad_norm": 0.14162738621234894,
      "learning_rate": 0.00025269320843091333,
      "loss": 0.0319,
      "step": 1033
    },
    {
      "epoch": 2.420128730251609,
      "grad_norm": 0.087401382625103,
      "learning_rate": 0.0002526151444184231,
      "loss": 0.0211,
      "step": 1034
    },
    {
      "epoch": 2.422469280280866,
      "grad_norm": 0.14514465630054474,
      "learning_rate": 0.00025253708040593283,
      "loss": 0.0436,
      "step": 1035
    },
    {
      "epoch": 2.4248098303101226,
      "grad_norm": 0.09677361696958542,
      "learning_rate": 0.0002524590163934426,
      "loss": 0.0149,
      "step": 1036
    },
    {
      "epoch": 2.4271503803393797,
      "grad_norm": 0.23034736514091492,
      "learning_rate": 0.0002523809523809524,
      "loss": 0.0494,
      "step": 1037
    },
    {
      "epoch": 2.4294909303686367,
      "grad_norm": 0.12659481167793274,
      "learning_rate": 0.0002523028883684621,
      "loss": 0.0327,
      "step": 1038
    },
    {
      "epoch": 2.4318314803978933,
      "grad_norm": 0.1079239472746849,
      "learning_rate": 0.0002522248243559719,
      "loss": 0.019,
      "step": 1039
    },
    {
      "epoch": 2.4341720304271504,
      "grad_norm": 0.14641530811786652,
      "learning_rate": 0.0002521467603434816,
      "loss": 0.0337,
      "step": 1040
    },
    {
      "epoch": 2.4365125804564074,
      "grad_norm": 0.15304706990718842,
      "learning_rate": 0.0002520686963309914,
      "loss": 0.0441,
      "step": 1041
    },
    {
      "epoch": 2.438853130485664,
      "grad_norm": 0.19332581758499146,
      "learning_rate": 0.00025199063231850117,
      "loss": 0.0503,
      "step": 1042
    },
    {
      "epoch": 2.441193680514921,
      "grad_norm": 0.11848800629377365,
      "learning_rate": 0.0002519125683060109,
      "loss": 0.0283,
      "step": 1043
    },
    {
      "epoch": 2.443534230544178,
      "grad_norm": 0.23907431960105896,
      "learning_rate": 0.00025183450429352067,
      "loss": 0.0572,
      "step": 1044
    },
    {
      "epoch": 2.4458747805734347,
      "grad_norm": 0.11675949394702911,
      "learning_rate": 0.00025175644028103044,
      "loss": 0.0379,
      "step": 1045
    },
    {
      "epoch": 2.4482153306026917,
      "grad_norm": 0.13307523727416992,
      "learning_rate": 0.00025167837626854017,
      "loss": 0.0309,
      "step": 1046
    },
    {
      "epoch": 2.4505558806319483,
      "grad_norm": 0.10828687250614166,
      "learning_rate": 0.00025160031225604995,
      "loss": 0.0305,
      "step": 1047
    },
    {
      "epoch": 2.4528964306612053,
      "grad_norm": 0.13177725672721863,
      "learning_rate": 0.0002515222482435597,
      "loss": 0.0398,
      "step": 1048
    },
    {
      "epoch": 2.4552369806904624,
      "grad_norm": 0.17451375722885132,
      "learning_rate": 0.00025144418423106945,
      "loss": 0.0454,
      "step": 1049
    },
    {
      "epoch": 2.457577530719719,
      "grad_norm": 0.17015419900417328,
      "learning_rate": 0.0002513661202185792,
      "loss": 0.053,
      "step": 1050
    },
    {
      "epoch": 2.459918080748976,
      "grad_norm": 0.15839393436908722,
      "learning_rate": 0.000251288056206089,
      "loss": 0.0339,
      "step": 1051
    },
    {
      "epoch": 2.462258630778233,
      "grad_norm": 0.17221593856811523,
      "learning_rate": 0.0002512099921935987,
      "loss": 0.0226,
      "step": 1052
    },
    {
      "epoch": 2.4645991808074896,
      "grad_norm": 0.444538950920105,
      "learning_rate": 0.0002511319281811085,
      "loss": 0.0441,
      "step": 1053
    },
    {
      "epoch": 2.4669397308367467,
      "grad_norm": 0.09441439807415009,
      "learning_rate": 0.0002510538641686183,
      "loss": 0.0233,
      "step": 1054
    },
    {
      "epoch": 2.4692802808660037,
      "grad_norm": 0.12369625270366669,
      "learning_rate": 0.000250975800156128,
      "loss": 0.0421,
      "step": 1055
    },
    {
      "epoch": 2.4716208308952603,
      "grad_norm": 0.1609996259212494,
      "learning_rate": 0.0002508977361436377,
      "loss": 0.0321,
      "step": 1056
    },
    {
      "epoch": 2.4739613809245173,
      "grad_norm": 0.1631430834531784,
      "learning_rate": 0.00025081967213114756,
      "loss": 0.0359,
      "step": 1057
    },
    {
      "epoch": 2.476301930953774,
      "grad_norm": 0.1614106446504593,
      "learning_rate": 0.0002507416081186573,
      "loss": 0.0481,
      "step": 1058
    },
    {
      "epoch": 2.478642480983031,
      "grad_norm": 0.16209039092063904,
      "learning_rate": 0.000250663544106167,
      "loss": 0.0336,
      "step": 1059
    },
    {
      "epoch": 2.480983031012288,
      "grad_norm": 0.07705186307430267,
      "learning_rate": 0.0002505854800936768,
      "loss": 0.0145,
      "step": 1060
    },
    {
      "epoch": 2.4833235810415446,
      "grad_norm": 0.1928015798330307,
      "learning_rate": 0.00025050741608118656,
      "loss": 0.0509,
      "step": 1061
    },
    {
      "epoch": 2.4856641310708016,
      "grad_norm": 0.11276659369468689,
      "learning_rate": 0.0002504293520686963,
      "loss": 0.0284,
      "step": 1062
    },
    {
      "epoch": 2.4880046811000587,
      "grad_norm": 0.3223297595977783,
      "learning_rate": 0.00025035128805620606,
      "loss": 0.0313,
      "step": 1063
    },
    {
      "epoch": 2.4903452311293153,
      "grad_norm": 0.19548363983631134,
      "learning_rate": 0.00025027322404371584,
      "loss": 0.0625,
      "step": 1064
    },
    {
      "epoch": 2.4926857811585723,
      "grad_norm": 0.10622328519821167,
      "learning_rate": 0.00025019516003122556,
      "loss": 0.0298,
      "step": 1065
    },
    {
      "epoch": 2.4950263311878293,
      "grad_norm": 0.1776764690876007,
      "learning_rate": 0.00025011709601873534,
      "loss": 0.0348,
      "step": 1066
    },
    {
      "epoch": 2.497366881217086,
      "grad_norm": 0.11766906082630157,
      "learning_rate": 0.0002500390320062451,
      "loss": 0.0274,
      "step": 1067
    },
    {
      "epoch": 2.499707431246343,
      "grad_norm": 0.19096557796001434,
      "learning_rate": 0.00024996096799375484,
      "loss": 0.0394,
      "step": 1068
    },
    {
      "epoch": 2.5020479812755996,
      "grad_norm": 0.6530598998069763,
      "learning_rate": 0.0002498829039812646,
      "loss": 0.0456,
      "step": 1069
    },
    {
      "epoch": 2.5043885313048566,
      "grad_norm": 0.16684535145759583,
      "learning_rate": 0.0002498048399687744,
      "loss": 0.0419,
      "step": 1070
    },
    {
      "epoch": 2.5067290813341137,
      "grad_norm": 0.2383490949869156,
      "learning_rate": 0.0002497267759562841,
      "loss": 0.0479,
      "step": 1071
    },
    {
      "epoch": 2.5090696313633702,
      "grad_norm": 0.20444390177726746,
      "learning_rate": 0.0002496487119437939,
      "loss": 0.0393,
      "step": 1072
    },
    {
      "epoch": 2.5114101813926273,
      "grad_norm": 0.15061092376708984,
      "learning_rate": 0.0002495706479313037,
      "loss": 0.0339,
      "step": 1073
    },
    {
      "epoch": 2.513750731421884,
      "grad_norm": 0.7039774656295776,
      "learning_rate": 0.0002494925839188134,
      "loss": 0.0578,
      "step": 1074
    },
    {
      "epoch": 2.516091281451141,
      "grad_norm": 0.1730530709028244,
      "learning_rate": 0.0002494145199063232,
      "loss": 0.0389,
      "step": 1075
    },
    {
      "epoch": 2.518431831480398,
      "grad_norm": 0.14046317338943481,
      "learning_rate": 0.0002493364558938329,
      "loss": 0.0266,
      "step": 1076
    },
    {
      "epoch": 2.520772381509655,
      "grad_norm": 0.1595158725976944,
      "learning_rate": 0.0002492583918813427,
      "loss": 0.0364,
      "step": 1077
    },
    {
      "epoch": 2.5231129315389116,
      "grad_norm": 0.14078718423843384,
      "learning_rate": 0.00024918032786885245,
      "loss": 0.0364,
      "step": 1078
    },
    {
      "epoch": 2.5254534815681686,
      "grad_norm": 0.17508582770824432,
      "learning_rate": 0.0002491022638563622,
      "loss": 0.0262,
      "step": 1079
    },
    {
      "epoch": 2.527794031597425,
      "grad_norm": 0.16896574199199677,
      "learning_rate": 0.00024902419984387195,
      "loss": 0.0267,
      "step": 1080
    },
    {
      "epoch": 2.5301345816266823,
      "grad_norm": 0.30066436529159546,
      "learning_rate": 0.00024894613583138173,
      "loss": 0.0537,
      "step": 1081
    },
    {
      "epoch": 2.5324751316559393,
      "grad_norm": 0.2529773712158203,
      "learning_rate": 0.00024886807181889145,
      "loss": 0.0527,
      "step": 1082
    },
    {
      "epoch": 2.534815681685196,
      "grad_norm": 0.3217189311981201,
      "learning_rate": 0.00024879000780640123,
      "loss": 0.0578,
      "step": 1083
    },
    {
      "epoch": 2.537156231714453,
      "grad_norm": 0.1433735489845276,
      "learning_rate": 0.000248711943793911,
      "loss": 0.0361,
      "step": 1084
    },
    {
      "epoch": 2.5394967817437095,
      "grad_norm": 0.18613183498382568,
      "learning_rate": 0.00024863387978142073,
      "loss": 0.0382,
      "step": 1085
    },
    {
      "epoch": 2.5418373317729666,
      "grad_norm": 0.12689648568630219,
      "learning_rate": 0.0002485558157689305,
      "loss": 0.0396,
      "step": 1086
    },
    {
      "epoch": 2.5441778818022236,
      "grad_norm": 0.19624421000480652,
      "learning_rate": 0.0002484777517564403,
      "loss": 0.0529,
      "step": 1087
    },
    {
      "epoch": 2.5465184318314806,
      "grad_norm": 0.17615985870361328,
      "learning_rate": 0.00024839968774395,
      "loss": 0.0346,
      "step": 1088
    },
    {
      "epoch": 2.5488589818607372,
      "grad_norm": 0.18937067687511444,
      "learning_rate": 0.0002483216237314598,
      "loss": 0.0339,
      "step": 1089
    },
    {
      "epoch": 2.5511995318899943,
      "grad_norm": 0.18154917657375336,
      "learning_rate": 0.00024824355971896957,
      "loss": 0.0296,
      "step": 1090
    },
    {
      "epoch": 2.553540081919251,
      "grad_norm": 0.16757522523403168,
      "learning_rate": 0.0002481654957064793,
      "loss": 0.034,
      "step": 1091
    },
    {
      "epoch": 2.555880631948508,
      "grad_norm": 0.1466427743434906,
      "learning_rate": 0.000248087431693989,
      "loss": 0.0432,
      "step": 1092
    },
    {
      "epoch": 2.558221181977765,
      "grad_norm": 0.16798414289951324,
      "learning_rate": 0.0002480093676814988,
      "loss": 0.0403,
      "step": 1093
    },
    {
      "epoch": 2.5605617320070215,
      "grad_norm": 0.14890316128730774,
      "learning_rate": 0.00024793130366900857,
      "loss": 0.044,
      "step": 1094
    },
    {
      "epoch": 2.5629022820362786,
      "grad_norm": 0.1446978747844696,
      "learning_rate": 0.0002478532396565183,
      "loss": 0.029,
      "step": 1095
    },
    {
      "epoch": 2.565242832065535,
      "grad_norm": 0.18228358030319214,
      "learning_rate": 0.00024777517564402807,
      "loss": 0.0383,
      "step": 1096
    },
    {
      "epoch": 2.567583382094792,
      "grad_norm": 0.1453358232975006,
      "learning_rate": 0.00024769711163153785,
      "loss": 0.032,
      "step": 1097
    },
    {
      "epoch": 2.5699239321240492,
      "grad_norm": 0.16980911791324615,
      "learning_rate": 0.00024761904761904757,
      "loss": 0.0461,
      "step": 1098
    },
    {
      "epoch": 2.5722644821533063,
      "grad_norm": 0.13601864874362946,
      "learning_rate": 0.00024754098360655735,
      "loss": 0.0197,
      "step": 1099
    },
    {
      "epoch": 2.574605032182563,
      "grad_norm": 0.1580611914396286,
      "learning_rate": 0.0002474629195940671,
      "loss": 0.0283,
      "step": 1100
    },
    {
      "epoch": 2.57694558221182,
      "grad_norm": 0.12771128118038177,
      "learning_rate": 0.00024738485558157685,
      "loss": 0.0204,
      "step": 1101
    },
    {
      "epoch": 2.5792861322410765,
      "grad_norm": 0.17797237634658813,
      "learning_rate": 0.0002473067915690866,
      "loss": 0.0552,
      "step": 1102
    },
    {
      "epoch": 2.5816266822703335,
      "grad_norm": 0.4163614809513092,
      "learning_rate": 0.0002472287275565964,
      "loss": 0.0705,
      "step": 1103
    },
    {
      "epoch": 2.5839672322995906,
      "grad_norm": 0.18222448229789734,
      "learning_rate": 0.0002471506635441061,
      "loss": 0.0432,
      "step": 1104
    },
    {
      "epoch": 2.586307782328847,
      "grad_norm": 0.13392803072929382,
      "learning_rate": 0.0002470725995316159,
      "loss": 0.0243,
      "step": 1105
    },
    {
      "epoch": 2.588648332358104,
      "grad_norm": 0.14001008868217468,
      "learning_rate": 0.0002469945355191257,
      "loss": 0.0178,
      "step": 1106
    },
    {
      "epoch": 2.590988882387361,
      "grad_norm": 0.1255687177181244,
      "learning_rate": 0.0002469164715066354,
      "loss": 0.0306,
      "step": 1107
    },
    {
      "epoch": 2.593329432416618,
      "grad_norm": 0.12939198315143585,
      "learning_rate": 0.0002468384074941452,
      "loss": 0.0467,
      "step": 1108
    },
    {
      "epoch": 2.595669982445875,
      "grad_norm": 0.15124526619911194,
      "learning_rate": 0.0002467603434816549,
      "loss": 0.049,
      "step": 1109
    },
    {
      "epoch": 2.598010532475132,
      "grad_norm": 0.12024585902690887,
      "learning_rate": 0.0002466822794691647,
      "loss": 0.034,
      "step": 1110
    },
    {
      "epoch": 2.6003510825043885,
      "grad_norm": 0.6099231243133545,
      "learning_rate": 0.00024660421545667446,
      "loss": 0.0332,
      "step": 1111
    },
    {
      "epoch": 2.6026916325336455,
      "grad_norm": 0.1357792317867279,
      "learning_rate": 0.0002465261514441842,
      "loss": 0.0416,
      "step": 1112
    },
    {
      "epoch": 2.605032182562902,
      "grad_norm": 0.16690124571323395,
      "learning_rate": 0.00024644808743169396,
      "loss": 0.0275,
      "step": 1113
    },
    {
      "epoch": 2.607372732592159,
      "grad_norm": 0.13866722583770752,
      "learning_rate": 0.00024637002341920374,
      "loss": 0.0366,
      "step": 1114
    },
    {
      "epoch": 2.609713282621416,
      "grad_norm": 0.21318526566028595,
      "learning_rate": 0.00024629195940671346,
      "loss": 0.0246,
      "step": 1115
    },
    {
      "epoch": 2.612053832650673,
      "grad_norm": 0.14073528349399567,
      "learning_rate": 0.00024621389539422324,
      "loss": 0.04,
      "step": 1116
    },
    {
      "epoch": 2.61439438267993,
      "grad_norm": 0.5926475524902344,
      "learning_rate": 0.000246135831381733,
      "loss": 0.034,
      "step": 1117
    },
    {
      "epoch": 2.6167349327091864,
      "grad_norm": 0.14639423787593842,
      "learning_rate": 0.00024605776736924274,
      "loss": 0.0251,
      "step": 1118
    },
    {
      "epoch": 2.6190754827384435,
      "grad_norm": 0.1623770147562027,
      "learning_rate": 0.0002459797033567525,
      "loss": 0.0289,
      "step": 1119
    },
    {
      "epoch": 2.6214160327677005,
      "grad_norm": 0.1700720638036728,
      "learning_rate": 0.0002459016393442623,
      "loss": 0.0508,
      "step": 1120
    },
    {
      "epoch": 2.6237565827969576,
      "grad_norm": 0.19474580883979797,
      "learning_rate": 0.000245823575331772,
      "loss": 0.0464,
      "step": 1121
    },
    {
      "epoch": 2.626097132826214,
      "grad_norm": 0.28535163402557373,
      "learning_rate": 0.0002457455113192818,
      "loss": 0.0386,
      "step": 1122
    },
    {
      "epoch": 2.628437682855471,
      "grad_norm": 0.33004310727119446,
      "learning_rate": 0.0002456674473067916,
      "loss": 0.0394,
      "step": 1123
    },
    {
      "epoch": 2.630778232884728,
      "grad_norm": 0.17245735228061676,
      "learning_rate": 0.0002455893832943013,
      "loss": 0.0457,
      "step": 1124
    },
    {
      "epoch": 2.633118782913985,
      "grad_norm": 0.7708487510681152,
      "learning_rate": 0.0002455113192818111,
      "loss": 0.0537,
      "step": 1125
    },
    {
      "epoch": 2.635459332943242,
      "grad_norm": 0.14950279891490936,
      "learning_rate": 0.00024543325526932085,
      "loss": 0.0194,
      "step": 1126
    },
    {
      "epoch": 2.6377998829724985,
      "grad_norm": 0.2395920306444168,
      "learning_rate": 0.0002453551912568306,
      "loss": 0.0513,
      "step": 1127
    },
    {
      "epoch": 2.6401404330017555,
      "grad_norm": 0.11610233038663864,
      "learning_rate": 0.0002452771272443403,
      "loss": 0.0197,
      "step": 1128
    },
    {
      "epoch": 2.642480983031012,
      "grad_norm": 0.41937392950057983,
      "learning_rate": 0.0002451990632318501,
      "loss": 0.0411,
      "step": 1129
    },
    {
      "epoch": 2.644821533060269,
      "grad_norm": 0.13324229419231415,
      "learning_rate": 0.00024512099921935985,
      "loss": 0.0311,
      "step": 1130
    },
    {
      "epoch": 2.647162083089526,
      "grad_norm": 0.36824169754981995,
      "learning_rate": 0.0002450429352068696,
      "loss": 0.0374,
      "step": 1131
    },
    {
      "epoch": 2.6495026331187828,
      "grad_norm": 0.23287276923656464,
      "learning_rate": 0.00024496487119437935,
      "loss": 0.0484,
      "step": 1132
    },
    {
      "epoch": 2.65184318314804,
      "grad_norm": 0.16633769869804382,
      "learning_rate": 0.00024488680718188913,
      "loss": 0.0343,
      "step": 1133
    },
    {
      "epoch": 2.6541837331772964,
      "grad_norm": 0.17929475009441376,
      "learning_rate": 0.0002448087431693989,
      "loss": 0.0538,
      "step": 1134
    },
    {
      "epoch": 2.6565242832065534,
      "grad_norm": 0.11951635777950287,
      "learning_rate": 0.00024473067915690863,
      "loss": 0.0203,
      "step": 1135
    },
    {
      "epoch": 2.6588648332358105,
      "grad_norm": 0.13836915791034698,
      "learning_rate": 0.0002446526151444184,
      "loss": 0.0304,
      "step": 1136
    },
    {
      "epoch": 2.6612053832650675,
      "grad_norm": 0.2001093178987503,
      "learning_rate": 0.0002445745511319282,
      "loss": 0.0492,
      "step": 1137
    },
    {
      "epoch": 2.663545933294324,
      "grad_norm": 0.14886504411697388,
      "learning_rate": 0.0002444964871194379,
      "loss": 0.042,
      "step": 1138
    },
    {
      "epoch": 2.665886483323581,
      "grad_norm": 0.11885561794042587,
      "learning_rate": 0.0002444184231069477,
      "loss": 0.0325,
      "step": 1139
    },
    {
      "epoch": 2.6682270333528377,
      "grad_norm": 0.14859133958816528,
      "learning_rate": 0.00024434035909445747,
      "loss": 0.0441,
      "step": 1140
    },
    {
      "epoch": 2.6705675833820948,
      "grad_norm": 0.08291304856538773,
      "learning_rate": 0.0002442622950819672,
      "loss": 0.0176,
      "step": 1141
    },
    {
      "epoch": 2.672908133411352,
      "grad_norm": 0.15549534559249878,
      "learning_rate": 0.00024418423106947697,
      "loss": 0.0311,
      "step": 1142
    },
    {
      "epoch": 2.6752486834406084,
      "grad_norm": 0.1522829234600067,
      "learning_rate": 0.00024410616705698672,
      "loss": 0.0383,
      "step": 1143
    },
    {
      "epoch": 2.6775892334698654,
      "grad_norm": 0.1980469822883606,
      "learning_rate": 0.00024402810304449647,
      "loss": 0.0478,
      "step": 1144
    },
    {
      "epoch": 2.679929783499122,
      "grad_norm": 0.14017528295516968,
      "learning_rate": 0.00024395003903200622,
      "loss": 0.0257,
      "step": 1145
    },
    {
      "epoch": 2.682270333528379,
      "grad_norm": 0.17396663129329681,
      "learning_rate": 0.000243871975019516,
      "loss": 0.066,
      "step": 1146
    },
    {
      "epoch": 2.684610883557636,
      "grad_norm": 0.1401783972978592,
      "learning_rate": 0.00024379391100702575,
      "loss": 0.0289,
      "step": 1147
    },
    {
      "epoch": 2.686951433586893,
      "grad_norm": 0.19858652353286743,
      "learning_rate": 0.0002437158469945355,
      "loss": 0.041,
      "step": 1148
    },
    {
      "epoch": 2.6892919836161497,
      "grad_norm": 0.16466042399406433,
      "learning_rate": 0.00024363778298204527,
      "loss": 0.0496,
      "step": 1149
    },
    {
      "epoch": 2.6916325336454068,
      "grad_norm": 0.13857507705688477,
      "learning_rate": 0.00024355971896955502,
      "loss": 0.0241,
      "step": 1150
    },
    {
      "epoch": 2.6939730836746634,
      "grad_norm": 0.1938145011663437,
      "learning_rate": 0.00024348165495706477,
      "loss": 0.0257,
      "step": 1151
    },
    {
      "epoch": 2.6963136337039204,
      "grad_norm": 0.1180470660328865,
      "learning_rate": 0.00024340359094457455,
      "loss": 0.0199,
      "step": 1152
    },
    {
      "epoch": 2.6986541837331774,
      "grad_norm": 0.15940873324871063,
      "learning_rate": 0.0002433255269320843,
      "loss": 0.0223,
      "step": 1153
    },
    {
      "epoch": 2.700994733762434,
      "grad_norm": 0.14549356698989868,
      "learning_rate": 0.00024324746291959403,
      "loss": 0.0356,
      "step": 1154
    },
    {
      "epoch": 2.703335283791691,
      "grad_norm": 0.13310910761356354,
      "learning_rate": 0.00024316939890710383,
      "loss": 0.0278,
      "step": 1155
    },
    {
      "epoch": 2.7056758338209477,
      "grad_norm": 0.2595076262950897,
      "learning_rate": 0.00024309133489461355,
      "loss": 0.0473,
      "step": 1156
    },
    {
      "epoch": 2.7080163838502047,
      "grad_norm": 0.22383517026901245,
      "learning_rate": 0.0002430132708821233,
      "loss": 0.0595,
      "step": 1157
    },
    {
      "epoch": 2.7103569338794617,
      "grad_norm": 0.19691644608974457,
      "learning_rate": 0.00024293520686963308,
      "loss": 0.0368,
      "step": 1158
    },
    {
      "epoch": 2.712697483908719,
      "grad_norm": 0.13673819601535797,
      "learning_rate": 0.00024285714285714283,
      "loss": 0.0364,
      "step": 1159
    },
    {
      "epoch": 2.7150380339379754,
      "grad_norm": 0.15932543575763702,
      "learning_rate": 0.00024277907884465258,
      "loss": 0.0363,
      "step": 1160
    },
    {
      "epoch": 2.7173785839672324,
      "grad_norm": 0.17010629177093506,
      "learning_rate": 0.00024270101483216236,
      "loss": 0.0299,
      "step": 1161
    },
    {
      "epoch": 2.719719133996489,
      "grad_norm": 0.12893036007881165,
      "learning_rate": 0.0002426229508196721,
      "loss": 0.0302,
      "step": 1162
    },
    {
      "epoch": 2.722059684025746,
      "grad_norm": 0.1263464093208313,
      "learning_rate": 0.00024254488680718186,
      "loss": 0.0285,
      "step": 1163
    },
    {
      "epoch": 2.724400234055003,
      "grad_norm": 0.10807981342077255,
      "learning_rate": 0.00024246682279469164,
      "loss": 0.0182,
      "step": 1164
    },
    {
      "epoch": 2.7267407840842597,
      "grad_norm": 0.13316601514816284,
      "learning_rate": 0.0002423887587822014,
      "loss": 0.0327,
      "step": 1165
    },
    {
      "epoch": 2.7290813341135167,
      "grad_norm": 0.18393951654434204,
      "learning_rate": 0.00024231069476971114,
      "loss": 0.0425,
      "step": 1166
    },
    {
      "epoch": 2.7314218841427733,
      "grad_norm": 0.12939660251140594,
      "learning_rate": 0.00024223263075722092,
      "loss": 0.0325,
      "step": 1167
    },
    {
      "epoch": 2.7337624341720304,
      "grad_norm": 0.10539431124925613,
      "learning_rate": 0.00024215456674473067,
      "loss": 0.0372,
      "step": 1168
    },
    {
      "epoch": 2.7361029842012874,
      "grad_norm": 0.12059652805328369,
      "learning_rate": 0.00024207650273224042,
      "loss": 0.0263,
      "step": 1169
    },
    {
      "epoch": 2.7384435342305444,
      "grad_norm": 0.13936661183834076,
      "learning_rate": 0.0002419984387197502,
      "loss": 0.0382,
      "step": 1170
    },
    {
      "epoch": 2.740784084259801,
      "grad_norm": 0.22689850628376007,
      "learning_rate": 0.00024192037470725995,
      "loss": 0.0429,
      "step": 1171
    },
    {
      "epoch": 2.743124634289058,
      "grad_norm": 0.15679332613945007,
      "learning_rate": 0.00024184231069476967,
      "loss": 0.025,
      "step": 1172
    },
    {
      "epoch": 2.7454651843183147,
      "grad_norm": 0.14682579040527344,
      "learning_rate": 0.00024176424668227947,
      "loss": 0.0291,
      "step": 1173
    },
    {
      "epoch": 2.7478057343475717,
      "grad_norm": 0.14205075800418854,
      "learning_rate": 0.0002416861826697892,
      "loss": 0.0385,
      "step": 1174
    },
    {
      "epoch": 2.7501462843768287,
      "grad_norm": 0.0934063270688057,
      "learning_rate": 0.00024160811865729895,
      "loss": 0.0233,
      "step": 1175
    },
    {
      "epoch": 2.7524868344060853,
      "grad_norm": 0.1412014663219452,
      "learning_rate": 0.00024153005464480872,
      "loss": 0.046,
      "step": 1176
    },
    {
      "epoch": 2.7548273844353424,
      "grad_norm": 0.10229551047086716,
      "learning_rate": 0.00024145199063231847,
      "loss": 0.017,
      "step": 1177
    },
    {
      "epoch": 2.757167934464599,
      "grad_norm": 0.13539062440395355,
      "learning_rate": 0.00024137392661982823,
      "loss": 0.0454,
      "step": 1178
    },
    {
      "epoch": 2.759508484493856,
      "grad_norm": 0.18981195986270905,
      "learning_rate": 0.000241295862607338,
      "loss": 0.0358,
      "step": 1179
    },
    {
      "epoch": 2.761849034523113,
      "grad_norm": 0.16513127088546753,
      "learning_rate": 0.00024121779859484775,
      "loss": 0.0268,
      "step": 1180
    },
    {
      "epoch": 2.76418958455237,
      "grad_norm": 0.17870423197746277,
      "learning_rate": 0.0002411397345823575,
      "loss": 0.0403,
      "step": 1181
    },
    {
      "epoch": 2.7665301345816267,
      "grad_norm": 0.07980091124773026,
      "learning_rate": 0.00024106167056986728,
      "loss": 0.0142,
      "step": 1182
    },
    {
      "epoch": 2.7688706846108837,
      "grad_norm": 0.15749453008174896,
      "learning_rate": 0.00024098360655737703,
      "loss": 0.0348,
      "step": 1183
    },
    {
      "epoch": 2.7712112346401403,
      "grad_norm": 0.15455996990203857,
      "learning_rate": 0.00024090554254488678,
      "loss": 0.0414,
      "step": 1184
    },
    {
      "epoch": 2.7735517846693973,
      "grad_norm": 0.10519219934940338,
      "learning_rate": 0.00024082747853239656,
      "loss": 0.0266,
      "step": 1185
    },
    {
      "epoch": 2.7758923346986544,
      "grad_norm": 0.12648683786392212,
      "learning_rate": 0.0002407494145199063,
      "loss": 0.0277,
      "step": 1186
    },
    {
      "epoch": 2.778232884727911,
      "grad_norm": 0.15475602447986603,
      "learning_rate": 0.00024067135050741606,
      "loss": 0.0293,
      "step": 1187
    },
    {
      "epoch": 2.780573434757168,
      "grad_norm": 0.16218909621238708,
      "learning_rate": 0.00024059328649492584,
      "loss": 0.0247,
      "step": 1188
    },
    {
      "epoch": 2.7829139847864246,
      "grad_norm": 0.17498449981212616,
      "learning_rate": 0.0002405152224824356,
      "loss": 0.0207,
      "step": 1189
    },
    {
      "epoch": 2.7852545348156816,
      "grad_norm": 0.22348661720752716,
      "learning_rate": 0.0002404371584699453,
      "loss": 0.0511,
      "step": 1190
    },
    {
      "epoch": 2.7875950848449387,
      "grad_norm": 0.21648339927196503,
      "learning_rate": 0.00024035909445745512,
      "loss": 0.0556,
      "step": 1191
    },
    {
      "epoch": 2.7899356348741957,
      "grad_norm": 0.1575039178133011,
      "learning_rate": 0.00024028103044496484,
      "loss": 0.0243,
      "step": 1192
    },
    {
      "epoch": 2.7922761849034523,
      "grad_norm": 0.1759902536869049,
      "learning_rate": 0.0002402029664324746,
      "loss": 0.0318,
      "step": 1193
    },
    {
      "epoch": 2.7946167349327093,
      "grad_norm": 0.1690262109041214,
      "learning_rate": 0.00024012490241998437,
      "loss": 0.0348,
      "step": 1194
    },
    {
      "epoch": 2.796957284961966,
      "grad_norm": 0.1133214607834816,
      "learning_rate": 0.00024004683840749412,
      "loss": 0.0273,
      "step": 1195
    },
    {
      "epoch": 2.799297834991223,
      "grad_norm": 0.19130034744739532,
      "learning_rate": 0.00023996877439500387,
      "loss": 0.0478,
      "step": 1196
    },
    {
      "epoch": 2.80163838502048,
      "grad_norm": 0.2012093961238861,
      "learning_rate": 0.00023989071038251365,
      "loss": 0.0377,
      "step": 1197
    },
    {
      "epoch": 2.8039789350497366,
      "grad_norm": 0.2381415069103241,
      "learning_rate": 0.0002398126463700234,
      "loss": 0.0308,
      "step": 1198
    },
    {
      "epoch": 2.8063194850789936,
      "grad_norm": 0.14176422357559204,
      "learning_rate": 0.00023973458235753315,
      "loss": 0.0244,
      "step": 1199
    },
    {
      "epoch": 2.8086600351082502,
      "grad_norm": 0.26589012145996094,
      "learning_rate": 0.00023965651834504292,
      "loss": 0.0444,
      "step": 1200
    },
    {
      "epoch": 2.8110005851375073,
      "grad_norm": 0.12399069219827652,
      "learning_rate": 0.00023957845433255267,
      "loss": 0.0262,
      "step": 1201
    },
    {
      "epoch": 2.8133411351667643,
      "grad_norm": 0.10960663110017776,
      "learning_rate": 0.00023950039032006242,
      "loss": 0.0236,
      "step": 1202
    },
    {
      "epoch": 2.815681685196021,
      "grad_norm": 0.24482281506061554,
      "learning_rate": 0.0002394223263075722,
      "loss": 0.0687,
      "step": 1203
    },
    {
      "epoch": 2.818022235225278,
      "grad_norm": 0.137302428483963,
      "learning_rate": 0.00023934426229508195,
      "loss": 0.0214,
      "step": 1204
    },
    {
      "epoch": 2.8203627852545345,
      "grad_norm": 0.12178017944097519,
      "learning_rate": 0.0002392661982825917,
      "loss": 0.0424,
      "step": 1205
    },
    {
      "epoch": 2.8227033352837916,
      "grad_norm": 0.11737432330846786,
      "learning_rate": 0.00023918813427010148,
      "loss": 0.0233,
      "step": 1206
    },
    {
      "epoch": 2.8250438853130486,
      "grad_norm": 0.13349102437496185,
      "learning_rate": 0.00023911007025761123,
      "loss": 0.032,
      "step": 1207
    },
    {
      "epoch": 2.8273844353423057,
      "grad_norm": 0.19183847308158875,
      "learning_rate": 0.00023903200624512095,
      "loss": 0.052,
      "step": 1208
    },
    {
      "epoch": 2.8297249853715623,
      "grad_norm": 0.19841381907463074,
      "learning_rate": 0.00023895394223263076,
      "loss": 0.0386,
      "step": 1209
    },
    {
      "epoch": 2.8320655354008193,
      "grad_norm": 0.12177771329879761,
      "learning_rate": 0.00023887587822014048,
      "loss": 0.0274,
      "step": 1210
    },
    {
      "epoch": 2.834406085430076,
      "grad_norm": 0.16942842304706573,
      "learning_rate": 0.00023879781420765023,
      "loss": 0.0445,
      "step": 1211
    },
    {
      "epoch": 2.836746635459333,
      "grad_norm": 0.11109824478626251,
      "learning_rate": 0.00023871975019516,
      "loss": 0.0311,
      "step": 1212
    },
    {
      "epoch": 2.83908718548859,
      "grad_norm": 0.1339188665151596,
      "learning_rate": 0.00023864168618266976,
      "loss": 0.026,
      "step": 1213
    },
    {
      "epoch": 2.8414277355178466,
      "grad_norm": 0.15275420248508453,
      "learning_rate": 0.0002385636221701795,
      "loss": 0.0386,
      "step": 1214
    },
    {
      "epoch": 2.8437682855471036,
      "grad_norm": 0.13918040692806244,
      "learning_rate": 0.0002384855581576893,
      "loss": 0.0274,
      "step": 1215
    },
    {
      "epoch": 2.84610883557636,
      "grad_norm": 0.1645088493824005,
      "learning_rate": 0.00023840749414519904,
      "loss": 0.0428,
      "step": 1216
    },
    {
      "epoch": 2.8484493856056172,
      "grad_norm": 0.16379746794700623,
      "learning_rate": 0.0002383294301327088,
      "loss": 0.0506,
      "step": 1217
    },
    {
      "epoch": 2.8507899356348743,
      "grad_norm": 0.14185769855976105,
      "learning_rate": 0.00023825136612021857,
      "loss": 0.0415,
      "step": 1218
    },
    {
      "epoch": 2.8531304856641313,
      "grad_norm": 0.13144901394844055,
      "learning_rate": 0.00023817330210772832,
      "loss": 0.0455,
      "step": 1219
    },
    {
      "epoch": 2.855471035693388,
      "grad_norm": 0.09487463533878326,
      "learning_rate": 0.00023809523809523807,
      "loss": 0.0292,
      "step": 1220
    },
    {
      "epoch": 2.857811585722645,
      "grad_norm": 0.10216221958398819,
      "learning_rate": 0.00023801717408274785,
      "loss": 0.04,
      "step": 1221
    },
    {
      "epoch": 2.8601521357519015,
      "grad_norm": 0.13052646815776825,
      "learning_rate": 0.0002379391100702576,
      "loss": 0.0353,
      "step": 1222
    },
    {
      "epoch": 2.8624926857811586,
      "grad_norm": 0.22304390370845795,
      "learning_rate": 0.00023786104605776735,
      "loss": 0.0612,
      "step": 1223
    },
    {
      "epoch": 2.8648332358104156,
      "grad_norm": 0.14231480658054352,
      "learning_rate": 0.00023778298204527712,
      "loss": 0.053,
      "step": 1224
    },
    {
      "epoch": 2.867173785839672,
      "grad_norm": 0.1511310636997223,
      "learning_rate": 0.00023770491803278687,
      "loss": 0.0367,
      "step": 1225
    },
    {
      "epoch": 2.8695143358689292,
      "grad_norm": 0.09496200829744339,
      "learning_rate": 0.0002376268540202966,
      "loss": 0.0255,
      "step": 1226
    },
    {
      "epoch": 2.871854885898186,
      "grad_norm": 0.13212694227695465,
      "learning_rate": 0.0002375487900078064,
      "loss": 0.0416,
      "step": 1227
    },
    {
      "epoch": 2.874195435927443,
      "grad_norm": 0.14376330375671387,
      "learning_rate": 0.00023747072599531613,
      "loss": 0.0228,
      "step": 1228
    },
    {
      "epoch": 2.8765359859567,
      "grad_norm": 0.1415289342403412,
      "learning_rate": 0.00023739266198282588,
      "loss": 0.0376,
      "step": 1229
    },
    {
      "epoch": 2.878876535985957,
      "grad_norm": 0.17344824969768524,
      "learning_rate": 0.00023731459797033565,
      "loss": 0.0458,
      "step": 1230
    },
    {
      "epoch": 2.8812170860152135,
      "grad_norm": 0.17953307926654816,
      "learning_rate": 0.0002372365339578454,
      "loss": 0.0478,
      "step": 1231
    },
    {
      "epoch": 2.8835576360444706,
      "grad_norm": 0.13327524065971375,
      "learning_rate": 0.00023715846994535515,
      "loss": 0.0197,
      "step": 1232
    },
    {
      "epoch": 2.885898186073727,
      "grad_norm": 0.16059961915016174,
      "learning_rate": 0.00023708040593286493,
      "loss": 0.0438,
      "step": 1233
    },
    {
      "epoch": 2.888238736102984,
      "grad_norm": 0.17901143431663513,
      "learning_rate": 0.00023700234192037468,
      "loss": 0.0346,
      "step": 1234
    },
    {
      "epoch": 2.8905792861322412,
      "grad_norm": 0.11246979236602783,
      "learning_rate": 0.00023692427790788443,
      "loss": 0.0195,
      "step": 1235
    },
    {
      "epoch": 2.892919836161498,
      "grad_norm": 0.2836630046367645,
      "learning_rate": 0.0002368462138953942,
      "loss": 0.0469,
      "step": 1236
    },
    {
      "epoch": 2.895260386190755,
      "grad_norm": 0.19341720640659332,
      "learning_rate": 0.00023676814988290396,
      "loss": 0.0373,
      "step": 1237
    },
    {
      "epoch": 2.8976009362200115,
      "grad_norm": 0.23168227076530457,
      "learning_rate": 0.0002366900858704137,
      "loss": 0.0544,
      "step": 1238
    },
    {
      "epoch": 2.8999414862492685,
      "grad_norm": 0.1874198615550995,
      "learning_rate": 0.0002366120218579235,
      "loss": 0.0449,
      "step": 1239
    },
    {
      "epoch": 2.9022820362785255,
      "grad_norm": 0.15686996281147003,
      "learning_rate": 0.00023653395784543324,
      "loss": 0.0232,
      "step": 1240
    },
    {
      "epoch": 2.9046225863077826,
      "grad_norm": 0.1632692813873291,
      "learning_rate": 0.000236455893832943,
      "loss": 0.038,
      "step": 1241
    },
    {
      "epoch": 2.906963136337039,
      "grad_norm": 0.2247207909822464,
      "learning_rate": 0.00023637782982045277,
      "loss": 0.0502,
      "step": 1242
    },
    {
      "epoch": 2.909303686366296,
      "grad_norm": 0.14821074903011322,
      "learning_rate": 0.00023629976580796252,
      "loss": 0.0286,
      "step": 1243
    },
    {
      "epoch": 2.911644236395553,
      "grad_norm": 0.17476068437099457,
      "learning_rate": 0.00023622170179547224,
      "loss": 0.0203,
      "step": 1244
    },
    {
      "epoch": 2.91398478642481,
      "grad_norm": 0.13647128641605377,
      "learning_rate": 0.00023614363778298202,
      "loss": 0.0445,
      "step": 1245
    },
    {
      "epoch": 2.916325336454067,
      "grad_norm": 0.22386398911476135,
      "learning_rate": 0.00023606557377049177,
      "loss": 0.057,
      "step": 1246
    },
    {
      "epoch": 2.9186658864833235,
      "grad_norm": 0.19335301220417023,
      "learning_rate": 0.00023598750975800152,
      "loss": 0.0327,
      "step": 1247
    },
    {
      "epoch": 2.9210064365125805,
      "grad_norm": 0.1027449294924736,
      "learning_rate": 0.0002359094457455113,
      "loss": 0.0243,
      "step": 1248
    },
    {
      "epoch": 2.923346986541837,
      "grad_norm": 0.13876155018806458,
      "learning_rate": 0.00023583138173302105,
      "loss": 0.0328,
      "step": 1249
    },
    {
      "epoch": 2.925687536571094,
      "grad_norm": 0.12918855249881744,
      "learning_rate": 0.0002357533177205308,
      "loss": 0.0237,
      "step": 1250
    },
    {
      "epoch": 2.928028086600351,
      "grad_norm": 0.1649649292230606,
      "learning_rate": 0.00023567525370804057,
      "loss": 0.0392,
      "step": 1251
    },
    {
      "epoch": 2.9303686366296082,
      "grad_norm": 0.16105733811855316,
      "learning_rate": 0.00023559718969555033,
      "loss": 0.0399,
      "step": 1252
    },
    {
      "epoch": 2.932709186658865,
      "grad_norm": 0.21902082860469818,
      "learning_rate": 0.00023551912568306008,
      "loss": 0.0446,
      "step": 1253
    },
    {
      "epoch": 2.935049736688122,
      "grad_norm": 0.12122392654418945,
      "learning_rate": 0.00023544106167056985,
      "loss": 0.0351,
      "step": 1254
    },
    {
      "epoch": 2.9373902867173785,
      "grad_norm": 0.16643092036247253,
      "learning_rate": 0.0002353629976580796,
      "loss": 0.0335,
      "step": 1255
    },
    {
      "epoch": 2.9397308367466355,
      "grad_norm": 0.13985592126846313,
      "learning_rate": 0.00023528493364558935,
      "loss": 0.0378,
      "step": 1256
    },
    {
      "epoch": 2.9420713867758925,
      "grad_norm": 0.16394980251789093,
      "learning_rate": 0.00023520686963309913,
      "loss": 0.0484,
      "step": 1257
    },
    {
      "epoch": 2.944411936805149,
      "grad_norm": 0.12066218256950378,
      "learning_rate": 0.00023512880562060888,
      "loss": 0.0225,
      "step": 1258
    },
    {
      "epoch": 2.946752486834406,
      "grad_norm": 0.14281274378299713,
      "learning_rate": 0.00023505074160811863,
      "loss": 0.0251,
      "step": 1259
    },
    {
      "epoch": 2.9490930368636628,
      "grad_norm": 0.1657869964838028,
      "learning_rate": 0.0002349726775956284,
      "loss": 0.0266,
      "step": 1260
    },
    {
      "epoch": 2.95143358689292,
      "grad_norm": 0.18406622111797333,
      "learning_rate": 0.00023489461358313816,
      "loss": 0.0303,
      "step": 1261
    },
    {
      "epoch": 2.953774136922177,
      "grad_norm": 0.15708091855049133,
      "learning_rate": 0.00023481654957064794,
      "loss": 0.0407,
      "step": 1262
    },
    {
      "epoch": 2.9561146869514334,
      "grad_norm": 0.09315934032201767,
      "learning_rate": 0.00023473848555815766,
      "loss": 0.0212,
      "step": 1263
    },
    {
      "epoch": 2.9584552369806905,
      "grad_norm": 0.1472426801919937,
      "learning_rate": 0.0002346604215456674,
      "loss": 0.0513,
      "step": 1264
    },
    {
      "epoch": 2.9607957870099475,
      "grad_norm": 0.15580421686172485,
      "learning_rate": 0.0002345823575331772,
      "loss": 0.0415,
      "step": 1265
    },
    {
      "epoch": 2.963136337039204,
      "grad_norm": 0.13084374368190765,
      "learning_rate": 0.00023450429352068694,
      "loss": 0.0344,
      "step": 1266
    },
    {
      "epoch": 2.965476887068461,
      "grad_norm": 0.15941587090492249,
      "learning_rate": 0.0002344262295081967,
      "loss": 0.0427,
      "step": 1267
    },
    {
      "epoch": 2.967817437097718,
      "grad_norm": 0.17825108766555786,
      "learning_rate": 0.00023434816549570647,
      "loss": 0.0424,
      "step": 1268
    },
    {
      "epoch": 2.9701579871269748,
      "grad_norm": 0.12541423738002777,
      "learning_rate": 0.00023427010148321622,
      "loss": 0.0394,
      "step": 1269
    },
    {
      "epoch": 2.972498537156232,
      "grad_norm": 0.11164876818656921,
      "learning_rate": 0.00023419203747072597,
      "loss": 0.0326,
      "step": 1270
    },
    {
      "epoch": 2.9748390871854884,
      "grad_norm": 0.12508653104305267,
      "learning_rate": 0.00023411397345823575,
      "loss": 0.0284,
      "step": 1271
    },
    {
      "epoch": 2.9771796372147454,
      "grad_norm": 0.16088107228279114,
      "learning_rate": 0.0002340359094457455,
      "loss": 0.0292,
      "step": 1272
    },
    {
      "epoch": 2.9795201872440025,
      "grad_norm": 0.13672153651714325,
      "learning_rate": 0.00023395784543325525,
      "loss": 0.0478,
      "step": 1273
    },
    {
      "epoch": 2.981860737273259,
      "grad_norm": 0.08722897619009018,
      "learning_rate": 0.00023387978142076502,
      "loss": 0.0247,
      "step": 1274
    },
    {
      "epoch": 2.984201287302516,
      "grad_norm": 0.08282168209552765,
      "learning_rate": 0.00023380171740827477,
      "loss": 0.0275,
      "step": 1275
    },
    {
      "epoch": 2.9865418373317727,
      "grad_norm": 0.13167735934257507,
      "learning_rate": 0.00023372365339578452,
      "loss": 0.0272,
      "step": 1276
    },
    {
      "epoch": 2.9888823873610297,
      "grad_norm": 0.09788568317890167,
      "learning_rate": 0.0002336455893832943,
      "loss": 0.0256,
      "step": 1277
    },
    {
      "epoch": 2.9912229373902868,
      "grad_norm": 0.13199156522750854,
      "learning_rate": 0.00023356752537080405,
      "loss": 0.0245,
      "step": 1278
    },
    {
      "epoch": 2.993563487419544,
      "grad_norm": 0.12902653217315674,
      "learning_rate": 0.0002334894613583138,
      "loss": 0.0342,
      "step": 1279
    },
    {
      "epoch": 2.9959040374488004,
      "grad_norm": 0.14209356904029846,
      "learning_rate": 0.00023341139734582358,
      "loss": 0.0425,
      "step": 1280
    },
    {
      "epoch": 2.9982445874780574,
      "grad_norm": 0.27155059576034546,
      "learning_rate": 0.0002333333333333333,
      "loss": 0.038,
      "step": 1281
    },
    {
      "epoch": 2.9982445874780574,
      "eval_loss": 0.0431111678481102,
      "eval_runtime": 127.2899,
      "eval_samples_per_second": 4.337,
      "eval_steps_per_second": 0.542,
      "step": 1281
    }
  ],
  "logging_steps": 1,
  "max_steps": 4270,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 3.557541729099448e+17,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
