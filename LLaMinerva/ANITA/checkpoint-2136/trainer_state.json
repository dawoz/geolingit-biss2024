{
  "best_metric": 0.25438231229782104,
  "best_model_checkpoint": "LLaMinerva/ANITA/checkpoint-2136",
  "epoch": 4.999414862492686,
  "eval_steps": 500,
  "global_step": 2136,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0023405500292568754,
      "grad_norm": 13.065563201904297,
      "learning_rate": 7.025761124121779e-07,
      "loss": 5.1436,
      "step": 1
    },
    {
      "epoch": 0.004681100058513751,
      "grad_norm": 12.436966896057129,
      "learning_rate": 1.4051522248243558e-06,
      "loss": 5.1867,
      "step": 2
    },
    {
      "epoch": 0.007021650087770626,
      "grad_norm": 12.954062461853027,
      "learning_rate": 2.107728337236534e-06,
      "loss": 5.4132,
      "step": 3
    },
    {
      "epoch": 0.009362200117027502,
      "grad_norm": 12.695781707763672,
      "learning_rate": 2.8103044496487116e-06,
      "loss": 5.4519,
      "step": 4
    },
    {
      "epoch": 0.011702750146284377,
      "grad_norm": 12.558113098144531,
      "learning_rate": 3.5128805620608897e-06,
      "loss": 5.5445,
      "step": 5
    },
    {
      "epoch": 0.014043300175541252,
      "grad_norm": 12.493412971496582,
      "learning_rate": 4.215456674473068e-06,
      "loss": 5.4375,
      "step": 6
    },
    {
      "epoch": 0.016383850204798128,
      "grad_norm": 12.889161109924316,
      "learning_rate": 4.9180327868852455e-06,
      "loss": 5.41,
      "step": 7
    },
    {
      "epoch": 0.018724400234055003,
      "grad_norm": 12.968704223632812,
      "learning_rate": 5.620608899297423e-06,
      "loss": 5.2092,
      "step": 8
    },
    {
      "epoch": 0.021064950263311878,
      "grad_norm": 12.401562690734863,
      "learning_rate": 6.323185011709601e-06,
      "loss": 5.2474,
      "step": 9
    },
    {
      "epoch": 0.023405500292568753,
      "grad_norm": 12.470664024353027,
      "learning_rate": 7.025761124121779e-06,
      "loss": 5.0532,
      "step": 10
    },
    {
      "epoch": 0.025746050321825628,
      "grad_norm": 13.91598892211914,
      "learning_rate": 7.728337236533957e-06,
      "loss": 5.2857,
      "step": 11
    },
    {
      "epoch": 0.028086600351082503,
      "grad_norm": 13.194491386413574,
      "learning_rate": 8.430913348946136e-06,
      "loss": 5.1412,
      "step": 12
    },
    {
      "epoch": 0.030427150380339378,
      "grad_norm": 13.185876846313477,
      "learning_rate": 9.133489461358312e-06,
      "loss": 5.1603,
      "step": 13
    },
    {
      "epoch": 0.032767700409596257,
      "grad_norm": 14.172979354858398,
      "learning_rate": 9.836065573770491e-06,
      "loss": 5.0395,
      "step": 14
    },
    {
      "epoch": 0.03510825043885313,
      "grad_norm": 13.406156539916992,
      "learning_rate": 1.0538641686182668e-05,
      "loss": 4.8048,
      "step": 15
    },
    {
      "epoch": 0.037448800468110006,
      "grad_norm": 12.577102661132812,
      "learning_rate": 1.1241217798594846e-05,
      "loss": 4.8353,
      "step": 16
    },
    {
      "epoch": 0.03978935049736688,
      "grad_norm": 12.921100616455078,
      "learning_rate": 1.1943793911007025e-05,
      "loss": 4.5877,
      "step": 17
    },
    {
      "epoch": 0.042129900526623756,
      "grad_norm": 13.393592834472656,
      "learning_rate": 1.2646370023419202e-05,
      "loss": 4.4159,
      "step": 18
    },
    {
      "epoch": 0.044470450555880635,
      "grad_norm": 13.247405052185059,
      "learning_rate": 1.334894613583138e-05,
      "loss": 4.6138,
      "step": 19
    },
    {
      "epoch": 0.046811000585137506,
      "grad_norm": 12.863821983337402,
      "learning_rate": 1.4051522248243559e-05,
      "loss": 4.4841,
      "step": 20
    },
    {
      "epoch": 0.049151550614394385,
      "grad_norm": 12.684087753295898,
      "learning_rate": 1.4754098360655736e-05,
      "loss": 4.1337,
      "step": 21
    },
    {
      "epoch": 0.051492100643651256,
      "grad_norm": 12.068134307861328,
      "learning_rate": 1.5456674473067914e-05,
      "loss": 4.1717,
      "step": 22
    },
    {
      "epoch": 0.053832650672908135,
      "grad_norm": 12.536057472229004,
      "learning_rate": 1.6159250585480093e-05,
      "loss": 3.9362,
      "step": 23
    },
    {
      "epoch": 0.056173200702165006,
      "grad_norm": 11.144207954406738,
      "learning_rate": 1.686182669789227e-05,
      "loss": 3.7747,
      "step": 24
    },
    {
      "epoch": 0.058513750731421885,
      "grad_norm": 10.859487533569336,
      "learning_rate": 1.756440281030445e-05,
      "loss": 3.5226,
      "step": 25
    },
    {
      "epoch": 0.060854300760678756,
      "grad_norm": 10.834660530090332,
      "learning_rate": 1.8266978922716625e-05,
      "loss": 3.5079,
      "step": 26
    },
    {
      "epoch": 0.06319485078993564,
      "grad_norm": 10.024247169494629,
      "learning_rate": 1.8969555035128803e-05,
      "loss": 3.2925,
      "step": 27
    },
    {
      "epoch": 0.06553540081919251,
      "grad_norm": 9.644912719726562,
      "learning_rate": 1.9672131147540982e-05,
      "loss": 3.2627,
      "step": 28
    },
    {
      "epoch": 0.06787595084844938,
      "grad_norm": 9.501051902770996,
      "learning_rate": 2.037470725995316e-05,
      "loss": 3.0781,
      "step": 29
    },
    {
      "epoch": 0.07021650087770626,
      "grad_norm": 11.407389640808105,
      "learning_rate": 2.1077283372365335e-05,
      "loss": 2.698,
      "step": 30
    },
    {
      "epoch": 0.07255705090696314,
      "grad_norm": 9.6383638381958,
      "learning_rate": 2.1779859484777514e-05,
      "loss": 2.5236,
      "step": 31
    },
    {
      "epoch": 0.07489760093622001,
      "grad_norm": 12.722529411315918,
      "learning_rate": 2.2482435597189693e-05,
      "loss": 2.545,
      "step": 32
    },
    {
      "epoch": 0.07723815096547688,
      "grad_norm": 13.642773628234863,
      "learning_rate": 2.318501170960187e-05,
      "loss": 2.3674,
      "step": 33
    },
    {
      "epoch": 0.07957870099473376,
      "grad_norm": 8.836204528808594,
      "learning_rate": 2.388758782201405e-05,
      "loss": 2.1249,
      "step": 34
    },
    {
      "epoch": 0.08191925102399064,
      "grad_norm": 8.181549072265625,
      "learning_rate": 2.4590163934426225e-05,
      "loss": 2.1045,
      "step": 35
    },
    {
      "epoch": 0.08425980105324751,
      "grad_norm": 8.411561012268066,
      "learning_rate": 2.5292740046838403e-05,
      "loss": 1.8462,
      "step": 36
    },
    {
      "epoch": 0.08660035108250438,
      "grad_norm": 7.99399471282959,
      "learning_rate": 2.5995316159250582e-05,
      "loss": 1.7375,
      "step": 37
    },
    {
      "epoch": 0.08894090111176127,
      "grad_norm": 7.9506611824035645,
      "learning_rate": 2.669789227166276e-05,
      "loss": 1.5808,
      "step": 38
    },
    {
      "epoch": 0.09128145114101814,
      "grad_norm": 7.286951065063477,
      "learning_rate": 2.740046838407494e-05,
      "loss": 1.5382,
      "step": 39
    },
    {
      "epoch": 0.09362200117027501,
      "grad_norm": 8.454571723937988,
      "learning_rate": 2.8103044496487117e-05,
      "loss": 1.3748,
      "step": 40
    },
    {
      "epoch": 0.09596255119953188,
      "grad_norm": 6.9411821365356445,
      "learning_rate": 2.8805620608899293e-05,
      "loss": 1.2825,
      "step": 41
    },
    {
      "epoch": 0.09830310122878877,
      "grad_norm": 5.675450801849365,
      "learning_rate": 2.950819672131147e-05,
      "loss": 1.1869,
      "step": 42
    },
    {
      "epoch": 0.10064365125804564,
      "grad_norm": 4.878019332885742,
      "learning_rate": 3.021077283372365e-05,
      "loss": 1.0004,
      "step": 43
    },
    {
      "epoch": 0.10298420128730251,
      "grad_norm": 4.561619281768799,
      "learning_rate": 3.091334894613583e-05,
      "loss": 0.8759,
      "step": 44
    },
    {
      "epoch": 0.1053247513165594,
      "grad_norm": 4.37693452835083,
      "learning_rate": 3.161592505854801e-05,
      "loss": 0.8639,
      "step": 45
    },
    {
      "epoch": 0.10766530134581627,
      "grad_norm": 3.7386996746063232,
      "learning_rate": 3.2318501170960185e-05,
      "loss": 0.7974,
      "step": 46
    },
    {
      "epoch": 0.11000585137507314,
      "grad_norm": 2.8982880115509033,
      "learning_rate": 3.3021077283372364e-05,
      "loss": 0.7903,
      "step": 47
    },
    {
      "epoch": 0.11234640140433001,
      "grad_norm": 2.725463628768921,
      "learning_rate": 3.372365339578454e-05,
      "loss": 0.792,
      "step": 48
    },
    {
      "epoch": 0.1146869514335869,
      "grad_norm": 2.750868558883667,
      "learning_rate": 3.442622950819672e-05,
      "loss": 0.7492,
      "step": 49
    },
    {
      "epoch": 0.11702750146284377,
      "grad_norm": 1.8370558023452759,
      "learning_rate": 3.51288056206089e-05,
      "loss": 0.6162,
      "step": 50
    },
    {
      "epoch": 0.11936805149210064,
      "grad_norm": 2.274932384490967,
      "learning_rate": 3.583138173302107e-05,
      "loss": 0.6516,
      "step": 51
    },
    {
      "epoch": 0.12170860152135751,
      "grad_norm": 1.6456327438354492,
      "learning_rate": 3.653395784543325e-05,
      "loss": 0.6626,
      "step": 52
    },
    {
      "epoch": 0.1240491515506144,
      "grad_norm": 1.4292540550231934,
      "learning_rate": 3.723653395784543e-05,
      "loss": 0.5297,
      "step": 53
    },
    {
      "epoch": 0.12638970157987128,
      "grad_norm": 1.4451825618743896,
      "learning_rate": 3.7939110070257607e-05,
      "loss": 0.5273,
      "step": 54
    },
    {
      "epoch": 0.12873025160912815,
      "grad_norm": 1.4839122295379639,
      "learning_rate": 3.8641686182669785e-05,
      "loss": 0.4825,
      "step": 55
    },
    {
      "epoch": 0.13107080163838503,
      "grad_norm": 1.5569746494293213,
      "learning_rate": 3.9344262295081964e-05,
      "loss": 0.4913,
      "step": 56
    },
    {
      "epoch": 0.1334113516676419,
      "grad_norm": 1.4359103441238403,
      "learning_rate": 4.004683840749414e-05,
      "loss": 0.5846,
      "step": 57
    },
    {
      "epoch": 0.13575190169689877,
      "grad_norm": 1.2249810695648193,
      "learning_rate": 4.074941451990632e-05,
      "loss": 0.5801,
      "step": 58
    },
    {
      "epoch": 0.13809245172615564,
      "grad_norm": 1.1146870851516724,
      "learning_rate": 4.145199063231849e-05,
      "loss": 0.5293,
      "step": 59
    },
    {
      "epoch": 0.1404330017554125,
      "grad_norm": 1.509322166442871,
      "learning_rate": 4.215456674473067e-05,
      "loss": 0.5211,
      "step": 60
    },
    {
      "epoch": 0.14277355178466938,
      "grad_norm": 1.577278971672058,
      "learning_rate": 4.285714285714285e-05,
      "loss": 0.5447,
      "step": 61
    },
    {
      "epoch": 0.14511410181392628,
      "grad_norm": 1.2614620923995972,
      "learning_rate": 4.355971896955503e-05,
      "loss": 0.5636,
      "step": 62
    },
    {
      "epoch": 0.14745465184318315,
      "grad_norm": 1.3166784048080444,
      "learning_rate": 4.4262295081967207e-05,
      "loss": 0.5075,
      "step": 63
    },
    {
      "epoch": 0.14979520187244003,
      "grad_norm": 1.0203850269317627,
      "learning_rate": 4.4964871194379385e-05,
      "loss": 0.449,
      "step": 64
    },
    {
      "epoch": 0.1521357519016969,
      "grad_norm": 1.131546139717102,
      "learning_rate": 4.5667447306791564e-05,
      "loss": 0.4082,
      "step": 65
    },
    {
      "epoch": 0.15447630193095377,
      "grad_norm": 1.352973222732544,
      "learning_rate": 4.637002341920374e-05,
      "loss": 0.5338,
      "step": 66
    },
    {
      "epoch": 0.15681685196021064,
      "grad_norm": 1.0258926153182983,
      "learning_rate": 4.707259953161592e-05,
      "loss": 0.404,
      "step": 67
    },
    {
      "epoch": 0.1591574019894675,
      "grad_norm": 1.342631220817566,
      "learning_rate": 4.77751756440281e-05,
      "loss": 0.3898,
      "step": 68
    },
    {
      "epoch": 0.1614979520187244,
      "grad_norm": 0.900435745716095,
      "learning_rate": 4.847775175644028e-05,
      "loss": 0.3793,
      "step": 69
    },
    {
      "epoch": 0.16383850204798128,
      "grad_norm": 1.0378607511520386,
      "learning_rate": 4.918032786885245e-05,
      "loss": 0.4307,
      "step": 70
    },
    {
      "epoch": 0.16617905207723815,
      "grad_norm": 1.000948190689087,
      "learning_rate": 4.988290398126463e-05,
      "loss": 0.4156,
      "step": 71
    },
    {
      "epoch": 0.16851960210649503,
      "grad_norm": 1.191450834274292,
      "learning_rate": 5.0585480093676807e-05,
      "loss": 0.4486,
      "step": 72
    },
    {
      "epoch": 0.1708601521357519,
      "grad_norm": 1.6728452444076538,
      "learning_rate": 5.1288056206088985e-05,
      "loss": 0.4659,
      "step": 73
    },
    {
      "epoch": 0.17320070216500877,
      "grad_norm": 1.6473792791366577,
      "learning_rate": 5.1990632318501164e-05,
      "loss": 0.5502,
      "step": 74
    },
    {
      "epoch": 0.17554125219426564,
      "grad_norm": 0.9899210929870605,
      "learning_rate": 5.269320843091334e-05,
      "loss": 0.4226,
      "step": 75
    },
    {
      "epoch": 0.17788180222352254,
      "grad_norm": 0.8230892419815063,
      "learning_rate": 5.339578454332552e-05,
      "loss": 0.3849,
      "step": 76
    },
    {
      "epoch": 0.1802223522527794,
      "grad_norm": 0.9706488847732544,
      "learning_rate": 5.40983606557377e-05,
      "loss": 0.4601,
      "step": 77
    },
    {
      "epoch": 0.18256290228203628,
      "grad_norm": 1.149086594581604,
      "learning_rate": 5.480093676814988e-05,
      "loss": 0.5298,
      "step": 78
    },
    {
      "epoch": 0.18490345231129315,
      "grad_norm": 1.0372658967971802,
      "learning_rate": 5.5503512880562056e-05,
      "loss": 0.3684,
      "step": 79
    },
    {
      "epoch": 0.18724400234055003,
      "grad_norm": 1.241874098777771,
      "learning_rate": 5.6206088992974235e-05,
      "loss": 0.4499,
      "step": 80
    },
    {
      "epoch": 0.1895845523698069,
      "grad_norm": 0.9262474775314331,
      "learning_rate": 5.6908665105386407e-05,
      "loss": 0.4794,
      "step": 81
    },
    {
      "epoch": 0.19192510239906377,
      "grad_norm": 0.8479219675064087,
      "learning_rate": 5.7611241217798585e-05,
      "loss": 0.4458,
      "step": 82
    },
    {
      "epoch": 0.19426565242832067,
      "grad_norm": 0.9695971608161926,
      "learning_rate": 5.8313817330210764e-05,
      "loss": 0.4592,
      "step": 83
    },
    {
      "epoch": 0.19660620245757754,
      "grad_norm": 0.8076009154319763,
      "learning_rate": 5.901639344262294e-05,
      "loss": 0.5281,
      "step": 84
    },
    {
      "epoch": 0.1989467524868344,
      "grad_norm": 0.8015292286872864,
      "learning_rate": 5.971896955503512e-05,
      "loss": 0.3764,
      "step": 85
    },
    {
      "epoch": 0.20128730251609128,
      "grad_norm": 0.7655457258224487,
      "learning_rate": 6.04215456674473e-05,
      "loss": 0.3809,
      "step": 86
    },
    {
      "epoch": 0.20362785254534815,
      "grad_norm": 0.8127506375312805,
      "learning_rate": 6.112412177985948e-05,
      "loss": 0.4183,
      "step": 87
    },
    {
      "epoch": 0.20596840257460503,
      "grad_norm": 0.6713958382606506,
      "learning_rate": 6.182669789227166e-05,
      "loss": 0.4047,
      "step": 88
    },
    {
      "epoch": 0.2083089526038619,
      "grad_norm": 0.7936638593673706,
      "learning_rate": 6.252927400468383e-05,
      "loss": 0.3736,
      "step": 89
    },
    {
      "epoch": 0.2106495026331188,
      "grad_norm": 0.7458532452583313,
      "learning_rate": 6.323185011709601e-05,
      "loss": 0.4444,
      "step": 90
    },
    {
      "epoch": 0.21299005266237567,
      "grad_norm": 0.7988500595092773,
      "learning_rate": 6.393442622950819e-05,
      "loss": 0.4169,
      "step": 91
    },
    {
      "epoch": 0.21533060269163254,
      "grad_norm": 0.7354991436004639,
      "learning_rate": 6.463700234192037e-05,
      "loss": 0.4339,
      "step": 92
    },
    {
      "epoch": 0.2176711527208894,
      "grad_norm": 0.6346981525421143,
      "learning_rate": 6.533957845433255e-05,
      "loss": 0.3915,
      "step": 93
    },
    {
      "epoch": 0.22001170275014628,
      "grad_norm": 0.9554297924041748,
      "learning_rate": 6.604215456674473e-05,
      "loss": 0.4579,
      "step": 94
    },
    {
      "epoch": 0.22235225277940315,
      "grad_norm": 0.6940909624099731,
      "learning_rate": 6.67447306791569e-05,
      "loss": 0.3473,
      "step": 95
    },
    {
      "epoch": 0.22469280280866002,
      "grad_norm": 0.7664311528205872,
      "learning_rate": 6.744730679156908e-05,
      "loss": 0.3816,
      "step": 96
    },
    {
      "epoch": 0.22703335283791692,
      "grad_norm": 0.8163902163505554,
      "learning_rate": 6.814988290398126e-05,
      "loss": 0.4227,
      "step": 97
    },
    {
      "epoch": 0.2293739028671738,
      "grad_norm": 0.6987481117248535,
      "learning_rate": 6.885245901639344e-05,
      "loss": 0.439,
      "step": 98
    },
    {
      "epoch": 0.23171445289643067,
      "grad_norm": 0.6710060238838196,
      "learning_rate": 6.955503512880562e-05,
      "loss": 0.389,
      "step": 99
    },
    {
      "epoch": 0.23405500292568754,
      "grad_norm": 0.8058071136474609,
      "learning_rate": 7.02576112412178e-05,
      "loss": 0.4323,
      "step": 100
    },
    {
      "epoch": 0.2363955529549444,
      "grad_norm": 0.7885583639144897,
      "learning_rate": 7.096018735362998e-05,
      "loss": 0.4323,
      "step": 101
    },
    {
      "epoch": 0.23873610298420128,
      "grad_norm": 0.8813993334770203,
      "learning_rate": 7.166276346604214e-05,
      "loss": 0.4133,
      "step": 102
    },
    {
      "epoch": 0.24107665301345815,
      "grad_norm": 0.8128343224525452,
      "learning_rate": 7.236533957845432e-05,
      "loss": 0.3777,
      "step": 103
    },
    {
      "epoch": 0.24341720304271502,
      "grad_norm": 0.8295542597770691,
      "learning_rate": 7.30679156908665e-05,
      "loss": 0.3546,
      "step": 104
    },
    {
      "epoch": 0.24575775307197192,
      "grad_norm": 0.8525824546813965,
      "learning_rate": 7.377049180327868e-05,
      "loss": 0.3441,
      "step": 105
    },
    {
      "epoch": 0.2480983031012288,
      "grad_norm": 0.7605139017105103,
      "learning_rate": 7.447306791569086e-05,
      "loss": 0.3607,
      "step": 106
    },
    {
      "epoch": 0.25043885313048564,
      "grad_norm": 0.813829243183136,
      "learning_rate": 7.517564402810303e-05,
      "loss": 0.3125,
      "step": 107
    },
    {
      "epoch": 0.25277940315974257,
      "grad_norm": 1.3707455396652222,
      "learning_rate": 7.587822014051521e-05,
      "loss": 0.5018,
      "step": 108
    },
    {
      "epoch": 0.25511995318899944,
      "grad_norm": 1.263526201248169,
      "learning_rate": 7.658079625292739e-05,
      "loss": 0.3397,
      "step": 109
    },
    {
      "epoch": 0.2574605032182563,
      "grad_norm": 0.7508626580238342,
      "learning_rate": 7.728337236533957e-05,
      "loss": 0.3621,
      "step": 110
    },
    {
      "epoch": 0.2598010532475132,
      "grad_norm": 0.7096715569496155,
      "learning_rate": 7.798594847775175e-05,
      "loss": 0.3465,
      "step": 111
    },
    {
      "epoch": 0.26214160327677005,
      "grad_norm": 0.7590633034706116,
      "learning_rate": 7.868852459016393e-05,
      "loss": 0.4255,
      "step": 112
    },
    {
      "epoch": 0.2644821533060269,
      "grad_norm": 0.862315833568573,
      "learning_rate": 7.93911007025761e-05,
      "loss": 0.5004,
      "step": 113
    },
    {
      "epoch": 0.2668227033352838,
      "grad_norm": 0.8720811009407043,
      "learning_rate": 8.009367681498828e-05,
      "loss": 0.4649,
      "step": 114
    },
    {
      "epoch": 0.26916325336454067,
      "grad_norm": 0.8996695876121521,
      "learning_rate": 8.079625292740046e-05,
      "loss": 0.4156,
      "step": 115
    },
    {
      "epoch": 0.27150380339379754,
      "grad_norm": 0.774932324886322,
      "learning_rate": 8.149882903981264e-05,
      "loss": 0.3973,
      "step": 116
    },
    {
      "epoch": 0.2738443534230544,
      "grad_norm": 0.7526772022247314,
      "learning_rate": 8.220140515222482e-05,
      "loss": 0.3657,
      "step": 117
    },
    {
      "epoch": 0.2761849034523113,
      "grad_norm": 0.7550967335700989,
      "learning_rate": 8.290398126463698e-05,
      "loss": 0.4133,
      "step": 118
    },
    {
      "epoch": 0.27852545348156815,
      "grad_norm": 0.7642532587051392,
      "learning_rate": 8.360655737704916e-05,
      "loss": 0.3484,
      "step": 119
    },
    {
      "epoch": 0.280866003510825,
      "grad_norm": 0.7900882363319397,
      "learning_rate": 8.430913348946134e-05,
      "loss": 0.3494,
      "step": 120
    },
    {
      "epoch": 0.2832065535400819,
      "grad_norm": 0.9133102297782898,
      "learning_rate": 8.501170960187352e-05,
      "loss": 0.44,
      "step": 121
    },
    {
      "epoch": 0.28554710356933877,
      "grad_norm": 1.1136574745178223,
      "learning_rate": 8.57142857142857e-05,
      "loss": 0.4973,
      "step": 122
    },
    {
      "epoch": 0.2878876535985957,
      "grad_norm": 0.7968728542327881,
      "learning_rate": 8.641686182669788e-05,
      "loss": 0.416,
      "step": 123
    },
    {
      "epoch": 0.29022820362785257,
      "grad_norm": 1.2420848608016968,
      "learning_rate": 8.711943793911006e-05,
      "loss": 0.392,
      "step": 124
    },
    {
      "epoch": 0.29256875365710944,
      "grad_norm": 0.9437037110328674,
      "learning_rate": 8.782201405152223e-05,
      "loss": 0.4762,
      "step": 125
    },
    {
      "epoch": 0.2949093036863663,
      "grad_norm": 1.2322052717208862,
      "learning_rate": 8.852459016393441e-05,
      "loss": 0.3429,
      "step": 126
    },
    {
      "epoch": 0.2972498537156232,
      "grad_norm": 0.9347396492958069,
      "learning_rate": 8.922716627634659e-05,
      "loss": 0.3878,
      "step": 127
    },
    {
      "epoch": 0.29959040374488005,
      "grad_norm": 1.1062064170837402,
      "learning_rate": 8.992974238875877e-05,
      "loss": 0.3154,
      "step": 128
    },
    {
      "epoch": 0.3019309537741369,
      "grad_norm": 0.8188717365264893,
      "learning_rate": 9.063231850117095e-05,
      "loss": 0.3202,
      "step": 129
    },
    {
      "epoch": 0.3042715038033938,
      "grad_norm": 0.8155307769775391,
      "learning_rate": 9.133489461358313e-05,
      "loss": 0.2835,
      "step": 130
    },
    {
      "epoch": 0.30661205383265067,
      "grad_norm": 1.326997995376587,
      "learning_rate": 9.20374707259953e-05,
      "loss": 0.3983,
      "step": 131
    },
    {
      "epoch": 0.30895260386190754,
      "grad_norm": 1.3389486074447632,
      "learning_rate": 9.274004683840748e-05,
      "loss": 0.4008,
      "step": 132
    },
    {
      "epoch": 0.3112931538911644,
      "grad_norm": 1.2613228559494019,
      "learning_rate": 9.344262295081966e-05,
      "loss": 0.429,
      "step": 133
    },
    {
      "epoch": 0.3136337039204213,
      "grad_norm": 1.0957224369049072,
      "learning_rate": 9.414519906323184e-05,
      "loss": 0.3334,
      "step": 134
    },
    {
      "epoch": 0.31597425394967815,
      "grad_norm": 0.7746673822402954,
      "learning_rate": 9.484777517564402e-05,
      "loss": 0.3093,
      "step": 135
    },
    {
      "epoch": 0.318314803978935,
      "grad_norm": 1.0181710720062256,
      "learning_rate": 9.55503512880562e-05,
      "loss": 0.3529,
      "step": 136
    },
    {
      "epoch": 0.32065535400819195,
      "grad_norm": 1.151749849319458,
      "learning_rate": 9.625292740046838e-05,
      "loss": 0.3312,
      "step": 137
    },
    {
      "epoch": 0.3229959040374488,
      "grad_norm": 0.8722494840621948,
      "learning_rate": 9.695550351288056e-05,
      "loss": 0.442,
      "step": 138
    },
    {
      "epoch": 0.3253364540667057,
      "grad_norm": 1.0850363969802856,
      "learning_rate": 9.765807962529272e-05,
      "loss": 0.267,
      "step": 139
    },
    {
      "epoch": 0.32767700409596257,
      "grad_norm": 0.7542566061019897,
      "learning_rate": 9.83606557377049e-05,
      "loss": 0.2792,
      "step": 140
    },
    {
      "epoch": 0.33001755412521944,
      "grad_norm": 0.6872401833534241,
      "learning_rate": 9.906323185011708e-05,
      "loss": 0.3068,
      "step": 141
    },
    {
      "epoch": 0.3323581041544763,
      "grad_norm": 1.059293270111084,
      "learning_rate": 9.976580796252926e-05,
      "loss": 0.3946,
      "step": 142
    },
    {
      "epoch": 0.3346986541837332,
      "grad_norm": 0.8540185689926147,
      "learning_rate": 0.00010046838407494143,
      "loss": 0.343,
      "step": 143
    },
    {
      "epoch": 0.33703920421299005,
      "grad_norm": 0.7936590313911438,
      "learning_rate": 0.00010117096018735361,
      "loss": 0.3356,
      "step": 144
    },
    {
      "epoch": 0.3393797542422469,
      "grad_norm": 0.9551578164100647,
      "learning_rate": 0.00010187353629976579,
      "loss": 0.4182,
      "step": 145
    },
    {
      "epoch": 0.3417203042715038,
      "grad_norm": 0.7838213443756104,
      "learning_rate": 0.00010257611241217797,
      "loss": 0.2828,
      "step": 146
    },
    {
      "epoch": 0.34406085430076067,
      "grad_norm": 0.8282880187034607,
      "learning_rate": 0.00010327868852459015,
      "loss": 0.2552,
      "step": 147
    },
    {
      "epoch": 0.34640140433001754,
      "grad_norm": 0.8054252862930298,
      "learning_rate": 0.00010398126463700233,
      "loss": 0.3487,
      "step": 148
    },
    {
      "epoch": 0.3487419543592744,
      "grad_norm": 0.7495320439338684,
      "learning_rate": 0.0001046838407494145,
      "loss": 0.3226,
      "step": 149
    },
    {
      "epoch": 0.3510825043885313,
      "grad_norm": 1.0357683897018433,
      "learning_rate": 0.00010538641686182668,
      "loss": 0.473,
      "step": 150
    },
    {
      "epoch": 0.3534230544177882,
      "grad_norm": 0.7600712180137634,
      "learning_rate": 0.00010608899297423886,
      "loss": 0.2851,
      "step": 151
    },
    {
      "epoch": 0.3557636044470451,
      "grad_norm": 0.951691210269928,
      "learning_rate": 0.00010679156908665104,
      "loss": 0.3571,
      "step": 152
    },
    {
      "epoch": 0.35810415447630195,
      "grad_norm": 1.3457950353622437,
      "learning_rate": 0.00010749414519906322,
      "loss": 0.3833,
      "step": 153
    },
    {
      "epoch": 0.3604447045055588,
      "grad_norm": 0.9108506441116333,
      "learning_rate": 0.0001081967213114754,
      "loss": 0.3451,
      "step": 154
    },
    {
      "epoch": 0.3627852545348157,
      "grad_norm": 0.9987760782241821,
      "learning_rate": 0.00010889929742388758,
      "loss": 0.3307,
      "step": 155
    },
    {
      "epoch": 0.36512580456407256,
      "grad_norm": 0.7679513096809387,
      "learning_rate": 0.00010960187353629976,
      "loss": 0.4359,
      "step": 156
    },
    {
      "epoch": 0.36746635459332944,
      "grad_norm": 0.9045163989067078,
      "learning_rate": 0.00011030444964871193,
      "loss": 0.5049,
      "step": 157
    },
    {
      "epoch": 0.3698069046225863,
      "grad_norm": 0.7412815093994141,
      "learning_rate": 0.00011100702576112411,
      "loss": 0.2754,
      "step": 158
    },
    {
      "epoch": 0.3721474546518432,
      "grad_norm": 0.8463777899742126,
      "learning_rate": 0.00011170960187353629,
      "loss": 0.3437,
      "step": 159
    },
    {
      "epoch": 0.37448800468110005,
      "grad_norm": 0.984266996383667,
      "learning_rate": 0.00011241217798594847,
      "loss": 0.2674,
      "step": 160
    },
    {
      "epoch": 0.3768285547103569,
      "grad_norm": 0.8293250203132629,
      "learning_rate": 0.00011311475409836063,
      "loss": 0.352,
      "step": 161
    },
    {
      "epoch": 0.3791691047396138,
      "grad_norm": 0.7257052659988403,
      "learning_rate": 0.00011381733021077281,
      "loss": 0.2737,
      "step": 162
    },
    {
      "epoch": 0.38150965476887067,
      "grad_norm": 0.8711684346199036,
      "learning_rate": 0.00011451990632318499,
      "loss": 0.3364,
      "step": 163
    },
    {
      "epoch": 0.38385020479812754,
      "grad_norm": 0.9221113324165344,
      "learning_rate": 0.00011522248243559717,
      "loss": 0.3693,
      "step": 164
    },
    {
      "epoch": 0.3861907548273844,
      "grad_norm": 0.904936671257019,
      "learning_rate": 0.00011592505854800935,
      "loss": 0.3463,
      "step": 165
    },
    {
      "epoch": 0.38853130485664134,
      "grad_norm": 0.8101233243942261,
      "learning_rate": 0.00011662763466042153,
      "loss": 0.3207,
      "step": 166
    },
    {
      "epoch": 0.3908718548858982,
      "grad_norm": 0.7913277745246887,
      "learning_rate": 0.0001173302107728337,
      "loss": 0.2045,
      "step": 167
    },
    {
      "epoch": 0.3932124049151551,
      "grad_norm": 0.8968319892883301,
      "learning_rate": 0.00011803278688524588,
      "loss": 0.3951,
      "step": 168
    },
    {
      "epoch": 0.39555295494441195,
      "grad_norm": 0.7595136761665344,
      "learning_rate": 0.00011873536299765806,
      "loss": 0.3648,
      "step": 169
    },
    {
      "epoch": 0.3978935049736688,
      "grad_norm": 1.0067402124404907,
      "learning_rate": 0.00011943793911007024,
      "loss": 0.3423,
      "step": 170
    },
    {
      "epoch": 0.4002340550029257,
      "grad_norm": 0.9176244735717773,
      "learning_rate": 0.00012014051522248242,
      "loss": 0.3841,
      "step": 171
    },
    {
      "epoch": 0.40257460503218256,
      "grad_norm": 1.1742318868637085,
      "learning_rate": 0.0001208430913348946,
      "loss": 0.3835,
      "step": 172
    },
    {
      "epoch": 0.40491515506143944,
      "grad_norm": 0.9005343914031982,
      "learning_rate": 0.00012154566744730678,
      "loss": 0.4138,
      "step": 173
    },
    {
      "epoch": 0.4072557050906963,
      "grad_norm": 1.1643309593200684,
      "learning_rate": 0.00012224824355971896,
      "loss": 0.3755,
      "step": 174
    },
    {
      "epoch": 0.4095962551199532,
      "grad_norm": 1.1504607200622559,
      "learning_rate": 0.00012295081967213115,
      "loss": 0.4531,
      "step": 175
    },
    {
      "epoch": 0.41193680514921005,
      "grad_norm": 1.005748987197876,
      "learning_rate": 0.0001236533957845433,
      "loss": 0.4799,
      "step": 176
    },
    {
      "epoch": 0.4142773551784669,
      "grad_norm": 0.9053893089294434,
      "learning_rate": 0.0001243559718969555,
      "loss": 0.3186,
      "step": 177
    },
    {
      "epoch": 0.4166179052077238,
      "grad_norm": 0.9120492339134216,
      "learning_rate": 0.00012505854800936767,
      "loss": 0.3354,
      "step": 178
    },
    {
      "epoch": 0.41895845523698066,
      "grad_norm": 0.6608469486236572,
      "learning_rate": 0.00012576112412177986,
      "loss": 0.3457,
      "step": 179
    },
    {
      "epoch": 0.4212990052662376,
      "grad_norm": 0.754196047782898,
      "learning_rate": 0.00012646370023419203,
      "loss": 0.3938,
      "step": 180
    },
    {
      "epoch": 0.42363955529549446,
      "grad_norm": 0.7131773233413696,
      "learning_rate": 0.00012716627634660422,
      "loss": 0.324,
      "step": 181
    },
    {
      "epoch": 0.42598010532475133,
      "grad_norm": 0.7748252153396606,
      "learning_rate": 0.00012786885245901638,
      "loss": 0.3367,
      "step": 182
    },
    {
      "epoch": 0.4283206553540082,
      "grad_norm": 0.8880994915962219,
      "learning_rate": 0.00012857142857142855,
      "loss": 0.3869,
      "step": 183
    },
    {
      "epoch": 0.4306612053832651,
      "grad_norm": 0.689612090587616,
      "learning_rate": 0.00012927400468384074,
      "loss": 0.3051,
      "step": 184
    },
    {
      "epoch": 0.43300175541252195,
      "grad_norm": 0.9844192266464233,
      "learning_rate": 0.0001299765807962529,
      "loss": 0.3177,
      "step": 185
    },
    {
      "epoch": 0.4353423054417788,
      "grad_norm": 0.8074104189872742,
      "learning_rate": 0.0001306791569086651,
      "loss": 0.3366,
      "step": 186
    },
    {
      "epoch": 0.4376828554710357,
      "grad_norm": 0.8782227039337158,
      "learning_rate": 0.00013138173302107726,
      "loss": 0.396,
      "step": 187
    },
    {
      "epoch": 0.44002340550029256,
      "grad_norm": 0.8334122896194458,
      "learning_rate": 0.00013208430913348945,
      "loss": 0.2879,
      "step": 188
    },
    {
      "epoch": 0.44236395552954944,
      "grad_norm": 0.7570309042930603,
      "learning_rate": 0.00013278688524590162,
      "loss": 0.2898,
      "step": 189
    },
    {
      "epoch": 0.4447045055588063,
      "grad_norm": 0.7665412425994873,
      "learning_rate": 0.0001334894613583138,
      "loss": 0.3242,
      "step": 190
    },
    {
      "epoch": 0.4470450555880632,
      "grad_norm": 0.7382681965827942,
      "learning_rate": 0.00013419203747072598,
      "loss": 0.2786,
      "step": 191
    },
    {
      "epoch": 0.44938560561732005,
      "grad_norm": 0.9432976245880127,
      "learning_rate": 0.00013489461358313817,
      "loss": 0.3458,
      "step": 192
    },
    {
      "epoch": 0.4517261556465769,
      "grad_norm": 0.7787358164787292,
      "learning_rate": 0.00013559718969555033,
      "loss": 0.331,
      "step": 193
    },
    {
      "epoch": 0.45406670567583385,
      "grad_norm": 0.6562229990959167,
      "learning_rate": 0.00013629976580796253,
      "loss": 0.2997,
      "step": 194
    },
    {
      "epoch": 0.4564072557050907,
      "grad_norm": 0.7338793873786926,
      "learning_rate": 0.0001370023419203747,
      "loss": 0.3462,
      "step": 195
    },
    {
      "epoch": 0.4587478057343476,
      "grad_norm": 1.0394604206085205,
      "learning_rate": 0.00013770491803278688,
      "loss": 0.4397,
      "step": 196
    },
    {
      "epoch": 0.46108835576360446,
      "grad_norm": 0.5967330932617188,
      "learning_rate": 0.00013840749414519905,
      "loss": 0.3607,
      "step": 197
    },
    {
      "epoch": 0.46342890579286133,
      "grad_norm": 0.812515914440155,
      "learning_rate": 0.00013911007025761124,
      "loss": 0.3479,
      "step": 198
    },
    {
      "epoch": 0.4657694558221182,
      "grad_norm": 0.8287981748580933,
      "learning_rate": 0.0001398126463700234,
      "loss": 0.3575,
      "step": 199
    },
    {
      "epoch": 0.4681100058513751,
      "grad_norm": 0.9529695510864258,
      "learning_rate": 0.0001405152224824356,
      "loss": 0.3268,
      "step": 200
    },
    {
      "epoch": 0.47045055588063195,
      "grad_norm": 0.790066659450531,
      "learning_rate": 0.00014121779859484776,
      "loss": 0.3574,
      "step": 201
    },
    {
      "epoch": 0.4727911059098888,
      "grad_norm": 0.931782066822052,
      "learning_rate": 0.00014192037470725995,
      "loss": 0.3071,
      "step": 202
    },
    {
      "epoch": 0.4751316559391457,
      "grad_norm": 0.9707749485969543,
      "learning_rate": 0.00014262295081967212,
      "loss": 0.309,
      "step": 203
    },
    {
      "epoch": 0.47747220596840256,
      "grad_norm": 1.1802873611450195,
      "learning_rate": 0.00014332552693208428,
      "loss": 0.4508,
      "step": 204
    },
    {
      "epoch": 0.47981275599765943,
      "grad_norm": 0.7642672657966614,
      "learning_rate": 0.00014402810304449648,
      "loss": 0.3486,
      "step": 205
    },
    {
      "epoch": 0.4821533060269163,
      "grad_norm": 0.870254397392273,
      "learning_rate": 0.00014473067915690864,
      "loss": 0.2927,
      "step": 206
    },
    {
      "epoch": 0.4844938560561732,
      "grad_norm": 0.8024106025695801,
      "learning_rate": 0.00014543325526932083,
      "loss": 0.3216,
      "step": 207
    },
    {
      "epoch": 0.48683440608543005,
      "grad_norm": 0.6712639331817627,
      "learning_rate": 0.000146135831381733,
      "loss": 0.2695,
      "step": 208
    },
    {
      "epoch": 0.489174956114687,
      "grad_norm": 0.8002272248268127,
      "learning_rate": 0.0001468384074941452,
      "loss": 0.3039,
      "step": 209
    },
    {
      "epoch": 0.49151550614394385,
      "grad_norm": 0.8009308576583862,
      "learning_rate": 0.00014754098360655736,
      "loss": 0.2763,
      "step": 210
    },
    {
      "epoch": 0.4938560561732007,
      "grad_norm": 0.6622499227523804,
      "learning_rate": 0.00014824355971896955,
      "loss": 0.2505,
      "step": 211
    },
    {
      "epoch": 0.4961966062024576,
      "grad_norm": 1.0322037935256958,
      "learning_rate": 0.0001489461358313817,
      "loss": 0.2765,
      "step": 212
    },
    {
      "epoch": 0.49853715623171446,
      "grad_norm": 1.13362717628479,
      "learning_rate": 0.0001496487119437939,
      "loss": 0.3491,
      "step": 213
    },
    {
      "epoch": 0.5008777062609713,
      "grad_norm": 1.1269359588623047,
      "learning_rate": 0.00015035128805620607,
      "loss": 0.4358,
      "step": 214
    },
    {
      "epoch": 0.5032182562902282,
      "grad_norm": 0.8983644247055054,
      "learning_rate": 0.00015105386416861826,
      "loss": 0.2513,
      "step": 215
    },
    {
      "epoch": 0.5055588063194851,
      "grad_norm": 0.8077988624572754,
      "learning_rate": 0.00015175644028103043,
      "loss": 0.2503,
      "step": 216
    },
    {
      "epoch": 0.507899356348742,
      "grad_norm": 0.7783879041671753,
      "learning_rate": 0.00015245901639344262,
      "loss": 0.3342,
      "step": 217
    },
    {
      "epoch": 0.5102399063779989,
      "grad_norm": 0.6813510060310364,
      "learning_rate": 0.00015316159250585478,
      "loss": 0.2558,
      "step": 218
    },
    {
      "epoch": 0.5125804564072557,
      "grad_norm": 0.9250956177711487,
      "learning_rate": 0.00015386416861826698,
      "loss": 0.4221,
      "step": 219
    },
    {
      "epoch": 0.5149210064365126,
      "grad_norm": 1.049975872039795,
      "learning_rate": 0.00015456674473067914,
      "loss": 0.4224,
      "step": 220
    },
    {
      "epoch": 0.5172615564657694,
      "grad_norm": 0.8974314332008362,
      "learning_rate": 0.00015526932084309133,
      "loss": 0.3143,
      "step": 221
    },
    {
      "epoch": 0.5196021064950264,
      "grad_norm": 1.0100222826004028,
      "learning_rate": 0.0001559718969555035,
      "loss": 0.4167,
      "step": 222
    },
    {
      "epoch": 0.5219426565242832,
      "grad_norm": 0.8053030967712402,
      "learning_rate": 0.0001566744730679157,
      "loss": 0.2989,
      "step": 223
    },
    {
      "epoch": 0.5242832065535401,
      "grad_norm": 0.8120437264442444,
      "learning_rate": 0.00015737704918032785,
      "loss": 0.3261,
      "step": 224
    },
    {
      "epoch": 0.5266237565827969,
      "grad_norm": 0.7552171945571899,
      "learning_rate": 0.00015807962529274005,
      "loss": 0.3116,
      "step": 225
    },
    {
      "epoch": 0.5289643066120538,
      "grad_norm": 0.8436841368675232,
      "learning_rate": 0.0001587822014051522,
      "loss": 0.4024,
      "step": 226
    },
    {
      "epoch": 0.5313048566413107,
      "grad_norm": 1.274009108543396,
      "learning_rate": 0.0001594847775175644,
      "loss": 0.3696,
      "step": 227
    },
    {
      "epoch": 0.5336454066705676,
      "grad_norm": 1.118312954902649,
      "learning_rate": 0.00016018735362997657,
      "loss": 0.3243,
      "step": 228
    },
    {
      "epoch": 0.5359859566998244,
      "grad_norm": 0.7144562602043152,
      "learning_rate": 0.00016088992974238876,
      "loss": 0.3434,
      "step": 229
    },
    {
      "epoch": 0.5383265067290813,
      "grad_norm": 0.6685822010040283,
      "learning_rate": 0.00016159250585480093,
      "loss": 0.2745,
      "step": 230
    },
    {
      "epoch": 0.5406670567583383,
      "grad_norm": 0.8116910457611084,
      "learning_rate": 0.00016229508196721312,
      "loss": 0.2025,
      "step": 231
    },
    {
      "epoch": 0.5430076067875951,
      "grad_norm": 0.7808611989021301,
      "learning_rate": 0.00016299765807962528,
      "loss": 0.2594,
      "step": 232
    },
    {
      "epoch": 0.545348156816852,
      "grad_norm": 0.7717172503471375,
      "learning_rate": 0.00016370023419203747,
      "loss": 0.2612,
      "step": 233
    },
    {
      "epoch": 0.5476887068461088,
      "grad_norm": 0.9180221557617188,
      "learning_rate": 0.00016440281030444964,
      "loss": 0.3182,
      "step": 234
    },
    {
      "epoch": 0.5500292568753657,
      "grad_norm": 0.8899121880531311,
      "learning_rate": 0.0001651053864168618,
      "loss": 0.3108,
      "step": 235
    },
    {
      "epoch": 0.5523698069046226,
      "grad_norm": 0.9751710295677185,
      "learning_rate": 0.00016580796252927397,
      "loss": 0.3497,
      "step": 236
    },
    {
      "epoch": 0.5547103569338795,
      "grad_norm": 0.8485270738601685,
      "learning_rate": 0.00016651053864168616,
      "loss": 0.2886,
      "step": 237
    },
    {
      "epoch": 0.5570509069631363,
      "grad_norm": 1.0112067461013794,
      "learning_rate": 0.00016721311475409833,
      "loss": 0.4076,
      "step": 238
    },
    {
      "epoch": 0.5593914569923932,
      "grad_norm": 0.8735426664352417,
      "learning_rate": 0.00016791569086651052,
      "loss": 0.2724,
      "step": 239
    },
    {
      "epoch": 0.56173200702165,
      "grad_norm": 0.8048049211502075,
      "learning_rate": 0.00016861826697892268,
      "loss": 0.3325,
      "step": 240
    },
    {
      "epoch": 0.564072557050907,
      "grad_norm": 0.8083825707435608,
      "learning_rate": 0.00016932084309133488,
      "loss": 0.3276,
      "step": 241
    },
    {
      "epoch": 0.5664131070801638,
      "grad_norm": 0.9437170028686523,
      "learning_rate": 0.00017002341920374704,
      "loss": 0.2948,
      "step": 242
    },
    {
      "epoch": 0.5687536571094207,
      "grad_norm": 0.8170874714851379,
      "learning_rate": 0.00017072599531615923,
      "loss": 0.2872,
      "step": 243
    },
    {
      "epoch": 0.5710942071386775,
      "grad_norm": 0.8284502029418945,
      "learning_rate": 0.0001714285714285714,
      "loss": 0.3277,
      "step": 244
    },
    {
      "epoch": 0.5734347571679345,
      "grad_norm": 0.9857319593429565,
      "learning_rate": 0.0001721311475409836,
      "loss": 0.3491,
      "step": 245
    },
    {
      "epoch": 0.5757753071971914,
      "grad_norm": 1.1376831531524658,
      "learning_rate": 0.00017283372365339576,
      "loss": 0.3595,
      "step": 246
    },
    {
      "epoch": 0.5781158572264482,
      "grad_norm": 0.8215055465698242,
      "learning_rate": 0.00017353629976580795,
      "loss": 0.2187,
      "step": 247
    },
    {
      "epoch": 0.5804564072557051,
      "grad_norm": 0.8776955604553223,
      "learning_rate": 0.0001742388758782201,
      "loss": 0.3252,
      "step": 248
    },
    {
      "epoch": 0.582796957284962,
      "grad_norm": 1.0241743326187134,
      "learning_rate": 0.0001749414519906323,
      "loss": 0.3919,
      "step": 249
    },
    {
      "epoch": 0.5851375073142189,
      "grad_norm": 0.8148260116577148,
      "learning_rate": 0.00017564402810304447,
      "loss": 0.2867,
      "step": 250
    },
    {
      "epoch": 0.5874780573434757,
      "grad_norm": 0.7936868667602539,
      "learning_rate": 0.00017634660421545666,
      "loss": 0.2148,
      "step": 251
    },
    {
      "epoch": 0.5898186073727326,
      "grad_norm": 0.9350371360778809,
      "learning_rate": 0.00017704918032786883,
      "loss": 0.3372,
      "step": 252
    },
    {
      "epoch": 0.5921591574019894,
      "grad_norm": 0.8334981203079224,
      "learning_rate": 0.00017775175644028102,
      "loss": 0.2583,
      "step": 253
    },
    {
      "epoch": 0.5944997074312464,
      "grad_norm": 0.9310334920883179,
      "learning_rate": 0.00017845433255269318,
      "loss": 0.3705,
      "step": 254
    },
    {
      "epoch": 0.5968402574605032,
      "grad_norm": 0.8228564262390137,
      "learning_rate": 0.00017915690866510538,
      "loss": 0.2061,
      "step": 255
    },
    {
      "epoch": 0.5991808074897601,
      "grad_norm": 0.992598295211792,
      "learning_rate": 0.00017985948477751754,
      "loss": 0.3534,
      "step": 256
    },
    {
      "epoch": 0.6015213575190169,
      "grad_norm": 0.6125496029853821,
      "learning_rate": 0.00018056206088992973,
      "loss": 0.2296,
      "step": 257
    },
    {
      "epoch": 0.6038619075482738,
      "grad_norm": 0.7566415667533875,
      "learning_rate": 0.0001812646370023419,
      "loss": 0.2631,
      "step": 258
    },
    {
      "epoch": 0.6062024575775308,
      "grad_norm": 0.812052845954895,
      "learning_rate": 0.0001819672131147541,
      "loss": 0.2717,
      "step": 259
    },
    {
      "epoch": 0.6085430076067876,
      "grad_norm": 0.9575058817863464,
      "learning_rate": 0.00018266978922716625,
      "loss": 0.3718,
      "step": 260
    },
    {
      "epoch": 0.6108835576360445,
      "grad_norm": 1.0426186323165894,
      "learning_rate": 0.00018337236533957845,
      "loss": 0.3479,
      "step": 261
    },
    {
      "epoch": 0.6132241076653013,
      "grad_norm": 1.1214020252227783,
      "learning_rate": 0.0001840749414519906,
      "loss": 0.3813,
      "step": 262
    },
    {
      "epoch": 0.6155646576945583,
      "grad_norm": 0.802934467792511,
      "learning_rate": 0.0001847775175644028,
      "loss": 0.2671,
      "step": 263
    },
    {
      "epoch": 0.6179052077238151,
      "grad_norm": 0.818509042263031,
      "learning_rate": 0.00018548009367681497,
      "loss": 0.2731,
      "step": 264
    },
    {
      "epoch": 0.620245757753072,
      "grad_norm": 0.9830486178398132,
      "learning_rate": 0.00018618266978922716,
      "loss": 0.3369,
      "step": 265
    },
    {
      "epoch": 0.6225863077823288,
      "grad_norm": 0.8779183030128479,
      "learning_rate": 0.00018688524590163933,
      "loss": 0.2826,
      "step": 266
    },
    {
      "epoch": 0.6249268578115857,
      "grad_norm": 0.8914856314659119,
      "learning_rate": 0.00018758782201405152,
      "loss": 0.26,
      "step": 267
    },
    {
      "epoch": 0.6272674078408426,
      "grad_norm": 0.7880075573921204,
      "learning_rate": 0.00018829039812646368,
      "loss": 0.266,
      "step": 268
    },
    {
      "epoch": 0.6296079578700995,
      "grad_norm": 0.9338579177856445,
      "learning_rate": 0.00018899297423887587,
      "loss": 0.2571,
      "step": 269
    },
    {
      "epoch": 0.6319485078993563,
      "grad_norm": 0.7965431213378906,
      "learning_rate": 0.00018969555035128804,
      "loss": 0.305,
      "step": 270
    },
    {
      "epoch": 0.6342890579286132,
      "grad_norm": 1.3529499769210815,
      "learning_rate": 0.00019039812646370023,
      "loss": 0.3283,
      "step": 271
    },
    {
      "epoch": 0.63662960795787,
      "grad_norm": 0.868913471698761,
      "learning_rate": 0.0001911007025761124,
      "loss": 0.2906,
      "step": 272
    },
    {
      "epoch": 0.638970157987127,
      "grad_norm": 0.8220526576042175,
      "learning_rate": 0.0001918032786885246,
      "loss": 0.2879,
      "step": 273
    },
    {
      "epoch": 0.6413107080163839,
      "grad_norm": 0.5826690793037415,
      "learning_rate": 0.00019250585480093675,
      "loss": 0.155,
      "step": 274
    },
    {
      "epoch": 0.6436512580456407,
      "grad_norm": 0.6483204960823059,
      "learning_rate": 0.00019320843091334895,
      "loss": 0.2506,
      "step": 275
    },
    {
      "epoch": 0.6459918080748976,
      "grad_norm": 0.8303985595703125,
      "learning_rate": 0.0001939110070257611,
      "loss": 0.3384,
      "step": 276
    },
    {
      "epoch": 0.6483323581041545,
      "grad_norm": 0.8473712801933289,
      "learning_rate": 0.0001946135831381733,
      "loss": 0.1944,
      "step": 277
    },
    {
      "epoch": 0.6506729081334114,
      "grad_norm": 0.7447115778923035,
      "learning_rate": 0.00019531615925058544,
      "loss": 0.2855,
      "step": 278
    },
    {
      "epoch": 0.6530134581626682,
      "grad_norm": 0.5787239074707031,
      "learning_rate": 0.00019601873536299763,
      "loss": 0.2239,
      "step": 279
    },
    {
      "epoch": 0.6553540081919251,
      "grad_norm": 0.8331506252288818,
      "learning_rate": 0.0001967213114754098,
      "loss": 0.2896,
      "step": 280
    },
    {
      "epoch": 0.657694558221182,
      "grad_norm": 0.7897618412971497,
      "learning_rate": 0.000197423887587822,
      "loss": 0.3045,
      "step": 281
    },
    {
      "epoch": 0.6600351082504389,
      "grad_norm": 0.8021056056022644,
      "learning_rate": 0.00019812646370023416,
      "loss": 0.3166,
      "step": 282
    },
    {
      "epoch": 0.6623756582796957,
      "grad_norm": 0.8148608207702637,
      "learning_rate": 0.00019882903981264635,
      "loss": 0.3041,
      "step": 283
    },
    {
      "epoch": 0.6647162083089526,
      "grad_norm": 0.7896011471748352,
      "learning_rate": 0.0001995316159250585,
      "loss": 0.3452,
      "step": 284
    },
    {
      "epoch": 0.6670567583382094,
      "grad_norm": 0.815863311290741,
      "learning_rate": 0.0002002341920374707,
      "loss": 0.3195,
      "step": 285
    },
    {
      "epoch": 0.6693973083674664,
      "grad_norm": 0.7610185146331787,
      "learning_rate": 0.00020093676814988287,
      "loss": 0.2967,
      "step": 286
    },
    {
      "epoch": 0.6717378583967232,
      "grad_norm": 0.7069740295410156,
      "learning_rate": 0.00020163934426229506,
      "loss": 0.3195,
      "step": 287
    },
    {
      "epoch": 0.6740784084259801,
      "grad_norm": 0.748857319355011,
      "learning_rate": 0.00020234192037470723,
      "loss": 0.2677,
      "step": 288
    },
    {
      "epoch": 0.676418958455237,
      "grad_norm": 0.8485399484634399,
      "learning_rate": 0.00020304449648711942,
      "loss": 0.3026,
      "step": 289
    },
    {
      "epoch": 0.6787595084844938,
      "grad_norm": 0.7038848996162415,
      "learning_rate": 0.00020374707259953158,
      "loss": 0.3477,
      "step": 290
    },
    {
      "epoch": 0.6811000585137508,
      "grad_norm": 0.7077156901359558,
      "learning_rate": 0.00020444964871194378,
      "loss": 0.2118,
      "step": 291
    },
    {
      "epoch": 0.6834406085430076,
      "grad_norm": 0.6178346276283264,
      "learning_rate": 0.00020515222482435594,
      "loss": 0.2732,
      "step": 292
    },
    {
      "epoch": 0.6857811585722645,
      "grad_norm": 0.8396615982055664,
      "learning_rate": 0.00020585480093676813,
      "loss": 0.2746,
      "step": 293
    },
    {
      "epoch": 0.6881217086015213,
      "grad_norm": 1.079005479812622,
      "learning_rate": 0.0002065573770491803,
      "loss": 0.3688,
      "step": 294
    },
    {
      "epoch": 0.6904622586307783,
      "grad_norm": 1.0508195161819458,
      "learning_rate": 0.0002072599531615925,
      "loss": 0.3486,
      "step": 295
    },
    {
      "epoch": 0.6928028086600351,
      "grad_norm": 1.0372776985168457,
      "learning_rate": 0.00020796252927400465,
      "loss": 0.3225,
      "step": 296
    },
    {
      "epoch": 0.695143358689292,
      "grad_norm": 0.8074139952659607,
      "learning_rate": 0.00020866510538641685,
      "loss": 0.2688,
      "step": 297
    },
    {
      "epoch": 0.6974839087185488,
      "grad_norm": 1.2325739860534668,
      "learning_rate": 0.000209367681498829,
      "loss": 0.3313,
      "step": 298
    },
    {
      "epoch": 0.6998244587478057,
      "grad_norm": 1.0929648876190186,
      "learning_rate": 0.0002100702576112412,
      "loss": 0.2843,
      "step": 299
    },
    {
      "epoch": 0.7021650087770626,
      "grad_norm": 0.6566285490989685,
      "learning_rate": 0.00021077283372365337,
      "loss": 0.2697,
      "step": 300
    },
    {
      "epoch": 0.7045055588063195,
      "grad_norm": 0.9044969081878662,
      "learning_rate": 0.00021147540983606556,
      "loss": 0.3291,
      "step": 301
    },
    {
      "epoch": 0.7068461088355764,
      "grad_norm": 0.8426467180252075,
      "learning_rate": 0.00021217798594847773,
      "loss": 0.2457,
      "step": 302
    },
    {
      "epoch": 0.7091866588648332,
      "grad_norm": 0.8207939863204956,
      "learning_rate": 0.00021288056206088992,
      "loss": 0.2835,
      "step": 303
    },
    {
      "epoch": 0.7115272088940902,
      "grad_norm": 0.9453349709510803,
      "learning_rate": 0.00021358313817330208,
      "loss": 0.4088,
      "step": 304
    },
    {
      "epoch": 0.713867758923347,
      "grad_norm": 0.7949888110160828,
      "learning_rate": 0.00021428571428571427,
      "loss": 0.2713,
      "step": 305
    },
    {
      "epoch": 0.7162083089526039,
      "grad_norm": 0.6692084670066833,
      "learning_rate": 0.00021498829039812644,
      "loss": 0.276,
      "step": 306
    },
    {
      "epoch": 0.7185488589818607,
      "grad_norm": 0.7563417553901672,
      "learning_rate": 0.00021569086651053863,
      "loss": 0.2624,
      "step": 307
    },
    {
      "epoch": 0.7208894090111176,
      "grad_norm": 0.666782021522522,
      "learning_rate": 0.0002163934426229508,
      "loss": 0.2577,
      "step": 308
    },
    {
      "epoch": 0.7232299590403745,
      "grad_norm": 0.9448810815811157,
      "learning_rate": 0.000217096018735363,
      "loss": 0.3172,
      "step": 309
    },
    {
      "epoch": 0.7255705090696314,
      "grad_norm": 0.8934730291366577,
      "learning_rate": 0.00021779859484777515,
      "loss": 0.2329,
      "step": 310
    },
    {
      "epoch": 0.7279110590988882,
      "grad_norm": 0.8392424583435059,
      "learning_rate": 0.00021850117096018735,
      "loss": 0.2231,
      "step": 311
    },
    {
      "epoch": 0.7302516091281451,
      "grad_norm": 0.9634220004081726,
      "learning_rate": 0.0002192037470725995,
      "loss": 0.2824,
      "step": 312
    },
    {
      "epoch": 0.732592159157402,
      "grad_norm": 1.0798089504241943,
      "learning_rate": 0.0002199063231850117,
      "loss": 0.3116,
      "step": 313
    },
    {
      "epoch": 0.7349327091866589,
      "grad_norm": 0.9124124050140381,
      "learning_rate": 0.00022060889929742387,
      "loss": 0.2576,
      "step": 314
    },
    {
      "epoch": 0.7372732592159157,
      "grad_norm": 0.923702597618103,
      "learning_rate": 0.00022131147540983606,
      "loss": 0.3184,
      "step": 315
    },
    {
      "epoch": 0.7396138092451726,
      "grad_norm": 0.8832808136940002,
      "learning_rate": 0.00022201405152224822,
      "loss": 0.2952,
      "step": 316
    },
    {
      "epoch": 0.7419543592744295,
      "grad_norm": 0.716823399066925,
      "learning_rate": 0.00022271662763466042,
      "loss": 0.2624,
      "step": 317
    },
    {
      "epoch": 0.7442949093036864,
      "grad_norm": 1.0510703325271606,
      "learning_rate": 0.00022341920374707258,
      "loss": 0.3256,
      "step": 318
    },
    {
      "epoch": 0.7466354593329433,
      "grad_norm": 0.7439059019088745,
      "learning_rate": 0.00022412177985948477,
      "loss": 0.2816,
      "step": 319
    },
    {
      "epoch": 0.7489760093622001,
      "grad_norm": 0.6908542513847351,
      "learning_rate": 0.00022482435597189694,
      "loss": 0.2342,
      "step": 320
    },
    {
      "epoch": 0.751316559391457,
      "grad_norm": 0.7447423338890076,
      "learning_rate": 0.0002255269320843091,
      "loss": 0.2338,
      "step": 321
    },
    {
      "epoch": 0.7536571094207138,
      "grad_norm": 0.8214778900146484,
      "learning_rate": 0.00022622950819672127,
      "loss": 0.2781,
      "step": 322
    },
    {
      "epoch": 0.7559976594499708,
      "grad_norm": 0.9183831214904785,
      "learning_rate": 0.00022693208430913346,
      "loss": 0.3102,
      "step": 323
    },
    {
      "epoch": 0.7583382094792276,
      "grad_norm": 0.9467938542366028,
      "learning_rate": 0.00022763466042154563,
      "loss": 0.3455,
      "step": 324
    },
    {
      "epoch": 0.7606787595084845,
      "grad_norm": 0.8338228464126587,
      "learning_rate": 0.00022833723653395782,
      "loss": 0.3078,
      "step": 325
    },
    {
      "epoch": 0.7630193095377413,
      "grad_norm": 0.6009271144866943,
      "learning_rate": 0.00022903981264636998,
      "loss": 0.2088,
      "step": 326
    },
    {
      "epoch": 0.7653598595669983,
      "grad_norm": 0.8621292114257812,
      "learning_rate": 0.00022974238875878218,
      "loss": 0.2794,
      "step": 327
    },
    {
      "epoch": 0.7677004095962551,
      "grad_norm": 0.717658519744873,
      "learning_rate": 0.00023044496487119434,
      "loss": 0.2684,
      "step": 328
    },
    {
      "epoch": 0.770040959625512,
      "grad_norm": 0.7445611953735352,
      "learning_rate": 0.00023114754098360653,
      "loss": 0.2485,
      "step": 329
    },
    {
      "epoch": 0.7723815096547688,
      "grad_norm": 0.8459638357162476,
      "learning_rate": 0.0002318501170960187,
      "loss": 0.2503,
      "step": 330
    },
    {
      "epoch": 0.7747220596840257,
      "grad_norm": 0.9163710474967957,
      "learning_rate": 0.0002325526932084309,
      "loss": 0.2819,
      "step": 331
    },
    {
      "epoch": 0.7770626097132827,
      "grad_norm": 0.850212574005127,
      "learning_rate": 0.00023325526932084305,
      "loss": 0.2684,
      "step": 332
    },
    {
      "epoch": 0.7794031597425395,
      "grad_norm": 1.0362321138381958,
      "learning_rate": 0.00023395784543325525,
      "loss": 0.3188,
      "step": 333
    },
    {
      "epoch": 0.7817437097717964,
      "grad_norm": 0.8670958876609802,
      "learning_rate": 0.0002346604215456674,
      "loss": 0.3496,
      "step": 334
    },
    {
      "epoch": 0.7840842598010532,
      "grad_norm": 0.7150039076805115,
      "learning_rate": 0.0002353629976580796,
      "loss": 0.2337,
      "step": 335
    },
    {
      "epoch": 0.7864248098303102,
      "grad_norm": 1.020265817642212,
      "learning_rate": 0.00023606557377049177,
      "loss": 0.3825,
      "step": 336
    },
    {
      "epoch": 0.788765359859567,
      "grad_norm": 0.8281091451644897,
      "learning_rate": 0.00023676814988290396,
      "loss": 0.346,
      "step": 337
    },
    {
      "epoch": 0.7911059098888239,
      "grad_norm": 0.7513360381126404,
      "learning_rate": 0.00023747072599531613,
      "loss": 0.3105,
      "step": 338
    },
    {
      "epoch": 0.7934464599180807,
      "grad_norm": 0.9792047739028931,
      "learning_rate": 0.00023817330210772832,
      "loss": 0.2626,
      "step": 339
    },
    {
      "epoch": 0.7957870099473376,
      "grad_norm": 0.7730432748794556,
      "learning_rate": 0.00023887587822014048,
      "loss": 0.2307,
      "step": 340
    },
    {
      "epoch": 0.7981275599765945,
      "grad_norm": 0.7367051839828491,
      "learning_rate": 0.00023957845433255267,
      "loss": 0.2741,
      "step": 341
    },
    {
      "epoch": 0.8004681100058514,
      "grad_norm": 0.726537823677063,
      "learning_rate": 0.00024028103044496484,
      "loss": 0.2936,
      "step": 342
    },
    {
      "epoch": 0.8028086600351082,
      "grad_norm": 0.8798702359199524,
      "learning_rate": 0.00024098360655737703,
      "loss": 0.226,
      "step": 343
    },
    {
      "epoch": 0.8051492100643651,
      "grad_norm": 0.8926436901092529,
      "learning_rate": 0.0002416861826697892,
      "loss": 0.2779,
      "step": 344
    },
    {
      "epoch": 0.807489760093622,
      "grad_norm": 0.9528531432151794,
      "learning_rate": 0.0002423887587822014,
      "loss": 0.2684,
      "step": 345
    },
    {
      "epoch": 0.8098303101228789,
      "grad_norm": 1.1053122282028198,
      "learning_rate": 0.00024309133489461355,
      "loss": 0.3224,
      "step": 346
    },
    {
      "epoch": 0.8121708601521358,
      "grad_norm": 0.8521222472190857,
      "learning_rate": 0.00024379391100702575,
      "loss": 0.2274,
      "step": 347
    },
    {
      "epoch": 0.8145114101813926,
      "grad_norm": 0.9013952016830444,
      "learning_rate": 0.0002444964871194379,
      "loss": 0.271,
      "step": 348
    },
    {
      "epoch": 0.8168519602106495,
      "grad_norm": 0.904913067817688,
      "learning_rate": 0.0002451990632318501,
      "loss": 0.2747,
      "step": 349
    },
    {
      "epoch": 0.8191925102399064,
      "grad_norm": 0.8059370517730713,
      "learning_rate": 0.0002459016393442623,
      "loss": 0.2872,
      "step": 350
    },
    {
      "epoch": 0.8215330602691633,
      "grad_norm": 0.8024557828903198,
      "learning_rate": 0.00024660421545667446,
      "loss": 0.2851,
      "step": 351
    },
    {
      "epoch": 0.8238736102984201,
      "grad_norm": 0.7901968955993652,
      "learning_rate": 0.0002473067915690866,
      "loss": 0.2817,
      "step": 352
    },
    {
      "epoch": 0.826214160327677,
      "grad_norm": 0.8495548367500305,
      "learning_rate": 0.0002480093676814988,
      "loss": 0.3014,
      "step": 353
    },
    {
      "epoch": 0.8285547103569338,
      "grad_norm": 0.7101137042045593,
      "learning_rate": 0.000248711943793911,
      "loss": 0.3574,
      "step": 354
    },
    {
      "epoch": 0.8308952603861908,
      "grad_norm": 0.7687988877296448,
      "learning_rate": 0.0002494145199063232,
      "loss": 0.2826,
      "step": 355
    },
    {
      "epoch": 0.8332358104154476,
      "grad_norm": 0.797540545463562,
      "learning_rate": 0.00025011709601873534,
      "loss": 0.3014,
      "step": 356
    },
    {
      "epoch": 0.8355763604447045,
      "grad_norm": 0.5225883722305298,
      "learning_rate": 0.00025081967213114756,
      "loss": 0.2199,
      "step": 357
    },
    {
      "epoch": 0.8379169104739613,
      "grad_norm": 0.7238962054252625,
      "learning_rate": 0.0002515222482435597,
      "loss": 0.29,
      "step": 358
    },
    {
      "epoch": 0.8402574605032183,
      "grad_norm": 0.7726640105247498,
      "learning_rate": 0.0002522248243559719,
      "loss": 0.2769,
      "step": 359
    },
    {
      "epoch": 0.8425980105324752,
      "grad_norm": 0.6499150395393372,
      "learning_rate": 0.00025292740046838405,
      "loss": 0.2301,
      "step": 360
    },
    {
      "epoch": 0.844938560561732,
      "grad_norm": 0.6252822279930115,
      "learning_rate": 0.00025362997658079627,
      "loss": 0.1951,
      "step": 361
    },
    {
      "epoch": 0.8472791105909889,
      "grad_norm": 0.8380541801452637,
      "learning_rate": 0.00025433255269320844,
      "loss": 0.3082,
      "step": 362
    },
    {
      "epoch": 0.8496196606202457,
      "grad_norm": 0.9813821315765381,
      "learning_rate": 0.00025503512880562055,
      "loss": 0.3864,
      "step": 363
    },
    {
      "epoch": 0.8519602106495027,
      "grad_norm": 0.8308704495429993,
      "learning_rate": 0.00025573770491803277,
      "loss": 0.2559,
      "step": 364
    },
    {
      "epoch": 0.8543007606787595,
      "grad_norm": 0.7838255167007446,
      "learning_rate": 0.00025644028103044493,
      "loss": 0.2665,
      "step": 365
    },
    {
      "epoch": 0.8566413107080164,
      "grad_norm": 0.8600631356239319,
      "learning_rate": 0.0002571428571428571,
      "loss": 0.2779,
      "step": 366
    },
    {
      "epoch": 0.8589818607372732,
      "grad_norm": 0.7499675154685974,
      "learning_rate": 0.00025784543325526926,
      "loss": 0.3098,
      "step": 367
    },
    {
      "epoch": 0.8613224107665302,
      "grad_norm": 0.7447799444198608,
      "learning_rate": 0.0002585480093676815,
      "loss": 0.2698,
      "step": 368
    },
    {
      "epoch": 0.863662960795787,
      "grad_norm": 0.623314380645752,
      "learning_rate": 0.00025925058548009365,
      "loss": 0.198,
      "step": 369
    },
    {
      "epoch": 0.8660035108250439,
      "grad_norm": 0.999742329120636,
      "learning_rate": 0.0002599531615925058,
      "loss": 0.3428,
      "step": 370
    },
    {
      "epoch": 0.8683440608543007,
      "grad_norm": 0.747599184513092,
      "learning_rate": 0.000260655737704918,
      "loss": 0.2299,
      "step": 371
    },
    {
      "epoch": 0.8706846108835576,
      "grad_norm": 0.6604514718055725,
      "learning_rate": 0.0002613583138173302,
      "loss": 0.2108,
      "step": 372
    },
    {
      "epoch": 0.8730251609128145,
      "grad_norm": 0.6126849055290222,
      "learning_rate": 0.00026206088992974236,
      "loss": 0.2333,
      "step": 373
    },
    {
      "epoch": 0.8753657109420714,
      "grad_norm": 0.6385489702224731,
      "learning_rate": 0.0002627634660421545,
      "loss": 0.2395,
      "step": 374
    },
    {
      "epoch": 0.8777062609713283,
      "grad_norm": 0.8062438368797302,
      "learning_rate": 0.0002634660421545667,
      "loss": 0.2335,
      "step": 375
    },
    {
      "epoch": 0.8800468110005851,
      "grad_norm": 1.01720130443573,
      "learning_rate": 0.0002641686182669789,
      "loss": 0.287,
      "step": 376
    },
    {
      "epoch": 0.882387361029842,
      "grad_norm": 0.7702879905700684,
      "learning_rate": 0.0002648711943793911,
      "loss": 0.2718,
      "step": 377
    },
    {
      "epoch": 0.8847279110590989,
      "grad_norm": 0.6759740114212036,
      "learning_rate": 0.00026557377049180324,
      "loss": 0.2422,
      "step": 378
    },
    {
      "epoch": 0.8870684610883558,
      "grad_norm": 0.7438066601753235,
      "learning_rate": 0.0002662763466042154,
      "loss": 0.2423,
      "step": 379
    },
    {
      "epoch": 0.8894090111176126,
      "grad_norm": 0.8555342555046082,
      "learning_rate": 0.0002669789227166276,
      "loss": 0.3004,
      "step": 380
    },
    {
      "epoch": 0.8917495611468695,
      "grad_norm": 0.672016441822052,
      "learning_rate": 0.0002676814988290398,
      "loss": 0.1967,
      "step": 381
    },
    {
      "epoch": 0.8940901111761264,
      "grad_norm": 0.6702294945716858,
      "learning_rate": 0.00026838407494145195,
      "loss": 0.2421,
      "step": 382
    },
    {
      "epoch": 0.8964306612053833,
      "grad_norm": 0.5782638192176819,
      "learning_rate": 0.0002690866510538641,
      "loss": 0.1877,
      "step": 383
    },
    {
      "epoch": 0.8987712112346401,
      "grad_norm": 0.5666160583496094,
      "learning_rate": 0.00026978922716627634,
      "loss": 0.2695,
      "step": 384
    },
    {
      "epoch": 0.901111761263897,
      "grad_norm": 0.5848038196563721,
      "learning_rate": 0.0002704918032786885,
      "loss": 0.2327,
      "step": 385
    },
    {
      "epoch": 0.9034523112931538,
      "grad_norm": 0.6383506655693054,
      "learning_rate": 0.00027119437939110067,
      "loss": 0.2039,
      "step": 386
    },
    {
      "epoch": 0.9057928613224108,
      "grad_norm": 0.7257751226425171,
      "learning_rate": 0.00027189695550351283,
      "loss": 0.2847,
      "step": 387
    },
    {
      "epoch": 0.9081334113516677,
      "grad_norm": 0.6868221163749695,
      "learning_rate": 0.00027259953161592505,
      "loss": 0.2683,
      "step": 388
    },
    {
      "epoch": 0.9104739613809245,
      "grad_norm": 0.7763496041297913,
      "learning_rate": 0.0002733021077283372,
      "loss": 0.2774,
      "step": 389
    },
    {
      "epoch": 0.9128145114101814,
      "grad_norm": 0.518866777420044,
      "learning_rate": 0.0002740046838407494,
      "loss": 0.2275,
      "step": 390
    },
    {
      "epoch": 0.9151550614394383,
      "grad_norm": 0.7594648003578186,
      "learning_rate": 0.00027470725995316155,
      "loss": 0.2565,
      "step": 391
    },
    {
      "epoch": 0.9174956114686952,
      "grad_norm": 0.7469357252120972,
      "learning_rate": 0.00027540983606557377,
      "loss": 0.2003,
      "step": 392
    },
    {
      "epoch": 0.919836161497952,
      "grad_norm": 0.9138466715812683,
      "learning_rate": 0.00027611241217798593,
      "loss": 0.3228,
      "step": 393
    },
    {
      "epoch": 0.9221767115272089,
      "grad_norm": 0.703883171081543,
      "learning_rate": 0.0002768149882903981,
      "loss": 0.2024,
      "step": 394
    },
    {
      "epoch": 0.9245172615564657,
      "grad_norm": 0.5915929079055786,
      "learning_rate": 0.0002775175644028103,
      "loss": 0.1996,
      "step": 395
    },
    {
      "epoch": 0.9268578115857227,
      "grad_norm": 0.7257809042930603,
      "learning_rate": 0.0002782201405152225,
      "loss": 0.2316,
      "step": 396
    },
    {
      "epoch": 0.9291983616149795,
      "grad_norm": 0.4937823414802551,
      "learning_rate": 0.00027892271662763465,
      "loss": 0.1784,
      "step": 397
    },
    {
      "epoch": 0.9315389116442364,
      "grad_norm": 0.770487904548645,
      "learning_rate": 0.0002796252927400468,
      "loss": 0.258,
      "step": 398
    },
    {
      "epoch": 0.9338794616734932,
      "grad_norm": 0.9403736591339111,
      "learning_rate": 0.00028032786885245903,
      "loss": 0.2732,
      "step": 399
    },
    {
      "epoch": 0.9362200117027502,
      "grad_norm": 0.9174203872680664,
      "learning_rate": 0.0002810304449648712,
      "loss": 0.3708,
      "step": 400
    },
    {
      "epoch": 0.938560561732007,
      "grad_norm": 0.7126749753952026,
      "learning_rate": 0.00028173302107728336,
      "loss": 0.3056,
      "step": 401
    },
    {
      "epoch": 0.9409011117612639,
      "grad_norm": 1.404272437095642,
      "learning_rate": 0.0002824355971896955,
      "loss": 0.415,
      "step": 402
    },
    {
      "epoch": 0.9432416617905208,
      "grad_norm": 0.7033040523529053,
      "learning_rate": 0.00028313817330210774,
      "loss": 0.2627,
      "step": 403
    },
    {
      "epoch": 0.9455822118197776,
      "grad_norm": 0.7291741371154785,
      "learning_rate": 0.0002838407494145199,
      "loss": 0.2337,
      "step": 404
    },
    {
      "epoch": 0.9479227618490346,
      "grad_norm": 0.6631123423576355,
      "learning_rate": 0.0002845433255269321,
      "loss": 0.306,
      "step": 405
    },
    {
      "epoch": 0.9502633118782914,
      "grad_norm": 0.5684021711349487,
      "learning_rate": 0.00028524590163934424,
      "loss": 0.1913,
      "step": 406
    },
    {
      "epoch": 0.9526038619075483,
      "grad_norm": 0.7982754111289978,
      "learning_rate": 0.0002859484777517564,
      "loss": 0.4223,
      "step": 407
    },
    {
      "epoch": 0.9549444119368051,
      "grad_norm": 0.6310368180274963,
      "learning_rate": 0.00028665105386416857,
      "loss": 0.2186,
      "step": 408
    },
    {
      "epoch": 0.957284961966062,
      "grad_norm": 0.662272036075592,
      "learning_rate": 0.00028735362997658073,
      "loss": 0.2865,
      "step": 409
    },
    {
      "epoch": 0.9596255119953189,
      "grad_norm": 0.698244035243988,
      "learning_rate": 0.00028805620608899295,
      "loss": 0.2136,
      "step": 410
    },
    {
      "epoch": 0.9619660620245758,
      "grad_norm": 0.7368411421775818,
      "learning_rate": 0.0002887587822014051,
      "loss": 0.3028,
      "step": 411
    },
    {
      "epoch": 0.9643066120538326,
      "grad_norm": 0.8223541378974915,
      "learning_rate": 0.0002894613583138173,
      "loss": 0.3215,
      "step": 412
    },
    {
      "epoch": 0.9666471620830895,
      "grad_norm": 0.6687695980072021,
      "learning_rate": 0.00029016393442622945,
      "loss": 0.2725,
      "step": 413
    },
    {
      "epoch": 0.9689877121123464,
      "grad_norm": 0.6152300238609314,
      "learning_rate": 0.00029086651053864167,
      "loss": 0.2487,
      "step": 414
    },
    {
      "epoch": 0.9713282621416033,
      "grad_norm": 0.8728123903274536,
      "learning_rate": 0.00029156908665105383,
      "loss": 0.3293,
      "step": 415
    },
    {
      "epoch": 0.9736688121708601,
      "grad_norm": 0.7942933440208435,
      "learning_rate": 0.000292271662763466,
      "loss": 0.2762,
      "step": 416
    },
    {
      "epoch": 0.976009362200117,
      "grad_norm": 0.6914966106414795,
      "learning_rate": 0.00029297423887587816,
      "loss": 0.2384,
      "step": 417
    },
    {
      "epoch": 0.978349912229374,
      "grad_norm": 0.6247174143791199,
      "learning_rate": 0.0002936768149882904,
      "loss": 0.2343,
      "step": 418
    },
    {
      "epoch": 0.9806904622586308,
      "grad_norm": 0.7358412146568298,
      "learning_rate": 0.00029437939110070255,
      "loss": 0.2039,
      "step": 419
    },
    {
      "epoch": 0.9830310122878877,
      "grad_norm": 0.8992546200752258,
      "learning_rate": 0.0002950819672131147,
      "loss": 0.3345,
      "step": 420
    },
    {
      "epoch": 0.9853715623171445,
      "grad_norm": 0.7271407246589661,
      "learning_rate": 0.0002957845433255269,
      "loss": 0.2039,
      "step": 421
    },
    {
      "epoch": 0.9877121123464014,
      "grad_norm": 0.6563349962234497,
      "learning_rate": 0.0002964871194379391,
      "loss": 0.2082,
      "step": 422
    },
    {
      "epoch": 0.9900526623756583,
      "grad_norm": 0.7759718298912048,
      "learning_rate": 0.00029718969555035126,
      "loss": 0.2925,
      "step": 423
    },
    {
      "epoch": 0.9923932124049152,
      "grad_norm": 0.8780991435050964,
      "learning_rate": 0.0002978922716627634,
      "loss": 0.3326,
      "step": 424
    },
    {
      "epoch": 0.994733762434172,
      "grad_norm": 0.6562137603759766,
      "learning_rate": 0.0002985948477751756,
      "loss": 0.2609,
      "step": 425
    },
    {
      "epoch": 0.9970743124634289,
      "grad_norm": 0.6585081815719604,
      "learning_rate": 0.0002992974238875878,
      "loss": 0.2897,
      "step": 426
    },
    {
      "epoch": 0.9994148624926857,
      "grad_norm": 0.7281473278999329,
      "learning_rate": 0.0003,
      "loss": 0.3261,
      "step": 427
    },
    {
      "epoch": 0.9994148624926857,
      "eval_loss": 0.35606974363327026,
      "eval_runtime": 129.3498,
      "eval_samples_per_second": 4.267,
      "eval_steps_per_second": 0.533,
      "step": 427
    },
    {
      "epoch": 1.0017554125219426,
      "grad_norm": 0.5050309896469116,
      "learning_rate": 0.0002999219359875097,
      "loss": 0.2159,
      "step": 428
    },
    {
      "epoch": 1.0040959625511996,
      "grad_norm": 0.8154605031013489,
      "learning_rate": 0.0002998438719750195,
      "loss": 0.2875,
      "step": 429
    },
    {
      "epoch": 1.0064365125804564,
      "grad_norm": 0.6112671494483948,
      "learning_rate": 0.00029976580796252925,
      "loss": 0.2432,
      "step": 430
    },
    {
      "epoch": 1.0087770626097132,
      "grad_norm": 0.6362435817718506,
      "learning_rate": 0.000299687743950039,
      "loss": 0.2511,
      "step": 431
    },
    {
      "epoch": 1.0111176126389703,
      "grad_norm": 0.8063945174217224,
      "learning_rate": 0.00029960967993754875,
      "loss": 0.2664,
      "step": 432
    },
    {
      "epoch": 1.013458162668227,
      "grad_norm": 0.6813740730285645,
      "learning_rate": 0.00029953161592505853,
      "loss": 0.2801,
      "step": 433
    },
    {
      "epoch": 1.015798712697484,
      "grad_norm": 0.7944533228874207,
      "learning_rate": 0.00029945355191256825,
      "loss": 0.2899,
      "step": 434
    },
    {
      "epoch": 1.0181392627267407,
      "grad_norm": 0.8454523086547852,
      "learning_rate": 0.00029937548790007803,
      "loss": 0.2689,
      "step": 435
    },
    {
      "epoch": 1.0204798127559978,
      "grad_norm": 0.7268518209457397,
      "learning_rate": 0.0002992974238875878,
      "loss": 0.255,
      "step": 436
    },
    {
      "epoch": 1.0228203627852546,
      "grad_norm": 0.6365455389022827,
      "learning_rate": 0.00029921935987509753,
      "loss": 0.2078,
      "step": 437
    },
    {
      "epoch": 1.0251609128145114,
      "grad_norm": 0.7786256074905396,
      "learning_rate": 0.0002991412958626073,
      "loss": 0.2108,
      "step": 438
    },
    {
      "epoch": 1.0275014628437682,
      "grad_norm": 0.6109829545021057,
      "learning_rate": 0.0002990632318501171,
      "loss": 0.1809,
      "step": 439
    },
    {
      "epoch": 1.0298420128730252,
      "grad_norm": 1.0196893215179443,
      "learning_rate": 0.0002989851678376268,
      "loss": 0.2846,
      "step": 440
    },
    {
      "epoch": 1.032182562902282,
      "grad_norm": 1.0831577777862549,
      "learning_rate": 0.0002989071038251366,
      "loss": 0.2623,
      "step": 441
    },
    {
      "epoch": 1.0345231129315389,
      "grad_norm": 0.9477410316467285,
      "learning_rate": 0.00029882903981264637,
      "loss": 0.2263,
      "step": 442
    },
    {
      "epoch": 1.0368636629607957,
      "grad_norm": 0.9750642776489258,
      "learning_rate": 0.0002987509758001561,
      "loss": 0.2981,
      "step": 443
    },
    {
      "epoch": 1.0392042129900527,
      "grad_norm": 0.7206982970237732,
      "learning_rate": 0.00029867291178766587,
      "loss": 0.2561,
      "step": 444
    },
    {
      "epoch": 1.0415447630193095,
      "grad_norm": 0.7715057134628296,
      "learning_rate": 0.0002985948477751756,
      "loss": 0.2968,
      "step": 445
    },
    {
      "epoch": 1.0438853130485664,
      "grad_norm": 0.8223751187324524,
      "learning_rate": 0.00029851678376268537,
      "loss": 0.2672,
      "step": 446
    },
    {
      "epoch": 1.0462258630778234,
      "grad_norm": 0.6764557957649231,
      "learning_rate": 0.00029843871975019514,
      "loss": 0.2005,
      "step": 447
    },
    {
      "epoch": 1.0485664131070802,
      "grad_norm": 0.8856650590896606,
      "learning_rate": 0.00029836065573770487,
      "loss": 0.3006,
      "step": 448
    },
    {
      "epoch": 1.050906963136337,
      "grad_norm": 0.973075270652771,
      "learning_rate": 0.00029828259172521465,
      "loss": 0.2881,
      "step": 449
    },
    {
      "epoch": 1.0532475131655938,
      "grad_norm": 0.5626999139785767,
      "learning_rate": 0.0002982045277127244,
      "loss": 0.2303,
      "step": 450
    },
    {
      "epoch": 1.0555880631948509,
      "grad_norm": 0.5795462131500244,
      "learning_rate": 0.00029812646370023415,
      "loss": 0.16,
      "step": 451
    },
    {
      "epoch": 1.0579286132241077,
      "grad_norm": 0.9316964149475098,
      "learning_rate": 0.0002980483996877439,
      "loss": 0.2948,
      "step": 452
    },
    {
      "epoch": 1.0602691632533645,
      "grad_norm": 0.6391876339912415,
      "learning_rate": 0.0002979703356752537,
      "loss": 0.2311,
      "step": 453
    },
    {
      "epoch": 1.0626097132826213,
      "grad_norm": 0.7908627390861511,
      "learning_rate": 0.0002978922716627634,
      "loss": 0.2626,
      "step": 454
    },
    {
      "epoch": 1.0649502633118784,
      "grad_norm": 0.9043421745300293,
      "learning_rate": 0.0002978142076502732,
      "loss": 0.2187,
      "step": 455
    },
    {
      "epoch": 1.0672908133411352,
      "grad_norm": 0.6904999613761902,
      "learning_rate": 0.000297736143637783,
      "loss": 0.2216,
      "step": 456
    },
    {
      "epoch": 1.069631363370392,
      "grad_norm": 0.7768654227256775,
      "learning_rate": 0.0002976580796252927,
      "loss": 0.2485,
      "step": 457
    },
    {
      "epoch": 1.0719719133996488,
      "grad_norm": 0.651447057723999,
      "learning_rate": 0.0002975800156128025,
      "loss": 0.2646,
      "step": 458
    },
    {
      "epoch": 1.0743124634289059,
      "grad_norm": 0.6048499345779419,
      "learning_rate": 0.00029750195160031226,
      "loss": 0.2227,
      "step": 459
    },
    {
      "epoch": 1.0766530134581627,
      "grad_norm": 0.5928725600242615,
      "learning_rate": 0.000297423887587822,
      "loss": 0.2302,
      "step": 460
    },
    {
      "epoch": 1.0789935634874195,
      "grad_norm": 0.677352249622345,
      "learning_rate": 0.00029734582357533176,
      "loss": 0.1933,
      "step": 461
    },
    {
      "epoch": 1.0813341135166765,
      "grad_norm": 0.5292572975158691,
      "learning_rate": 0.00029726775956284154,
      "loss": 0.1907,
      "step": 462
    },
    {
      "epoch": 1.0836746635459333,
      "grad_norm": 0.6755881905555725,
      "learning_rate": 0.00029718969555035126,
      "loss": 0.2266,
      "step": 463
    },
    {
      "epoch": 1.0860152135751902,
      "grad_norm": 0.8752768039703369,
      "learning_rate": 0.000297111631537861,
      "loss": 0.2532,
      "step": 464
    },
    {
      "epoch": 1.088355763604447,
      "grad_norm": 0.8456672430038452,
      "learning_rate": 0.00029703356752537076,
      "loss": 0.2543,
      "step": 465
    },
    {
      "epoch": 1.090696313633704,
      "grad_norm": 1.1317094564437866,
      "learning_rate": 0.00029695550351288054,
      "loss": 0.296,
      "step": 466
    },
    {
      "epoch": 1.0930368636629608,
      "grad_norm": 0.87260502576828,
      "learning_rate": 0.00029687743950039026,
      "loss": 0.3478,
      "step": 467
    },
    {
      "epoch": 1.0953774136922176,
      "grad_norm": 0.7057528495788574,
      "learning_rate": 0.00029679937548790004,
      "loss": 0.2673,
      "step": 468
    },
    {
      "epoch": 1.0977179637214745,
      "grad_norm": 0.7826763391494751,
      "learning_rate": 0.0002967213114754098,
      "loss": 0.2583,
      "step": 469
    },
    {
      "epoch": 1.1000585137507315,
      "grad_norm": 0.8345116376876831,
      "learning_rate": 0.00029664324746291954,
      "loss": 0.2814,
      "step": 470
    },
    {
      "epoch": 1.1023990637799883,
      "grad_norm": 0.7592636346817017,
      "learning_rate": 0.0002965651834504293,
      "loss": 0.2085,
      "step": 471
    },
    {
      "epoch": 1.1047396138092451,
      "grad_norm": 0.6565026044845581,
      "learning_rate": 0.0002964871194379391,
      "loss": 0.2254,
      "step": 472
    },
    {
      "epoch": 1.1070801638385022,
      "grad_norm": 0.758868396282196,
      "learning_rate": 0.0002964090554254488,
      "loss": 0.3058,
      "step": 473
    },
    {
      "epoch": 1.109420713867759,
      "grad_norm": 0.7649089694023132,
      "learning_rate": 0.0002963309914129586,
      "loss": 0.2454,
      "step": 474
    },
    {
      "epoch": 1.1117612638970158,
      "grad_norm": 0.6524908542633057,
      "learning_rate": 0.0002962529274004684,
      "loss": 0.2372,
      "step": 475
    },
    {
      "epoch": 1.1141018139262726,
      "grad_norm": 0.7699429988861084,
      "learning_rate": 0.0002961748633879781,
      "loss": 0.2947,
      "step": 476
    },
    {
      "epoch": 1.1164423639555296,
      "grad_norm": 0.6539528369903564,
      "learning_rate": 0.0002960967993754879,
      "loss": 0.2113,
      "step": 477
    },
    {
      "epoch": 1.1187829139847865,
      "grad_norm": 0.5816998481750488,
      "learning_rate": 0.00029601873536299765,
      "loss": 0.2317,
      "step": 478
    },
    {
      "epoch": 1.1211234640140433,
      "grad_norm": 0.5895833969116211,
      "learning_rate": 0.0002959406713505074,
      "loss": 0.202,
      "step": 479
    },
    {
      "epoch": 1.1234640140433,
      "grad_norm": 0.6303694844245911,
      "learning_rate": 0.00029586260733801715,
      "loss": 0.2227,
      "step": 480
    },
    {
      "epoch": 1.1258045640725571,
      "grad_norm": 0.5357046127319336,
      "learning_rate": 0.0002957845433255269,
      "loss": 0.1775,
      "step": 481
    },
    {
      "epoch": 1.128145114101814,
      "grad_norm": 0.6826846599578857,
      "learning_rate": 0.00029570647931303665,
      "loss": 0.2715,
      "step": 482
    },
    {
      "epoch": 1.1304856641310708,
      "grad_norm": 0.6173509359359741,
      "learning_rate": 0.00029562841530054643,
      "loss": 0.1723,
      "step": 483
    },
    {
      "epoch": 1.1328262141603276,
      "grad_norm": 0.4984360635280609,
      "learning_rate": 0.00029555035128805615,
      "loss": 0.1497,
      "step": 484
    },
    {
      "epoch": 1.1351667641895846,
      "grad_norm": 0.6284852027893066,
      "learning_rate": 0.00029547228727556593,
      "loss": 0.2366,
      "step": 485
    },
    {
      "epoch": 1.1375073142188414,
      "grad_norm": 0.8268014192581177,
      "learning_rate": 0.0002953942232630757,
      "loss": 0.2461,
      "step": 486
    },
    {
      "epoch": 1.1398478642480983,
      "grad_norm": 0.5946990847587585,
      "learning_rate": 0.00029531615925058543,
      "loss": 0.2078,
      "step": 487
    },
    {
      "epoch": 1.142188414277355,
      "grad_norm": 0.6397935152053833,
      "learning_rate": 0.0002952380952380952,
      "loss": 0.2493,
      "step": 488
    },
    {
      "epoch": 1.144528964306612,
      "grad_norm": 0.5340433120727539,
      "learning_rate": 0.000295160031225605,
      "loss": 0.1993,
      "step": 489
    },
    {
      "epoch": 1.146869514335869,
      "grad_norm": 0.6934782862663269,
      "learning_rate": 0.0002950819672131147,
      "loss": 0.253,
      "step": 490
    },
    {
      "epoch": 1.1492100643651257,
      "grad_norm": 0.6427504420280457,
      "learning_rate": 0.0002950039032006245,
      "loss": 0.2846,
      "step": 491
    },
    {
      "epoch": 1.1515506143943828,
      "grad_norm": 0.7751436829566956,
      "learning_rate": 0.00029492583918813427,
      "loss": 0.1821,
      "step": 492
    },
    {
      "epoch": 1.1538911644236396,
      "grad_norm": 0.5971342325210571,
      "learning_rate": 0.000294847775175644,
      "loss": 0.258,
      "step": 493
    },
    {
      "epoch": 1.1562317144528964,
      "grad_norm": 0.5593274235725403,
      "learning_rate": 0.00029476971116315377,
      "loss": 0.1955,
      "step": 494
    },
    {
      "epoch": 1.1585722644821532,
      "grad_norm": 0.8067606687545776,
      "learning_rate": 0.00029469164715066354,
      "loss": 0.2773,
      "step": 495
    },
    {
      "epoch": 1.1609128145114103,
      "grad_norm": 0.7453831434249878,
      "learning_rate": 0.00029461358313817327,
      "loss": 0.2296,
      "step": 496
    },
    {
      "epoch": 1.163253364540667,
      "grad_norm": 0.5657895803451538,
      "learning_rate": 0.00029453551912568304,
      "loss": 0.2239,
      "step": 497
    },
    {
      "epoch": 1.165593914569924,
      "grad_norm": 0.6481044292449951,
      "learning_rate": 0.0002944574551131928,
      "loss": 0.1942,
      "step": 498
    },
    {
      "epoch": 1.1679344645991807,
      "grad_norm": 0.9121841192245483,
      "learning_rate": 0.00029437939110070255,
      "loss": 0.3228,
      "step": 499
    },
    {
      "epoch": 1.1702750146284377,
      "grad_norm": 0.7603395581245422,
      "learning_rate": 0.0002943013270882123,
      "loss": 0.1857,
      "step": 500
    },
    {
      "epoch": 1.1726155646576946,
      "grad_norm": 0.6005155444145203,
      "learning_rate": 0.00029422326307572205,
      "loss": 0.1956,
      "step": 501
    },
    {
      "epoch": 1.1749561146869514,
      "grad_norm": 0.8601025342941284,
      "learning_rate": 0.0002941451990632318,
      "loss": 0.2868,
      "step": 502
    },
    {
      "epoch": 1.1772966647162084,
      "grad_norm": 0.9121009707450867,
      "learning_rate": 0.0002940671350507416,
      "loss": 0.2986,
      "step": 503
    },
    {
      "epoch": 1.1796372147454652,
      "grad_norm": 1.0804035663604736,
      "learning_rate": 0.0002939890710382513,
      "loss": 0.3832,
      "step": 504
    },
    {
      "epoch": 1.181977764774722,
      "grad_norm": 0.6650076508522034,
      "learning_rate": 0.0002939110070257611,
      "loss": 0.227,
      "step": 505
    },
    {
      "epoch": 1.1843183148039789,
      "grad_norm": 0.6589468717575073,
      "learning_rate": 0.0002938329430132709,
      "loss": 0.2425,
      "step": 506
    },
    {
      "epoch": 1.186658864833236,
      "grad_norm": 0.8205167055130005,
      "learning_rate": 0.0002937548790007806,
      "loss": 0.2854,
      "step": 507
    },
    {
      "epoch": 1.1889994148624927,
      "grad_norm": 0.6020083427429199,
      "learning_rate": 0.0002936768149882904,
      "loss": 0.2595,
      "step": 508
    },
    {
      "epoch": 1.1913399648917495,
      "grad_norm": 0.6945433616638184,
      "learning_rate": 0.00029359875097580016,
      "loss": 0.2645,
      "step": 509
    },
    {
      "epoch": 1.1936805149210064,
      "grad_norm": 0.7395508885383606,
      "learning_rate": 0.0002935206869633099,
      "loss": 0.2631,
      "step": 510
    },
    {
      "epoch": 1.1960210649502634,
      "grad_norm": 0.5649994015693665,
      "learning_rate": 0.00029344262295081966,
      "loss": 0.2727,
      "step": 511
    },
    {
      "epoch": 1.1983616149795202,
      "grad_norm": 0.5422025918960571,
      "learning_rate": 0.00029336455893832944,
      "loss": 0.194,
      "step": 512
    },
    {
      "epoch": 1.200702165008777,
      "grad_norm": 0.6710071563720703,
      "learning_rate": 0.00029328649492583916,
      "loss": 0.2474,
      "step": 513
    },
    {
      "epoch": 1.203042715038034,
      "grad_norm": 0.5874468088150024,
      "learning_rate": 0.00029320843091334894,
      "loss": 0.2019,
      "step": 514
    },
    {
      "epoch": 1.2053832650672909,
      "grad_norm": 0.5673450231552124,
      "learning_rate": 0.0002931303669008587,
      "loss": 0.2076,
      "step": 515
    },
    {
      "epoch": 1.2077238150965477,
      "grad_norm": 1.0013337135314941,
      "learning_rate": 0.00029305230288836844,
      "loss": 0.3155,
      "step": 516
    },
    {
      "epoch": 1.2100643651258045,
      "grad_norm": 0.8517215251922607,
      "learning_rate": 0.00029297423887587816,
      "loss": 0.275,
      "step": 517
    },
    {
      "epoch": 1.2124049151550613,
      "grad_norm": 0.6122258305549622,
      "learning_rate": 0.000292896174863388,
      "loss": 0.2297,
      "step": 518
    },
    {
      "epoch": 1.2147454651843184,
      "grad_norm": 0.7337661981582642,
      "learning_rate": 0.0002928181108508977,
      "loss": 0.2442,
      "step": 519
    },
    {
      "epoch": 1.2170860152135752,
      "grad_norm": 0.6227125525474548,
      "learning_rate": 0.00029274004683840744,
      "loss": 0.2464,
      "step": 520
    },
    {
      "epoch": 1.219426565242832,
      "grad_norm": 0.8081520199775696,
      "learning_rate": 0.0002926619828259172,
      "loss": 0.3273,
      "step": 521
    },
    {
      "epoch": 1.221767115272089,
      "grad_norm": 0.5950573682785034,
      "learning_rate": 0.000292583918813427,
      "loss": 0.195,
      "step": 522
    },
    {
      "epoch": 1.2241076653013458,
      "grad_norm": 0.7184973955154419,
      "learning_rate": 0.0002925058548009367,
      "loss": 0.3021,
      "step": 523
    },
    {
      "epoch": 1.2264482153306027,
      "grad_norm": 0.6171027421951294,
      "learning_rate": 0.0002924277907884465,
      "loss": 0.2768,
      "step": 524
    },
    {
      "epoch": 1.2287887653598595,
      "grad_norm": 0.6472241878509521,
      "learning_rate": 0.0002923497267759563,
      "loss": 0.2994,
      "step": 525
    },
    {
      "epoch": 1.2311293153891165,
      "grad_norm": 0.8520017862319946,
      "learning_rate": 0.000292271662763466,
      "loss": 0.3318,
      "step": 526
    },
    {
      "epoch": 1.2334698654183733,
      "grad_norm": 0.8127416372299194,
      "learning_rate": 0.0002921935987509758,
      "loss": 0.2799,
      "step": 527
    },
    {
      "epoch": 1.2358104154476302,
      "grad_norm": 0.7194250226020813,
      "learning_rate": 0.00029211553473848555,
      "loss": 0.2356,
      "step": 528
    },
    {
      "epoch": 1.238150965476887,
      "grad_norm": 0.7163675427436829,
      "learning_rate": 0.0002920374707259953,
      "loss": 0.2669,
      "step": 529
    },
    {
      "epoch": 1.240491515506144,
      "grad_norm": 0.7990010380744934,
      "learning_rate": 0.00029195940671350505,
      "loss": 0.2407,
      "step": 530
    },
    {
      "epoch": 1.2428320655354008,
      "grad_norm": 0.9868305921554565,
      "learning_rate": 0.00029188134270101483,
      "loss": 0.2928,
      "step": 531
    },
    {
      "epoch": 1.2451726155646576,
      "grad_norm": 0.9033278226852417,
      "learning_rate": 0.00029180327868852455,
      "loss": 0.2988,
      "step": 532
    },
    {
      "epoch": 1.2475131655939147,
      "grad_norm": 0.7177594304084778,
      "learning_rate": 0.00029172521467603433,
      "loss": 0.2459,
      "step": 533
    },
    {
      "epoch": 1.2498537156231715,
      "grad_norm": 0.6541428565979004,
      "learning_rate": 0.0002916471506635441,
      "loss": 0.218,
      "step": 534
    },
    {
      "epoch": 1.2521942656524283,
      "grad_norm": 0.5379924774169922,
      "learning_rate": 0.00029156908665105383,
      "loss": 0.2104,
      "step": 535
    },
    {
      "epoch": 1.2545348156816851,
      "grad_norm": 0.5531273484230042,
      "learning_rate": 0.0002914910226385636,
      "loss": 0.2369,
      "step": 536
    },
    {
      "epoch": 1.256875365710942,
      "grad_norm": 0.5540961027145386,
      "learning_rate": 0.00029141295862607333,
      "loss": 0.2403,
      "step": 537
    },
    {
      "epoch": 1.259215915740199,
      "grad_norm": 0.6757596135139465,
      "learning_rate": 0.0002913348946135831,
      "loss": 0.3076,
      "step": 538
    },
    {
      "epoch": 1.2615564657694558,
      "grad_norm": 0.5237883925437927,
      "learning_rate": 0.0002912568306010929,
      "loss": 0.2344,
      "step": 539
    },
    {
      "epoch": 1.2638970157987126,
      "grad_norm": 0.5778980851173401,
      "learning_rate": 0.0002911787665886026,
      "loss": 0.2543,
      "step": 540
    },
    {
      "epoch": 1.2662375658279696,
      "grad_norm": 0.5987311005592346,
      "learning_rate": 0.0002911007025761124,
      "loss": 0.2363,
      "step": 541
    },
    {
      "epoch": 1.2685781158572265,
      "grad_norm": 0.7165778279304504,
      "learning_rate": 0.00029102263856362217,
      "loss": 0.3284,
      "step": 542
    },
    {
      "epoch": 1.2709186658864833,
      "grad_norm": 0.7146481871604919,
      "learning_rate": 0.0002909445745511319,
      "loss": 0.3187,
      "step": 543
    },
    {
      "epoch": 1.2732592159157403,
      "grad_norm": 0.6384750008583069,
      "learning_rate": 0.00029086651053864167,
      "loss": 0.2268,
      "step": 544
    },
    {
      "epoch": 1.2755997659449971,
      "grad_norm": 0.5602327585220337,
      "learning_rate": 0.00029078844652615144,
      "loss": 0.2316,
      "step": 545
    },
    {
      "epoch": 1.277940315974254,
      "grad_norm": 0.5184253454208374,
      "learning_rate": 0.00029071038251366117,
      "loss": 0.2243,
      "step": 546
    },
    {
      "epoch": 1.2802808660035108,
      "grad_norm": 0.9245814085006714,
      "learning_rate": 0.00029063231850117094,
      "loss": 0.3342,
      "step": 547
    },
    {
      "epoch": 1.2826214160327676,
      "grad_norm": 0.7357866764068604,
      "learning_rate": 0.0002905542544886807,
      "loss": 0.3192,
      "step": 548
    },
    {
      "epoch": 1.2849619660620246,
      "grad_norm": 0.7512485384941101,
      "learning_rate": 0.00029047619047619045,
      "loss": 0.2686,
      "step": 549
    },
    {
      "epoch": 1.2873025160912814,
      "grad_norm": 0.6658631563186646,
      "learning_rate": 0.0002903981264637002,
      "loss": 0.2332,
      "step": 550
    },
    {
      "epoch": 1.2896430661205383,
      "grad_norm": 0.5521184802055359,
      "learning_rate": 0.00029032006245121,
      "loss": 0.2443,
      "step": 551
    },
    {
      "epoch": 1.2919836161497953,
      "grad_norm": 0.6237401366233826,
      "learning_rate": 0.0002902419984387197,
      "loss": 0.2233,
      "step": 552
    },
    {
      "epoch": 1.294324166179052,
      "grad_norm": 0.7881461977958679,
      "learning_rate": 0.00029016393442622945,
      "loss": 0.2879,
      "step": 553
    },
    {
      "epoch": 1.296664716208309,
      "grad_norm": 0.8292672038078308,
      "learning_rate": 0.0002900858704137393,
      "loss": 0.2704,
      "step": 554
    },
    {
      "epoch": 1.299005266237566,
      "grad_norm": 0.6894605159759521,
      "learning_rate": 0.000290007806401249,
      "loss": 0.2243,
      "step": 555
    },
    {
      "epoch": 1.3013458162668228,
      "grad_norm": 0.53110671043396,
      "learning_rate": 0.0002899297423887587,
      "loss": 0.1777,
      "step": 556
    },
    {
      "epoch": 1.3036863662960796,
      "grad_norm": 0.5812824964523315,
      "learning_rate": 0.0002898516783762685,
      "loss": 0.2665,
      "step": 557
    },
    {
      "epoch": 1.3060269163253364,
      "grad_norm": 0.6719763278961182,
      "learning_rate": 0.0002897736143637783,
      "loss": 0.3055,
      "step": 558
    },
    {
      "epoch": 1.3083674663545932,
      "grad_norm": 0.69077467918396,
      "learning_rate": 0.000289695550351288,
      "loss": 0.3206,
      "step": 559
    },
    {
      "epoch": 1.3107080163838503,
      "grad_norm": 0.5300419926643372,
      "learning_rate": 0.0002896174863387978,
      "loss": 0.2199,
      "step": 560
    },
    {
      "epoch": 1.313048566413107,
      "grad_norm": 0.5657298564910889,
      "learning_rate": 0.00028953942232630756,
      "loss": 0.195,
      "step": 561
    },
    {
      "epoch": 1.315389116442364,
      "grad_norm": 0.5899434089660645,
      "learning_rate": 0.0002894613583138173,
      "loss": 0.2671,
      "step": 562
    },
    {
      "epoch": 1.317729666471621,
      "grad_norm": 0.5920374989509583,
      "learning_rate": 0.00028938329430132706,
      "loss": 0.1929,
      "step": 563
    },
    {
      "epoch": 1.3200702165008777,
      "grad_norm": 0.43756523728370667,
      "learning_rate": 0.00028930523028883684,
      "loss": 0.1245,
      "step": 564
    },
    {
      "epoch": 1.3224107665301346,
      "grad_norm": 0.7356793880462646,
      "learning_rate": 0.00028922716627634656,
      "loss": 0.2729,
      "step": 565
    },
    {
      "epoch": 1.3247513165593914,
      "grad_norm": 0.6701174974441528,
      "learning_rate": 0.00028914910226385634,
      "loss": 0.2852,
      "step": 566
    },
    {
      "epoch": 1.3270918665886482,
      "grad_norm": 0.6318825483322144,
      "learning_rate": 0.0002890710382513661,
      "loss": 0.2512,
      "step": 567
    },
    {
      "epoch": 1.3294324166179052,
      "grad_norm": 0.8392045497894287,
      "learning_rate": 0.00028899297423887584,
      "loss": 0.3008,
      "step": 568
    },
    {
      "epoch": 1.331772966647162,
      "grad_norm": 0.9940409660339355,
      "learning_rate": 0.0002889149102263856,
      "loss": 0.2411,
      "step": 569
    },
    {
      "epoch": 1.3341135166764189,
      "grad_norm": 0.6284676194190979,
      "learning_rate": 0.0002888368462138954,
      "loss": 0.2348,
      "step": 570
    },
    {
      "epoch": 1.336454066705676,
      "grad_norm": 0.6901156306266785,
      "learning_rate": 0.0002887587822014051,
      "loss": 0.2429,
      "step": 571
    },
    {
      "epoch": 1.3387946167349327,
      "grad_norm": 0.6001614928245544,
      "learning_rate": 0.0002886807181889149,
      "loss": 0.2661,
      "step": 572
    },
    {
      "epoch": 1.3411351667641895,
      "grad_norm": 0.7244064807891846,
      "learning_rate": 0.0002886026541764246,
      "loss": 0.2763,
      "step": 573
    },
    {
      "epoch": 1.3434757167934466,
      "grad_norm": 0.6005414724349976,
      "learning_rate": 0.0002885245901639344,
      "loss": 0.2523,
      "step": 574
    },
    {
      "epoch": 1.3458162668227034,
      "grad_norm": 0.6772350072860718,
      "learning_rate": 0.0002884465261514442,
      "loss": 0.2794,
      "step": 575
    },
    {
      "epoch": 1.3481568168519602,
      "grad_norm": 0.7443065643310547,
      "learning_rate": 0.0002883684621389539,
      "loss": 0.2982,
      "step": 576
    },
    {
      "epoch": 1.350497366881217,
      "grad_norm": 0.518945574760437,
      "learning_rate": 0.0002882903981264637,
      "loss": 0.1676,
      "step": 577
    },
    {
      "epoch": 1.3528379169104738,
      "grad_norm": 0.6287896037101746,
      "learning_rate": 0.00028821233411397345,
      "loss": 0.2293,
      "step": 578
    },
    {
      "epoch": 1.3551784669397309,
      "grad_norm": 0.8302507996559143,
      "learning_rate": 0.0002881342701014832,
      "loss": 0.3073,
      "step": 579
    },
    {
      "epoch": 1.3575190169689877,
      "grad_norm": 0.6603302359580994,
      "learning_rate": 0.00028805620608899295,
      "loss": 0.243,
      "step": 580
    },
    {
      "epoch": 1.3598595669982445,
      "grad_norm": 0.6392195820808411,
      "learning_rate": 0.00028797814207650273,
      "loss": 0.2782,
      "step": 581
    },
    {
      "epoch": 1.3622001170275015,
      "grad_norm": 0.6984876990318298,
      "learning_rate": 0.00028790007806401245,
      "loss": 0.2969,
      "step": 582
    },
    {
      "epoch": 1.3645406670567584,
      "grad_norm": 0.5231770873069763,
      "learning_rate": 0.00028782201405152223,
      "loss": 0.2089,
      "step": 583
    },
    {
      "epoch": 1.3668812170860152,
      "grad_norm": 0.46662577986717224,
      "learning_rate": 0.000287743950039032,
      "loss": 0.2181,
      "step": 584
    },
    {
      "epoch": 1.3692217671152722,
      "grad_norm": 0.6155534386634827,
      "learning_rate": 0.00028766588602654173,
      "loss": 0.2407,
      "step": 585
    },
    {
      "epoch": 1.371562317144529,
      "grad_norm": 0.5466418266296387,
      "learning_rate": 0.0002875878220140515,
      "loss": 0.2334,
      "step": 586
    },
    {
      "epoch": 1.3739028671737858,
      "grad_norm": 0.5026735663414001,
      "learning_rate": 0.0002875097580015613,
      "loss": 0.1973,
      "step": 587
    },
    {
      "epoch": 1.3762434172030427,
      "grad_norm": 0.555044412612915,
      "learning_rate": 0.000287431693989071,
      "loss": 0.182,
      "step": 588
    },
    {
      "epoch": 1.3785839672322995,
      "grad_norm": 0.6249412298202515,
      "learning_rate": 0.00028735362997658073,
      "loss": 0.2526,
      "step": 589
    },
    {
      "epoch": 1.3809245172615565,
      "grad_norm": 0.47771355509757996,
      "learning_rate": 0.0002872755659640905,
      "loss": 0.2028,
      "step": 590
    },
    {
      "epoch": 1.3832650672908133,
      "grad_norm": 0.7397273182868958,
      "learning_rate": 0.0002871975019516003,
      "loss": 0.2285,
      "step": 591
    },
    {
      "epoch": 1.3856056173200701,
      "grad_norm": 0.7555626034736633,
      "learning_rate": 0.00028711943793911,
      "loss": 0.3012,
      "step": 592
    },
    {
      "epoch": 1.3879461673493272,
      "grad_norm": 0.6772009134292603,
      "learning_rate": 0.0002870413739266198,
      "loss": 0.2479,
      "step": 593
    },
    {
      "epoch": 1.390286717378584,
      "grad_norm": 0.6025837659835815,
      "learning_rate": 0.00028696330991412957,
      "loss": 0.2377,
      "step": 594
    },
    {
      "epoch": 1.3926272674078408,
      "grad_norm": 0.5084841251373291,
      "learning_rate": 0.0002868852459016393,
      "loss": 0.2333,
      "step": 595
    },
    {
      "epoch": 1.3949678174370979,
      "grad_norm": 0.628978967666626,
      "learning_rate": 0.00028680718188914907,
      "loss": 0.2636,
      "step": 596
    },
    {
      "epoch": 1.3973083674663547,
      "grad_norm": 0.623611330986023,
      "learning_rate": 0.00028672911787665884,
      "loss": 0.2319,
      "step": 597
    },
    {
      "epoch": 1.3996489174956115,
      "grad_norm": 0.652924656867981,
      "learning_rate": 0.00028665105386416857,
      "loss": 0.2488,
      "step": 598
    },
    {
      "epoch": 1.4019894675248683,
      "grad_norm": 0.7072157859802246,
      "learning_rate": 0.00028657298985167835,
      "loss": 0.2422,
      "step": 599
    },
    {
      "epoch": 1.4043300175541251,
      "grad_norm": 0.5705933570861816,
      "learning_rate": 0.0002864949258391881,
      "loss": 0.2343,
      "step": 600
    },
    {
      "epoch": 1.4066705675833822,
      "grad_norm": 0.4577906131744385,
      "learning_rate": 0.00028641686182669785,
      "loss": 0.2077,
      "step": 601
    },
    {
      "epoch": 1.409011117612639,
      "grad_norm": 0.5749457478523254,
      "learning_rate": 0.0002863387978142076,
      "loss": 0.2143,
      "step": 602
    },
    {
      "epoch": 1.4113516676418958,
      "grad_norm": 0.7785137891769409,
      "learning_rate": 0.0002862607338017174,
      "loss": 0.2612,
      "step": 603
    },
    {
      "epoch": 1.4136922176711528,
      "grad_norm": 0.7207542061805725,
      "learning_rate": 0.0002861826697892271,
      "loss": 0.2262,
      "step": 604
    },
    {
      "epoch": 1.4160327677004096,
      "grad_norm": 0.6884204149246216,
      "learning_rate": 0.0002861046057767369,
      "loss": 0.2177,
      "step": 605
    },
    {
      "epoch": 1.4183733177296665,
      "grad_norm": 0.6028758883476257,
      "learning_rate": 0.0002860265417642467,
      "loss": 0.2404,
      "step": 606
    },
    {
      "epoch": 1.4207138677589233,
      "grad_norm": 0.5336012244224548,
      "learning_rate": 0.0002859484777517564,
      "loss": 0.1844,
      "step": 607
    },
    {
      "epoch": 1.42305441778818,
      "grad_norm": 0.6133540272712708,
      "learning_rate": 0.0002858704137392662,
      "loss": 0.1968,
      "step": 608
    },
    {
      "epoch": 1.4253949678174371,
      "grad_norm": 0.9098119735717773,
      "learning_rate": 0.0002857923497267759,
      "loss": 0.2623,
      "step": 609
    },
    {
      "epoch": 1.427735517846694,
      "grad_norm": 0.5520263910293579,
      "learning_rate": 0.0002857142857142857,
      "loss": 0.1769,
      "step": 610
    },
    {
      "epoch": 1.4300760678759508,
      "grad_norm": 0.686767041683197,
      "learning_rate": 0.00028563622170179546,
      "loss": 0.2177,
      "step": 611
    },
    {
      "epoch": 1.4324166179052078,
      "grad_norm": 0.865389883518219,
      "learning_rate": 0.0002855581576893052,
      "loss": 0.2984,
      "step": 612
    },
    {
      "epoch": 1.4347571679344646,
      "grad_norm": 0.5799633264541626,
      "learning_rate": 0.00028548009367681496,
      "loss": 0.186,
      "step": 613
    },
    {
      "epoch": 1.4370977179637214,
      "grad_norm": 0.8131377696990967,
      "learning_rate": 0.00028540202966432474,
      "loss": 0.3131,
      "step": 614
    },
    {
      "epoch": 1.4394382679929785,
      "grad_norm": 0.6694985032081604,
      "learning_rate": 0.00028532396565183446,
      "loss": 0.2581,
      "step": 615
    },
    {
      "epoch": 1.4417788180222353,
      "grad_norm": 0.4860536456108093,
      "learning_rate": 0.00028524590163934424,
      "loss": 0.1759,
      "step": 616
    },
    {
      "epoch": 1.444119368051492,
      "grad_norm": 0.5634688138961792,
      "learning_rate": 0.000285167837626854,
      "loss": 0.2128,
      "step": 617
    },
    {
      "epoch": 1.446459918080749,
      "grad_norm": 0.6111524701118469,
      "learning_rate": 0.00028508977361436374,
      "loss": 0.2395,
      "step": 618
    },
    {
      "epoch": 1.4488004681100057,
      "grad_norm": 0.5212950706481934,
      "learning_rate": 0.0002850117096018735,
      "loss": 0.1774,
      "step": 619
    },
    {
      "epoch": 1.4511410181392628,
      "grad_norm": 0.5473677515983582,
      "learning_rate": 0.0002849336455893833,
      "loss": 0.2594,
      "step": 620
    },
    {
      "epoch": 1.4534815681685196,
      "grad_norm": 0.6897675395011902,
      "learning_rate": 0.000284855581576893,
      "loss": 0.2785,
      "step": 621
    },
    {
      "epoch": 1.4558221181977764,
      "grad_norm": 0.6480953693389893,
      "learning_rate": 0.0002847775175644028,
      "loss": 0.2549,
      "step": 622
    },
    {
      "epoch": 1.4581626682270334,
      "grad_norm": 0.6107755899429321,
      "learning_rate": 0.00028469945355191257,
      "loss": 0.2835,
      "step": 623
    },
    {
      "epoch": 1.4605032182562903,
      "grad_norm": 0.49352195858955383,
      "learning_rate": 0.0002846213895394223,
      "loss": 0.2149,
      "step": 624
    },
    {
      "epoch": 1.462843768285547,
      "grad_norm": 0.6523193120956421,
      "learning_rate": 0.0002845433255269321,
      "loss": 0.2721,
      "step": 625
    },
    {
      "epoch": 1.4651843183148041,
      "grad_norm": 0.4899730980396271,
      "learning_rate": 0.0002844652615144418,
      "loss": 0.2022,
      "step": 626
    },
    {
      "epoch": 1.467524868344061,
      "grad_norm": 0.5783730149269104,
      "learning_rate": 0.0002843871975019516,
      "loss": 0.231,
      "step": 627
    },
    {
      "epoch": 1.4698654183733177,
      "grad_norm": 0.5825036764144897,
      "learning_rate": 0.00028430913348946135,
      "loss": 0.2604,
      "step": 628
    },
    {
      "epoch": 1.4722059684025746,
      "grad_norm": 0.5253364443778992,
      "learning_rate": 0.0002842310694769711,
      "loss": 0.2393,
      "step": 629
    },
    {
      "epoch": 1.4745465184318314,
      "grad_norm": 0.642986536026001,
      "learning_rate": 0.00028415300546448085,
      "loss": 0.2631,
      "step": 630
    },
    {
      "epoch": 1.4768870684610884,
      "grad_norm": 0.6848770976066589,
      "learning_rate": 0.00028407494145199063,
      "loss": 0.2583,
      "step": 631
    },
    {
      "epoch": 1.4792276184903452,
      "grad_norm": 0.6893572211265564,
      "learning_rate": 0.00028399687743950035,
      "loss": 0.2989,
      "step": 632
    },
    {
      "epoch": 1.481568168519602,
      "grad_norm": 0.7069861888885498,
      "learning_rate": 0.00028391881342701013,
      "loss": 0.2454,
      "step": 633
    },
    {
      "epoch": 1.483908718548859,
      "grad_norm": 0.4989096224308014,
      "learning_rate": 0.0002838407494145199,
      "loss": 0.1885,
      "step": 634
    },
    {
      "epoch": 1.486249268578116,
      "grad_norm": 0.6261564493179321,
      "learning_rate": 0.00028376268540202963,
      "loss": 0.2687,
      "step": 635
    },
    {
      "epoch": 1.4885898186073727,
      "grad_norm": 0.46156424283981323,
      "learning_rate": 0.0002836846213895394,
      "loss": 0.2193,
      "step": 636
    },
    {
      "epoch": 1.4909303686366295,
      "grad_norm": 0.5582564473152161,
      "learning_rate": 0.0002836065573770492,
      "loss": 0.2452,
      "step": 637
    },
    {
      "epoch": 1.4932709186658863,
      "grad_norm": 0.6125344634056091,
      "learning_rate": 0.0002835284933645589,
      "loss": 0.2513,
      "step": 638
    },
    {
      "epoch": 1.4956114686951434,
      "grad_norm": 0.6385429501533508,
      "learning_rate": 0.0002834504293520687,
      "loss": 0.255,
      "step": 639
    },
    {
      "epoch": 1.4979520187244002,
      "grad_norm": 0.8917161226272583,
      "learning_rate": 0.00028337236533957846,
      "loss": 0.3537,
      "step": 640
    },
    {
      "epoch": 1.500292568753657,
      "grad_norm": 0.6927987337112427,
      "learning_rate": 0.0002832943013270882,
      "loss": 0.2407,
      "step": 641
    },
    {
      "epoch": 1.502633118782914,
      "grad_norm": 0.7918349504470825,
      "learning_rate": 0.0002832162373145979,
      "loss": 0.2676,
      "step": 642
    },
    {
      "epoch": 1.5049736688121709,
      "grad_norm": 0.6118308305740356,
      "learning_rate": 0.00028313817330210774,
      "loss": 0.2144,
      "step": 643
    },
    {
      "epoch": 1.5073142188414277,
      "grad_norm": 0.5387856364250183,
      "learning_rate": 0.00028306010928961747,
      "loss": 0.204,
      "step": 644
    },
    {
      "epoch": 1.5096547688706847,
      "grad_norm": 0.6031796932220459,
      "learning_rate": 0.0002829820452771272,
      "loss": 0.1617,
      "step": 645
    },
    {
      "epoch": 1.5119953188999413,
      "grad_norm": 0.4384934902191162,
      "learning_rate": 0.00028290398126463697,
      "loss": 0.1351,
      "step": 646
    },
    {
      "epoch": 1.5143358689291984,
      "grad_norm": 0.457932710647583,
      "learning_rate": 0.00028282591725214674,
      "loss": 0.1648,
      "step": 647
    },
    {
      "epoch": 1.5166764189584554,
      "grad_norm": 0.8926905989646912,
      "learning_rate": 0.00028274785323965647,
      "loss": 0.2425,
      "step": 648
    },
    {
      "epoch": 1.519016968987712,
      "grad_norm": 0.8112468719482422,
      "learning_rate": 0.00028266978922716625,
      "loss": 0.3313,
      "step": 649
    },
    {
      "epoch": 1.521357519016969,
      "grad_norm": 0.7009965181350708,
      "learning_rate": 0.000282591725214676,
      "loss": 0.2348,
      "step": 650
    },
    {
      "epoch": 1.5236980690462258,
      "grad_norm": 0.5742040276527405,
      "learning_rate": 0.00028251366120218575,
      "loss": 0.1932,
      "step": 651
    },
    {
      "epoch": 1.5260386190754827,
      "grad_norm": 0.7283630967140198,
      "learning_rate": 0.0002824355971896955,
      "loss": 0.2698,
      "step": 652
    },
    {
      "epoch": 1.5283791691047397,
      "grad_norm": 0.7550937533378601,
      "learning_rate": 0.0002823575331772053,
      "loss": 0.2539,
      "step": 653
    },
    {
      "epoch": 1.5307197191339965,
      "grad_norm": 0.49791306257247925,
      "learning_rate": 0.000282279469164715,
      "loss": 0.2001,
      "step": 654
    },
    {
      "epoch": 1.5330602691632533,
      "grad_norm": 0.7167301774024963,
      "learning_rate": 0.0002822014051522248,
      "loss": 0.2496,
      "step": 655
    },
    {
      "epoch": 1.5354008191925104,
      "grad_norm": 0.5204850435256958,
      "learning_rate": 0.0002821233411397346,
      "loss": 0.2285,
      "step": 656
    },
    {
      "epoch": 1.537741369221767,
      "grad_norm": 0.7286834716796875,
      "learning_rate": 0.0002820452771272443,
      "loss": 0.2576,
      "step": 657
    },
    {
      "epoch": 1.540081919251024,
      "grad_norm": 0.5982232689857483,
      "learning_rate": 0.0002819672131147541,
      "loss": 0.2645,
      "step": 658
    },
    {
      "epoch": 1.5424224692802808,
      "grad_norm": 0.5953397750854492,
      "learning_rate": 0.00028188914910226386,
      "loss": 0.2919,
      "step": 659
    },
    {
      "epoch": 1.5447630193095376,
      "grad_norm": 0.667203426361084,
      "learning_rate": 0.0002818110850897736,
      "loss": 0.2607,
      "step": 660
    },
    {
      "epoch": 1.5471035693387947,
      "grad_norm": 0.650166392326355,
      "learning_rate": 0.00028173302107728336,
      "loss": 0.2539,
      "step": 661
    },
    {
      "epoch": 1.5494441193680515,
      "grad_norm": 0.5674072504043579,
      "learning_rate": 0.0002816549570647931,
      "loss": 0.2299,
      "step": 662
    },
    {
      "epoch": 1.5517846693973083,
      "grad_norm": 0.47287213802337646,
      "learning_rate": 0.00028157689305230286,
      "loss": 0.2373,
      "step": 663
    },
    {
      "epoch": 1.5541252194265653,
      "grad_norm": 0.6204575300216675,
      "learning_rate": 0.00028149882903981264,
      "loss": 0.2878,
      "step": 664
    },
    {
      "epoch": 1.5564657694558222,
      "grad_norm": 0.561775803565979,
      "learning_rate": 0.00028142076502732236,
      "loss": 0.2692,
      "step": 665
    },
    {
      "epoch": 1.558806319485079,
      "grad_norm": 0.8078715801239014,
      "learning_rate": 0.00028134270101483214,
      "loss": 0.3064,
      "step": 666
    },
    {
      "epoch": 1.561146869514336,
      "grad_norm": 0.6117017865180969,
      "learning_rate": 0.0002812646370023419,
      "loss": 0.2247,
      "step": 667
    },
    {
      "epoch": 1.5634874195435926,
      "grad_norm": 0.5907092690467834,
      "learning_rate": 0.00028118657298985164,
      "loss": 0.193,
      "step": 668
    },
    {
      "epoch": 1.5658279695728496,
      "grad_norm": 0.5228221416473389,
      "learning_rate": 0.0002811085089773614,
      "loss": 0.2287,
      "step": 669
    },
    {
      "epoch": 1.5681685196021065,
      "grad_norm": 0.460018128156662,
      "learning_rate": 0.0002810304449648712,
      "loss": 0.2307,
      "step": 670
    },
    {
      "epoch": 1.5705090696313633,
      "grad_norm": 0.7554489970207214,
      "learning_rate": 0.0002809523809523809,
      "loss": 0.297,
      "step": 671
    },
    {
      "epoch": 1.5728496196606203,
      "grad_norm": 0.5426353812217712,
      "learning_rate": 0.0002808743169398907,
      "loss": 0.2112,
      "step": 672
    },
    {
      "epoch": 1.5751901696898771,
      "grad_norm": 0.7513291239738464,
      "learning_rate": 0.00028079625292740047,
      "loss": 0.2244,
      "step": 673
    },
    {
      "epoch": 1.577530719719134,
      "grad_norm": 0.6991456151008606,
      "learning_rate": 0.0002807181889149102,
      "loss": 0.2363,
      "step": 674
    },
    {
      "epoch": 1.579871269748391,
      "grad_norm": 0.5201103091239929,
      "learning_rate": 0.00028064012490242,
      "loss": 0.1957,
      "step": 675
    },
    {
      "epoch": 1.5822118197776478,
      "grad_norm": 0.7303555011749268,
      "learning_rate": 0.00028056206088992975,
      "loss": 0.2588,
      "step": 676
    },
    {
      "epoch": 1.5845523698069046,
      "grad_norm": 0.6132664084434509,
      "learning_rate": 0.0002804839968774395,
      "loss": 0.1767,
      "step": 677
    },
    {
      "epoch": 1.5868929198361617,
      "grad_norm": 0.6769293546676636,
      "learning_rate": 0.0002804059328649492,
      "loss": 0.2445,
      "step": 678
    },
    {
      "epoch": 1.5892334698654182,
      "grad_norm": 0.6693732142448425,
      "learning_rate": 0.00028032786885245903,
      "loss": 0.2751,
      "step": 679
    },
    {
      "epoch": 1.5915740198946753,
      "grad_norm": 0.6024100184440613,
      "learning_rate": 0.00028024980483996875,
      "loss": 0.236,
      "step": 680
    },
    {
      "epoch": 1.593914569923932,
      "grad_norm": 0.5290104150772095,
      "learning_rate": 0.0002801717408274785,
      "loss": 0.1779,
      "step": 681
    },
    {
      "epoch": 1.596255119953189,
      "grad_norm": 0.3973540663719177,
      "learning_rate": 0.00028009367681498825,
      "loss": 0.1312,
      "step": 682
    },
    {
      "epoch": 1.598595669982446,
      "grad_norm": 0.521659255027771,
      "learning_rate": 0.00028001561280249803,
      "loss": 0.2114,
      "step": 683
    },
    {
      "epoch": 1.6009362200117028,
      "grad_norm": 0.5992465615272522,
      "learning_rate": 0.00027993754879000775,
      "loss": 0.2468,
      "step": 684
    },
    {
      "epoch": 1.6032767700409596,
      "grad_norm": 0.49036291241645813,
      "learning_rate": 0.00027985948477751753,
      "loss": 0.1933,
      "step": 685
    },
    {
      "epoch": 1.6056173200702166,
      "grad_norm": 0.5267480611801147,
      "learning_rate": 0.0002797814207650273,
      "loss": 0.2546,
      "step": 686
    },
    {
      "epoch": 1.6079578700994732,
      "grad_norm": 0.606681764125824,
      "learning_rate": 0.00027970335675253703,
      "loss": 0.2094,
      "step": 687
    },
    {
      "epoch": 1.6102984201287303,
      "grad_norm": 0.6096392869949341,
      "learning_rate": 0.0002796252927400468,
      "loss": 0.3194,
      "step": 688
    },
    {
      "epoch": 1.612638970157987,
      "grad_norm": 0.5532101988792419,
      "learning_rate": 0.0002795472287275566,
      "loss": 0.2236,
      "step": 689
    },
    {
      "epoch": 1.6149795201872439,
      "grad_norm": 0.5864980220794678,
      "learning_rate": 0.0002794691647150663,
      "loss": 0.2378,
      "step": 690
    },
    {
      "epoch": 1.617320070216501,
      "grad_norm": 0.6072813868522644,
      "learning_rate": 0.0002793911007025761,
      "loss": 0.3016,
      "step": 691
    },
    {
      "epoch": 1.6196606202457577,
      "grad_norm": 0.5610007047653198,
      "learning_rate": 0.00027931303669008587,
      "loss": 0.2125,
      "step": 692
    },
    {
      "epoch": 1.6220011702750146,
      "grad_norm": 0.6549259424209595,
      "learning_rate": 0.0002792349726775956,
      "loss": 0.2152,
      "step": 693
    },
    {
      "epoch": 1.6243417203042716,
      "grad_norm": 0.6827576756477356,
      "learning_rate": 0.00027915690866510537,
      "loss": 0.2582,
      "step": 694
    },
    {
      "epoch": 1.6266822703335284,
      "grad_norm": 0.49631842970848083,
      "learning_rate": 0.00027907884465261514,
      "loss": 0.235,
      "step": 695
    },
    {
      "epoch": 1.6290228203627852,
      "grad_norm": 0.5490763783454895,
      "learning_rate": 0.00027900078064012487,
      "loss": 0.2008,
      "step": 696
    },
    {
      "epoch": 1.6313633703920423,
      "grad_norm": 0.52680504322052,
      "learning_rate": 0.00027892271662763465,
      "loss": 0.2128,
      "step": 697
    },
    {
      "epoch": 1.6337039204212989,
      "grad_norm": 0.6150671243667603,
      "learning_rate": 0.00027884465261514437,
      "loss": 0.2515,
      "step": 698
    },
    {
      "epoch": 1.636044470450556,
      "grad_norm": 0.7758251428604126,
      "learning_rate": 0.00027876658860265415,
      "loss": 0.2783,
      "step": 699
    },
    {
      "epoch": 1.6383850204798127,
      "grad_norm": 0.6165770292282104,
      "learning_rate": 0.0002786885245901639,
      "loss": 0.2032,
      "step": 700
    },
    {
      "epoch": 1.6407255705090695,
      "grad_norm": 0.661266565322876,
      "learning_rate": 0.00027861046057767365,
      "loss": 0.2732,
      "step": 701
    },
    {
      "epoch": 1.6430661205383266,
      "grad_norm": 0.5020604133605957,
      "learning_rate": 0.0002785323965651834,
      "loss": 0.1695,
      "step": 702
    },
    {
      "epoch": 1.6454066705675834,
      "grad_norm": 0.504331111907959,
      "learning_rate": 0.0002784543325526932,
      "loss": 0.2166,
      "step": 703
    },
    {
      "epoch": 1.6477472205968402,
      "grad_norm": 0.6139036417007446,
      "learning_rate": 0.0002783762685402029,
      "loss": 0.2484,
      "step": 704
    },
    {
      "epoch": 1.6500877706260972,
      "grad_norm": 0.8033633828163147,
      "learning_rate": 0.0002782982045277127,
      "loss": 0.3071,
      "step": 705
    },
    {
      "epoch": 1.652428320655354,
      "grad_norm": 0.5183423161506653,
      "learning_rate": 0.0002782201405152225,
      "loss": 0.1801,
      "step": 706
    },
    {
      "epoch": 1.6547688706846109,
      "grad_norm": 0.6041838526725769,
      "learning_rate": 0.0002781420765027322,
      "loss": 0.2321,
      "step": 707
    },
    {
      "epoch": 1.657109420713868,
      "grad_norm": 0.5242636799812317,
      "learning_rate": 0.000278064012490242,
      "loss": 0.2152,
      "step": 708
    },
    {
      "epoch": 1.6594499707431245,
      "grad_norm": 0.44494619965553284,
      "learning_rate": 0.00027798594847775176,
      "loss": 0.1975,
      "step": 709
    },
    {
      "epoch": 1.6617905207723815,
      "grad_norm": 0.4873981177806854,
      "learning_rate": 0.0002779078844652615,
      "loss": 0.2033,
      "step": 710
    },
    {
      "epoch": 1.6641310708016384,
      "grad_norm": 0.6010719537734985,
      "learning_rate": 0.00027782982045277126,
      "loss": 0.2417,
      "step": 711
    },
    {
      "epoch": 1.6664716208308952,
      "grad_norm": 0.562192440032959,
      "learning_rate": 0.00027775175644028104,
      "loss": 0.19,
      "step": 712
    },
    {
      "epoch": 1.6688121708601522,
      "grad_norm": 0.7465546727180481,
      "learning_rate": 0.00027767369242779076,
      "loss": 0.3312,
      "step": 713
    },
    {
      "epoch": 1.671152720889409,
      "grad_norm": 0.6206865906715393,
      "learning_rate": 0.0002775956284153005,
      "loss": 0.2386,
      "step": 714
    },
    {
      "epoch": 1.6734932709186658,
      "grad_norm": 0.5167813897132874,
      "learning_rate": 0.0002775175644028103,
      "loss": 0.2253,
      "step": 715
    },
    {
      "epoch": 1.6758338209479229,
      "grad_norm": 0.40686678886413574,
      "learning_rate": 0.00027743950039032004,
      "loss": 0.1671,
      "step": 716
    },
    {
      "epoch": 1.6781743709771795,
      "grad_norm": 0.5560344457626343,
      "learning_rate": 0.00027736143637782976,
      "loss": 0.2531,
      "step": 717
    },
    {
      "epoch": 1.6805149210064365,
      "grad_norm": 0.5033082365989685,
      "learning_rate": 0.00027728337236533954,
      "loss": 0.2334,
      "step": 718
    },
    {
      "epoch": 1.6828554710356936,
      "grad_norm": 0.45155149698257446,
      "learning_rate": 0.0002772053083528493,
      "loss": 0.1896,
      "step": 719
    },
    {
      "epoch": 1.6851960210649501,
      "grad_norm": 0.5657597780227661,
      "learning_rate": 0.00027712724434035904,
      "loss": 0.2572,
      "step": 720
    },
    {
      "epoch": 1.6875365710942072,
      "grad_norm": 0.5262623429298401,
      "learning_rate": 0.0002770491803278688,
      "loss": 0.2357,
      "step": 721
    },
    {
      "epoch": 1.689877121123464,
      "grad_norm": 0.5919508337974548,
      "learning_rate": 0.0002769711163153786,
      "loss": 0.2541,
      "step": 722
    },
    {
      "epoch": 1.6922176711527208,
      "grad_norm": 0.6522839665412903,
      "learning_rate": 0.0002768930523028883,
      "loss": 0.2958,
      "step": 723
    },
    {
      "epoch": 1.6945582211819779,
      "grad_norm": 0.6113526225090027,
      "learning_rate": 0.0002768149882903981,
      "loss": 0.2912,
      "step": 724
    },
    {
      "epoch": 1.6968987712112347,
      "grad_norm": 0.5861015915870667,
      "learning_rate": 0.0002767369242779079,
      "loss": 0.2565,
      "step": 725
    },
    {
      "epoch": 1.6992393212404915,
      "grad_norm": 0.48906755447387695,
      "learning_rate": 0.0002766588602654176,
      "loss": 0.1831,
      "step": 726
    },
    {
      "epoch": 1.7015798712697485,
      "grad_norm": 0.4905660152435303,
      "learning_rate": 0.0002765807962529274,
      "loss": 0.2053,
      "step": 727
    },
    {
      "epoch": 1.7039204212990051,
      "grad_norm": 0.5822237730026245,
      "learning_rate": 0.00027650273224043715,
      "loss": 0.1973,
      "step": 728
    },
    {
      "epoch": 1.7062609713282622,
      "grad_norm": 0.682068407535553,
      "learning_rate": 0.0002764246682279469,
      "loss": 0.2346,
      "step": 729
    },
    {
      "epoch": 1.708601521357519,
      "grad_norm": 0.6827600002288818,
      "learning_rate": 0.00027634660421545665,
      "loss": 0.2527,
      "step": 730
    },
    {
      "epoch": 1.7109420713867758,
      "grad_norm": 0.536926805973053,
      "learning_rate": 0.00027626854020296643,
      "loss": 0.1822,
      "step": 731
    },
    {
      "epoch": 1.7132826214160328,
      "grad_norm": 0.6540880799293518,
      "learning_rate": 0.00027619047619047615,
      "loss": 0.2209,
      "step": 732
    },
    {
      "epoch": 1.7156231714452896,
      "grad_norm": 0.7602537870407104,
      "learning_rate": 0.00027611241217798593,
      "loss": 0.2331,
      "step": 733
    },
    {
      "epoch": 1.7179637214745465,
      "grad_norm": 0.7365424633026123,
      "learning_rate": 0.00027603434816549565,
      "loss": 0.1799,
      "step": 734
    },
    {
      "epoch": 1.7203042715038035,
      "grad_norm": 0.7974913120269775,
      "learning_rate": 0.00027595628415300543,
      "loss": 0.2945,
      "step": 735
    },
    {
      "epoch": 1.7226448215330603,
      "grad_norm": 0.45312586426734924,
      "learning_rate": 0.0002758782201405152,
      "loss": 0.1233,
      "step": 736
    },
    {
      "epoch": 1.7249853715623171,
      "grad_norm": 0.8432010412216187,
      "learning_rate": 0.00027580015612802493,
      "loss": 0.247,
      "step": 737
    },
    {
      "epoch": 1.7273259215915742,
      "grad_norm": 0.521790087223053,
      "learning_rate": 0.0002757220921155347,
      "loss": 0.1493,
      "step": 738
    },
    {
      "epoch": 1.7296664716208308,
      "grad_norm": 0.6555188298225403,
      "learning_rate": 0.0002756440281030445,
      "loss": 0.2216,
      "step": 739
    },
    {
      "epoch": 1.7320070216500878,
      "grad_norm": 0.6334725022315979,
      "learning_rate": 0.0002755659640905542,
      "loss": 0.1703,
      "step": 740
    },
    {
      "epoch": 1.7343475716793446,
      "grad_norm": 0.7421914339065552,
      "learning_rate": 0.000275487900078064,
      "loss": 0.2258,
      "step": 741
    },
    {
      "epoch": 1.7366881217086014,
      "grad_norm": 0.5720406174659729,
      "learning_rate": 0.00027540983606557377,
      "loss": 0.2275,
      "step": 742
    },
    {
      "epoch": 1.7390286717378585,
      "grad_norm": 0.7311261892318726,
      "learning_rate": 0.0002753317720530835,
      "loss": 0.2499,
      "step": 743
    },
    {
      "epoch": 1.7413692217671153,
      "grad_norm": 0.6101363301277161,
      "learning_rate": 0.00027525370804059327,
      "loss": 0.2553,
      "step": 744
    },
    {
      "epoch": 1.743709771796372,
      "grad_norm": 0.5356411337852478,
      "learning_rate": 0.00027517564402810304,
      "loss": 0.1896,
      "step": 745
    },
    {
      "epoch": 1.7460503218256291,
      "grad_norm": 0.5517138242721558,
      "learning_rate": 0.00027509758001561277,
      "loss": 0.1901,
      "step": 746
    },
    {
      "epoch": 1.748390871854886,
      "grad_norm": 0.5939951539039612,
      "learning_rate": 0.00027501951600312255,
      "loss": 0.2681,
      "step": 747
    },
    {
      "epoch": 1.7507314218841428,
      "grad_norm": 0.38924968242645264,
      "learning_rate": 0.0002749414519906323,
      "loss": 0.1664,
      "step": 748
    },
    {
      "epoch": 1.7530719719133998,
      "grad_norm": 0.4265928864479065,
      "learning_rate": 0.00027486338797814205,
      "loss": 0.1423,
      "step": 749
    },
    {
      "epoch": 1.7554125219426564,
      "grad_norm": 0.4819576144218445,
      "learning_rate": 0.0002747853239656518,
      "loss": 0.2399,
      "step": 750
    },
    {
      "epoch": 1.7577530719719134,
      "grad_norm": 0.5317783355712891,
      "learning_rate": 0.00027470725995316155,
      "loss": 0.2002,
      "step": 751
    },
    {
      "epoch": 1.7600936220011703,
      "grad_norm": 0.5584965348243713,
      "learning_rate": 0.0002746291959406713,
      "loss": 0.2449,
      "step": 752
    },
    {
      "epoch": 1.762434172030427,
      "grad_norm": 0.78631991147995,
      "learning_rate": 0.0002745511319281811,
      "loss": 0.2902,
      "step": 753
    },
    {
      "epoch": 1.764774722059684,
      "grad_norm": 0.6081402897834778,
      "learning_rate": 0.0002744730679156908,
      "loss": 0.286,
      "step": 754
    },
    {
      "epoch": 1.767115272088941,
      "grad_norm": 0.7590538263320923,
      "learning_rate": 0.0002743950039032006,
      "loss": 0.321,
      "step": 755
    },
    {
      "epoch": 1.7694558221181977,
      "grad_norm": 0.49087315797805786,
      "learning_rate": 0.0002743169398907104,
      "loss": 0.2241,
      "step": 756
    },
    {
      "epoch": 1.7717963721474548,
      "grad_norm": 0.5779684782028198,
      "learning_rate": 0.0002742388758782201,
      "loss": 0.1969,
      "step": 757
    },
    {
      "epoch": 1.7741369221767114,
      "grad_norm": 0.5391296744346619,
      "learning_rate": 0.0002741608118657299,
      "loss": 0.1959,
      "step": 758
    },
    {
      "epoch": 1.7764774722059684,
      "grad_norm": 0.8086673617362976,
      "learning_rate": 0.00027408274785323966,
      "loss": 0.2359,
      "step": 759
    },
    {
      "epoch": 1.7788180222352252,
      "grad_norm": 0.5644164681434631,
      "learning_rate": 0.0002740046838407494,
      "loss": 0.2315,
      "step": 760
    },
    {
      "epoch": 1.781158572264482,
      "grad_norm": 0.4708375930786133,
      "learning_rate": 0.00027392661982825916,
      "loss": 0.1628,
      "step": 761
    },
    {
      "epoch": 1.783499122293739,
      "grad_norm": 0.4742141664028168,
      "learning_rate": 0.00027384855581576894,
      "loss": 0.2075,
      "step": 762
    },
    {
      "epoch": 1.785839672322996,
      "grad_norm": 0.8035599589347839,
      "learning_rate": 0.00027377049180327866,
      "loss": 0.2555,
      "step": 763
    },
    {
      "epoch": 1.7881802223522527,
      "grad_norm": 0.8653469085693359,
      "learning_rate": 0.00027369242779078844,
      "loss": 0.2585,
      "step": 764
    },
    {
      "epoch": 1.7905207723815098,
      "grad_norm": 0.5505135655403137,
      "learning_rate": 0.0002736143637782982,
      "loss": 0.2642,
      "step": 765
    },
    {
      "epoch": 1.7928613224107666,
      "grad_norm": 0.5076479911804199,
      "learning_rate": 0.00027353629976580794,
      "loss": 0.2351,
      "step": 766
    },
    {
      "epoch": 1.7952018724400234,
      "grad_norm": 0.4671745002269745,
      "learning_rate": 0.0002734582357533177,
      "loss": 0.1936,
      "step": 767
    },
    {
      "epoch": 1.7975424224692804,
      "grad_norm": 0.5481442809104919,
      "learning_rate": 0.0002733801717408275,
      "loss": 0.1565,
      "step": 768
    },
    {
      "epoch": 1.799882972498537,
      "grad_norm": 0.5334978103637695,
      "learning_rate": 0.0002733021077283372,
      "loss": 0.1588,
      "step": 769
    },
    {
      "epoch": 1.802223522527794,
      "grad_norm": 0.6470242142677307,
      "learning_rate": 0.00027322404371584694,
      "loss": 0.2764,
      "step": 770
    },
    {
      "epoch": 1.8045640725570509,
      "grad_norm": 0.550394594669342,
      "learning_rate": 0.0002731459797033567,
      "loss": 0.2073,
      "step": 771
    },
    {
      "epoch": 1.8069046225863077,
      "grad_norm": 0.5893709659576416,
      "learning_rate": 0.0002730679156908665,
      "loss": 0.2417,
      "step": 772
    },
    {
      "epoch": 1.8092451726155647,
      "grad_norm": 0.5497284531593323,
      "learning_rate": 0.0002729898516783762,
      "loss": 0.2291,
      "step": 773
    },
    {
      "epoch": 1.8115857226448215,
      "grad_norm": 0.7012251615524292,
      "learning_rate": 0.000272911787665886,
      "loss": 0.2882,
      "step": 774
    },
    {
      "epoch": 1.8139262726740784,
      "grad_norm": 0.66656494140625,
      "learning_rate": 0.0002728337236533958,
      "loss": 0.2561,
      "step": 775
    },
    {
      "epoch": 1.8162668227033354,
      "grad_norm": 0.5444290041923523,
      "learning_rate": 0.0002727556596409055,
      "loss": 0.2254,
      "step": 776
    },
    {
      "epoch": 1.8186073727325922,
      "grad_norm": 0.5455798506736755,
      "learning_rate": 0.0002726775956284153,
      "loss": 0.2103,
      "step": 777
    },
    {
      "epoch": 1.820947922761849,
      "grad_norm": 0.6486136317253113,
      "learning_rate": 0.00027259953161592505,
      "loss": 0.2264,
      "step": 778
    },
    {
      "epoch": 1.823288472791106,
      "grad_norm": 0.5139883160591125,
      "learning_rate": 0.0002725214676034348,
      "loss": 0.2014,
      "step": 779
    },
    {
      "epoch": 1.8256290228203627,
      "grad_norm": 0.6467077136039734,
      "learning_rate": 0.00027244340359094455,
      "loss": 0.2759,
      "step": 780
    },
    {
      "epoch": 1.8279695728496197,
      "grad_norm": 0.4878312945365906,
      "learning_rate": 0.00027236533957845433,
      "loss": 0.1604,
      "step": 781
    },
    {
      "epoch": 1.8303101228788765,
      "grad_norm": 0.5923587679862976,
      "learning_rate": 0.00027228727556596405,
      "loss": 0.2172,
      "step": 782
    },
    {
      "epoch": 1.8326506729081333,
      "grad_norm": 0.7178249359130859,
      "learning_rate": 0.00027220921155347383,
      "loss": 0.2683,
      "step": 783
    },
    {
      "epoch": 1.8349912229373904,
      "grad_norm": 0.72164386510849,
      "learning_rate": 0.0002721311475409836,
      "loss": 0.2703,
      "step": 784
    },
    {
      "epoch": 1.8373317729666472,
      "grad_norm": 0.6346378326416016,
      "learning_rate": 0.00027205308352849333,
      "loss": 0.2059,
      "step": 785
    },
    {
      "epoch": 1.839672322995904,
      "grad_norm": 0.6277174949645996,
      "learning_rate": 0.0002719750195160031,
      "loss": 0.2349,
      "step": 786
    },
    {
      "epoch": 1.842012873025161,
      "grad_norm": 0.5228105187416077,
      "learning_rate": 0.00027189695550351283,
      "loss": 0.1905,
      "step": 787
    },
    {
      "epoch": 1.8443534230544176,
      "grad_norm": 0.7643714547157288,
      "learning_rate": 0.0002718188914910226,
      "loss": 0.3105,
      "step": 788
    },
    {
      "epoch": 1.8466939730836747,
      "grad_norm": 0.8075363039970398,
      "learning_rate": 0.0002717408274785324,
      "loss": 0.3029,
      "step": 789
    },
    {
      "epoch": 1.8490345231129317,
      "grad_norm": 0.9182232618331909,
      "learning_rate": 0.0002716627634660421,
      "loss": 0.3304,
      "step": 790
    },
    {
      "epoch": 1.8513750731421883,
      "grad_norm": 0.6882867217063904,
      "learning_rate": 0.0002715846994535519,
      "loss": 0.2673,
      "step": 791
    },
    {
      "epoch": 1.8537156231714453,
      "grad_norm": 0.55961674451828,
      "learning_rate": 0.00027150663544106167,
      "loss": 0.1923,
      "step": 792
    },
    {
      "epoch": 1.8560561732007022,
      "grad_norm": 0.6214730143547058,
      "learning_rate": 0.0002714285714285714,
      "loss": 0.2063,
      "step": 793
    },
    {
      "epoch": 1.858396723229959,
      "grad_norm": 0.6436156630516052,
      "learning_rate": 0.00027135050741608117,
      "loss": 0.2636,
      "step": 794
    },
    {
      "epoch": 1.860737273259216,
      "grad_norm": 0.5332072377204895,
      "learning_rate": 0.00027127244340359094,
      "loss": 0.1974,
      "step": 795
    },
    {
      "epoch": 1.8630778232884728,
      "grad_norm": 0.43869930505752563,
      "learning_rate": 0.00027119437939110067,
      "loss": 0.1644,
      "step": 796
    },
    {
      "epoch": 1.8654183733177296,
      "grad_norm": 0.6218580007553101,
      "learning_rate": 0.00027111631537861045,
      "loss": 0.2542,
      "step": 797
    },
    {
      "epoch": 1.8677589233469867,
      "grad_norm": 0.6987879276275635,
      "learning_rate": 0.0002710382513661202,
      "loss": 0.2627,
      "step": 798
    },
    {
      "epoch": 1.8700994733762433,
      "grad_norm": 0.653878927230835,
      "learning_rate": 0.00027096018735362995,
      "loss": 0.2312,
      "step": 799
    },
    {
      "epoch": 1.8724400234055003,
      "grad_norm": 0.537924587726593,
      "learning_rate": 0.0002708821233411397,
      "loss": 0.1958,
      "step": 800
    },
    {
      "epoch": 1.8747805734347571,
      "grad_norm": 0.45428264141082764,
      "learning_rate": 0.0002708040593286495,
      "loss": 0.1978,
      "step": 801
    },
    {
      "epoch": 1.877121123464014,
      "grad_norm": 0.6978995203971863,
      "learning_rate": 0.0002707259953161592,
      "loss": 0.2695,
      "step": 802
    },
    {
      "epoch": 1.879461673493271,
      "grad_norm": 0.5626063942909241,
      "learning_rate": 0.00027064793130366895,
      "loss": 0.2305,
      "step": 803
    },
    {
      "epoch": 1.8818022235225278,
      "grad_norm": 0.5256984829902649,
      "learning_rate": 0.0002705698672911788,
      "loss": 0.1969,
      "step": 804
    },
    {
      "epoch": 1.8841427735517846,
      "grad_norm": 0.5547666549682617,
      "learning_rate": 0.0002704918032786885,
      "loss": 0.2235,
      "step": 805
    },
    {
      "epoch": 1.8864833235810416,
      "grad_norm": 0.659089207649231,
      "learning_rate": 0.0002704137392661982,
      "loss": 0.3021,
      "step": 806
    },
    {
      "epoch": 1.8888238736102985,
      "grad_norm": 0.5463134050369263,
      "learning_rate": 0.000270335675253708,
      "loss": 0.2408,
      "step": 807
    },
    {
      "epoch": 1.8911644236395553,
      "grad_norm": 0.5192107558250427,
      "learning_rate": 0.0002702576112412178,
      "loss": 0.1279,
      "step": 808
    },
    {
      "epoch": 1.8935049736688123,
      "grad_norm": 0.6804971098899841,
      "learning_rate": 0.0002701795472287275,
      "loss": 0.3268,
      "step": 809
    },
    {
      "epoch": 1.895845523698069,
      "grad_norm": 0.48195257782936096,
      "learning_rate": 0.0002701014832162373,
      "loss": 0.2455,
      "step": 810
    },
    {
      "epoch": 1.898186073727326,
      "grad_norm": 0.6296635270118713,
      "learning_rate": 0.00027002341920374706,
      "loss": 0.2869,
      "step": 811
    },
    {
      "epoch": 1.9005266237565828,
      "grad_norm": 0.5535439848899841,
      "learning_rate": 0.0002699453551912568,
      "loss": 0.2373,
      "step": 812
    },
    {
      "epoch": 1.9028671737858396,
      "grad_norm": 0.4696044325828552,
      "learning_rate": 0.00026986729117876656,
      "loss": 0.1618,
      "step": 813
    },
    {
      "epoch": 1.9052077238150966,
      "grad_norm": 0.5369753241539001,
      "learning_rate": 0.00026978922716627634,
      "loss": 0.2377,
      "step": 814
    },
    {
      "epoch": 1.9075482738443534,
      "grad_norm": 0.4376494884490967,
      "learning_rate": 0.00026971116315378606,
      "loss": 0.1449,
      "step": 815
    },
    {
      "epoch": 1.9098888238736103,
      "grad_norm": 0.6258947849273682,
      "learning_rate": 0.00026963309914129584,
      "loss": 0.2684,
      "step": 816
    },
    {
      "epoch": 1.9122293739028673,
      "grad_norm": 0.5808405876159668,
      "learning_rate": 0.0002695550351288056,
      "loss": 0.2528,
      "step": 817
    },
    {
      "epoch": 1.9145699239321239,
      "grad_norm": 0.778304398059845,
      "learning_rate": 0.00026947697111631534,
      "loss": 0.2762,
      "step": 818
    },
    {
      "epoch": 1.916910473961381,
      "grad_norm": 0.50103360414505,
      "learning_rate": 0.0002693989071038251,
      "loss": 0.1819,
      "step": 819
    },
    {
      "epoch": 1.919251023990638,
      "grad_norm": 0.7469761371612549,
      "learning_rate": 0.0002693208430913349,
      "loss": 0.1841,
      "step": 820
    },
    {
      "epoch": 1.9215915740198946,
      "grad_norm": 0.6073131561279297,
      "learning_rate": 0.0002692427790788446,
      "loss": 0.1962,
      "step": 821
    },
    {
      "epoch": 1.9239321240491516,
      "grad_norm": 0.6821140646934509,
      "learning_rate": 0.0002691647150663544,
      "loss": 0.2721,
      "step": 822
    },
    {
      "epoch": 1.9262726740784084,
      "grad_norm": 0.5471162796020508,
      "learning_rate": 0.0002690866510538641,
      "loss": 0.1945,
      "step": 823
    },
    {
      "epoch": 1.9286132241076652,
      "grad_norm": 0.6032418608665466,
      "learning_rate": 0.0002690085870413739,
      "loss": 0.2373,
      "step": 824
    },
    {
      "epoch": 1.9309537741369223,
      "grad_norm": 0.5110908150672913,
      "learning_rate": 0.0002689305230288837,
      "loss": 0.1763,
      "step": 825
    },
    {
      "epoch": 1.933294324166179,
      "grad_norm": 0.6219080686569214,
      "learning_rate": 0.0002688524590163934,
      "loss": 0.2806,
      "step": 826
    },
    {
      "epoch": 1.935634874195436,
      "grad_norm": 0.4919954240322113,
      "learning_rate": 0.0002687743950039032,
      "loss": 0.1855,
      "step": 827
    },
    {
      "epoch": 1.937975424224693,
      "grad_norm": 0.571054220199585,
      "learning_rate": 0.00026869633099141295,
      "loss": 0.2496,
      "step": 828
    },
    {
      "epoch": 1.9403159742539495,
      "grad_norm": 0.5339367985725403,
      "learning_rate": 0.0002686182669789227,
      "loss": 0.251,
      "step": 829
    },
    {
      "epoch": 1.9426565242832066,
      "grad_norm": 0.5161534547805786,
      "learning_rate": 0.00026854020296643245,
      "loss": 0.1736,
      "step": 830
    },
    {
      "epoch": 1.9449970743124634,
      "grad_norm": 0.5479429960250854,
      "learning_rate": 0.00026846213895394223,
      "loss": 0.2668,
      "step": 831
    },
    {
      "epoch": 1.9473376243417202,
      "grad_norm": 0.44095662236213684,
      "learning_rate": 0.00026838407494145195,
      "loss": 0.1839,
      "step": 832
    },
    {
      "epoch": 1.9496781743709772,
      "grad_norm": 0.5769118666648865,
      "learning_rate": 0.00026830601092896173,
      "loss": 0.2609,
      "step": 833
    },
    {
      "epoch": 1.952018724400234,
      "grad_norm": 0.6615362763404846,
      "learning_rate": 0.0002682279469164715,
      "loss": 0.2904,
      "step": 834
    },
    {
      "epoch": 1.9543592744294909,
      "grad_norm": 0.5203760266304016,
      "learning_rate": 0.00026814988290398123,
      "loss": 0.2145,
      "step": 835
    },
    {
      "epoch": 1.956699824458748,
      "grad_norm": 0.8211543560028076,
      "learning_rate": 0.000268071818891491,
      "loss": 0.2856,
      "step": 836
    },
    {
      "epoch": 1.9590403744880047,
      "grad_norm": 0.5366610288619995,
      "learning_rate": 0.0002679937548790008,
      "loss": 0.1984,
      "step": 837
    },
    {
      "epoch": 1.9613809245172615,
      "grad_norm": 0.4643138349056244,
      "learning_rate": 0.0002679156908665105,
      "loss": 0.1747,
      "step": 838
    },
    {
      "epoch": 1.9637214745465186,
      "grad_norm": 0.5854538083076477,
      "learning_rate": 0.00026783762685402023,
      "loss": 0.2221,
      "step": 839
    },
    {
      "epoch": 1.9660620245757752,
      "grad_norm": 0.682101309299469,
      "learning_rate": 0.00026775956284153007,
      "loss": 0.2336,
      "step": 840
    },
    {
      "epoch": 1.9684025746050322,
      "grad_norm": 0.5742960572242737,
      "learning_rate": 0.0002676814988290398,
      "loss": 0.2383,
      "step": 841
    },
    {
      "epoch": 1.970743124634289,
      "grad_norm": 0.5557207465171814,
      "learning_rate": 0.0002676034348165495,
      "loss": 0.2453,
      "step": 842
    },
    {
      "epoch": 1.9730836746635458,
      "grad_norm": 0.46166861057281494,
      "learning_rate": 0.0002675253708040593,
      "loss": 0.1309,
      "step": 843
    },
    {
      "epoch": 1.9754242246928029,
      "grad_norm": 0.6618620753288269,
      "learning_rate": 0.00026744730679156907,
      "loss": 0.227,
      "step": 844
    },
    {
      "epoch": 1.9777647747220597,
      "grad_norm": 0.6840081810951233,
      "learning_rate": 0.0002673692427790788,
      "loss": 0.2785,
      "step": 845
    },
    {
      "epoch": 1.9801053247513165,
      "grad_norm": 0.6777881383895874,
      "learning_rate": 0.00026729117876658857,
      "loss": 0.2705,
      "step": 846
    },
    {
      "epoch": 1.9824458747805735,
      "grad_norm": 0.9218038320541382,
      "learning_rate": 0.00026721311475409835,
      "loss": 0.3038,
      "step": 847
    },
    {
      "epoch": 1.9847864248098304,
      "grad_norm": 0.6290381550788879,
      "learning_rate": 0.00026713505074160807,
      "loss": 0.2112,
      "step": 848
    },
    {
      "epoch": 1.9871269748390872,
      "grad_norm": 0.5866653919219971,
      "learning_rate": 0.00026705698672911785,
      "loss": 0.224,
      "step": 849
    },
    {
      "epoch": 1.9894675248683442,
      "grad_norm": 0.6355277895927429,
      "learning_rate": 0.0002669789227166276,
      "loss": 0.2253,
      "step": 850
    },
    {
      "epoch": 1.9918080748976008,
      "grad_norm": 0.5861966609954834,
      "learning_rate": 0.00026690085870413735,
      "loss": 0.2184,
      "step": 851
    },
    {
      "epoch": 1.9941486249268578,
      "grad_norm": 0.5497576594352722,
      "learning_rate": 0.0002668227946916471,
      "loss": 0.2046,
      "step": 852
    },
    {
      "epoch": 1.9964891749561147,
      "grad_norm": 0.5302874445915222,
      "learning_rate": 0.0002667447306791569,
      "loss": 0.1803,
      "step": 853
    },
    {
      "epoch": 1.9988297249853715,
      "grad_norm": 0.527915358543396,
      "learning_rate": 0.0002666666666666666,
      "loss": 0.2355,
      "step": 854
    },
    {
      "epoch": 1.9988297249853715,
      "eval_loss": 0.2895708680152893,
      "eval_runtime": 129.0793,
      "eval_samples_per_second": 4.276,
      "eval_steps_per_second": 0.535,
      "step": 854
    },
    {
      "epoch": 2.0011702750146285,
      "grad_norm": 0.7639491558074951,
      "learning_rate": 0.0002665886026541764,
      "loss": 0.2387,
      "step": 855
    },
    {
      "epoch": 2.003510825043885,
      "grad_norm": 0.7000420689582825,
      "learning_rate": 0.0002665105386416862,
      "loss": 0.2499,
      "step": 856
    },
    {
      "epoch": 2.005851375073142,
      "grad_norm": 0.5706763863563538,
      "learning_rate": 0.0002664324746291959,
      "loss": 0.215,
      "step": 857
    },
    {
      "epoch": 2.008191925102399,
      "grad_norm": 0.49367555975914,
      "learning_rate": 0.0002663544106167057,
      "loss": 0.2185,
      "step": 858
    },
    {
      "epoch": 2.010532475131656,
      "grad_norm": 0.48258697986602783,
      "learning_rate": 0.0002662763466042154,
      "loss": 0.2044,
      "step": 859
    },
    {
      "epoch": 2.012873025160913,
      "grad_norm": 0.502260684967041,
      "learning_rate": 0.0002661982825917252,
      "loss": 0.1315,
      "step": 860
    },
    {
      "epoch": 2.01521357519017,
      "grad_norm": 0.5383921265602112,
      "learning_rate": 0.00026612021857923496,
      "loss": 0.1629,
      "step": 861
    },
    {
      "epoch": 2.0175541252194265,
      "grad_norm": 0.6826454997062683,
      "learning_rate": 0.0002660421545667447,
      "loss": 0.2353,
      "step": 862
    },
    {
      "epoch": 2.0198946752486835,
      "grad_norm": 0.547300398349762,
      "learning_rate": 0.00026596409055425446,
      "loss": 0.1706,
      "step": 863
    },
    {
      "epoch": 2.0222352252779405,
      "grad_norm": 0.5754042267799377,
      "learning_rate": 0.00026588602654176424,
      "loss": 0.1877,
      "step": 864
    },
    {
      "epoch": 2.024575775307197,
      "grad_norm": 0.70657879114151,
      "learning_rate": 0.00026580796252927396,
      "loss": 0.2353,
      "step": 865
    },
    {
      "epoch": 2.026916325336454,
      "grad_norm": 0.7880088686943054,
      "learning_rate": 0.00026572989851678374,
      "loss": 0.2625,
      "step": 866
    },
    {
      "epoch": 2.0292568753657108,
      "grad_norm": 0.6884554028511047,
      "learning_rate": 0.0002656518345042935,
      "loss": 0.2175,
      "step": 867
    },
    {
      "epoch": 2.031597425394968,
      "grad_norm": 0.5240896940231323,
      "learning_rate": 0.00026557377049180324,
      "loss": 0.187,
      "step": 868
    },
    {
      "epoch": 2.033937975424225,
      "grad_norm": 0.7075437903404236,
      "learning_rate": 0.000265495706479313,
      "loss": 0.2269,
      "step": 869
    },
    {
      "epoch": 2.0362785254534814,
      "grad_norm": 0.48684102296829224,
      "learning_rate": 0.0002654176424668228,
      "loss": 0.1729,
      "step": 870
    },
    {
      "epoch": 2.0386190754827385,
      "grad_norm": 0.4453898072242737,
      "learning_rate": 0.0002653395784543325,
      "loss": 0.1323,
      "step": 871
    },
    {
      "epoch": 2.0409596255119955,
      "grad_norm": 0.6750168204307556,
      "learning_rate": 0.0002652615144418423,
      "loss": 0.2351,
      "step": 872
    },
    {
      "epoch": 2.043300175541252,
      "grad_norm": 0.6549996137619019,
      "learning_rate": 0.00026518345042935207,
      "loss": 0.2056,
      "step": 873
    },
    {
      "epoch": 2.045640725570509,
      "grad_norm": 0.46010783314704895,
      "learning_rate": 0.0002651053864168618,
      "loss": 0.1462,
      "step": 874
    },
    {
      "epoch": 2.0479812755997657,
      "grad_norm": 0.5767553448677063,
      "learning_rate": 0.0002650273224043715,
      "loss": 0.2294,
      "step": 875
    },
    {
      "epoch": 2.0503218256290228,
      "grad_norm": 0.5145044922828674,
      "learning_rate": 0.00026494925839188135,
      "loss": 0.1439,
      "step": 876
    },
    {
      "epoch": 2.05266237565828,
      "grad_norm": 0.5577744841575623,
      "learning_rate": 0.0002648711943793911,
      "loss": 0.1917,
      "step": 877
    },
    {
      "epoch": 2.0550029256875364,
      "grad_norm": 0.5300160646438599,
      "learning_rate": 0.00026479313036690085,
      "loss": 0.16,
      "step": 878
    },
    {
      "epoch": 2.0573434757167934,
      "grad_norm": 0.5248023867607117,
      "learning_rate": 0.0002647150663544106,
      "loss": 0.1894,
      "step": 879
    },
    {
      "epoch": 2.0596840257460505,
      "grad_norm": 0.5892680883407593,
      "learning_rate": 0.00026463700234192035,
      "loss": 0.2129,
      "step": 880
    },
    {
      "epoch": 2.062024575775307,
      "grad_norm": 0.5853282809257507,
      "learning_rate": 0.00026455893832943013,
      "loss": 0.1988,
      "step": 881
    },
    {
      "epoch": 2.064365125804564,
      "grad_norm": 0.5019909143447876,
      "learning_rate": 0.00026448087431693985,
      "loss": 0.2,
      "step": 882
    },
    {
      "epoch": 2.066705675833821,
      "grad_norm": 0.6280490159988403,
      "learning_rate": 0.00026440281030444963,
      "loss": 0.194,
      "step": 883
    },
    {
      "epoch": 2.0690462258630777,
      "grad_norm": 0.6305379867553711,
      "learning_rate": 0.0002643247462919594,
      "loss": 0.1958,
      "step": 884
    },
    {
      "epoch": 2.0713867758923348,
      "grad_norm": 0.55721515417099,
      "learning_rate": 0.00026424668227946913,
      "loss": 0.1956,
      "step": 885
    },
    {
      "epoch": 2.0737273259215914,
      "grad_norm": 0.49251705408096313,
      "learning_rate": 0.0002641686182669789,
      "loss": 0.1324,
      "step": 886
    },
    {
      "epoch": 2.0760678759508484,
      "grad_norm": 0.5969528555870056,
      "learning_rate": 0.0002640905542544887,
      "loss": 0.1278,
      "step": 887
    },
    {
      "epoch": 2.0784084259801054,
      "grad_norm": 0.4529361128807068,
      "learning_rate": 0.0002640124902419984,
      "loss": 0.1478,
      "step": 888
    },
    {
      "epoch": 2.080748976009362,
      "grad_norm": 0.3189336359500885,
      "learning_rate": 0.0002639344262295082,
      "loss": 0.1068,
      "step": 889
    },
    {
      "epoch": 2.083089526038619,
      "grad_norm": 0.7132315635681152,
      "learning_rate": 0.00026385636221701797,
      "loss": 0.1934,
      "step": 890
    },
    {
      "epoch": 2.085430076067876,
      "grad_norm": 0.5648661255836487,
      "learning_rate": 0.0002637782982045277,
      "loss": 0.1827,
      "step": 891
    },
    {
      "epoch": 2.0877706260971327,
      "grad_norm": 0.7809880375862122,
      "learning_rate": 0.00026370023419203747,
      "loss": 0.2446,
      "step": 892
    },
    {
      "epoch": 2.0901111761263897,
      "grad_norm": 0.7505423426628113,
      "learning_rate": 0.00026362217017954724,
      "loss": 0.227,
      "step": 893
    },
    {
      "epoch": 2.092451726155647,
      "grad_norm": 0.5580998063087463,
      "learning_rate": 0.00026354410616705697,
      "loss": 0.2033,
      "step": 894
    },
    {
      "epoch": 2.0947922761849034,
      "grad_norm": 0.6107498407363892,
      "learning_rate": 0.0002634660421545667,
      "loss": 0.1994,
      "step": 895
    },
    {
      "epoch": 2.0971328262141604,
      "grad_norm": 0.49791380763053894,
      "learning_rate": 0.00026338797814207647,
      "loss": 0.1566,
      "step": 896
    },
    {
      "epoch": 2.099473376243417,
      "grad_norm": 0.4464121460914612,
      "learning_rate": 0.00026330991412958625,
      "loss": 0.1683,
      "step": 897
    },
    {
      "epoch": 2.101813926272674,
      "grad_norm": 0.4983968138694763,
      "learning_rate": 0.00026323185011709597,
      "loss": 0.1967,
      "step": 898
    },
    {
      "epoch": 2.104154476301931,
      "grad_norm": 0.5016987323760986,
      "learning_rate": 0.00026315378610460575,
      "loss": 0.1536,
      "step": 899
    },
    {
      "epoch": 2.1064950263311877,
      "grad_norm": 0.7548088431358337,
      "learning_rate": 0.0002630757220921155,
      "loss": 0.2408,
      "step": 900
    },
    {
      "epoch": 2.1088355763604447,
      "grad_norm": 0.5500598549842834,
      "learning_rate": 0.00026299765807962525,
      "loss": 0.2181,
      "step": 901
    },
    {
      "epoch": 2.1111761263897018,
      "grad_norm": 0.5611492991447449,
      "learning_rate": 0.000262919594067135,
      "loss": 0.2148,
      "step": 902
    },
    {
      "epoch": 2.1135166764189584,
      "grad_norm": 0.4555947184562683,
      "learning_rate": 0.0002628415300546448,
      "loss": 0.1783,
      "step": 903
    },
    {
      "epoch": 2.1158572264482154,
      "grad_norm": 0.5554770827293396,
      "learning_rate": 0.0002627634660421545,
      "loss": 0.2249,
      "step": 904
    },
    {
      "epoch": 2.118197776477472,
      "grad_norm": 0.8416849374771118,
      "learning_rate": 0.0002626854020296643,
      "loss": 0.2221,
      "step": 905
    },
    {
      "epoch": 2.120538326506729,
      "grad_norm": 0.4351446330547333,
      "learning_rate": 0.0002626073380171741,
      "loss": 0.1487,
      "step": 906
    },
    {
      "epoch": 2.122878876535986,
      "grad_norm": 0.49367576837539673,
      "learning_rate": 0.0002625292740046838,
      "loss": 0.1611,
      "step": 907
    },
    {
      "epoch": 2.1252194265652427,
      "grad_norm": 0.5352911949157715,
      "learning_rate": 0.0002624512099921936,
      "loss": 0.2018,
      "step": 908
    },
    {
      "epoch": 2.1275599765944997,
      "grad_norm": 0.646111786365509,
      "learning_rate": 0.00026237314597970336,
      "loss": 0.2387,
      "step": 909
    },
    {
      "epoch": 2.1299005266237567,
      "grad_norm": 0.689520537853241,
      "learning_rate": 0.0002622950819672131,
      "loss": 0.2313,
      "step": 910
    },
    {
      "epoch": 2.1322410766530133,
      "grad_norm": 0.6351328492164612,
      "learning_rate": 0.00026221701795472286,
      "loss": 0.2401,
      "step": 911
    },
    {
      "epoch": 2.1345816266822704,
      "grad_norm": 0.5778254866600037,
      "learning_rate": 0.00026213895394223264,
      "loss": 0.1766,
      "step": 912
    },
    {
      "epoch": 2.1369221767115274,
      "grad_norm": 0.8184921145439148,
      "learning_rate": 0.00026206088992974236,
      "loss": 0.2448,
      "step": 913
    },
    {
      "epoch": 2.139262726740784,
      "grad_norm": 0.6472208499908447,
      "learning_rate": 0.00026198282591725214,
      "loss": 0.2176,
      "step": 914
    },
    {
      "epoch": 2.141603276770041,
      "grad_norm": 0.6248598694801331,
      "learning_rate": 0.00026190476190476186,
      "loss": 0.1998,
      "step": 915
    },
    {
      "epoch": 2.1439438267992976,
      "grad_norm": 0.7097126245498657,
      "learning_rate": 0.00026182669789227164,
      "loss": 0.2041,
      "step": 916
    },
    {
      "epoch": 2.1462843768285547,
      "grad_norm": 0.6655234694480896,
      "learning_rate": 0.0002617486338797814,
      "loss": 0.2214,
      "step": 917
    },
    {
      "epoch": 2.1486249268578117,
      "grad_norm": 0.5756528377532959,
      "learning_rate": 0.00026167056986729114,
      "loss": 0.1775,
      "step": 918
    },
    {
      "epoch": 2.1509654768870683,
      "grad_norm": 0.7117827534675598,
      "learning_rate": 0.0002615925058548009,
      "loss": 0.2089,
      "step": 919
    },
    {
      "epoch": 2.1533060269163253,
      "grad_norm": 0.6295044422149658,
      "learning_rate": 0.0002615144418423107,
      "loss": 0.1554,
      "step": 920
    },
    {
      "epoch": 2.1556465769455824,
      "grad_norm": 0.6209376454353333,
      "learning_rate": 0.0002614363778298204,
      "loss": 0.204,
      "step": 921
    },
    {
      "epoch": 2.157987126974839,
      "grad_norm": 0.5212801694869995,
      "learning_rate": 0.0002613583138173302,
      "loss": 0.1427,
      "step": 922
    },
    {
      "epoch": 2.160327677004096,
      "grad_norm": 0.7071346044540405,
      "learning_rate": 0.00026128024980483997,
      "loss": 0.1687,
      "step": 923
    },
    {
      "epoch": 2.162668227033353,
      "grad_norm": 0.7214221954345703,
      "learning_rate": 0.0002612021857923497,
      "loss": 0.1939,
      "step": 924
    },
    {
      "epoch": 2.1650087770626096,
      "grad_norm": 0.7196916937828064,
      "learning_rate": 0.0002611241217798595,
      "loss": 0.2371,
      "step": 925
    },
    {
      "epoch": 2.1673493270918667,
      "grad_norm": 0.7737578749656677,
      "learning_rate": 0.00026104605776736925,
      "loss": 0.2344,
      "step": 926
    },
    {
      "epoch": 2.1696898771211233,
      "grad_norm": 0.8240876793861389,
      "learning_rate": 0.000260967993754879,
      "loss": 0.2001,
      "step": 927
    },
    {
      "epoch": 2.1720304271503803,
      "grad_norm": 0.6428293585777283,
      "learning_rate": 0.00026088992974238875,
      "loss": 0.2067,
      "step": 928
    },
    {
      "epoch": 2.1743709771796373,
      "grad_norm": 0.557721734046936,
      "learning_rate": 0.00026081186572989853,
      "loss": 0.1858,
      "step": 929
    },
    {
      "epoch": 2.176711527208894,
      "grad_norm": 0.5163233280181885,
      "learning_rate": 0.00026073380171740825,
      "loss": 0.1843,
      "step": 930
    },
    {
      "epoch": 2.179052077238151,
      "grad_norm": 0.5854336619377136,
      "learning_rate": 0.000260655737704918,
      "loss": 0.1868,
      "step": 931
    },
    {
      "epoch": 2.181392627267408,
      "grad_norm": 0.6547084450721741,
      "learning_rate": 0.00026057767369242775,
      "loss": 0.2134,
      "step": 932
    },
    {
      "epoch": 2.1837331772966646,
      "grad_norm": 0.6649954915046692,
      "learning_rate": 0.00026049960967993753,
      "loss": 0.1805,
      "step": 933
    },
    {
      "epoch": 2.1860737273259216,
      "grad_norm": 0.46423137187957764,
      "learning_rate": 0.00026042154566744725,
      "loss": 0.1391,
      "step": 934
    },
    {
      "epoch": 2.1884142773551787,
      "grad_norm": 0.7446958422660828,
      "learning_rate": 0.00026034348165495703,
      "loss": 0.2088,
      "step": 935
    },
    {
      "epoch": 2.1907548273844353,
      "grad_norm": 0.78547602891922,
      "learning_rate": 0.0002602654176424668,
      "loss": 0.2634,
      "step": 936
    },
    {
      "epoch": 2.1930953774136923,
      "grad_norm": 0.4045257568359375,
      "learning_rate": 0.00026018735362997653,
      "loss": 0.1859,
      "step": 937
    },
    {
      "epoch": 2.195435927442949,
      "grad_norm": 0.7495725750923157,
      "learning_rate": 0.0002601092896174863,
      "loss": 0.2154,
      "step": 938
    },
    {
      "epoch": 2.197776477472206,
      "grad_norm": 0.586438000202179,
      "learning_rate": 0.0002600312256049961,
      "loss": 0.1701,
      "step": 939
    },
    {
      "epoch": 2.200117027501463,
      "grad_norm": 0.6785942316055298,
      "learning_rate": 0.0002599531615925058,
      "loss": 0.1996,
      "step": 940
    },
    {
      "epoch": 2.2024575775307196,
      "grad_norm": 0.5798131823539734,
      "learning_rate": 0.0002598750975800156,
      "loss": 0.2661,
      "step": 941
    },
    {
      "epoch": 2.2047981275599766,
      "grad_norm": 0.4760452210903168,
      "learning_rate": 0.00025979703356752537,
      "loss": 0.1795,
      "step": 942
    },
    {
      "epoch": 2.2071386775892337,
      "grad_norm": 0.5085640549659729,
      "learning_rate": 0.0002597189695550351,
      "loss": 0.1605,
      "step": 943
    },
    {
      "epoch": 2.2094792276184902,
      "grad_norm": 0.5608185529708862,
      "learning_rate": 0.00025964090554254487,
      "loss": 0.1755,
      "step": 944
    },
    {
      "epoch": 2.2118197776477473,
      "grad_norm": 0.5160544514656067,
      "learning_rate": 0.00025956284153005464,
      "loss": 0.1811,
      "step": 945
    },
    {
      "epoch": 2.2141603276770043,
      "grad_norm": 0.4332812428474426,
      "learning_rate": 0.00025948477751756437,
      "loss": 0.181,
      "step": 946
    },
    {
      "epoch": 2.216500877706261,
      "grad_norm": 0.5154505372047424,
      "learning_rate": 0.00025940671350507415,
      "loss": 0.1829,
      "step": 947
    },
    {
      "epoch": 2.218841427735518,
      "grad_norm": 0.5300213098526001,
      "learning_rate": 0.00025932864949258387,
      "loss": 0.1475,
      "step": 948
    },
    {
      "epoch": 2.2211819777647746,
      "grad_norm": 0.6875470280647278,
      "learning_rate": 0.00025925058548009365,
      "loss": 0.2158,
      "step": 949
    },
    {
      "epoch": 2.2235225277940316,
      "grad_norm": 0.5973475575447083,
      "learning_rate": 0.0002591725214676034,
      "loss": 0.1944,
      "step": 950
    },
    {
      "epoch": 2.2258630778232886,
      "grad_norm": 0.7063873410224915,
      "learning_rate": 0.00025909445745511315,
      "loss": 0.2313,
      "step": 951
    },
    {
      "epoch": 2.228203627852545,
      "grad_norm": 0.6947303414344788,
      "learning_rate": 0.0002590163934426229,
      "loss": 0.2786,
      "step": 952
    },
    {
      "epoch": 2.2305441778818023,
      "grad_norm": 0.34421369433403015,
      "learning_rate": 0.0002589383294301327,
      "loss": 0.0865,
      "step": 953
    },
    {
      "epoch": 2.2328847279110593,
      "grad_norm": 0.4698253273963928,
      "learning_rate": 0.0002588602654176424,
      "loss": 0.1878,
      "step": 954
    },
    {
      "epoch": 2.235225277940316,
      "grad_norm": 0.5010329484939575,
      "learning_rate": 0.0002587822014051522,
      "loss": 0.2038,
      "step": 955
    },
    {
      "epoch": 2.237565827969573,
      "grad_norm": 0.6057751774787903,
      "learning_rate": 0.000258704137392662,
      "loss": 0.2546,
      "step": 956
    },
    {
      "epoch": 2.2399063779988295,
      "grad_norm": 0.5219748020172119,
      "learning_rate": 0.0002586260733801717,
      "loss": 0.1865,
      "step": 957
    },
    {
      "epoch": 2.2422469280280866,
      "grad_norm": 0.4613109529018402,
      "learning_rate": 0.0002585480093676815,
      "loss": 0.1985,
      "step": 958
    },
    {
      "epoch": 2.2445874780573436,
      "grad_norm": 0.49692609906196594,
      "learning_rate": 0.00025846994535519126,
      "loss": 0.1538,
      "step": 959
    },
    {
      "epoch": 2.2469280280866,
      "grad_norm": 0.6900572180747986,
      "learning_rate": 0.000258391881342701,
      "loss": 0.2708,
      "step": 960
    },
    {
      "epoch": 2.2492685781158572,
      "grad_norm": 0.5943496227264404,
      "learning_rate": 0.00025831381733021076,
      "loss": 0.2152,
      "step": 961
    },
    {
      "epoch": 2.2516091281451143,
      "grad_norm": 0.47768351435661316,
      "learning_rate": 0.00025823575331772054,
      "loss": 0.1976,
      "step": 962
    },
    {
      "epoch": 2.253949678174371,
      "grad_norm": 0.458148330450058,
      "learning_rate": 0.00025815768930523026,
      "loss": 0.1235,
      "step": 963
    },
    {
      "epoch": 2.256290228203628,
      "grad_norm": 0.6359423398971558,
      "learning_rate": 0.00025807962529274004,
      "loss": 0.228,
      "step": 964
    },
    {
      "epoch": 2.2586307782328845,
      "grad_norm": 0.8001506924629211,
      "learning_rate": 0.0002580015612802498,
      "loss": 0.2353,
      "step": 965
    },
    {
      "epoch": 2.2609713282621415,
      "grad_norm": 0.6896068453788757,
      "learning_rate": 0.00025792349726775954,
      "loss": 0.2251,
      "step": 966
    },
    {
      "epoch": 2.2633118782913986,
      "grad_norm": 0.6396545171737671,
      "learning_rate": 0.00025784543325526926,
      "loss": 0.1596,
      "step": 967
    },
    {
      "epoch": 2.265652428320655,
      "grad_norm": 0.5112870335578918,
      "learning_rate": 0.00025776736924277904,
      "loss": 0.147,
      "step": 968
    },
    {
      "epoch": 2.267992978349912,
      "grad_norm": 0.7208767533302307,
      "learning_rate": 0.0002576893052302888,
      "loss": 0.2373,
      "step": 969
    },
    {
      "epoch": 2.2703335283791692,
      "grad_norm": 0.7102628350257874,
      "learning_rate": 0.00025761124121779854,
      "loss": 0.2132,
      "step": 970
    },
    {
      "epoch": 2.272674078408426,
      "grad_norm": 0.7964979410171509,
      "learning_rate": 0.0002575331772053083,
      "loss": 0.163,
      "step": 971
    },
    {
      "epoch": 2.275014628437683,
      "grad_norm": 0.5566372275352478,
      "learning_rate": 0.0002574551131928181,
      "loss": 0.1595,
      "step": 972
    },
    {
      "epoch": 2.27735517846694,
      "grad_norm": 0.5734760165214539,
      "learning_rate": 0.0002573770491803278,
      "loss": 0.1738,
      "step": 973
    },
    {
      "epoch": 2.2796957284961965,
      "grad_norm": 0.7127610445022583,
      "learning_rate": 0.0002572989851678376,
      "loss": 0.241,
      "step": 974
    },
    {
      "epoch": 2.2820362785254535,
      "grad_norm": 0.6798275113105774,
      "learning_rate": 0.0002572209211553474,
      "loss": 0.2204,
      "step": 975
    },
    {
      "epoch": 2.28437682855471,
      "grad_norm": 0.7708956599235535,
      "learning_rate": 0.0002571428571428571,
      "loss": 0.1932,
      "step": 976
    },
    {
      "epoch": 2.286717378583967,
      "grad_norm": 0.40489500761032104,
      "learning_rate": 0.0002570647931303669,
      "loss": 0.1363,
      "step": 977
    },
    {
      "epoch": 2.289057928613224,
      "grad_norm": 0.5371028184890747,
      "learning_rate": 0.00025698672911787665,
      "loss": 0.2162,
      "step": 978
    },
    {
      "epoch": 2.291398478642481,
      "grad_norm": 0.6391522884368896,
      "learning_rate": 0.0002569086651053864,
      "loss": 0.1928,
      "step": 979
    },
    {
      "epoch": 2.293739028671738,
      "grad_norm": 0.6401144862174988,
      "learning_rate": 0.00025683060109289615,
      "loss": 0.2236,
      "step": 980
    },
    {
      "epoch": 2.296079578700995,
      "grad_norm": 0.559574544429779,
      "learning_rate": 0.00025675253708040593,
      "loss": 0.1823,
      "step": 981
    },
    {
      "epoch": 2.2984201287302515,
      "grad_norm": 0.5404853224754333,
      "learning_rate": 0.00025667447306791565,
      "loss": 0.1784,
      "step": 982
    },
    {
      "epoch": 2.3007606787595085,
      "grad_norm": 0.7083843350410461,
      "learning_rate": 0.00025659640905542543,
      "loss": 0.224,
      "step": 983
    },
    {
      "epoch": 2.3031012287887656,
      "grad_norm": 0.7199651598930359,
      "learning_rate": 0.00025651834504293515,
      "loss": 0.2755,
      "step": 984
    },
    {
      "epoch": 2.305441778818022,
      "grad_norm": 0.6794219613075256,
      "learning_rate": 0.00025644028103044493,
      "loss": 0.2737,
      "step": 985
    },
    {
      "epoch": 2.307782328847279,
      "grad_norm": 0.4287308156490326,
      "learning_rate": 0.0002563622170179547,
      "loss": 0.164,
      "step": 986
    },
    {
      "epoch": 2.310122878876536,
      "grad_norm": 0.3584732711315155,
      "learning_rate": 0.00025628415300546443,
      "loss": 0.1608,
      "step": 987
    },
    {
      "epoch": 2.312463428905793,
      "grad_norm": 0.49361753463745117,
      "learning_rate": 0.0002562060889929742,
      "loss": 0.1697,
      "step": 988
    },
    {
      "epoch": 2.31480397893505,
      "grad_norm": 0.47864335775375366,
      "learning_rate": 0.000256128024980484,
      "loss": 0.1767,
      "step": 989
    },
    {
      "epoch": 2.3171445289643064,
      "grad_norm": 0.5606163740158081,
      "learning_rate": 0.0002560499609679937,
      "loss": 0.2266,
      "step": 990
    },
    {
      "epoch": 2.3194850789935635,
      "grad_norm": 0.5888628363609314,
      "learning_rate": 0.0002559718969555035,
      "loss": 0.2086,
      "step": 991
    },
    {
      "epoch": 2.3218256290228205,
      "grad_norm": 0.5427351593971252,
      "learning_rate": 0.00025589383294301327,
      "loss": 0.2006,
      "step": 992
    },
    {
      "epoch": 2.324166179052077,
      "grad_norm": 0.5588325262069702,
      "learning_rate": 0.000255815768930523,
      "loss": 0.1629,
      "step": 993
    },
    {
      "epoch": 2.326506729081334,
      "grad_norm": 0.7040112614631653,
      "learning_rate": 0.00025573770491803277,
      "loss": 0.1977,
      "step": 994
    },
    {
      "epoch": 2.328847279110591,
      "grad_norm": 0.7019592523574829,
      "learning_rate": 0.00025565964090554254,
      "loss": 0.2059,
      "step": 995
    },
    {
      "epoch": 2.331187829139848,
      "grad_norm": 0.6393070816993713,
      "learning_rate": 0.00025558157689305227,
      "loss": 0.1765,
      "step": 996
    },
    {
      "epoch": 2.333528379169105,
      "grad_norm": 0.6131064295768738,
      "learning_rate": 0.00025550351288056205,
      "loss": 0.1726,
      "step": 997
    },
    {
      "epoch": 2.3358689291983614,
      "grad_norm": 0.6071540117263794,
      "learning_rate": 0.0002554254488680718,
      "loss": 0.1788,
      "step": 998
    },
    {
      "epoch": 2.3382094792276185,
      "grad_norm": 0.5434771776199341,
      "learning_rate": 0.00025534738485558155,
      "loss": 0.1589,
      "step": 999
    },
    {
      "epoch": 2.3405500292568755,
      "grad_norm": 0.7612959146499634,
      "learning_rate": 0.00025526932084309127,
      "loss": 0.2227,
      "step": 1000
    },
    {
      "epoch": 2.342890579286132,
      "grad_norm": 0.6392298340797424,
      "learning_rate": 0.0002551912568306011,
      "loss": 0.2237,
      "step": 1001
    },
    {
      "epoch": 2.345231129315389,
      "grad_norm": 0.6047635078430176,
      "learning_rate": 0.0002551131928181108,
      "loss": 0.1592,
      "step": 1002
    },
    {
      "epoch": 2.347571679344646,
      "grad_norm": 0.7538284659385681,
      "learning_rate": 0.00025503512880562055,
      "loss": 0.2084,
      "step": 1003
    },
    {
      "epoch": 2.3499122293739028,
      "grad_norm": 0.6366420984268188,
      "learning_rate": 0.0002549570647931303,
      "loss": 0.2225,
      "step": 1004
    },
    {
      "epoch": 2.35225277940316,
      "grad_norm": 0.554097592830658,
      "learning_rate": 0.0002548790007806401,
      "loss": 0.2009,
      "step": 1005
    },
    {
      "epoch": 2.354593329432417,
      "grad_norm": 0.6622557044029236,
      "learning_rate": 0.0002548009367681499,
      "loss": 0.1842,
      "step": 1006
    },
    {
      "epoch": 2.3569338794616734,
      "grad_norm": 0.8204147815704346,
      "learning_rate": 0.0002547228727556596,
      "loss": 0.2515,
      "step": 1007
    },
    {
      "epoch": 2.3592744294909305,
      "grad_norm": 0.5750885605812073,
      "learning_rate": 0.0002546448087431694,
      "loss": 0.2291,
      "step": 1008
    },
    {
      "epoch": 2.361614979520187,
      "grad_norm": 0.6147986650466919,
      "learning_rate": 0.00025456674473067916,
      "loss": 0.2066,
      "step": 1009
    },
    {
      "epoch": 2.363955529549444,
      "grad_norm": 0.5185875296592712,
      "learning_rate": 0.0002544886807181889,
      "loss": 0.1678,
      "step": 1010
    },
    {
      "epoch": 2.366296079578701,
      "grad_norm": 0.8484250903129578,
      "learning_rate": 0.00025441061670569866,
      "loss": 0.1597,
      "step": 1011
    },
    {
      "epoch": 2.3686366296079577,
      "grad_norm": 0.47349676489830017,
      "learning_rate": 0.00025433255269320844,
      "loss": 0.2029,
      "step": 1012
    },
    {
      "epoch": 2.3709771796372148,
      "grad_norm": 0.5819757580757141,
      "learning_rate": 0.00025425448868071816,
      "loss": 0.2023,
      "step": 1013
    },
    {
      "epoch": 2.373317729666472,
      "grad_norm": 0.6766828894615173,
      "learning_rate": 0.00025417642466822794,
      "loss": 0.2573,
      "step": 1014
    },
    {
      "epoch": 2.3756582796957284,
      "grad_norm": 0.6040202379226685,
      "learning_rate": 0.0002540983606557377,
      "loss": 0.1674,
      "step": 1015
    },
    {
      "epoch": 2.3779988297249854,
      "grad_norm": 0.415242463350296,
      "learning_rate": 0.00025402029664324744,
      "loss": 0.1686,
      "step": 1016
    },
    {
      "epoch": 2.3803393797542425,
      "grad_norm": 0.45707136392593384,
      "learning_rate": 0.0002539422326307572,
      "loss": 0.1755,
      "step": 1017
    },
    {
      "epoch": 2.382679929783499,
      "grad_norm": 0.6164835095405579,
      "learning_rate": 0.000253864168618267,
      "loss": 0.2207,
      "step": 1018
    },
    {
      "epoch": 2.385020479812756,
      "grad_norm": 0.40656185150146484,
      "learning_rate": 0.0002537861046057767,
      "loss": 0.1573,
      "step": 1019
    },
    {
      "epoch": 2.3873610298420127,
      "grad_norm": 0.4744901657104492,
      "learning_rate": 0.00025370804059328644,
      "loss": 0.1579,
      "step": 1020
    },
    {
      "epoch": 2.3897015798712697,
      "grad_norm": 0.7154913544654846,
      "learning_rate": 0.00025362997658079627,
      "loss": 0.278,
      "step": 1021
    },
    {
      "epoch": 2.392042129900527,
      "grad_norm": 0.7787860631942749,
      "learning_rate": 0.000253551912568306,
      "loss": 0.1792,
      "step": 1022
    },
    {
      "epoch": 2.3943826799297834,
      "grad_norm": 0.8275811672210693,
      "learning_rate": 0.0002534738485558157,
      "loss": 0.2954,
      "step": 1023
    },
    {
      "epoch": 2.3967232299590404,
      "grad_norm": 0.6582648754119873,
      "learning_rate": 0.0002533957845433255,
      "loss": 0.2034,
      "step": 1024
    },
    {
      "epoch": 2.399063779988297,
      "grad_norm": 0.5611529350280762,
      "learning_rate": 0.0002533177205308353,
      "loss": 0.2015,
      "step": 1025
    },
    {
      "epoch": 2.401404330017554,
      "grad_norm": 0.71223384141922,
      "learning_rate": 0.000253239656518345,
      "loss": 0.2917,
      "step": 1026
    },
    {
      "epoch": 2.403744880046811,
      "grad_norm": 0.37677350640296936,
      "learning_rate": 0.0002531615925058548,
      "loss": 0.1285,
      "step": 1027
    },
    {
      "epoch": 2.406085430076068,
      "grad_norm": 0.5945034623146057,
      "learning_rate": 0.00025308352849336455,
      "loss": 0.2623,
      "step": 1028
    },
    {
      "epoch": 2.4084259801053247,
      "grad_norm": 0.544708788394928,
      "learning_rate": 0.0002530054644808743,
      "loss": 0.1597,
      "step": 1029
    },
    {
      "epoch": 2.4107665301345818,
      "grad_norm": 0.6424126029014587,
      "learning_rate": 0.00025292740046838405,
      "loss": 0.2298,
      "step": 1030
    },
    {
      "epoch": 2.4131070801638383,
      "grad_norm": 0.6364026665687561,
      "learning_rate": 0.00025284933645589383,
      "loss": 0.2811,
      "step": 1031
    },
    {
      "epoch": 2.4154476301930954,
      "grad_norm": 0.6036072969436646,
      "learning_rate": 0.00025277127244340355,
      "loss": 0.2082,
      "step": 1032
    },
    {
      "epoch": 2.4177881802223524,
      "grad_norm": 0.6295894980430603,
      "learning_rate": 0.00025269320843091333,
      "loss": 0.2253,
      "step": 1033
    },
    {
      "epoch": 2.420128730251609,
      "grad_norm": 0.6124255657196045,
      "learning_rate": 0.0002526151444184231,
      "loss": 0.1877,
      "step": 1034
    },
    {
      "epoch": 2.422469280280866,
      "grad_norm": 0.5233687162399292,
      "learning_rate": 0.00025253708040593283,
      "loss": 0.2189,
      "step": 1035
    },
    {
      "epoch": 2.4248098303101226,
      "grad_norm": 0.6615109443664551,
      "learning_rate": 0.0002524590163934426,
      "loss": 0.2781,
      "step": 1036
    },
    {
      "epoch": 2.4271503803393797,
      "grad_norm": 0.5064884424209595,
      "learning_rate": 0.0002523809523809524,
      "loss": 0.1788,
      "step": 1037
    },
    {
      "epoch": 2.4294909303686367,
      "grad_norm": 0.6146239042282104,
      "learning_rate": 0.0002523028883684621,
      "loss": 0.2377,
      "step": 1038
    },
    {
      "epoch": 2.4318314803978933,
      "grad_norm": 0.5866500735282898,
      "learning_rate": 0.0002522248243559719,
      "loss": 0.1782,
      "step": 1039
    },
    {
      "epoch": 2.4341720304271504,
      "grad_norm": 0.5470282435417175,
      "learning_rate": 0.0002521467603434816,
      "loss": 0.1975,
      "step": 1040
    },
    {
      "epoch": 2.4365125804564074,
      "grad_norm": 0.48098674416542053,
      "learning_rate": 0.0002520686963309914,
      "loss": 0.1162,
      "step": 1041
    },
    {
      "epoch": 2.438853130485664,
      "grad_norm": 0.6938203573226929,
      "learning_rate": 0.00025199063231850117,
      "loss": 0.2409,
      "step": 1042
    },
    {
      "epoch": 2.441193680514921,
      "grad_norm": 0.6353834271430969,
      "learning_rate": 0.0002519125683060109,
      "loss": 0.1962,
      "step": 1043
    },
    {
      "epoch": 2.443534230544178,
      "grad_norm": 0.518335223197937,
      "learning_rate": 0.00025183450429352067,
      "loss": 0.1246,
      "step": 1044
    },
    {
      "epoch": 2.4458747805734347,
      "grad_norm": 0.4395870864391327,
      "learning_rate": 0.00025175644028103044,
      "loss": 0.1748,
      "step": 1045
    },
    {
      "epoch": 2.4482153306026917,
      "grad_norm": 0.5668835043907166,
      "learning_rate": 0.00025167837626854017,
      "loss": 0.1804,
      "step": 1046
    },
    {
      "epoch": 2.4505558806319483,
      "grad_norm": 0.6073132157325745,
      "learning_rate": 0.00025160031225604995,
      "loss": 0.1571,
      "step": 1047
    },
    {
      "epoch": 2.4528964306612053,
      "grad_norm": 0.6482378244400024,
      "learning_rate": 0.0002515222482435597,
      "loss": 0.2176,
      "step": 1048
    },
    {
      "epoch": 2.4552369806904624,
      "grad_norm": 0.6219510436058044,
      "learning_rate": 0.00025144418423106945,
      "loss": 0.1873,
      "step": 1049
    },
    {
      "epoch": 2.457577530719719,
      "grad_norm": 0.662648618221283,
      "learning_rate": 0.0002513661202185792,
      "loss": 0.2493,
      "step": 1050
    },
    {
      "epoch": 2.459918080748976,
      "grad_norm": 0.97642982006073,
      "learning_rate": 0.000251288056206089,
      "loss": 0.2225,
      "step": 1051
    },
    {
      "epoch": 2.462258630778233,
      "grad_norm": 0.7120621800422668,
      "learning_rate": 0.0002512099921935987,
      "loss": 0.229,
      "step": 1052
    },
    {
      "epoch": 2.4645991808074896,
      "grad_norm": 0.7384864687919617,
      "learning_rate": 0.0002511319281811085,
      "loss": 0.1838,
      "step": 1053
    },
    {
      "epoch": 2.4669397308367467,
      "grad_norm": 0.6085290908813477,
      "learning_rate": 0.0002510538641686183,
      "loss": 0.1694,
      "step": 1054
    },
    {
      "epoch": 2.4692802808660037,
      "grad_norm": 0.6609886288642883,
      "learning_rate": 0.000250975800156128,
      "loss": 0.2091,
      "step": 1055
    },
    {
      "epoch": 2.4716208308952603,
      "grad_norm": 0.47252678871154785,
      "learning_rate": 0.0002508977361436377,
      "loss": 0.146,
      "step": 1056
    },
    {
      "epoch": 2.4739613809245173,
      "grad_norm": 0.5804997086524963,
      "learning_rate": 0.00025081967213114756,
      "loss": 0.2022,
      "step": 1057
    },
    {
      "epoch": 2.476301930953774,
      "grad_norm": 0.6334847807884216,
      "learning_rate": 0.0002507416081186573,
      "loss": 0.2066,
      "step": 1058
    },
    {
      "epoch": 2.478642480983031,
      "grad_norm": 0.7257189154624939,
      "learning_rate": 0.000250663544106167,
      "loss": 0.2678,
      "step": 1059
    },
    {
      "epoch": 2.480983031012288,
      "grad_norm": 0.6872127056121826,
      "learning_rate": 0.0002505854800936768,
      "loss": 0.233,
      "step": 1060
    },
    {
      "epoch": 2.4833235810415446,
      "grad_norm": 0.6547925472259521,
      "learning_rate": 0.00025050741608118656,
      "loss": 0.2564,
      "step": 1061
    },
    {
      "epoch": 2.4856641310708016,
      "grad_norm": 0.6860712766647339,
      "learning_rate": 0.0002504293520686963,
      "loss": 0.2356,
      "step": 1062
    },
    {
      "epoch": 2.4880046811000587,
      "grad_norm": 0.6819084286689758,
      "learning_rate": 0.00025035128805620606,
      "loss": 0.2369,
      "step": 1063
    },
    {
      "epoch": 2.4903452311293153,
      "grad_norm": 0.6589412689208984,
      "learning_rate": 0.00025027322404371584,
      "loss": 0.1545,
      "step": 1064
    },
    {
      "epoch": 2.4926857811585723,
      "grad_norm": 0.6298510432243347,
      "learning_rate": 0.00025019516003122556,
      "loss": 0.2238,
      "step": 1065
    },
    {
      "epoch": 2.4950263311878293,
      "grad_norm": 0.614559531211853,
      "learning_rate": 0.00025011709601873534,
      "loss": 0.2607,
      "step": 1066
    },
    {
      "epoch": 2.497366881217086,
      "grad_norm": 0.4697611331939697,
      "learning_rate": 0.0002500390320062451,
      "loss": 0.1005,
      "step": 1067
    },
    {
      "epoch": 2.499707431246343,
      "grad_norm": 0.5492181181907654,
      "learning_rate": 0.00024996096799375484,
      "loss": 0.1643,
      "step": 1068
    },
    {
      "epoch": 2.5020479812755996,
      "grad_norm": 0.5910841226577759,
      "learning_rate": 0.0002498829039812646,
      "loss": 0.224,
      "step": 1069
    },
    {
      "epoch": 2.5043885313048566,
      "grad_norm": 0.48175233602523804,
      "learning_rate": 0.0002498048399687744,
      "loss": 0.1665,
      "step": 1070
    },
    {
      "epoch": 2.5067290813341137,
      "grad_norm": 0.6121644377708435,
      "learning_rate": 0.0002497267759562841,
      "loss": 0.2054,
      "step": 1071
    },
    {
      "epoch": 2.5090696313633702,
      "grad_norm": 0.7375407218933105,
      "learning_rate": 0.0002496487119437939,
      "loss": 0.2382,
      "step": 1072
    },
    {
      "epoch": 2.5114101813926273,
      "grad_norm": 0.7231982946395874,
      "learning_rate": 0.0002495706479313037,
      "loss": 0.236,
      "step": 1073
    },
    {
      "epoch": 2.513750731421884,
      "grad_norm": 0.742344081401825,
      "learning_rate": 0.0002494925839188134,
      "loss": 0.265,
      "step": 1074
    },
    {
      "epoch": 2.516091281451141,
      "grad_norm": 0.6507247090339661,
      "learning_rate": 0.0002494145199063232,
      "loss": 0.2336,
      "step": 1075
    },
    {
      "epoch": 2.518431831480398,
      "grad_norm": 0.3814326822757721,
      "learning_rate": 0.0002493364558938329,
      "loss": 0.1518,
      "step": 1076
    },
    {
      "epoch": 2.520772381509655,
      "grad_norm": 0.47029414772987366,
      "learning_rate": 0.0002492583918813427,
      "loss": 0.1685,
      "step": 1077
    },
    {
      "epoch": 2.5231129315389116,
      "grad_norm": 0.578022837638855,
      "learning_rate": 0.00024918032786885245,
      "loss": 0.2456,
      "step": 1078
    },
    {
      "epoch": 2.5254534815681686,
      "grad_norm": 0.41818171739578247,
      "learning_rate": 0.0002491022638563622,
      "loss": 0.1321,
      "step": 1079
    },
    {
      "epoch": 2.527794031597425,
      "grad_norm": 0.5945963859558105,
      "learning_rate": 0.00024902419984387195,
      "loss": 0.183,
      "step": 1080
    },
    {
      "epoch": 2.5301345816266823,
      "grad_norm": 0.6473022103309631,
      "learning_rate": 0.00024894613583138173,
      "loss": 0.2478,
      "step": 1081
    },
    {
      "epoch": 2.5324751316559393,
      "grad_norm": 0.5958973169326782,
      "learning_rate": 0.00024886807181889145,
      "loss": 0.207,
      "step": 1082
    },
    {
      "epoch": 2.534815681685196,
      "grad_norm": 0.604831337928772,
      "learning_rate": 0.00024879000780640123,
      "loss": 0.2128,
      "step": 1083
    },
    {
      "epoch": 2.537156231714453,
      "grad_norm": 0.5073763132095337,
      "learning_rate": 0.000248711943793911,
      "loss": 0.2184,
      "step": 1084
    },
    {
      "epoch": 2.5394967817437095,
      "grad_norm": 0.6755195260047913,
      "learning_rate": 0.00024863387978142073,
      "loss": 0.2469,
      "step": 1085
    },
    {
      "epoch": 2.5418373317729666,
      "grad_norm": 0.5423866510391235,
      "learning_rate": 0.0002485558157689305,
      "loss": 0.1791,
      "step": 1086
    },
    {
      "epoch": 2.5441778818022236,
      "grad_norm": 0.6119487881660461,
      "learning_rate": 0.0002484777517564403,
      "loss": 0.2322,
      "step": 1087
    },
    {
      "epoch": 2.5465184318314806,
      "grad_norm": 0.7182809114456177,
      "learning_rate": 0.00024839968774395,
      "loss": 0.2198,
      "step": 1088
    },
    {
      "epoch": 2.5488589818607372,
      "grad_norm": 0.7985907196998596,
      "learning_rate": 0.0002483216237314598,
      "loss": 0.1809,
      "step": 1089
    },
    {
      "epoch": 2.5511995318899943,
      "grad_norm": 0.6031938195228577,
      "learning_rate": 0.00024824355971896957,
      "loss": 0.2155,
      "step": 1090
    },
    {
      "epoch": 2.553540081919251,
      "grad_norm": 0.5345407724380493,
      "learning_rate": 0.0002481654957064793,
      "loss": 0.1696,
      "step": 1091
    },
    {
      "epoch": 2.555880631948508,
      "grad_norm": 0.7314776182174683,
      "learning_rate": 0.000248087431693989,
      "loss": 0.2196,
      "step": 1092
    },
    {
      "epoch": 2.558221181977765,
      "grad_norm": 0.6042296290397644,
      "learning_rate": 0.0002480093676814988,
      "loss": 0.1966,
      "step": 1093
    },
    {
      "epoch": 2.5605617320070215,
      "grad_norm": 0.6896017789840698,
      "learning_rate": 0.00024793130366900857,
      "loss": 0.215,
      "step": 1094
    },
    {
      "epoch": 2.5629022820362786,
      "grad_norm": 0.6125012636184692,
      "learning_rate": 0.0002478532396565183,
      "loss": 0.1653,
      "step": 1095
    },
    {
      "epoch": 2.565242832065535,
      "grad_norm": 0.7458984851837158,
      "learning_rate": 0.00024777517564402807,
      "loss": 0.1912,
      "step": 1096
    },
    {
      "epoch": 2.567583382094792,
      "grad_norm": 0.5428503751754761,
      "learning_rate": 0.00024769711163153785,
      "loss": 0.1396,
      "step": 1097
    },
    {
      "epoch": 2.5699239321240492,
      "grad_norm": 0.48287925124168396,
      "learning_rate": 0.00024761904761904757,
      "loss": 0.1744,
      "step": 1098
    },
    {
      "epoch": 2.5722644821533063,
      "grad_norm": 0.6164469718933105,
      "learning_rate": 0.00024754098360655735,
      "loss": 0.1946,
      "step": 1099
    },
    {
      "epoch": 2.574605032182563,
      "grad_norm": 0.5048424005508423,
      "learning_rate": 0.0002474629195940671,
      "loss": 0.1814,
      "step": 1100
    },
    {
      "epoch": 2.57694558221182,
      "grad_norm": 0.6088590621948242,
      "learning_rate": 0.00024738485558157685,
      "loss": 0.1804,
      "step": 1101
    },
    {
      "epoch": 2.5792861322410765,
      "grad_norm": 0.5920332074165344,
      "learning_rate": 0.0002473067915690866,
      "loss": 0.1833,
      "step": 1102
    },
    {
      "epoch": 2.5816266822703335,
      "grad_norm": 0.7340251207351685,
      "learning_rate": 0.0002472287275565964,
      "loss": 0.1932,
      "step": 1103
    },
    {
      "epoch": 2.5839672322995906,
      "grad_norm": 0.663944661617279,
      "learning_rate": 0.0002471506635441061,
      "loss": 0.2578,
      "step": 1104
    },
    {
      "epoch": 2.586307782328847,
      "grad_norm": 0.6157091856002808,
      "learning_rate": 0.0002470725995316159,
      "loss": 0.1887,
      "step": 1105
    },
    {
      "epoch": 2.588648332358104,
      "grad_norm": 0.5416165590286255,
      "learning_rate": 0.0002469945355191257,
      "loss": 0.1726,
      "step": 1106
    },
    {
      "epoch": 2.590988882387361,
      "grad_norm": 0.5770382881164551,
      "learning_rate": 0.0002469164715066354,
      "loss": 0.2299,
      "step": 1107
    },
    {
      "epoch": 2.593329432416618,
      "grad_norm": 0.6193284392356873,
      "learning_rate": 0.0002468384074941452,
      "loss": 0.1526,
      "step": 1108
    },
    {
      "epoch": 2.595669982445875,
      "grad_norm": 0.5916460156440735,
      "learning_rate": 0.0002467603434816549,
      "loss": 0.171,
      "step": 1109
    },
    {
      "epoch": 2.598010532475132,
      "grad_norm": 0.5322926640510559,
      "learning_rate": 0.0002466822794691647,
      "loss": 0.1724,
      "step": 1110
    },
    {
      "epoch": 2.6003510825043885,
      "grad_norm": 0.6826391220092773,
      "learning_rate": 0.00024660421545667446,
      "loss": 0.2744,
      "step": 1111
    },
    {
      "epoch": 2.6026916325336455,
      "grad_norm": 0.49006029963493347,
      "learning_rate": 0.0002465261514441842,
      "loss": 0.1489,
      "step": 1112
    },
    {
      "epoch": 2.605032182562902,
      "grad_norm": 0.5243620872497559,
      "learning_rate": 0.00024644808743169396,
      "loss": 0.1594,
      "step": 1113
    },
    {
      "epoch": 2.607372732592159,
      "grad_norm": 0.46721765398979187,
      "learning_rate": 0.00024637002341920374,
      "loss": 0.1776,
      "step": 1114
    },
    {
      "epoch": 2.609713282621416,
      "grad_norm": 0.6889635920524597,
      "learning_rate": 0.00024629195940671346,
      "loss": 0.244,
      "step": 1115
    },
    {
      "epoch": 2.612053832650673,
      "grad_norm": 0.6782718896865845,
      "learning_rate": 0.00024621389539422324,
      "loss": 0.253,
      "step": 1116
    },
    {
      "epoch": 2.61439438267993,
      "grad_norm": 0.5676100254058838,
      "learning_rate": 0.000246135831381733,
      "loss": 0.1372,
      "step": 1117
    },
    {
      "epoch": 2.6167349327091864,
      "grad_norm": 0.521324634552002,
      "learning_rate": 0.00024605776736924274,
      "loss": 0.192,
      "step": 1118
    },
    {
      "epoch": 2.6190754827384435,
      "grad_norm": 0.6442184448242188,
      "learning_rate": 0.0002459797033567525,
      "loss": 0.2383,
      "step": 1119
    },
    {
      "epoch": 2.6214160327677005,
      "grad_norm": 0.5796523094177246,
      "learning_rate": 0.0002459016393442623,
      "loss": 0.2426,
      "step": 1120
    },
    {
      "epoch": 2.6237565827969576,
      "grad_norm": 0.3810011148452759,
      "learning_rate": 0.000245823575331772,
      "loss": 0.0823,
      "step": 1121
    },
    {
      "epoch": 2.626097132826214,
      "grad_norm": 0.5404190421104431,
      "learning_rate": 0.0002457455113192818,
      "loss": 0.1961,
      "step": 1122
    },
    {
      "epoch": 2.628437682855471,
      "grad_norm": 0.6419076919555664,
      "learning_rate": 0.0002456674473067916,
      "loss": 0.204,
      "step": 1123
    },
    {
      "epoch": 2.630778232884728,
      "grad_norm": 0.5072348713874817,
      "learning_rate": 0.0002455893832943013,
      "loss": 0.1785,
      "step": 1124
    },
    {
      "epoch": 2.633118782913985,
      "grad_norm": 0.5226039886474609,
      "learning_rate": 0.0002455113192818111,
      "loss": 0.1983,
      "step": 1125
    },
    {
      "epoch": 2.635459332943242,
      "grad_norm": 0.5608882308006287,
      "learning_rate": 0.00024543325526932085,
      "loss": 0.1978,
      "step": 1126
    },
    {
      "epoch": 2.6377998829724985,
      "grad_norm": 0.5129210352897644,
      "learning_rate": 0.0002453551912568306,
      "loss": 0.1589,
      "step": 1127
    },
    {
      "epoch": 2.6401404330017555,
      "grad_norm": 0.7758574485778809,
      "learning_rate": 0.0002452771272443403,
      "loss": 0.2835,
      "step": 1128
    },
    {
      "epoch": 2.642480983031012,
      "grad_norm": 0.7465605735778809,
      "learning_rate": 0.0002451990632318501,
      "loss": 0.2197,
      "step": 1129
    },
    {
      "epoch": 2.644821533060269,
      "grad_norm": 0.5989202857017517,
      "learning_rate": 0.00024512099921935985,
      "loss": 0.19,
      "step": 1130
    },
    {
      "epoch": 2.647162083089526,
      "grad_norm": 0.5757240653038025,
      "learning_rate": 0.0002450429352068696,
      "loss": 0.1736,
      "step": 1131
    },
    {
      "epoch": 2.6495026331187828,
      "grad_norm": 0.6934990286827087,
      "learning_rate": 0.00024496487119437935,
      "loss": 0.1504,
      "step": 1132
    },
    {
      "epoch": 2.65184318314804,
      "grad_norm": 0.881618857383728,
      "learning_rate": 0.00024488680718188913,
      "loss": 0.2275,
      "step": 1133
    },
    {
      "epoch": 2.6541837331772964,
      "grad_norm": 0.6781178712844849,
      "learning_rate": 0.0002448087431693989,
      "loss": 0.2514,
      "step": 1134
    },
    {
      "epoch": 2.6565242832065534,
      "grad_norm": 0.7159805297851562,
      "learning_rate": 0.00024473067915690863,
      "loss": 0.2218,
      "step": 1135
    },
    {
      "epoch": 2.6588648332358105,
      "grad_norm": 0.5176329612731934,
      "learning_rate": 0.0002446526151444184,
      "loss": 0.1883,
      "step": 1136
    },
    {
      "epoch": 2.6612053832650675,
      "grad_norm": 0.5212604403495789,
      "learning_rate": 0.0002445745511319282,
      "loss": 0.1854,
      "step": 1137
    },
    {
      "epoch": 2.663545933294324,
      "grad_norm": 0.6024775505065918,
      "learning_rate": 0.0002444964871194379,
      "loss": 0.2043,
      "step": 1138
    },
    {
      "epoch": 2.665886483323581,
      "grad_norm": 0.6239482164382935,
      "learning_rate": 0.0002444184231069477,
      "loss": 0.2364,
      "step": 1139
    },
    {
      "epoch": 2.6682270333528377,
      "grad_norm": 0.4692019522190094,
      "learning_rate": 0.00024434035909445747,
      "loss": 0.1844,
      "step": 1140
    },
    {
      "epoch": 2.6705675833820948,
      "grad_norm": 0.7942224144935608,
      "learning_rate": 0.0002442622950819672,
      "loss": 0.2502,
      "step": 1141
    },
    {
      "epoch": 2.672908133411352,
      "grad_norm": 0.4739638566970825,
      "learning_rate": 0.00024418423106947697,
      "loss": 0.1742,
      "step": 1142
    },
    {
      "epoch": 2.6752486834406084,
      "grad_norm": 0.5596687197685242,
      "learning_rate": 0.00024410616705698672,
      "loss": 0.2257,
      "step": 1143
    },
    {
      "epoch": 2.6775892334698654,
      "grad_norm": 0.5867512822151184,
      "learning_rate": 0.00024402810304449647,
      "loss": 0.2274,
      "step": 1144
    },
    {
      "epoch": 2.679929783499122,
      "grad_norm": 0.547151505947113,
      "learning_rate": 0.00024395003903200622,
      "loss": 0.2116,
      "step": 1145
    },
    {
      "epoch": 2.682270333528379,
      "grad_norm": 0.5235583186149597,
      "learning_rate": 0.000243871975019516,
      "loss": 0.1863,
      "step": 1146
    },
    {
      "epoch": 2.684610883557636,
      "grad_norm": 0.5884446501731873,
      "learning_rate": 0.00024379391100702575,
      "loss": 0.2048,
      "step": 1147
    },
    {
      "epoch": 2.686951433586893,
      "grad_norm": 0.6500634551048279,
      "learning_rate": 0.0002437158469945355,
      "loss": 0.1992,
      "step": 1148
    },
    {
      "epoch": 2.6892919836161497,
      "grad_norm": 0.5862616300582886,
      "learning_rate": 0.00024363778298204527,
      "loss": 0.165,
      "step": 1149
    },
    {
      "epoch": 2.6916325336454068,
      "grad_norm": 0.5360001921653748,
      "learning_rate": 0.00024355971896955502,
      "loss": 0.2008,
      "step": 1150
    },
    {
      "epoch": 2.6939730836746634,
      "grad_norm": 0.6593778133392334,
      "learning_rate": 0.00024348165495706477,
      "loss": 0.2453,
      "step": 1151
    },
    {
      "epoch": 2.6963136337039204,
      "grad_norm": 0.48080527782440186,
      "learning_rate": 0.00024340359094457455,
      "loss": 0.1834,
      "step": 1152
    },
    {
      "epoch": 2.6986541837331774,
      "grad_norm": 0.5288419127464294,
      "learning_rate": 0.0002433255269320843,
      "loss": 0.1493,
      "step": 1153
    },
    {
      "epoch": 2.700994733762434,
      "grad_norm": 0.5044218301773071,
      "learning_rate": 0.00024324746291959403,
      "loss": 0.1283,
      "step": 1154
    },
    {
      "epoch": 2.703335283791691,
      "grad_norm": 0.5267982482910156,
      "learning_rate": 0.00024316939890710383,
      "loss": 0.2093,
      "step": 1155
    },
    {
      "epoch": 2.7056758338209477,
      "grad_norm": 0.5389242172241211,
      "learning_rate": 0.00024309133489461355,
      "loss": 0.2122,
      "step": 1156
    },
    {
      "epoch": 2.7080163838502047,
      "grad_norm": 0.5007054209709167,
      "learning_rate": 0.0002430132708821233,
      "loss": 0.1811,
      "step": 1157
    },
    {
      "epoch": 2.7103569338794617,
      "grad_norm": 0.4574107825756073,
      "learning_rate": 0.00024293520686963308,
      "loss": 0.1682,
      "step": 1158
    },
    {
      "epoch": 2.712697483908719,
      "grad_norm": 0.5995507836341858,
      "learning_rate": 0.00024285714285714283,
      "loss": 0.1734,
      "step": 1159
    },
    {
      "epoch": 2.7150380339379754,
      "grad_norm": 0.58919757604599,
      "learning_rate": 0.00024277907884465258,
      "loss": 0.2145,
      "step": 1160
    },
    {
      "epoch": 2.7173785839672324,
      "grad_norm": 0.4963042140007019,
      "learning_rate": 0.00024270101483216236,
      "loss": 0.1643,
      "step": 1161
    },
    {
      "epoch": 2.719719133996489,
      "grad_norm": 0.6143425107002258,
      "learning_rate": 0.0002426229508196721,
      "loss": 0.2411,
      "step": 1162
    },
    {
      "epoch": 2.722059684025746,
      "grad_norm": 0.6686487197875977,
      "learning_rate": 0.00024254488680718186,
      "loss": 0.2517,
      "step": 1163
    },
    {
      "epoch": 2.724400234055003,
      "grad_norm": 0.5367770791053772,
      "learning_rate": 0.00024246682279469164,
      "loss": 0.1742,
      "step": 1164
    },
    {
      "epoch": 2.7267407840842597,
      "grad_norm": 0.4787004292011261,
      "learning_rate": 0.0002423887587822014,
      "loss": 0.1718,
      "step": 1165
    },
    {
      "epoch": 2.7290813341135167,
      "grad_norm": 0.5632392168045044,
      "learning_rate": 0.00024231069476971114,
      "loss": 0.2087,
      "step": 1166
    },
    {
      "epoch": 2.7314218841427733,
      "grad_norm": 0.5472249388694763,
      "learning_rate": 0.00024223263075722092,
      "loss": 0.185,
      "step": 1167
    },
    {
      "epoch": 2.7337624341720304,
      "grad_norm": 0.7125111222267151,
      "learning_rate": 0.00024215456674473067,
      "loss": 0.2433,
      "step": 1168
    },
    {
      "epoch": 2.7361029842012874,
      "grad_norm": 0.5701318979263306,
      "learning_rate": 0.00024207650273224042,
      "loss": 0.1935,
      "step": 1169
    },
    {
      "epoch": 2.7384435342305444,
      "grad_norm": 0.571845293045044,
      "learning_rate": 0.0002419984387197502,
      "loss": 0.1815,
      "step": 1170
    },
    {
      "epoch": 2.740784084259801,
      "grad_norm": 0.5015832185745239,
      "learning_rate": 0.00024192037470725995,
      "loss": 0.2156,
      "step": 1171
    },
    {
      "epoch": 2.743124634289058,
      "grad_norm": 0.6518336534500122,
      "learning_rate": 0.00024184231069476967,
      "loss": 0.2074,
      "step": 1172
    },
    {
      "epoch": 2.7454651843183147,
      "grad_norm": 0.5806142687797546,
      "learning_rate": 0.00024176424668227947,
      "loss": 0.1866,
      "step": 1173
    },
    {
      "epoch": 2.7478057343475717,
      "grad_norm": 0.5740692019462585,
      "learning_rate": 0.0002416861826697892,
      "loss": 0.1426,
      "step": 1174
    },
    {
      "epoch": 2.7501462843768287,
      "grad_norm": 0.7557177543640137,
      "learning_rate": 0.00024160811865729895,
      "loss": 0.292,
      "step": 1175
    },
    {
      "epoch": 2.7524868344060853,
      "grad_norm": 0.8218662738800049,
      "learning_rate": 0.00024153005464480872,
      "loss": 0.2288,
      "step": 1176
    },
    {
      "epoch": 2.7548273844353424,
      "grad_norm": 0.6350980997085571,
      "learning_rate": 0.00024145199063231847,
      "loss": 0.2532,
      "step": 1177
    },
    {
      "epoch": 2.757167934464599,
      "grad_norm": 0.45491692423820496,
      "learning_rate": 0.00024137392661982823,
      "loss": 0.1697,
      "step": 1178
    },
    {
      "epoch": 2.759508484493856,
      "grad_norm": 0.5085026621818542,
      "learning_rate": 0.000241295862607338,
      "loss": 0.1756,
      "step": 1179
    },
    {
      "epoch": 2.761849034523113,
      "grad_norm": 0.4211353361606598,
      "learning_rate": 0.00024121779859484775,
      "loss": 0.1522,
      "step": 1180
    },
    {
      "epoch": 2.76418958455237,
      "grad_norm": 0.5759664177894592,
      "learning_rate": 0.0002411397345823575,
      "loss": 0.2285,
      "step": 1181
    },
    {
      "epoch": 2.7665301345816267,
      "grad_norm": 0.5849700570106506,
      "learning_rate": 0.00024106167056986728,
      "loss": 0.2016,
      "step": 1182
    },
    {
      "epoch": 2.7688706846108837,
      "grad_norm": 0.5489730834960938,
      "learning_rate": 0.00024098360655737703,
      "loss": 0.1868,
      "step": 1183
    },
    {
      "epoch": 2.7712112346401403,
      "grad_norm": 0.4170866012573242,
      "learning_rate": 0.00024090554254488678,
      "loss": 0.1243,
      "step": 1184
    },
    {
      "epoch": 2.7735517846693973,
      "grad_norm": 0.36720559000968933,
      "learning_rate": 0.00024082747853239656,
      "loss": 0.1352,
      "step": 1185
    },
    {
      "epoch": 2.7758923346986544,
      "grad_norm": 0.6230758428573608,
      "learning_rate": 0.0002407494145199063,
      "loss": 0.2627,
      "step": 1186
    },
    {
      "epoch": 2.778232884727911,
      "grad_norm": 0.7164156436920166,
      "learning_rate": 0.00024067135050741606,
      "loss": 0.1911,
      "step": 1187
    },
    {
      "epoch": 2.780573434757168,
      "grad_norm": 0.7442493438720703,
      "learning_rate": 0.00024059328649492584,
      "loss": 0.246,
      "step": 1188
    },
    {
      "epoch": 2.7829139847864246,
      "grad_norm": 0.4952000081539154,
      "learning_rate": 0.0002405152224824356,
      "loss": 0.214,
      "step": 1189
    },
    {
      "epoch": 2.7852545348156816,
      "grad_norm": 0.516975998878479,
      "learning_rate": 0.0002404371584699453,
      "loss": 0.2053,
      "step": 1190
    },
    {
      "epoch": 2.7875950848449387,
      "grad_norm": 0.6030364632606506,
      "learning_rate": 0.00024035909445745512,
      "loss": 0.2243,
      "step": 1191
    },
    {
      "epoch": 2.7899356348741957,
      "grad_norm": 0.6081345081329346,
      "learning_rate": 0.00024028103044496484,
      "loss": 0.1884,
      "step": 1192
    },
    {
      "epoch": 2.7922761849034523,
      "grad_norm": 0.4482404589653015,
      "learning_rate": 0.0002402029664324746,
      "loss": 0.1576,
      "step": 1193
    },
    {
      "epoch": 2.7946167349327093,
      "grad_norm": 0.5292582511901855,
      "learning_rate": 0.00024012490241998437,
      "loss": 0.2256,
      "step": 1194
    },
    {
      "epoch": 2.796957284961966,
      "grad_norm": 0.4556490480899811,
      "learning_rate": 0.00024004683840749412,
      "loss": 0.1687,
      "step": 1195
    },
    {
      "epoch": 2.799297834991223,
      "grad_norm": 0.5711034536361694,
      "learning_rate": 0.00023996877439500387,
      "loss": 0.2387,
      "step": 1196
    },
    {
      "epoch": 2.80163838502048,
      "grad_norm": 0.6690816283226013,
      "learning_rate": 0.00023989071038251365,
      "loss": 0.2524,
      "step": 1197
    },
    {
      "epoch": 2.8039789350497366,
      "grad_norm": 0.8365682363510132,
      "learning_rate": 0.0002398126463700234,
      "loss": 0.2422,
      "step": 1198
    },
    {
      "epoch": 2.8063194850789936,
      "grad_norm": 0.5091309547424316,
      "learning_rate": 0.00023973458235753315,
      "loss": 0.2151,
      "step": 1199
    },
    {
      "epoch": 2.8086600351082502,
      "grad_norm": 0.6482131481170654,
      "learning_rate": 0.00023965651834504292,
      "loss": 0.1826,
      "step": 1200
    },
    {
      "epoch": 2.8110005851375073,
      "grad_norm": 0.5607661008834839,
      "learning_rate": 0.00023957845433255267,
      "loss": 0.2683,
      "step": 1201
    },
    {
      "epoch": 2.8133411351667643,
      "grad_norm": 0.502094030380249,
      "learning_rate": 0.00023950039032006242,
      "loss": 0.1858,
      "step": 1202
    },
    {
      "epoch": 2.815681685196021,
      "grad_norm": 0.6329809427261353,
      "learning_rate": 0.0002394223263075722,
      "loss": 0.2011,
      "step": 1203
    },
    {
      "epoch": 2.818022235225278,
      "grad_norm": 0.38770490884780884,
      "learning_rate": 0.00023934426229508195,
      "loss": 0.1529,
      "step": 1204
    },
    {
      "epoch": 2.8203627852545345,
      "grad_norm": 0.561657726764679,
      "learning_rate": 0.0002392661982825917,
      "loss": 0.203,
      "step": 1205
    },
    {
      "epoch": 2.8227033352837916,
      "grad_norm": 0.6001734137535095,
      "learning_rate": 0.00023918813427010148,
      "loss": 0.2285,
      "step": 1206
    },
    {
      "epoch": 2.8250438853130486,
      "grad_norm": 0.8099863529205322,
      "learning_rate": 0.00023911007025761123,
      "loss": 0.2737,
      "step": 1207
    },
    {
      "epoch": 2.8273844353423057,
      "grad_norm": 0.4595024883747101,
      "learning_rate": 0.00023903200624512095,
      "loss": 0.1836,
      "step": 1208
    },
    {
      "epoch": 2.8297249853715623,
      "grad_norm": 0.5309516787528992,
      "learning_rate": 0.00023895394223263076,
      "loss": 0.201,
      "step": 1209
    },
    {
      "epoch": 2.8320655354008193,
      "grad_norm": 0.48662659525871277,
      "learning_rate": 0.00023887587822014048,
      "loss": 0.1908,
      "step": 1210
    },
    {
      "epoch": 2.834406085430076,
      "grad_norm": 0.5157158970832825,
      "learning_rate": 0.00023879781420765023,
      "loss": 0.1798,
      "step": 1211
    },
    {
      "epoch": 2.836746635459333,
      "grad_norm": 0.7626555562019348,
      "learning_rate": 0.00023871975019516,
      "loss": 0.2899,
      "step": 1212
    },
    {
      "epoch": 2.83908718548859,
      "grad_norm": 0.6312456727027893,
      "learning_rate": 0.00023864168618266976,
      "loss": 0.1719,
      "step": 1213
    },
    {
      "epoch": 2.8414277355178466,
      "grad_norm": 0.6275874972343445,
      "learning_rate": 0.0002385636221701795,
      "loss": 0.2622,
      "step": 1214
    },
    {
      "epoch": 2.8437682855471036,
      "grad_norm": 0.4905102550983429,
      "learning_rate": 0.0002384855581576893,
      "loss": 0.1687,
      "step": 1215
    },
    {
      "epoch": 2.84610883557636,
      "grad_norm": 0.5906692147254944,
      "learning_rate": 0.00023840749414519904,
      "loss": 0.2158,
      "step": 1216
    },
    {
      "epoch": 2.8484493856056172,
      "grad_norm": 0.41384372115135193,
      "learning_rate": 0.0002383294301327088,
      "loss": 0.1087,
      "step": 1217
    },
    {
      "epoch": 2.8507899356348743,
      "grad_norm": 0.5537968873977661,
      "learning_rate": 0.00023825136612021857,
      "loss": 0.2089,
      "step": 1218
    },
    {
      "epoch": 2.8531304856641313,
      "grad_norm": 0.6079428791999817,
      "learning_rate": 0.00023817330210772832,
      "loss": 0.1837,
      "step": 1219
    },
    {
      "epoch": 2.855471035693388,
      "grad_norm": 0.6076885461807251,
      "learning_rate": 0.00023809523809523807,
      "loss": 0.2041,
      "step": 1220
    },
    {
      "epoch": 2.857811585722645,
      "grad_norm": 0.5057610869407654,
      "learning_rate": 0.00023801717408274785,
      "loss": 0.2113,
      "step": 1221
    },
    {
      "epoch": 2.8601521357519015,
      "grad_norm": 0.6391615867614746,
      "learning_rate": 0.0002379391100702576,
      "loss": 0.2386,
      "step": 1222
    },
    {
      "epoch": 2.8624926857811586,
      "grad_norm": 0.7194927930831909,
      "learning_rate": 0.00023786104605776735,
      "loss": 0.2157,
      "step": 1223
    },
    {
      "epoch": 2.8648332358104156,
      "grad_norm": 0.5960976481437683,
      "learning_rate": 0.00023778298204527712,
      "loss": 0.19,
      "step": 1224
    },
    {
      "epoch": 2.867173785839672,
      "grad_norm": 0.5869261622428894,
      "learning_rate": 0.00023770491803278687,
      "loss": 0.2033,
      "step": 1225
    },
    {
      "epoch": 2.8695143358689292,
      "grad_norm": 0.5980136394500732,
      "learning_rate": 0.0002376268540202966,
      "loss": 0.1994,
      "step": 1226
    },
    {
      "epoch": 2.871854885898186,
      "grad_norm": 0.7171032428741455,
      "learning_rate": 0.0002375487900078064,
      "loss": 0.2324,
      "step": 1227
    },
    {
      "epoch": 2.874195435927443,
      "grad_norm": 0.7981472015380859,
      "learning_rate": 0.00023747072599531613,
      "loss": 0.1503,
      "step": 1228
    },
    {
      "epoch": 2.8765359859567,
      "grad_norm": 0.5462324023246765,
      "learning_rate": 0.00023739266198282588,
      "loss": 0.193,
      "step": 1229
    },
    {
      "epoch": 2.878876535985957,
      "grad_norm": 0.6286069750785828,
      "learning_rate": 0.00023731459797033565,
      "loss": 0.1806,
      "step": 1230
    },
    {
      "epoch": 2.8812170860152135,
      "grad_norm": 0.5017154216766357,
      "learning_rate": 0.0002372365339578454,
      "loss": 0.2135,
      "step": 1231
    },
    {
      "epoch": 2.8835576360444706,
      "grad_norm": 0.557508111000061,
      "learning_rate": 0.00023715846994535515,
      "loss": 0.2039,
      "step": 1232
    },
    {
      "epoch": 2.885898186073727,
      "grad_norm": 0.664644718170166,
      "learning_rate": 0.00023708040593286493,
      "loss": 0.1912,
      "step": 1233
    },
    {
      "epoch": 2.888238736102984,
      "grad_norm": 0.519435465335846,
      "learning_rate": 0.00023700234192037468,
      "loss": 0.2162,
      "step": 1234
    },
    {
      "epoch": 2.8905792861322412,
      "grad_norm": 0.6056510210037231,
      "learning_rate": 0.00023692427790788443,
      "loss": 0.233,
      "step": 1235
    },
    {
      "epoch": 2.892919836161498,
      "grad_norm": 0.7260335683822632,
      "learning_rate": 0.0002368462138953942,
      "loss": 0.2551,
      "step": 1236
    },
    {
      "epoch": 2.895260386190755,
      "grad_norm": 0.46436646580696106,
      "learning_rate": 0.00023676814988290396,
      "loss": 0.1644,
      "step": 1237
    },
    {
      "epoch": 2.8976009362200115,
      "grad_norm": 0.6439013481140137,
      "learning_rate": 0.0002366900858704137,
      "loss": 0.1785,
      "step": 1238
    },
    {
      "epoch": 2.8999414862492685,
      "grad_norm": 0.5602746605873108,
      "learning_rate": 0.0002366120218579235,
      "loss": 0.1379,
      "step": 1239
    },
    {
      "epoch": 2.9022820362785255,
      "grad_norm": 0.7848427891731262,
      "learning_rate": 0.00023653395784543324,
      "loss": 0.2213,
      "step": 1240
    },
    {
      "epoch": 2.9046225863077826,
      "grad_norm": 0.509490966796875,
      "learning_rate": 0.000236455893832943,
      "loss": 0.1607,
      "step": 1241
    },
    {
      "epoch": 2.906963136337039,
      "grad_norm": 0.45259296894073486,
      "learning_rate": 0.00023637782982045277,
      "loss": 0.1377,
      "step": 1242
    },
    {
      "epoch": 2.909303686366296,
      "grad_norm": 0.5790546536445618,
      "learning_rate": 0.00023629976580796252,
      "loss": 0.1952,
      "step": 1243
    },
    {
      "epoch": 2.911644236395553,
      "grad_norm": 0.4910257160663605,
      "learning_rate": 0.00023622170179547224,
      "loss": 0.1796,
      "step": 1244
    },
    {
      "epoch": 2.91398478642481,
      "grad_norm": 0.6360270380973816,
      "learning_rate": 0.00023614363778298202,
      "loss": 0.2516,
      "step": 1245
    },
    {
      "epoch": 2.916325336454067,
      "grad_norm": 0.7079678773880005,
      "learning_rate": 0.00023606557377049177,
      "loss": 0.2142,
      "step": 1246
    },
    {
      "epoch": 2.9186658864833235,
      "grad_norm": 0.46409496665000916,
      "learning_rate": 0.00023598750975800152,
      "loss": 0.1705,
      "step": 1247
    },
    {
      "epoch": 2.9210064365125805,
      "grad_norm": 0.6810775995254517,
      "learning_rate": 0.0002359094457455113,
      "loss": 0.2233,
      "step": 1248
    },
    {
      "epoch": 2.923346986541837,
      "grad_norm": 0.5215737819671631,
      "learning_rate": 0.00023583138173302105,
      "loss": 0.1457,
      "step": 1249
    },
    {
      "epoch": 2.925687536571094,
      "grad_norm": 0.471535325050354,
      "learning_rate": 0.0002357533177205308,
      "loss": 0.194,
      "step": 1250
    },
    {
      "epoch": 2.928028086600351,
      "grad_norm": 0.5822786092758179,
      "learning_rate": 0.00023567525370804057,
      "loss": 0.2513,
      "step": 1251
    },
    {
      "epoch": 2.9303686366296082,
      "grad_norm": 0.5844292640686035,
      "learning_rate": 0.00023559718969555033,
      "loss": 0.1921,
      "step": 1252
    },
    {
      "epoch": 2.932709186658865,
      "grad_norm": 0.5872007608413696,
      "learning_rate": 0.00023551912568306008,
      "loss": 0.2124,
      "step": 1253
    },
    {
      "epoch": 2.935049736688122,
      "grad_norm": 0.6890915632247925,
      "learning_rate": 0.00023544106167056985,
      "loss": 0.2347,
      "step": 1254
    },
    {
      "epoch": 2.9373902867173785,
      "grad_norm": 1.2237160205841064,
      "learning_rate": 0.0002353629976580796,
      "loss": 0.2649,
      "step": 1255
    },
    {
      "epoch": 2.9397308367466355,
      "grad_norm": 0.6909456849098206,
      "learning_rate": 0.00023528493364558935,
      "loss": 0.2292,
      "step": 1256
    },
    {
      "epoch": 2.9420713867758925,
      "grad_norm": 0.623920202255249,
      "learning_rate": 0.00023520686963309913,
      "loss": 0.2235,
      "step": 1257
    },
    {
      "epoch": 2.944411936805149,
      "grad_norm": 0.6392480134963989,
      "learning_rate": 0.00023512880562060888,
      "loss": 0.2098,
      "step": 1258
    },
    {
      "epoch": 2.946752486834406,
      "grad_norm": 0.5557429790496826,
      "learning_rate": 0.00023505074160811863,
      "loss": 0.2187,
      "step": 1259
    },
    {
      "epoch": 2.9490930368636628,
      "grad_norm": 0.596708357334137,
      "learning_rate": 0.0002349726775956284,
      "loss": 0.1742,
      "step": 1260
    },
    {
      "epoch": 2.95143358689292,
      "grad_norm": 0.7041770219802856,
      "learning_rate": 0.00023489461358313816,
      "loss": 0.241,
      "step": 1261
    },
    {
      "epoch": 2.953774136922177,
      "grad_norm": 0.5881533026695251,
      "learning_rate": 0.00023481654957064794,
      "loss": 0.2091,
      "step": 1262
    },
    {
      "epoch": 2.9561146869514334,
      "grad_norm": 0.5092427134513855,
      "learning_rate": 0.00023473848555815766,
      "loss": 0.2462,
      "step": 1263
    },
    {
      "epoch": 2.9584552369806905,
      "grad_norm": 0.5410852432250977,
      "learning_rate": 0.0002346604215456674,
      "loss": 0.2279,
      "step": 1264
    },
    {
      "epoch": 2.9607957870099475,
      "grad_norm": 0.45885157585144043,
      "learning_rate": 0.0002345823575331772,
      "loss": 0.1403,
      "step": 1265
    },
    {
      "epoch": 2.963136337039204,
      "grad_norm": 0.5435621738433838,
      "learning_rate": 0.00023450429352068694,
      "loss": 0.2174,
      "step": 1266
    },
    {
      "epoch": 2.965476887068461,
      "grad_norm": 0.5672848224639893,
      "learning_rate": 0.0002344262295081967,
      "loss": 0.2588,
      "step": 1267
    },
    {
      "epoch": 2.967817437097718,
      "grad_norm": 0.6171185970306396,
      "learning_rate": 0.00023434816549570647,
      "loss": 0.2288,
      "step": 1268
    },
    {
      "epoch": 2.9701579871269748,
      "grad_norm": 0.7087019085884094,
      "learning_rate": 0.00023427010148321622,
      "loss": 0.25,
      "step": 1269
    },
    {
      "epoch": 2.972498537156232,
      "grad_norm": 0.6590922474861145,
      "learning_rate": 0.00023419203747072597,
      "loss": 0.2793,
      "step": 1270
    },
    {
      "epoch": 2.9748390871854884,
      "grad_norm": 0.4534372389316559,
      "learning_rate": 0.00023411397345823575,
      "loss": 0.1527,
      "step": 1271
    },
    {
      "epoch": 2.9771796372147454,
      "grad_norm": 0.7592383027076721,
      "learning_rate": 0.0002340359094457455,
      "loss": 0.2531,
      "step": 1272
    },
    {
      "epoch": 2.9795201872440025,
      "grad_norm": 0.6710776686668396,
      "learning_rate": 0.00023395784543325525,
      "loss": 0.2343,
      "step": 1273
    },
    {
      "epoch": 2.981860737273259,
      "grad_norm": 0.6150983572006226,
      "learning_rate": 0.00023387978142076502,
      "loss": 0.1863,
      "step": 1274
    },
    {
      "epoch": 2.984201287302516,
      "grad_norm": 0.5954399704933167,
      "learning_rate": 0.00023380171740827477,
      "loss": 0.2163,
      "step": 1275
    },
    {
      "epoch": 2.9865418373317727,
      "grad_norm": 0.49767857789993286,
      "learning_rate": 0.00023372365339578452,
      "loss": 0.2021,
      "step": 1276
    },
    {
      "epoch": 2.9888823873610297,
      "grad_norm": 0.5821076035499573,
      "learning_rate": 0.0002336455893832943,
      "loss": 0.2124,
      "step": 1277
    },
    {
      "epoch": 2.9912229373902868,
      "grad_norm": 0.5345950126647949,
      "learning_rate": 0.00023356752537080405,
      "loss": 0.1808,
      "step": 1278
    },
    {
      "epoch": 2.993563487419544,
      "grad_norm": 0.608528196811676,
      "learning_rate": 0.0002334894613583138,
      "loss": 0.1617,
      "step": 1279
    },
    {
      "epoch": 2.9959040374488004,
      "grad_norm": 0.6931644082069397,
      "learning_rate": 0.00023341139734582358,
      "loss": 0.2162,
      "step": 1280
    },
    {
      "epoch": 2.9982445874780574,
      "grad_norm": 0.49315568804740906,
      "learning_rate": 0.0002333333333333333,
      "loss": 0.1766,
      "step": 1281
    },
    {
      "epoch": 2.9982445874780574,
      "eval_loss": 0.26117026805877686,
      "eval_runtime": 128.36,
      "eval_samples_per_second": 4.3,
      "eval_steps_per_second": 0.538,
      "step": 1281
    },
    {
      "epoch": 3.000585137507314,
      "grad_norm": 0.5800526142120361,
      "learning_rate": 0.00023325526932084305,
      "loss": 0.1429,
      "step": 1282
    },
    {
      "epoch": 3.002925687536571,
      "grad_norm": 0.41838687658309937,
      "learning_rate": 0.00023317720530835283,
      "loss": 0.1659,
      "step": 1283
    },
    {
      "epoch": 3.005266237565828,
      "grad_norm": 0.7760977149009705,
      "learning_rate": 0.00023309914129586258,
      "loss": 0.1596,
      "step": 1284
    },
    {
      "epoch": 3.0076067875950847,
      "grad_norm": 0.6656216979026794,
      "learning_rate": 0.00023302107728337233,
      "loss": 0.1976,
      "step": 1285
    },
    {
      "epoch": 3.0099473376243417,
      "grad_norm": 0.6415850520133972,
      "learning_rate": 0.0002329430132708821,
      "loss": 0.1941,
      "step": 1286
    },
    {
      "epoch": 3.012287887653599,
      "grad_norm": 0.6372498869895935,
      "learning_rate": 0.00023286494925839186,
      "loss": 0.1745,
      "step": 1287
    },
    {
      "epoch": 3.0146284376828554,
      "grad_norm": 0.498786985874176,
      "learning_rate": 0.0002327868852459016,
      "loss": 0.149,
      "step": 1288
    },
    {
      "epoch": 3.0169689877121124,
      "grad_norm": 0.7183017134666443,
      "learning_rate": 0.0002327088212334114,
      "loss": 0.2135,
      "step": 1289
    },
    {
      "epoch": 3.019309537741369,
      "grad_norm": 0.5497326850891113,
      "learning_rate": 0.00023263075722092114,
      "loss": 0.1943,
      "step": 1290
    },
    {
      "epoch": 3.021650087770626,
      "grad_norm": 0.6952658295631409,
      "learning_rate": 0.0002325526932084309,
      "loss": 0.2,
      "step": 1291
    },
    {
      "epoch": 3.023990637799883,
      "grad_norm": 0.825581967830658,
      "learning_rate": 0.00023247462919594067,
      "loss": 0.1798,
      "step": 1292
    },
    {
      "epoch": 3.0263311878291397,
      "grad_norm": 0.6282232403755188,
      "learning_rate": 0.00023239656518345042,
      "loss": 0.159,
      "step": 1293
    },
    {
      "epoch": 3.0286717378583967,
      "grad_norm": 0.5203448534011841,
      "learning_rate": 0.00023231850117096017,
      "loss": 0.1299,
      "step": 1294
    },
    {
      "epoch": 3.0310122878876538,
      "grad_norm": 0.7004000544548035,
      "learning_rate": 0.00023224043715846995,
      "loss": 0.1747,
      "step": 1295
    },
    {
      "epoch": 3.0333528379169103,
      "grad_norm": 0.8134536147117615,
      "learning_rate": 0.0002321623731459797,
      "loss": 0.146,
      "step": 1296
    },
    {
      "epoch": 3.0356933879461674,
      "grad_norm": 0.47460293769836426,
      "learning_rate": 0.00023208430913348942,
      "loss": 0.1532,
      "step": 1297
    },
    {
      "epoch": 3.0380339379754244,
      "grad_norm": 0.6863190531730652,
      "learning_rate": 0.00023200624512099922,
      "loss": 0.1979,
      "step": 1298
    },
    {
      "epoch": 3.040374488004681,
      "grad_norm": 0.5123138427734375,
      "learning_rate": 0.00023192818110850895,
      "loss": 0.1387,
      "step": 1299
    },
    {
      "epoch": 3.042715038033938,
      "grad_norm": 0.5128079056739807,
      "learning_rate": 0.0002318501170960187,
      "loss": 0.1189,
      "step": 1300
    },
    {
      "epoch": 3.0450555880631947,
      "grad_norm": 0.6429733633995056,
      "learning_rate": 0.00023177205308352847,
      "loss": 0.1975,
      "step": 1301
    },
    {
      "epoch": 3.0473961380924517,
      "grad_norm": 0.6890996694564819,
      "learning_rate": 0.00023169398907103823,
      "loss": 0.1844,
      "step": 1302
    },
    {
      "epoch": 3.0497366881217087,
      "grad_norm": 0.6831597089767456,
      "learning_rate": 0.00023161592505854798,
      "loss": 0.158,
      "step": 1303
    },
    {
      "epoch": 3.0520772381509653,
      "grad_norm": 0.47429388761520386,
      "learning_rate": 0.00023153786104605775,
      "loss": 0.1182,
      "step": 1304
    },
    {
      "epoch": 3.0544177881802224,
      "grad_norm": 0.635824978351593,
      "learning_rate": 0.0002314597970335675,
      "loss": 0.2089,
      "step": 1305
    },
    {
      "epoch": 3.0567583382094794,
      "grad_norm": 0.5654268860816956,
      "learning_rate": 0.00023138173302107725,
      "loss": 0.1739,
      "step": 1306
    },
    {
      "epoch": 3.059098888238736,
      "grad_norm": 0.7100324034690857,
      "learning_rate": 0.00023130366900858703,
      "loss": 0.2192,
      "step": 1307
    },
    {
      "epoch": 3.061439438267993,
      "grad_norm": 0.6158837080001831,
      "learning_rate": 0.00023122560499609678,
      "loss": 0.1406,
      "step": 1308
    },
    {
      "epoch": 3.06377998829725,
      "grad_norm": 0.38814136385917664,
      "learning_rate": 0.00023114754098360653,
      "loss": 0.1203,
      "step": 1309
    },
    {
      "epoch": 3.0661205383265067,
      "grad_norm": 0.5084823966026306,
      "learning_rate": 0.0002310694769711163,
      "loss": 0.1636,
      "step": 1310
    },
    {
      "epoch": 3.0684610883557637,
      "grad_norm": 0.556287944316864,
      "learning_rate": 0.00023099141295862606,
      "loss": 0.1961,
      "step": 1311
    },
    {
      "epoch": 3.0708016383850203,
      "grad_norm": 0.4499957263469696,
      "learning_rate": 0.0002309133489461358,
      "loss": 0.1533,
      "step": 1312
    },
    {
      "epoch": 3.0731421884142773,
      "grad_norm": 0.4619664251804352,
      "learning_rate": 0.0002308352849336456,
      "loss": 0.1186,
      "step": 1313
    },
    {
      "epoch": 3.0754827384435344,
      "grad_norm": 0.6165702939033508,
      "learning_rate": 0.00023075722092115534,
      "loss": 0.1899,
      "step": 1314
    },
    {
      "epoch": 3.077823288472791,
      "grad_norm": 0.7257070541381836,
      "learning_rate": 0.00023067915690866506,
      "loss": 0.1649,
      "step": 1315
    },
    {
      "epoch": 3.080163838502048,
      "grad_norm": 0.5415969491004944,
      "learning_rate": 0.00023060109289617487,
      "loss": 0.1726,
      "step": 1316
    },
    {
      "epoch": 3.082504388531305,
      "grad_norm": 0.5198619365692139,
      "learning_rate": 0.0002305230288836846,
      "loss": 0.1575,
      "step": 1317
    },
    {
      "epoch": 3.0848449385605616,
      "grad_norm": 0.5553296804428101,
      "learning_rate": 0.00023044496487119434,
      "loss": 0.1461,
      "step": 1318
    },
    {
      "epoch": 3.0871854885898187,
      "grad_norm": 0.777290940284729,
      "learning_rate": 0.00023036690085870412,
      "loss": 0.2335,
      "step": 1319
    },
    {
      "epoch": 3.0895260386190753,
      "grad_norm": 0.36522984504699707,
      "learning_rate": 0.00023028883684621387,
      "loss": 0.1083,
      "step": 1320
    },
    {
      "epoch": 3.0918665886483323,
      "grad_norm": 0.6469545364379883,
      "learning_rate": 0.00023021077283372362,
      "loss": 0.1614,
      "step": 1321
    },
    {
      "epoch": 3.0942071386775893,
      "grad_norm": 0.6828714609146118,
      "learning_rate": 0.0002301327088212334,
      "loss": 0.1202,
      "step": 1322
    },
    {
      "epoch": 3.096547688706846,
      "grad_norm": 0.5578635334968567,
      "learning_rate": 0.00023005464480874315,
      "loss": 0.1647,
      "step": 1323
    },
    {
      "epoch": 3.098888238736103,
      "grad_norm": 0.7730097770690918,
      "learning_rate": 0.0002299765807962529,
      "loss": 0.1978,
      "step": 1324
    },
    {
      "epoch": 3.10122878876536,
      "grad_norm": 0.8394127488136292,
      "learning_rate": 0.00022989851678376267,
      "loss": 0.1531,
      "step": 1325
    },
    {
      "epoch": 3.1035693387946166,
      "grad_norm": 0.8417275547981262,
      "learning_rate": 0.00022982045277127242,
      "loss": 0.225,
      "step": 1326
    },
    {
      "epoch": 3.1059098888238736,
      "grad_norm": 0.6003636121749878,
      "learning_rate": 0.00022974238875878218,
      "loss": 0.1511,
      "step": 1327
    },
    {
      "epoch": 3.1082504388531307,
      "grad_norm": 0.4870753586292267,
      "learning_rate": 0.00022966432474629195,
      "loss": 0.157,
      "step": 1328
    },
    {
      "epoch": 3.1105909888823873,
      "grad_norm": 0.743347704410553,
      "learning_rate": 0.0002295862607338017,
      "loss": 0.2009,
      "step": 1329
    },
    {
      "epoch": 3.1129315389116443,
      "grad_norm": 0.6434395909309387,
      "learning_rate": 0.00022950819672131145,
      "loss": 0.1786,
      "step": 1330
    },
    {
      "epoch": 3.115272088940901,
      "grad_norm": 0.6245706677436829,
      "learning_rate": 0.00022943013270882123,
      "loss": 0.15,
      "step": 1331
    },
    {
      "epoch": 3.117612638970158,
      "grad_norm": 0.922618567943573,
      "learning_rate": 0.00022935206869633098,
      "loss": 0.2189,
      "step": 1332
    },
    {
      "epoch": 3.119953188999415,
      "grad_norm": 0.46539250016212463,
      "learning_rate": 0.0002292740046838407,
      "loss": 0.1386,
      "step": 1333
    },
    {
      "epoch": 3.1222937390286716,
      "grad_norm": 0.5641117095947266,
      "learning_rate": 0.0002291959406713505,
      "loss": 0.1714,
      "step": 1334
    },
    {
      "epoch": 3.1246342890579286,
      "grad_norm": 0.801774799823761,
      "learning_rate": 0.00022911787665886023,
      "loss": 0.223,
      "step": 1335
    },
    {
      "epoch": 3.1269748390871857,
      "grad_norm": 0.7603920698165894,
      "learning_rate": 0.00022903981264636998,
      "loss": 0.2434,
      "step": 1336
    },
    {
      "epoch": 3.1293153891164422,
      "grad_norm": 0.47130176424980164,
      "learning_rate": 0.00022896174863387976,
      "loss": 0.1589,
      "step": 1337
    },
    {
      "epoch": 3.1316559391456993,
      "grad_norm": 0.5954605340957642,
      "learning_rate": 0.0002288836846213895,
      "loss": 0.2012,
      "step": 1338
    },
    {
      "epoch": 3.1339964891749563,
      "grad_norm": 0.6090272068977356,
      "learning_rate": 0.00022880562060889926,
      "loss": 0.1864,
      "step": 1339
    },
    {
      "epoch": 3.136337039204213,
      "grad_norm": 0.6025677919387817,
      "learning_rate": 0.00022872755659640904,
      "loss": 0.1828,
      "step": 1340
    },
    {
      "epoch": 3.13867758923347,
      "grad_norm": 0.6323486566543579,
      "learning_rate": 0.0002286494925839188,
      "loss": 0.1859,
      "step": 1341
    },
    {
      "epoch": 3.1410181392627265,
      "grad_norm": 0.6015905141830444,
      "learning_rate": 0.00022857142857142854,
      "loss": 0.1707,
      "step": 1342
    },
    {
      "epoch": 3.1433586892919836,
      "grad_norm": 0.6419717073440552,
      "learning_rate": 0.00022849336455893832,
      "loss": 0.1405,
      "step": 1343
    },
    {
      "epoch": 3.1456992393212406,
      "grad_norm": 0.7440172433853149,
      "learning_rate": 0.00022841530054644807,
      "loss": 0.1833,
      "step": 1344
    },
    {
      "epoch": 3.148039789350497,
      "grad_norm": 0.5189486145973206,
      "learning_rate": 0.00022833723653395782,
      "loss": 0.1438,
      "step": 1345
    },
    {
      "epoch": 3.1503803393797543,
      "grad_norm": 0.6803268194198608,
      "learning_rate": 0.0002282591725214676,
      "loss": 0.2283,
      "step": 1346
    },
    {
      "epoch": 3.1527208894090113,
      "grad_norm": 0.8236263394355774,
      "learning_rate": 0.00022818110850897735,
      "loss": 0.2221,
      "step": 1347
    },
    {
      "epoch": 3.155061439438268,
      "grad_norm": 0.5766244530677795,
      "learning_rate": 0.0002281030444964871,
      "loss": 0.1182,
      "step": 1348
    },
    {
      "epoch": 3.157401989467525,
      "grad_norm": 0.6829506158828735,
      "learning_rate": 0.00022802498048399687,
      "loss": 0.1974,
      "step": 1349
    },
    {
      "epoch": 3.159742539496782,
      "grad_norm": 0.6533176898956299,
      "learning_rate": 0.00022794691647150662,
      "loss": 0.1685,
      "step": 1350
    },
    {
      "epoch": 3.1620830895260386,
      "grad_norm": 0.7685233950614929,
      "learning_rate": 0.00022786885245901635,
      "loss": 0.1989,
      "step": 1351
    },
    {
      "epoch": 3.1644236395552956,
      "grad_norm": 0.7271565198898315,
      "learning_rate": 0.00022779078844652615,
      "loss": 0.2236,
      "step": 1352
    },
    {
      "epoch": 3.166764189584552,
      "grad_norm": 0.6897749304771423,
      "learning_rate": 0.00022771272443403588,
      "loss": 0.1667,
      "step": 1353
    },
    {
      "epoch": 3.1691047396138092,
      "grad_norm": 0.7050999999046326,
      "learning_rate": 0.00022763466042154563,
      "loss": 0.1984,
      "step": 1354
    },
    {
      "epoch": 3.1714452896430663,
      "grad_norm": 0.6516498327255249,
      "learning_rate": 0.0002275565964090554,
      "loss": 0.1716,
      "step": 1355
    },
    {
      "epoch": 3.173785839672323,
      "grad_norm": 0.5621217489242554,
      "learning_rate": 0.00022747853239656515,
      "loss": 0.1044,
      "step": 1356
    },
    {
      "epoch": 3.17612638970158,
      "grad_norm": 0.5505291223526001,
      "learning_rate": 0.0002274004683840749,
      "loss": 0.169,
      "step": 1357
    },
    {
      "epoch": 3.178466939730837,
      "grad_norm": 0.6886973977088928,
      "learning_rate": 0.00022732240437158468,
      "loss": 0.1843,
      "step": 1358
    },
    {
      "epoch": 3.1808074897600935,
      "grad_norm": 0.6168743371963501,
      "learning_rate": 0.00022724434035909443,
      "loss": 0.1965,
      "step": 1359
    },
    {
      "epoch": 3.1831480397893506,
      "grad_norm": 0.497203528881073,
      "learning_rate": 0.00022716627634660418,
      "loss": 0.1655,
      "step": 1360
    },
    {
      "epoch": 3.1854885898186076,
      "grad_norm": 0.7400798201560974,
      "learning_rate": 0.00022708821233411396,
      "loss": 0.2178,
      "step": 1361
    },
    {
      "epoch": 3.187829139847864,
      "grad_norm": 0.7204070091247559,
      "learning_rate": 0.0002270101483216237,
      "loss": 0.1776,
      "step": 1362
    },
    {
      "epoch": 3.1901696898771212,
      "grad_norm": 0.6366153359413147,
      "learning_rate": 0.00022693208430913346,
      "loss": 0.1274,
      "step": 1363
    },
    {
      "epoch": 3.192510239906378,
      "grad_norm": 0.8705403208732605,
      "learning_rate": 0.00022685402029664324,
      "loss": 0.1626,
      "step": 1364
    },
    {
      "epoch": 3.194850789935635,
      "grad_norm": 0.5921887755393982,
      "learning_rate": 0.000226775956284153,
      "loss": 0.1732,
      "step": 1365
    },
    {
      "epoch": 3.197191339964892,
      "grad_norm": 0.6283929944038391,
      "learning_rate": 0.00022669789227166274,
      "loss": 0.1866,
      "step": 1366
    },
    {
      "epoch": 3.1995318899941485,
      "grad_norm": 0.6237258911132812,
      "learning_rate": 0.00022661982825917252,
      "loss": 0.1839,
      "step": 1367
    },
    {
      "epoch": 3.2018724400234055,
      "grad_norm": 0.5582988858222961,
      "learning_rate": 0.00022654176424668227,
      "loss": 0.1493,
      "step": 1368
    },
    {
      "epoch": 3.2042129900526626,
      "grad_norm": 0.5091052055358887,
      "learning_rate": 0.000226463700234192,
      "loss": 0.1537,
      "step": 1369
    },
    {
      "epoch": 3.206553540081919,
      "grad_norm": 0.6662542223930359,
      "learning_rate": 0.0002263856362217018,
      "loss": 0.1588,
      "step": 1370
    },
    {
      "epoch": 3.208894090111176,
      "grad_norm": 0.5436694025993347,
      "learning_rate": 0.00022630757220921152,
      "loss": 0.1742,
      "step": 1371
    },
    {
      "epoch": 3.211234640140433,
      "grad_norm": 0.48271840810775757,
      "learning_rate": 0.00022622950819672127,
      "loss": 0.1607,
      "step": 1372
    },
    {
      "epoch": 3.21357519016969,
      "grad_norm": 0.6767146587371826,
      "learning_rate": 0.00022615144418423105,
      "loss": 0.2229,
      "step": 1373
    },
    {
      "epoch": 3.215915740198947,
      "grad_norm": 0.6384451985359192,
      "learning_rate": 0.0002260733801717408,
      "loss": 0.1709,
      "step": 1374
    },
    {
      "epoch": 3.2182562902282035,
      "grad_norm": 0.5653002858161926,
      "learning_rate": 0.00022599531615925055,
      "loss": 0.1672,
      "step": 1375
    },
    {
      "epoch": 3.2205968402574605,
      "grad_norm": 0.5712752342224121,
      "learning_rate": 0.00022591725214676032,
      "loss": 0.1583,
      "step": 1376
    },
    {
      "epoch": 3.2229373902867176,
      "grad_norm": 0.6481325030326843,
      "learning_rate": 0.00022583918813427008,
      "loss": 0.2143,
      "step": 1377
    },
    {
      "epoch": 3.225277940315974,
      "grad_norm": 0.6547248363494873,
      "learning_rate": 0.00022576112412177983,
      "loss": 0.2164,
      "step": 1378
    },
    {
      "epoch": 3.227618490345231,
      "grad_norm": 0.6581783294677734,
      "learning_rate": 0.0002256830601092896,
      "loss": 0.2143,
      "step": 1379
    },
    {
      "epoch": 3.2299590403744878,
      "grad_norm": 0.5070552825927734,
      "learning_rate": 0.00022560499609679935,
      "loss": 0.1201,
      "step": 1380
    },
    {
      "epoch": 3.232299590403745,
      "grad_norm": 0.6533962488174438,
      "learning_rate": 0.0002255269320843091,
      "loss": 0.1577,
      "step": 1381
    },
    {
      "epoch": 3.234640140433002,
      "grad_norm": 0.6204380393028259,
      "learning_rate": 0.00022544886807181888,
      "loss": 0.1482,
      "step": 1382
    },
    {
      "epoch": 3.2369806904622584,
      "grad_norm": 0.6946876645088196,
      "learning_rate": 0.00022537080405932863,
      "loss": 0.195,
      "step": 1383
    },
    {
      "epoch": 3.2393212404915155,
      "grad_norm": 0.6551868915557861,
      "learning_rate": 0.00022529274004683838,
      "loss": 0.2177,
      "step": 1384
    },
    {
      "epoch": 3.2416617905207725,
      "grad_norm": 0.7245994210243225,
      "learning_rate": 0.00022521467603434816,
      "loss": 0.1765,
      "step": 1385
    },
    {
      "epoch": 3.244002340550029,
      "grad_norm": 0.7422822713851929,
      "learning_rate": 0.0002251366120218579,
      "loss": 0.1682,
      "step": 1386
    },
    {
      "epoch": 3.246342890579286,
      "grad_norm": 0.7222872376441956,
      "learning_rate": 0.00022505854800936763,
      "loss": 0.1429,
      "step": 1387
    },
    {
      "epoch": 3.248683440608543,
      "grad_norm": 0.5641016960144043,
      "learning_rate": 0.00022498048399687744,
      "loss": 0.1607,
      "step": 1388
    },
    {
      "epoch": 3.2510239906378,
      "grad_norm": 0.6675763726234436,
      "learning_rate": 0.00022490241998438716,
      "loss": 0.2216,
      "step": 1389
    },
    {
      "epoch": 3.253364540667057,
      "grad_norm": 0.5920748710632324,
      "learning_rate": 0.00022482435597189694,
      "loss": 0.1835,
      "step": 1390
    },
    {
      "epoch": 3.2557050906963134,
      "grad_norm": 0.6629338264465332,
      "learning_rate": 0.0002247462919594067,
      "loss": 0.1245,
      "step": 1391
    },
    {
      "epoch": 3.2580456407255705,
      "grad_norm": 0.5395615696907043,
      "learning_rate": 0.00022466822794691644,
      "loss": 0.1334,
      "step": 1392
    },
    {
      "epoch": 3.2603861907548275,
      "grad_norm": 1.038041591644287,
      "learning_rate": 0.00022459016393442622,
      "loss": 0.2697,
      "step": 1393
    },
    {
      "epoch": 3.262726740784084,
      "grad_norm": 0.6979227066040039,
      "learning_rate": 0.00022451209992193597,
      "loss": 0.1787,
      "step": 1394
    },
    {
      "epoch": 3.265067290813341,
      "grad_norm": 0.6428722143173218,
      "learning_rate": 0.00022443403590944572,
      "loss": 0.1896,
      "step": 1395
    },
    {
      "epoch": 3.267407840842598,
      "grad_norm": 0.7296664714813232,
      "learning_rate": 0.0002243559718969555,
      "loss": 0.2022,
      "step": 1396
    },
    {
      "epoch": 3.2697483908718548,
      "grad_norm": 0.8246878981590271,
      "learning_rate": 0.00022427790788446525,
      "loss": 0.1936,
      "step": 1397
    },
    {
      "epoch": 3.272088940901112,
      "grad_norm": 0.6817263960838318,
      "learning_rate": 0.000224199843871975,
      "loss": 0.2009,
      "step": 1398
    },
    {
      "epoch": 3.274429490930369,
      "grad_norm": 0.7456212043762207,
      "learning_rate": 0.00022412177985948477,
      "loss": 0.1732,
      "step": 1399
    },
    {
      "epoch": 3.2767700409596254,
      "grad_norm": 0.5755385160446167,
      "learning_rate": 0.00022404371584699452,
      "loss": 0.1531,
      "step": 1400
    },
    {
      "epoch": 3.2791105909888825,
      "grad_norm": 0.47848984599113464,
      "learning_rate": 0.00022396565183450427,
      "loss": 0.1515,
      "step": 1401
    },
    {
      "epoch": 3.281451141018139,
      "grad_norm": 0.701788604259491,
      "learning_rate": 0.00022388758782201405,
      "loss": 0.2308,
      "step": 1402
    },
    {
      "epoch": 3.283791691047396,
      "grad_norm": 0.5025097727775574,
      "learning_rate": 0.0002238095238095238,
      "loss": 0.1347,
      "step": 1403
    },
    {
      "epoch": 3.286132241076653,
      "grad_norm": 0.5262550115585327,
      "learning_rate": 0.00022373145979703355,
      "loss": 0.167,
      "step": 1404
    },
    {
      "epoch": 3.2884727911059097,
      "grad_norm": 0.544891893863678,
      "learning_rate": 0.00022365339578454333,
      "loss": 0.1444,
      "step": 1405
    },
    {
      "epoch": 3.2908133411351668,
      "grad_norm": 0.6052914261817932,
      "learning_rate": 0.00022357533177205308,
      "loss": 0.1694,
      "step": 1406
    },
    {
      "epoch": 3.293153891164424,
      "grad_norm": 0.6863828301429749,
      "learning_rate": 0.0002234972677595628,
      "loss": 0.2343,
      "step": 1407
    },
    {
      "epoch": 3.2954944411936804,
      "grad_norm": 0.8260394930839539,
      "learning_rate": 0.00022341920374707258,
      "loss": 0.1529,
      "step": 1408
    },
    {
      "epoch": 3.2978349912229374,
      "grad_norm": 0.6990976929664612,
      "learning_rate": 0.00022334113973458233,
      "loss": 0.1956,
      "step": 1409
    },
    {
      "epoch": 3.3001755412521945,
      "grad_norm": 0.594677209854126,
      "learning_rate": 0.00022326307572209208,
      "loss": 0.1579,
      "step": 1410
    },
    {
      "epoch": 3.302516091281451,
      "grad_norm": 0.6768714189529419,
      "learning_rate": 0.00022318501170960186,
      "loss": 0.1925,
      "step": 1411
    },
    {
      "epoch": 3.304856641310708,
      "grad_norm": 0.6392573714256287,
      "learning_rate": 0.0002231069476971116,
      "loss": 0.2337,
      "step": 1412
    },
    {
      "epoch": 3.3071971913399647,
      "grad_norm": 0.5823093056678772,
      "learning_rate": 0.00022302888368462136,
      "loss": 0.1896,
      "step": 1413
    },
    {
      "epoch": 3.3095377413692217,
      "grad_norm": 0.4731728136539459,
      "learning_rate": 0.00022295081967213114,
      "loss": 0.1632,
      "step": 1414
    },
    {
      "epoch": 3.311878291398479,
      "grad_norm": 0.5545098185539246,
      "learning_rate": 0.0002228727556596409,
      "loss": 0.1584,
      "step": 1415
    },
    {
      "epoch": 3.3142188414277354,
      "grad_norm": 0.5607774257659912,
      "learning_rate": 0.00022279469164715064,
      "loss": 0.1486,
      "step": 1416
    },
    {
      "epoch": 3.3165593914569924,
      "grad_norm": 0.540725588798523,
      "learning_rate": 0.00022271662763466042,
      "loss": 0.1283,
      "step": 1417
    },
    {
      "epoch": 3.3188999414862494,
      "grad_norm": 0.5211367011070251,
      "learning_rate": 0.00022263856362217017,
      "loss": 0.1522,
      "step": 1418
    },
    {
      "epoch": 3.321240491515506,
      "grad_norm": 0.5445390939712524,
      "learning_rate": 0.00022256049960967992,
      "loss": 0.2058,
      "step": 1419
    },
    {
      "epoch": 3.323581041544763,
      "grad_norm": 0.5021021366119385,
      "learning_rate": 0.0002224824355971897,
      "loss": 0.1553,
      "step": 1420
    },
    {
      "epoch": 3.32592159157402,
      "grad_norm": 0.5392759442329407,
      "learning_rate": 0.00022240437158469945,
      "loss": 0.143,
      "step": 1421
    },
    {
      "epoch": 3.3282621416032767,
      "grad_norm": 0.8330765962600708,
      "learning_rate": 0.0002223263075722092,
      "loss": 0.2016,
      "step": 1422
    },
    {
      "epoch": 3.3306026916325338,
      "grad_norm": 0.673717737197876,
      "learning_rate": 0.00022224824355971897,
      "loss": 0.1559,
      "step": 1423
    },
    {
      "epoch": 3.3329432416617903,
      "grad_norm": 0.6247749924659729,
      "learning_rate": 0.0002221701795472287,
      "loss": 0.1948,
      "step": 1424
    },
    {
      "epoch": 3.3352837916910474,
      "grad_norm": 0.7931548953056335,
      "learning_rate": 0.00022209211553473845,
      "loss": 0.2313,
      "step": 1425
    },
    {
      "epoch": 3.3376243417203044,
      "grad_norm": 0.7387451529502869,
      "learning_rate": 0.00022201405152224822,
      "loss": 0.1463,
      "step": 1426
    },
    {
      "epoch": 3.339964891749561,
      "grad_norm": 0.5308867692947388,
      "learning_rate": 0.00022193598750975798,
      "loss": 0.1666,
      "step": 1427
    },
    {
      "epoch": 3.342305441778818,
      "grad_norm": 0.5561037659645081,
      "learning_rate": 0.00022185792349726773,
      "loss": 0.1715,
      "step": 1428
    },
    {
      "epoch": 3.344645991808075,
      "grad_norm": 0.6119531989097595,
      "learning_rate": 0.0002217798594847775,
      "loss": 0.1454,
      "step": 1429
    },
    {
      "epoch": 3.3469865418373317,
      "grad_norm": 0.524545431137085,
      "learning_rate": 0.00022170179547228725,
      "loss": 0.1373,
      "step": 1430
    },
    {
      "epoch": 3.3493270918665887,
      "grad_norm": 0.5362716913223267,
      "learning_rate": 0.000221623731459797,
      "loss": 0.1644,
      "step": 1431
    },
    {
      "epoch": 3.3516676418958458,
      "grad_norm": 0.49685198068618774,
      "learning_rate": 0.00022154566744730678,
      "loss": 0.153,
      "step": 1432
    },
    {
      "epoch": 3.3540081919251024,
      "grad_norm": 0.6386570930480957,
      "learning_rate": 0.00022146760343481653,
      "loss": 0.196,
      "step": 1433
    },
    {
      "epoch": 3.3563487419543594,
      "grad_norm": 0.598320722579956,
      "learning_rate": 0.00022138953942232628,
      "loss": 0.1567,
      "step": 1434
    },
    {
      "epoch": 3.358689291983616,
      "grad_norm": 0.5893160700798035,
      "learning_rate": 0.00022131147540983606,
      "loss": 0.1486,
      "step": 1435
    },
    {
      "epoch": 3.361029842012873,
      "grad_norm": 0.5868298411369324,
      "learning_rate": 0.0002212334113973458,
      "loss": 0.1715,
      "step": 1436
    },
    {
      "epoch": 3.36337039204213,
      "grad_norm": 0.7861813902854919,
      "learning_rate": 0.00022115534738485556,
      "loss": 0.2303,
      "step": 1437
    },
    {
      "epoch": 3.3657109420713867,
      "grad_norm": 0.7482436299324036,
      "learning_rate": 0.00022107728337236534,
      "loss": 0.2549,
      "step": 1438
    },
    {
      "epoch": 3.3680514921006437,
      "grad_norm": 0.5719582438468933,
      "learning_rate": 0.0002209992193598751,
      "loss": 0.1513,
      "step": 1439
    },
    {
      "epoch": 3.3703920421299003,
      "grad_norm": 0.6021864414215088,
      "learning_rate": 0.00022092115534738484,
      "loss": 0.1773,
      "step": 1440
    },
    {
      "epoch": 3.3727325921591573,
      "grad_norm": 0.4032193124294281,
      "learning_rate": 0.00022084309133489462,
      "loss": 0.1128,
      "step": 1441
    },
    {
      "epoch": 3.3750731421884144,
      "grad_norm": 0.4890616834163666,
      "learning_rate": 0.00022076502732240434,
      "loss": 0.1506,
      "step": 1442
    },
    {
      "epoch": 3.377413692217671,
      "grad_norm": 0.6948811411857605,
      "learning_rate": 0.0002206869633099141,
      "loss": 0.2419,
      "step": 1443
    },
    {
      "epoch": 3.379754242246928,
      "grad_norm": 0.4419439136981964,
      "learning_rate": 0.00022060889929742387,
      "loss": 0.1511,
      "step": 1444
    },
    {
      "epoch": 3.382094792276185,
      "grad_norm": 0.6519325375556946,
      "learning_rate": 0.00022053083528493362,
      "loss": 0.1641,
      "step": 1445
    },
    {
      "epoch": 3.3844353423054416,
      "grad_norm": 0.6245406866073608,
      "learning_rate": 0.00022045277127244337,
      "loss": 0.1467,
      "step": 1446
    },
    {
      "epoch": 3.3867758923346987,
      "grad_norm": 0.7779908180236816,
      "learning_rate": 0.00022037470725995315,
      "loss": 0.1741,
      "step": 1447
    },
    {
      "epoch": 3.3891164423639557,
      "grad_norm": 0.671749472618103,
      "learning_rate": 0.0002202966432474629,
      "loss": 0.1966,
      "step": 1448
    },
    {
      "epoch": 3.3914569923932123,
      "grad_norm": 0.839142918586731,
      "learning_rate": 0.00022021857923497265,
      "loss": 0.1583,
      "step": 1449
    },
    {
      "epoch": 3.3937975424224693,
      "grad_norm": 0.7363424897193909,
      "learning_rate": 0.00022014051522248242,
      "loss": 0.1812,
      "step": 1450
    },
    {
      "epoch": 3.396138092451726,
      "grad_norm": 0.7371634840965271,
      "learning_rate": 0.00022006245120999217,
      "loss": 0.1781,
      "step": 1451
    },
    {
      "epoch": 3.398478642480983,
      "grad_norm": 0.5832276344299316,
      "learning_rate": 0.00021998438719750193,
      "loss": 0.144,
      "step": 1452
    },
    {
      "epoch": 3.40081919251024,
      "grad_norm": 0.47440826892852783,
      "learning_rate": 0.0002199063231850117,
      "loss": 0.1562,
      "step": 1453
    },
    {
      "epoch": 3.4031597425394966,
      "grad_norm": 0.6579403877258301,
      "learning_rate": 0.00021982825917252145,
      "loss": 0.1934,
      "step": 1454
    },
    {
      "epoch": 3.4055002925687536,
      "grad_norm": 0.6441361308097839,
      "learning_rate": 0.0002197501951600312,
      "loss": 0.1831,
      "step": 1455
    },
    {
      "epoch": 3.4078408425980107,
      "grad_norm": 0.64493727684021,
      "learning_rate": 0.00021967213114754098,
      "loss": 0.1732,
      "step": 1456
    },
    {
      "epoch": 3.4101813926272673,
      "grad_norm": 0.6850014328956604,
      "learning_rate": 0.00021959406713505073,
      "loss": 0.1445,
      "step": 1457
    },
    {
      "epoch": 3.4125219426565243,
      "grad_norm": 0.7391971945762634,
      "learning_rate": 0.00021951600312256048,
      "loss": 0.1779,
      "step": 1458
    },
    {
      "epoch": 3.4148624926857813,
      "grad_norm": 0.952262818813324,
      "learning_rate": 0.00021943793911007026,
      "loss": 0.2354,
      "step": 1459
    },
    {
      "epoch": 3.417203042715038,
      "grad_norm": 0.6316167116165161,
      "learning_rate": 0.00021935987509757998,
      "loss": 0.1616,
      "step": 1460
    },
    {
      "epoch": 3.419543592744295,
      "grad_norm": 0.48781633377075195,
      "learning_rate": 0.00021928181108508973,
      "loss": 0.1267,
      "step": 1461
    },
    {
      "epoch": 3.4218841427735516,
      "grad_norm": 0.6178369522094727,
      "learning_rate": 0.0002192037470725995,
      "loss": 0.1678,
      "step": 1462
    },
    {
      "epoch": 3.4242246928028086,
      "grad_norm": 0.5884512066841125,
      "learning_rate": 0.00021912568306010926,
      "loss": 0.1273,
      "step": 1463
    },
    {
      "epoch": 3.4265652428320656,
      "grad_norm": 0.5416690707206726,
      "learning_rate": 0.000219047619047619,
      "loss": 0.1489,
      "step": 1464
    },
    {
      "epoch": 3.4289057928613222,
      "grad_norm": 0.5861112475395203,
      "learning_rate": 0.0002189695550351288,
      "loss": 0.1158,
      "step": 1465
    },
    {
      "epoch": 3.4312463428905793,
      "grad_norm": 0.8478973507881165,
      "learning_rate": 0.00021889149102263854,
      "loss": 0.1745,
      "step": 1466
    },
    {
      "epoch": 3.4335868929198363,
      "grad_norm": 0.8162992000579834,
      "learning_rate": 0.0002188134270101483,
      "loss": 0.1355,
      "step": 1467
    },
    {
      "epoch": 3.435927442949093,
      "grad_norm": 1.1442687511444092,
      "learning_rate": 0.00021873536299765807,
      "loss": 0.1739,
      "step": 1468
    },
    {
      "epoch": 3.43826799297835,
      "grad_norm": 0.5790355205535889,
      "learning_rate": 0.00021865729898516782,
      "loss": 0.1405,
      "step": 1469
    },
    {
      "epoch": 3.440608543007607,
      "grad_norm": 0.5630751252174377,
      "learning_rate": 0.00021857923497267757,
      "loss": 0.0924,
      "step": 1470
    },
    {
      "epoch": 3.4429490930368636,
      "grad_norm": 0.7131187319755554,
      "learning_rate": 0.00021850117096018735,
      "loss": 0.1527,
      "step": 1471
    },
    {
      "epoch": 3.4452896430661206,
      "grad_norm": 0.6666011214256287,
      "learning_rate": 0.0002184231069476971,
      "loss": 0.1783,
      "step": 1472
    },
    {
      "epoch": 3.447630193095377,
      "grad_norm": 0.48528826236724854,
      "learning_rate": 0.00021834504293520685,
      "loss": 0.1344,
      "step": 1473
    },
    {
      "epoch": 3.4499707431246343,
      "grad_norm": 0.7659273743629456,
      "learning_rate": 0.00021826697892271662,
      "loss": 0.2504,
      "step": 1474
    },
    {
      "epoch": 3.4523112931538913,
      "grad_norm": 0.8184679746627808,
      "learning_rate": 0.00021818891491022637,
      "loss": 0.2016,
      "step": 1475
    },
    {
      "epoch": 3.454651843183148,
      "grad_norm": 0.4363923966884613,
      "learning_rate": 0.0002181108508977361,
      "loss": 0.1263,
      "step": 1476
    },
    {
      "epoch": 3.456992393212405,
      "grad_norm": 0.45784294605255127,
      "learning_rate": 0.0002180327868852459,
      "loss": 0.1843,
      "step": 1477
    },
    {
      "epoch": 3.459332943241662,
      "grad_norm": 0.6622937321662903,
      "learning_rate": 0.00021795472287275563,
      "loss": 0.2096,
      "step": 1478
    },
    {
      "epoch": 3.4616734932709186,
      "grad_norm": 0.4444245994091034,
      "learning_rate": 0.00021787665886026538,
      "loss": 0.1706,
      "step": 1479
    },
    {
      "epoch": 3.4640140433001756,
      "grad_norm": 0.7439395785331726,
      "learning_rate": 0.00021779859484777515,
      "loss": 0.2097,
      "step": 1480
    },
    {
      "epoch": 3.4663545933294326,
      "grad_norm": 0.4871480166912079,
      "learning_rate": 0.0002177205308352849,
      "loss": 0.1339,
      "step": 1481
    },
    {
      "epoch": 3.4686951433586892,
      "grad_norm": 0.42891696095466614,
      "learning_rate": 0.00021764246682279465,
      "loss": 0.1164,
      "step": 1482
    },
    {
      "epoch": 3.4710356933879463,
      "grad_norm": 0.6357017755508423,
      "learning_rate": 0.00021756440281030443,
      "loss": 0.1914,
      "step": 1483
    },
    {
      "epoch": 3.473376243417203,
      "grad_norm": 0.5845524668693542,
      "learning_rate": 0.00021748633879781418,
      "loss": 0.2106,
      "step": 1484
    },
    {
      "epoch": 3.47571679344646,
      "grad_norm": 0.45394235849380493,
      "learning_rate": 0.00021740827478532393,
      "loss": 0.1447,
      "step": 1485
    },
    {
      "epoch": 3.478057343475717,
      "grad_norm": 0.6195104122161865,
      "learning_rate": 0.0002173302107728337,
      "loss": 0.1895,
      "step": 1486
    },
    {
      "epoch": 3.4803978935049735,
      "grad_norm": 0.5977752804756165,
      "learning_rate": 0.00021725214676034346,
      "loss": 0.139,
      "step": 1487
    },
    {
      "epoch": 3.4827384435342306,
      "grad_norm": 0.4997783899307251,
      "learning_rate": 0.0002171740827478532,
      "loss": 0.1547,
      "step": 1488
    },
    {
      "epoch": 3.4850789935634876,
      "grad_norm": 0.7871851921081543,
      "learning_rate": 0.000217096018735363,
      "loss": 0.2283,
      "step": 1489
    },
    {
      "epoch": 3.487419543592744,
      "grad_norm": 0.7539838552474976,
      "learning_rate": 0.00021701795472287274,
      "loss": 0.2013,
      "step": 1490
    },
    {
      "epoch": 3.4897600936220012,
      "grad_norm": 0.7457584142684937,
      "learning_rate": 0.0002169398907103825,
      "loss": 0.2027,
      "step": 1491
    },
    {
      "epoch": 3.4921006436512583,
      "grad_norm": 0.5152716040611267,
      "learning_rate": 0.00021686182669789227,
      "loss": 0.1305,
      "step": 1492
    },
    {
      "epoch": 3.494441193680515,
      "grad_norm": 0.6688769459724426,
      "learning_rate": 0.00021678376268540202,
      "loss": 0.2203,
      "step": 1493
    },
    {
      "epoch": 3.496781743709772,
      "grad_norm": 0.8076115250587463,
      "learning_rate": 0.00021670569867291174,
      "loss": 0.253,
      "step": 1494
    },
    {
      "epoch": 3.4991222937390285,
      "grad_norm": 0.5697046518325806,
      "learning_rate": 0.00021662763466042155,
      "loss": 0.1502,
      "step": 1495
    },
    {
      "epoch": 3.5014628437682855,
      "grad_norm": 0.5692278742790222,
      "learning_rate": 0.00021654957064793127,
      "loss": 0.176,
      "step": 1496
    },
    {
      "epoch": 3.5038033937975426,
      "grad_norm": 0.6500159502029419,
      "learning_rate": 0.00021647150663544102,
      "loss": 0.175,
      "step": 1497
    },
    {
      "epoch": 3.506143943826799,
      "grad_norm": 0.9068785905838013,
      "learning_rate": 0.0002163934426229508,
      "loss": 0.1859,
      "step": 1498
    },
    {
      "epoch": 3.508484493856056,
      "grad_norm": 0.7123134732246399,
      "learning_rate": 0.00021631537861046055,
      "loss": 0.1996,
      "step": 1499
    },
    {
      "epoch": 3.510825043885313,
      "grad_norm": 0.6285578608512878,
      "learning_rate": 0.0002162373145979703,
      "loss": 0.1424,
      "step": 1500
    },
    {
      "epoch": 3.51316559391457,
      "grad_norm": 0.785902738571167,
      "learning_rate": 0.00021615925058548008,
      "loss": 0.2136,
      "step": 1501
    },
    {
      "epoch": 3.515506143943827,
      "grad_norm": 0.45157599449157715,
      "learning_rate": 0.00021608118657298983,
      "loss": 0.1556,
      "step": 1502
    },
    {
      "epoch": 3.517846693973084,
      "grad_norm": 0.7937187552452087,
      "learning_rate": 0.00021600312256049958,
      "loss": 0.244,
      "step": 1503
    },
    {
      "epoch": 3.5201872440023405,
      "grad_norm": 0.6552695035934448,
      "learning_rate": 0.00021592505854800935,
      "loss": 0.1747,
      "step": 1504
    },
    {
      "epoch": 3.5225277940315975,
      "grad_norm": 0.6250075101852417,
      "learning_rate": 0.0002158469945355191,
      "loss": 0.142,
      "step": 1505
    },
    {
      "epoch": 3.524868344060854,
      "grad_norm": 0.6864503026008606,
      "learning_rate": 0.00021576893052302885,
      "loss": 0.2108,
      "step": 1506
    },
    {
      "epoch": 3.527208894090111,
      "grad_norm": 0.4228021800518036,
      "learning_rate": 0.00021569086651053863,
      "loss": 0.1166,
      "step": 1507
    },
    {
      "epoch": 3.529549444119368,
      "grad_norm": 0.5495626330375671,
      "learning_rate": 0.00021561280249804838,
      "loss": 0.1842,
      "step": 1508
    },
    {
      "epoch": 3.531889994148625,
      "grad_norm": 0.554324209690094,
      "learning_rate": 0.00021553473848555813,
      "loss": 0.1679,
      "step": 1509
    },
    {
      "epoch": 3.534230544177882,
      "grad_norm": 0.6398672461509705,
      "learning_rate": 0.0002154566744730679,
      "loss": 0.1388,
      "step": 1510
    },
    {
      "epoch": 3.5365710942071384,
      "grad_norm": 0.6860594153404236,
      "learning_rate": 0.00021537861046057766,
      "loss": 0.1666,
      "step": 1511
    },
    {
      "epoch": 3.5389116442363955,
      "grad_norm": 0.6027462482452393,
      "learning_rate": 0.00021530054644808738,
      "loss": 0.1765,
      "step": 1512
    },
    {
      "epoch": 3.5412521942656525,
      "grad_norm": 0.6945369839668274,
      "learning_rate": 0.0002152224824355972,
      "loss": 0.1954,
      "step": 1513
    },
    {
      "epoch": 3.5435927442949096,
      "grad_norm": 0.57414311170578,
      "learning_rate": 0.0002151444184231069,
      "loss": 0.1585,
      "step": 1514
    },
    {
      "epoch": 3.545933294324166,
      "grad_norm": 0.8609046339988708,
      "learning_rate": 0.00021506635441061666,
      "loss": 0.2206,
      "step": 1515
    },
    {
      "epoch": 3.548273844353423,
      "grad_norm": 0.6113066673278809,
      "learning_rate": 0.00021498829039812644,
      "loss": 0.1687,
      "step": 1516
    },
    {
      "epoch": 3.55061439438268,
      "grad_norm": 0.7164209485054016,
      "learning_rate": 0.0002149102263856362,
      "loss": 0.2065,
      "step": 1517
    },
    {
      "epoch": 3.552954944411937,
      "grad_norm": 0.7001926898956299,
      "learning_rate": 0.00021483216237314597,
      "loss": 0.2286,
      "step": 1518
    },
    {
      "epoch": 3.555295494441194,
      "grad_norm": 0.6710931062698364,
      "learning_rate": 0.00021475409836065572,
      "loss": 0.1858,
      "step": 1519
    },
    {
      "epoch": 3.5576360444704505,
      "grad_norm": 0.5015283823013306,
      "learning_rate": 0.00021467603434816547,
      "loss": 0.1389,
      "step": 1520
    },
    {
      "epoch": 3.5599765944997075,
      "grad_norm": 0.7016227841377258,
      "learning_rate": 0.00021459797033567525,
      "loss": 0.1975,
      "step": 1521
    },
    {
      "epoch": 3.562317144528964,
      "grad_norm": 0.6518599987030029,
      "learning_rate": 0.000214519906323185,
      "loss": 0.1785,
      "step": 1522
    },
    {
      "epoch": 3.564657694558221,
      "grad_norm": 0.4550316035747528,
      "learning_rate": 0.00021444184231069475,
      "loss": 0.1424,
      "step": 1523
    },
    {
      "epoch": 3.566998244587478,
      "grad_norm": 0.5365321636199951,
      "learning_rate": 0.00021436377829820452,
      "loss": 0.1439,
      "step": 1524
    },
    {
      "epoch": 3.569338794616735,
      "grad_norm": 0.40272200107574463,
      "learning_rate": 0.00021428571428571427,
      "loss": 0.1305,
      "step": 1525
    },
    {
      "epoch": 3.571679344645992,
      "grad_norm": 0.6928476095199585,
      "learning_rate": 0.00021420765027322403,
      "loss": 0.1656,
      "step": 1526
    },
    {
      "epoch": 3.574019894675249,
      "grad_norm": 0.7869302034378052,
      "learning_rate": 0.0002141295862607338,
      "loss": 0.1966,
      "step": 1527
    },
    {
      "epoch": 3.5763604447045054,
      "grad_norm": 0.8879262208938599,
      "learning_rate": 0.00021405152224824355,
      "loss": 0.2075,
      "step": 1528
    },
    {
      "epoch": 3.5787009947337625,
      "grad_norm": 0.7352405190467834,
      "learning_rate": 0.0002139734582357533,
      "loss": 0.1977,
      "step": 1529
    },
    {
      "epoch": 3.5810415447630195,
      "grad_norm": 0.6568158864974976,
      "learning_rate": 0.00021389539422326308,
      "loss": 0.1718,
      "step": 1530
    },
    {
      "epoch": 3.583382094792276,
      "grad_norm": 0.823495626449585,
      "learning_rate": 0.00021381733021077283,
      "loss": 0.1952,
      "step": 1531
    },
    {
      "epoch": 3.585722644821533,
      "grad_norm": 0.5792094469070435,
      "learning_rate": 0.00021373926619828255,
      "loss": 0.2215,
      "step": 1532
    },
    {
      "epoch": 3.5880631948507897,
      "grad_norm": 0.7681276202201843,
      "learning_rate": 0.00021366120218579236,
      "loss": 0.1709,
      "step": 1533
    },
    {
      "epoch": 3.5904037448800468,
      "grad_norm": 0.6770976781845093,
      "learning_rate": 0.00021358313817330208,
      "loss": 0.2112,
      "step": 1534
    },
    {
      "epoch": 3.592744294909304,
      "grad_norm": 0.6378010511398315,
      "learning_rate": 0.00021350507416081183,
      "loss": 0.1603,
      "step": 1535
    },
    {
      "epoch": 3.5950848449385604,
      "grad_norm": 0.3833310008049011,
      "learning_rate": 0.0002134270101483216,
      "loss": 0.0652,
      "step": 1536
    },
    {
      "epoch": 3.5974253949678174,
      "grad_norm": 0.4327932894229889,
      "learning_rate": 0.00021334894613583136,
      "loss": 0.1425,
      "step": 1537
    },
    {
      "epoch": 3.5997659449970745,
      "grad_norm": 0.601294219493866,
      "learning_rate": 0.0002132708821233411,
      "loss": 0.1229,
      "step": 1538
    },
    {
      "epoch": 3.602106495026331,
      "grad_norm": 0.5604562163352966,
      "learning_rate": 0.0002131928181108509,
      "loss": 0.1457,
      "step": 1539
    },
    {
      "epoch": 3.604447045055588,
      "grad_norm": 0.7543532252311707,
      "learning_rate": 0.00021311475409836064,
      "loss": 0.2027,
      "step": 1540
    },
    {
      "epoch": 3.606787595084845,
      "grad_norm": 0.6626271605491638,
      "learning_rate": 0.0002130366900858704,
      "loss": 0.1919,
      "step": 1541
    },
    {
      "epoch": 3.6091281451141017,
      "grad_norm": 0.5572352409362793,
      "learning_rate": 0.00021295862607338017,
      "loss": 0.1665,
      "step": 1542
    },
    {
      "epoch": 3.6114686951433588,
      "grad_norm": 0.6289747357368469,
      "learning_rate": 0.00021288056206088992,
      "loss": 0.1601,
      "step": 1543
    },
    {
      "epoch": 3.6138092451726154,
      "grad_norm": 0.6726326942443848,
      "learning_rate": 0.00021280249804839967,
      "loss": 0.172,
      "step": 1544
    },
    {
      "epoch": 3.6161497952018724,
      "grad_norm": 0.7956263422966003,
      "learning_rate": 0.00021272443403590945,
      "loss": 0.1949,
      "step": 1545
    },
    {
      "epoch": 3.6184903452311294,
      "grad_norm": 0.48547205328941345,
      "learning_rate": 0.0002126463700234192,
      "loss": 0.1734,
      "step": 1546
    },
    {
      "epoch": 3.620830895260386,
      "grad_norm": 0.6728026866912842,
      "learning_rate": 0.00021256830601092895,
      "loss": 0.1668,
      "step": 1547
    },
    {
      "epoch": 3.623171445289643,
      "grad_norm": 0.6972084641456604,
      "learning_rate": 0.00021249024199843872,
      "loss": 0.1829,
      "step": 1548
    },
    {
      "epoch": 3.6255119953188997,
      "grad_norm": 0.6678091287612915,
      "learning_rate": 0.00021241217798594847,
      "loss": 0.2019,
      "step": 1549
    },
    {
      "epoch": 3.6278525453481567,
      "grad_norm": 0.5063760876655579,
      "learning_rate": 0.0002123341139734582,
      "loss": 0.1498,
      "step": 1550
    },
    {
      "epoch": 3.6301930953774137,
      "grad_norm": 0.5624707341194153,
      "learning_rate": 0.000212256049960968,
      "loss": 0.1963,
      "step": 1551
    },
    {
      "epoch": 3.632533645406671,
      "grad_norm": 0.5500608682632446,
      "learning_rate": 0.00021217798594847773,
      "loss": 0.1431,
      "step": 1552
    },
    {
      "epoch": 3.6348741954359274,
      "grad_norm": 0.5599268078804016,
      "learning_rate": 0.00021209992193598748,
      "loss": 0.1474,
      "step": 1553
    },
    {
      "epoch": 3.6372147454651844,
      "grad_norm": 0.4734618663787842,
      "learning_rate": 0.00021202185792349725,
      "loss": 0.0966,
      "step": 1554
    },
    {
      "epoch": 3.639555295494441,
      "grad_norm": 0.48505645990371704,
      "learning_rate": 0.000211943793911007,
      "loss": 0.1581,
      "step": 1555
    },
    {
      "epoch": 3.641895845523698,
      "grad_norm": 0.4248795509338379,
      "learning_rate": 0.00021186572989851675,
      "loss": 0.1217,
      "step": 1556
    },
    {
      "epoch": 3.644236395552955,
      "grad_norm": 0.6979759335517883,
      "learning_rate": 0.00021178766588602653,
      "loss": 0.2142,
      "step": 1557
    },
    {
      "epoch": 3.6465769455822117,
      "grad_norm": 0.6322890520095825,
      "learning_rate": 0.00021170960187353628,
      "loss": 0.1891,
      "step": 1558
    },
    {
      "epoch": 3.6489174956114687,
      "grad_norm": 0.7115390300750732,
      "learning_rate": 0.00021163153786104603,
      "loss": 0.2098,
      "step": 1559
    },
    {
      "epoch": 3.6512580456407253,
      "grad_norm": 0.6783484220504761,
      "learning_rate": 0.0002115534738485558,
      "loss": 0.2135,
      "step": 1560
    },
    {
      "epoch": 3.6535985956699824,
      "grad_norm": 0.6483533978462219,
      "learning_rate": 0.00021147540983606556,
      "loss": 0.1309,
      "step": 1561
    },
    {
      "epoch": 3.6559391456992394,
      "grad_norm": 0.5501140356063843,
      "learning_rate": 0.0002113973458235753,
      "loss": 0.1695,
      "step": 1562
    },
    {
      "epoch": 3.6582796957284964,
      "grad_norm": 0.45882442593574524,
      "learning_rate": 0.0002113192818110851,
      "loss": 0.1346,
      "step": 1563
    },
    {
      "epoch": 3.660620245757753,
      "grad_norm": 0.5303383469581604,
      "learning_rate": 0.00021124121779859484,
      "loss": 0.1521,
      "step": 1564
    },
    {
      "epoch": 3.66296079578701,
      "grad_norm": 0.703274667263031,
      "learning_rate": 0.0002111631537861046,
      "loss": 0.1876,
      "step": 1565
    },
    {
      "epoch": 3.6653013458162667,
      "grad_norm": 0.524505078792572,
      "learning_rate": 0.00021108508977361437,
      "loss": 0.14,
      "step": 1566
    },
    {
      "epoch": 3.6676418958455237,
      "grad_norm": 0.5587984919548035,
      "learning_rate": 0.00021100702576112412,
      "loss": 0.1784,
      "step": 1567
    },
    {
      "epoch": 3.6699824458747807,
      "grad_norm": 0.7088616490364075,
      "learning_rate": 0.00021092896174863384,
      "loss": 0.2195,
      "step": 1568
    },
    {
      "epoch": 3.6723229959040373,
      "grad_norm": 0.8202693462371826,
      "learning_rate": 0.00021085089773614362,
      "loss": 0.2328,
      "step": 1569
    },
    {
      "epoch": 3.6746635459332944,
      "grad_norm": 0.6775091886520386,
      "learning_rate": 0.00021077283372365337,
      "loss": 0.1605,
      "step": 1570
    },
    {
      "epoch": 3.677004095962551,
      "grad_norm": 0.5766424536705017,
      "learning_rate": 0.00021069476971116312,
      "loss": 0.1529,
      "step": 1571
    },
    {
      "epoch": 3.679344645991808,
      "grad_norm": 0.5461920499801636,
      "learning_rate": 0.0002106167056986729,
      "loss": 0.2006,
      "step": 1572
    },
    {
      "epoch": 3.681685196021065,
      "grad_norm": 0.7805964946746826,
      "learning_rate": 0.00021053864168618265,
      "loss": 0.1992,
      "step": 1573
    },
    {
      "epoch": 3.684025746050322,
      "grad_norm": 0.502362072467804,
      "learning_rate": 0.0002104605776736924,
      "loss": 0.1211,
      "step": 1574
    },
    {
      "epoch": 3.6863662960795787,
      "grad_norm": 0.6429240107536316,
      "learning_rate": 0.00021038251366120217,
      "loss": 0.1737,
      "step": 1575
    },
    {
      "epoch": 3.6887068461088357,
      "grad_norm": 0.5196946263313293,
      "learning_rate": 0.00021030444964871193,
      "loss": 0.1719,
      "step": 1576
    },
    {
      "epoch": 3.6910473961380923,
      "grad_norm": 0.7087492942810059,
      "learning_rate": 0.00021022638563622168,
      "loss": 0.2265,
      "step": 1577
    },
    {
      "epoch": 3.6933879461673493,
      "grad_norm": 0.39969494938850403,
      "learning_rate": 0.00021014832162373145,
      "loss": 0.129,
      "step": 1578
    },
    {
      "epoch": 3.6957284961966064,
      "grad_norm": 0.4097781181335449,
      "learning_rate": 0.0002100702576112412,
      "loss": 0.1433,
      "step": 1579
    },
    {
      "epoch": 3.698069046225863,
      "grad_norm": 0.49884897470474243,
      "learning_rate": 0.00020999219359875095,
      "loss": 0.1658,
      "step": 1580
    },
    {
      "epoch": 3.70040959625512,
      "grad_norm": 0.816419243812561,
      "learning_rate": 0.00020991412958626073,
      "loss": 0.1909,
      "step": 1581
    },
    {
      "epoch": 3.7027501462843766,
      "grad_norm": 0.619023859500885,
      "learning_rate": 0.00020983606557377048,
      "loss": 0.1527,
      "step": 1582
    },
    {
      "epoch": 3.7050906963136336,
      "grad_norm": 0.541005551815033,
      "learning_rate": 0.00020975800156128023,
      "loss": 0.184,
      "step": 1583
    },
    {
      "epoch": 3.7074312463428907,
      "grad_norm": 0.46894463896751404,
      "learning_rate": 0.00020967993754879,
      "loss": 0.1707,
      "step": 1584
    },
    {
      "epoch": 3.7097717963721477,
      "grad_norm": 0.5068365335464478,
      "learning_rate": 0.00020960187353629976,
      "loss": 0.1661,
      "step": 1585
    },
    {
      "epoch": 3.7121123464014043,
      "grad_norm": 0.4680197834968567,
      "learning_rate": 0.00020952380952380948,
      "loss": 0.1451,
      "step": 1586
    },
    {
      "epoch": 3.7144528964306613,
      "grad_norm": 0.4912547171115875,
      "learning_rate": 0.00020944574551131926,
      "loss": 0.1444,
      "step": 1587
    },
    {
      "epoch": 3.716793446459918,
      "grad_norm": 0.662708044052124,
      "learning_rate": 0.000209367681498829,
      "loss": 0.2047,
      "step": 1588
    },
    {
      "epoch": 3.719133996489175,
      "grad_norm": 0.618775486946106,
      "learning_rate": 0.00020928961748633876,
      "loss": 0.1844,
      "step": 1589
    },
    {
      "epoch": 3.721474546518432,
      "grad_norm": 0.688965916633606,
      "learning_rate": 0.00020921155347384854,
      "loss": 0.1024,
      "step": 1590
    },
    {
      "epoch": 3.7238150965476886,
      "grad_norm": 0.8290074467658997,
      "learning_rate": 0.0002091334894613583,
      "loss": 0.2008,
      "step": 1591
    },
    {
      "epoch": 3.7261556465769456,
      "grad_norm": 0.6853097081184387,
      "learning_rate": 0.00020905542544886804,
      "loss": 0.1597,
      "step": 1592
    },
    {
      "epoch": 3.7284961966062022,
      "grad_norm": 0.617405891418457,
      "learning_rate": 0.00020897736143637782,
      "loss": 0.1873,
      "step": 1593
    },
    {
      "epoch": 3.7308367466354593,
      "grad_norm": 0.5168799757957458,
      "learning_rate": 0.00020889929742388757,
      "loss": 0.154,
      "step": 1594
    },
    {
      "epoch": 3.7331772966647163,
      "grad_norm": 0.687768816947937,
      "learning_rate": 0.00020882123341139732,
      "loss": 0.1879,
      "step": 1595
    },
    {
      "epoch": 3.7355178466939734,
      "grad_norm": 0.6268645524978638,
      "learning_rate": 0.0002087431693989071,
      "loss": 0.1871,
      "step": 1596
    },
    {
      "epoch": 3.73785839672323,
      "grad_norm": 0.7926755547523499,
      "learning_rate": 0.00020866510538641685,
      "loss": 0.1974,
      "step": 1597
    },
    {
      "epoch": 3.740198946752487,
      "grad_norm": 0.6084365248680115,
      "learning_rate": 0.0002085870413739266,
      "loss": 0.1398,
      "step": 1598
    },
    {
      "epoch": 3.7425394967817436,
      "grad_norm": 0.6665260791778564,
      "learning_rate": 0.00020850897736143637,
      "loss": 0.1724,
      "step": 1599
    },
    {
      "epoch": 3.7448800468110006,
      "grad_norm": 0.6243898272514343,
      "learning_rate": 0.00020843091334894612,
      "loss": 0.1789,
      "step": 1600
    },
    {
      "epoch": 3.7472205968402577,
      "grad_norm": 0.5896002054214478,
      "learning_rate": 0.00020835284933645588,
      "loss": 0.184,
      "step": 1601
    },
    {
      "epoch": 3.7495611468695142,
      "grad_norm": 0.6356264352798462,
      "learning_rate": 0.00020827478532396565,
      "loss": 0.1848,
      "step": 1602
    },
    {
      "epoch": 3.7519016968987713,
      "grad_norm": 0.6095094680786133,
      "learning_rate": 0.00020819672131147538,
      "loss": 0.1473,
      "step": 1603
    },
    {
      "epoch": 3.754242246928028,
      "grad_norm": 0.9061611890792847,
      "learning_rate": 0.00020811865729898513,
      "loss": 0.2103,
      "step": 1604
    },
    {
      "epoch": 3.756582796957285,
      "grad_norm": 0.5995348691940308,
      "learning_rate": 0.0002080405932864949,
      "loss": 0.1816,
      "step": 1605
    },
    {
      "epoch": 3.758923346986542,
      "grad_norm": 0.5521705150604248,
      "learning_rate": 0.00020796252927400465,
      "loss": 0.2098,
      "step": 1606
    },
    {
      "epoch": 3.7612638970157986,
      "grad_norm": 0.8171765208244324,
      "learning_rate": 0.0002078844652615144,
      "loss": 0.1888,
      "step": 1607
    },
    {
      "epoch": 3.7636044470450556,
      "grad_norm": 0.4879005253314972,
      "learning_rate": 0.00020780640124902418,
      "loss": 0.1492,
      "step": 1608
    },
    {
      "epoch": 3.7659449970743126,
      "grad_norm": 0.6981766223907471,
      "learning_rate": 0.00020772833723653393,
      "loss": 0.1495,
      "step": 1609
    },
    {
      "epoch": 3.768285547103569,
      "grad_norm": 0.5784435868263245,
      "learning_rate": 0.00020765027322404368,
      "loss": 0.1599,
      "step": 1610
    },
    {
      "epoch": 3.7706260971328263,
      "grad_norm": 0.630844235420227,
      "learning_rate": 0.00020757220921155346,
      "loss": 0.1688,
      "step": 1611
    },
    {
      "epoch": 3.7729666471620833,
      "grad_norm": 0.5554282665252686,
      "learning_rate": 0.0002074941451990632,
      "loss": 0.1583,
      "step": 1612
    },
    {
      "epoch": 3.77530719719134,
      "grad_norm": 0.6256376504898071,
      "learning_rate": 0.00020741608118657296,
      "loss": 0.1729,
      "step": 1613
    },
    {
      "epoch": 3.777647747220597,
      "grad_norm": 0.8248060941696167,
      "learning_rate": 0.00020733801717408274,
      "loss": 0.2372,
      "step": 1614
    },
    {
      "epoch": 3.7799882972498535,
      "grad_norm": 0.590976357460022,
      "learning_rate": 0.0002072599531615925,
      "loss": 0.1761,
      "step": 1615
    },
    {
      "epoch": 3.7823288472791106,
      "grad_norm": 1.0304532051086426,
      "learning_rate": 0.00020718188914910224,
      "loss": 0.1621,
      "step": 1616
    },
    {
      "epoch": 3.7846693973083676,
      "grad_norm": 0.5127116441726685,
      "learning_rate": 0.00020710382513661202,
      "loss": 0.1275,
      "step": 1617
    },
    {
      "epoch": 3.787009947337624,
      "grad_norm": 0.638020932674408,
      "learning_rate": 0.00020702576112412177,
      "loss": 0.1863,
      "step": 1618
    },
    {
      "epoch": 3.7893504973668812,
      "grad_norm": 0.6570270657539368,
      "learning_rate": 0.00020694769711163152,
      "loss": 0.1741,
      "step": 1619
    },
    {
      "epoch": 3.791691047396138,
      "grad_norm": 0.5085346102714539,
      "learning_rate": 0.0002068696330991413,
      "loss": 0.1418,
      "step": 1620
    },
    {
      "epoch": 3.794031597425395,
      "grad_norm": 0.5682501196861267,
      "learning_rate": 0.00020679156908665102,
      "loss": 0.1375,
      "step": 1621
    },
    {
      "epoch": 3.796372147454652,
      "grad_norm": 0.6577860713005066,
      "learning_rate": 0.00020671350507416077,
      "loss": 0.2143,
      "step": 1622
    },
    {
      "epoch": 3.798712697483909,
      "grad_norm": 0.508610725402832,
      "learning_rate": 0.00020663544106167055,
      "loss": 0.1519,
      "step": 1623
    },
    {
      "epoch": 3.8010532475131655,
      "grad_norm": 0.5497192740440369,
      "learning_rate": 0.0002065573770491803,
      "loss": 0.209,
      "step": 1624
    },
    {
      "epoch": 3.8033937975424226,
      "grad_norm": 0.6077954769134521,
      "learning_rate": 0.00020647931303669005,
      "loss": 0.182,
      "step": 1625
    },
    {
      "epoch": 3.805734347571679,
      "grad_norm": 0.5335986018180847,
      "learning_rate": 0.00020640124902419983,
      "loss": 0.1892,
      "step": 1626
    },
    {
      "epoch": 3.808074897600936,
      "grad_norm": 0.6579254865646362,
      "learning_rate": 0.00020632318501170958,
      "loss": 0.219,
      "step": 1627
    },
    {
      "epoch": 3.8104154476301932,
      "grad_norm": 0.5538986325263977,
      "learning_rate": 0.00020624512099921933,
      "loss": 0.1963,
      "step": 1628
    },
    {
      "epoch": 3.81275599765945,
      "grad_norm": 0.5710949301719666,
      "learning_rate": 0.0002061670569867291,
      "loss": 0.1927,
      "step": 1629
    },
    {
      "epoch": 3.815096547688707,
      "grad_norm": 0.6178343296051025,
      "learning_rate": 0.00020608899297423885,
      "loss": 0.1864,
      "step": 1630
    },
    {
      "epoch": 3.8174370977179635,
      "grad_norm": 0.6266618967056274,
      "learning_rate": 0.0002060109289617486,
      "loss": 0.1388,
      "step": 1631
    },
    {
      "epoch": 3.8197776477472205,
      "grad_norm": 0.6429165601730347,
      "learning_rate": 0.00020593286494925838,
      "loss": 0.1361,
      "step": 1632
    },
    {
      "epoch": 3.8221181977764775,
      "grad_norm": 0.6441013216972351,
      "learning_rate": 0.00020585480093676813,
      "loss": 0.1784,
      "step": 1633
    },
    {
      "epoch": 3.8244587478057346,
      "grad_norm": 0.6701741814613342,
      "learning_rate": 0.00020577673692427788,
      "loss": 0.1858,
      "step": 1634
    },
    {
      "epoch": 3.826799297834991,
      "grad_norm": 0.5283234119415283,
      "learning_rate": 0.00020569867291178766,
      "loss": 0.148,
      "step": 1635
    },
    {
      "epoch": 3.829139847864248,
      "grad_norm": 0.5019733309745789,
      "learning_rate": 0.0002056206088992974,
      "loss": 0.1391,
      "step": 1636
    },
    {
      "epoch": 3.831480397893505,
      "grad_norm": 0.6387123465538025,
      "learning_rate": 0.00020554254488680716,
      "loss": 0.1812,
      "step": 1637
    },
    {
      "epoch": 3.833820947922762,
      "grad_norm": 0.8454127907752991,
      "learning_rate": 0.00020546448087431694,
      "loss": 0.2115,
      "step": 1638
    },
    {
      "epoch": 3.836161497952019,
      "grad_norm": 0.5245240330696106,
      "learning_rate": 0.00020538641686182666,
      "loss": 0.1618,
      "step": 1639
    },
    {
      "epoch": 3.8385020479812755,
      "grad_norm": 0.5473023056983948,
      "learning_rate": 0.0002053083528493364,
      "loss": 0.1251,
      "step": 1640
    },
    {
      "epoch": 3.8408425980105325,
      "grad_norm": 0.5600388646125793,
      "learning_rate": 0.0002052302888368462,
      "loss": 0.1416,
      "step": 1641
    },
    {
      "epoch": 3.843183148039789,
      "grad_norm": 0.60538649559021,
      "learning_rate": 0.00020515222482435594,
      "loss": 0.1646,
      "step": 1642
    },
    {
      "epoch": 3.845523698069046,
      "grad_norm": 0.6566024422645569,
      "learning_rate": 0.0002050741608118657,
      "loss": 0.2334,
      "step": 1643
    },
    {
      "epoch": 3.847864248098303,
      "grad_norm": 0.7161768674850464,
      "learning_rate": 0.00020499609679937547,
      "loss": 0.1678,
      "step": 1644
    },
    {
      "epoch": 3.8502047981275602,
      "grad_norm": 0.5476476550102234,
      "learning_rate": 0.00020491803278688522,
      "loss": 0.1503,
      "step": 1645
    },
    {
      "epoch": 3.852545348156817,
      "grad_norm": 0.8302428722381592,
      "learning_rate": 0.000204839968774395,
      "loss": 0.23,
      "step": 1646
    },
    {
      "epoch": 3.854885898186074,
      "grad_norm": 0.5900845527648926,
      "learning_rate": 0.00020476190476190475,
      "loss": 0.1988,
      "step": 1647
    },
    {
      "epoch": 3.8572264482153304,
      "grad_norm": 0.5076303482055664,
      "learning_rate": 0.0002046838407494145,
      "loss": 0.1706,
      "step": 1648
    },
    {
      "epoch": 3.8595669982445875,
      "grad_norm": 0.9444072246551514,
      "learning_rate": 0.00020460577673692427,
      "loss": 0.2477,
      "step": 1649
    },
    {
      "epoch": 3.8619075482738445,
      "grad_norm": 0.5908685922622681,
      "learning_rate": 0.00020452771272443402,
      "loss": 0.1507,
      "step": 1650
    },
    {
      "epoch": 3.864248098303101,
      "grad_norm": 0.7712497711181641,
      "learning_rate": 0.00020444964871194378,
      "loss": 0.1731,
      "step": 1651
    },
    {
      "epoch": 3.866588648332358,
      "grad_norm": 0.5189889073371887,
      "learning_rate": 0.00020437158469945355,
      "loss": 0.1411,
      "step": 1652
    },
    {
      "epoch": 3.8689291983616148,
      "grad_norm": 0.4715965688228607,
      "learning_rate": 0.0002042935206869633,
      "loss": 0.104,
      "step": 1653
    },
    {
      "epoch": 3.871269748390872,
      "grad_norm": 0.6394662261009216,
      "learning_rate": 0.00020421545667447305,
      "loss": 0.1605,
      "step": 1654
    },
    {
      "epoch": 3.873610298420129,
      "grad_norm": 0.7174078226089478,
      "learning_rate": 0.00020413739266198283,
      "loss": 0.1589,
      "step": 1655
    },
    {
      "epoch": 3.875950848449386,
      "grad_norm": 0.5761550068855286,
      "learning_rate": 0.00020405932864949258,
      "loss": 0.1393,
      "step": 1656
    },
    {
      "epoch": 3.8782913984786425,
      "grad_norm": 0.9265554547309875,
      "learning_rate": 0.0002039812646370023,
      "loss": 0.2248,
      "step": 1657
    },
    {
      "epoch": 3.8806319485078995,
      "grad_norm": 0.7098698616027832,
      "learning_rate": 0.0002039032006245121,
      "loss": 0.1757,
      "step": 1658
    },
    {
      "epoch": 3.882972498537156,
      "grad_norm": 0.47429144382476807,
      "learning_rate": 0.00020382513661202183,
      "loss": 0.1452,
      "step": 1659
    },
    {
      "epoch": 3.885313048566413,
      "grad_norm": 0.5487303733825684,
      "learning_rate": 0.00020374707259953158,
      "loss": 0.1596,
      "step": 1660
    },
    {
      "epoch": 3.88765359859567,
      "grad_norm": 0.6116654872894287,
      "learning_rate": 0.00020366900858704136,
      "loss": 0.1886,
      "step": 1661
    },
    {
      "epoch": 3.8899941486249268,
      "grad_norm": 0.6851608157157898,
      "learning_rate": 0.0002035909445745511,
      "loss": 0.2224,
      "step": 1662
    },
    {
      "epoch": 3.892334698654184,
      "grad_norm": 0.9801551103591919,
      "learning_rate": 0.00020351288056206086,
      "loss": 0.2179,
      "step": 1663
    },
    {
      "epoch": 3.8946752486834404,
      "grad_norm": 0.6166954040527344,
      "learning_rate": 0.00020343481654957064,
      "loss": 0.1256,
      "step": 1664
    },
    {
      "epoch": 3.8970157987126974,
      "grad_norm": 0.5778745412826538,
      "learning_rate": 0.0002033567525370804,
      "loss": 0.1929,
      "step": 1665
    },
    {
      "epoch": 3.8993563487419545,
      "grad_norm": 0.5258333086967468,
      "learning_rate": 0.00020327868852459014,
      "loss": 0.1149,
      "step": 1666
    },
    {
      "epoch": 3.9016968987712115,
      "grad_norm": 0.6129694581031799,
      "learning_rate": 0.00020320062451209992,
      "loss": 0.1543,
      "step": 1667
    },
    {
      "epoch": 3.904037448800468,
      "grad_norm": 0.5919172167778015,
      "learning_rate": 0.00020312256049960967,
      "loss": 0.1479,
      "step": 1668
    },
    {
      "epoch": 3.906377998829725,
      "grad_norm": 0.7810109257698059,
      "learning_rate": 0.00020304449648711942,
      "loss": 0.1901,
      "step": 1669
    },
    {
      "epoch": 3.9087185488589817,
      "grad_norm": 0.7055559158325195,
      "learning_rate": 0.0002029664324746292,
      "loss": 0.1997,
      "step": 1670
    },
    {
      "epoch": 3.9110590988882388,
      "grad_norm": 0.47243732213974,
      "learning_rate": 0.00020288836846213895,
      "loss": 0.168,
      "step": 1671
    },
    {
      "epoch": 3.913399648917496,
      "grad_norm": 0.7424244284629822,
      "learning_rate": 0.0002028103044496487,
      "loss": 0.2053,
      "step": 1672
    },
    {
      "epoch": 3.9157401989467524,
      "grad_norm": 0.7063992619514465,
      "learning_rate": 0.00020273224043715847,
      "loss": 0.1506,
      "step": 1673
    },
    {
      "epoch": 3.9180807489760094,
      "grad_norm": 0.5567623972892761,
      "learning_rate": 0.00020265417642466822,
      "loss": 0.1666,
      "step": 1674
    },
    {
      "epoch": 3.920421299005266,
      "grad_norm": 0.7119470834732056,
      "learning_rate": 0.00020257611241217795,
      "loss": 0.2222,
      "step": 1675
    },
    {
      "epoch": 3.922761849034523,
      "grad_norm": 0.7597933411598206,
      "learning_rate": 0.00020249804839968775,
      "loss": 0.234,
      "step": 1676
    },
    {
      "epoch": 3.92510239906378,
      "grad_norm": 0.5810102820396423,
      "learning_rate": 0.00020241998438719748,
      "loss": 0.157,
      "step": 1677
    },
    {
      "epoch": 3.9274429490930367,
      "grad_norm": 0.5902822613716125,
      "learning_rate": 0.00020234192037470723,
      "loss": 0.191,
      "step": 1678
    },
    {
      "epoch": 3.9297834991222937,
      "grad_norm": 0.5420843958854675,
      "learning_rate": 0.000202263856362217,
      "loss": 0.158,
      "step": 1679
    },
    {
      "epoch": 3.9321240491515503,
      "grad_norm": 0.43748193979263306,
      "learning_rate": 0.00020218579234972675,
      "loss": 0.1094,
      "step": 1680
    },
    {
      "epoch": 3.9344645991808074,
      "grad_norm": 0.5516394376754761,
      "learning_rate": 0.0002021077283372365,
      "loss": 0.184,
      "step": 1681
    },
    {
      "epoch": 3.9368051492100644,
      "grad_norm": 0.6629353761672974,
      "learning_rate": 0.00020202966432474628,
      "loss": 0.1503,
      "step": 1682
    },
    {
      "epoch": 3.9391456992393215,
      "grad_norm": 0.5778688788414001,
      "learning_rate": 0.00020195160031225603,
      "loss": 0.1856,
      "step": 1683
    },
    {
      "epoch": 3.941486249268578,
      "grad_norm": 0.6099511981010437,
      "learning_rate": 0.00020187353629976578,
      "loss": 0.1978,
      "step": 1684
    },
    {
      "epoch": 3.943826799297835,
      "grad_norm": 0.6946933269500732,
      "learning_rate": 0.00020179547228727556,
      "loss": 0.1389,
      "step": 1685
    },
    {
      "epoch": 3.9461673493270917,
      "grad_norm": 0.5772628784179688,
      "learning_rate": 0.0002017174082747853,
      "loss": 0.1502,
      "step": 1686
    },
    {
      "epoch": 3.9485078993563487,
      "grad_norm": 0.533318817615509,
      "learning_rate": 0.00020163934426229506,
      "loss": 0.2058,
      "step": 1687
    },
    {
      "epoch": 3.9508484493856058,
      "grad_norm": 0.5993953347206116,
      "learning_rate": 0.00020156128024980484,
      "loss": 0.1676,
      "step": 1688
    },
    {
      "epoch": 3.9531889994148623,
      "grad_norm": 0.644546627998352,
      "learning_rate": 0.0002014832162373146,
      "loss": 0.1562,
      "step": 1689
    },
    {
      "epoch": 3.9555295494441194,
      "grad_norm": 0.5418927669525146,
      "learning_rate": 0.00020140515222482434,
      "loss": 0.1671,
      "step": 1690
    },
    {
      "epoch": 3.957870099473376,
      "grad_norm": 0.610008955001831,
      "learning_rate": 0.00020132708821233412,
      "loss": 0.1473,
      "step": 1691
    },
    {
      "epoch": 3.960210649502633,
      "grad_norm": 0.6715195775032043,
      "learning_rate": 0.00020124902419984387,
      "loss": 0.1555,
      "step": 1692
    },
    {
      "epoch": 3.96255119953189,
      "grad_norm": 0.7097546458244324,
      "learning_rate": 0.0002011709601873536,
      "loss": 0.2018,
      "step": 1693
    },
    {
      "epoch": 3.964891749561147,
      "grad_norm": 0.46393606066703796,
      "learning_rate": 0.0002010928961748634,
      "loss": 0.1278,
      "step": 1694
    },
    {
      "epoch": 3.9672322995904037,
      "grad_norm": 0.8241606950759888,
      "learning_rate": 0.00020101483216237312,
      "loss": 0.2449,
      "step": 1695
    },
    {
      "epoch": 3.9695728496196607,
      "grad_norm": 0.6668792963027954,
      "learning_rate": 0.00020093676814988287,
      "loss": 0.163,
      "step": 1696
    },
    {
      "epoch": 3.9719133996489173,
      "grad_norm": 0.7423110604286194,
      "learning_rate": 0.00020085870413739265,
      "loss": 0.2005,
      "step": 1697
    },
    {
      "epoch": 3.9742539496781744,
      "grad_norm": 1.2297395467758179,
      "learning_rate": 0.0002007806401249024,
      "loss": 0.1588,
      "step": 1698
    },
    {
      "epoch": 3.9765944997074314,
      "grad_norm": 0.6740990877151489,
      "learning_rate": 0.00020070257611241215,
      "loss": 0.1979,
      "step": 1699
    },
    {
      "epoch": 3.978935049736688,
      "grad_norm": 0.5424336194992065,
      "learning_rate": 0.00020062451209992192,
      "loss": 0.1775,
      "step": 1700
    },
    {
      "epoch": 3.981275599765945,
      "grad_norm": 0.610509991645813,
      "learning_rate": 0.00020054644808743168,
      "loss": 0.1149,
      "step": 1701
    },
    {
      "epoch": 3.9836161497952016,
      "grad_norm": 0.56388258934021,
      "learning_rate": 0.00020046838407494143,
      "loss": 0.1821,
      "step": 1702
    },
    {
      "epoch": 3.9859566998244587,
      "grad_norm": 0.7084858417510986,
      "learning_rate": 0.0002003903200624512,
      "loss": 0.1591,
      "step": 1703
    },
    {
      "epoch": 3.9882972498537157,
      "grad_norm": 0.588596522808075,
      "learning_rate": 0.00020031225604996095,
      "loss": 0.1535,
      "step": 1704
    },
    {
      "epoch": 3.9906377998829727,
      "grad_norm": 0.7037241458892822,
      "learning_rate": 0.0002002341920374707,
      "loss": 0.1961,
      "step": 1705
    },
    {
      "epoch": 3.9929783499122293,
      "grad_norm": 0.5723172426223755,
      "learning_rate": 0.00020015612802498048,
      "loss": 0.1678,
      "step": 1706
    },
    {
      "epoch": 3.9953188999414864,
      "grad_norm": 0.6943104267120361,
      "learning_rate": 0.00020007806401249023,
      "loss": 0.2187,
      "step": 1707
    },
    {
      "epoch": 3.997659449970743,
      "grad_norm": 0.6896638870239258,
      "learning_rate": 0.00019999999999999998,
      "loss": 0.186,
      "step": 1708
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.976698637008667,
      "learning_rate": 0.00019992193598750976,
      "loss": 0.2112,
      "step": 1709
    },
    {
      "epoch": 4.0,
      "eval_loss": 0.25576052069664,
      "eval_runtime": 128.9573,
      "eval_samples_per_second": 4.28,
      "eval_steps_per_second": 0.535,
      "step": 1709
    },
    {
      "epoch": 4.002340550029257,
      "grad_norm": 0.6521545052528381,
      "learning_rate": 0.0001998438719750195,
      "loss": 0.1884,
      "step": 1710
    },
    {
      "epoch": 4.004681100058514,
      "grad_norm": 0.612119734287262,
      "learning_rate": 0.00019976580796252923,
      "loss": 0.1466,
      "step": 1711
    },
    {
      "epoch": 4.00702165008777,
      "grad_norm": 0.6215600967407227,
      "learning_rate": 0.00019968774395003904,
      "loss": 0.1611,
      "step": 1712
    },
    {
      "epoch": 4.009362200117027,
      "grad_norm": 0.5412958860397339,
      "learning_rate": 0.00019960967993754876,
      "loss": 0.1405,
      "step": 1713
    },
    {
      "epoch": 4.011702750146284,
      "grad_norm": 0.49514660239219666,
      "learning_rate": 0.0001995316159250585,
      "loss": 0.1169,
      "step": 1714
    },
    {
      "epoch": 4.014043300175541,
      "grad_norm": 0.5565004348754883,
      "learning_rate": 0.0001994535519125683,
      "loss": 0.1389,
      "step": 1715
    },
    {
      "epoch": 4.016383850204798,
      "grad_norm": 0.5798431634902954,
      "learning_rate": 0.00019937548790007804,
      "loss": 0.1585,
      "step": 1716
    },
    {
      "epoch": 4.018724400234055,
      "grad_norm": 0.5151790976524353,
      "learning_rate": 0.0001992974238875878,
      "loss": 0.1319,
      "step": 1717
    },
    {
      "epoch": 4.021064950263312,
      "grad_norm": 0.5355885028839111,
      "learning_rate": 0.00019921935987509757,
      "loss": 0.1216,
      "step": 1718
    },
    {
      "epoch": 4.023405500292569,
      "grad_norm": 0.5517144799232483,
      "learning_rate": 0.00019914129586260732,
      "loss": 0.1132,
      "step": 1719
    },
    {
      "epoch": 4.025746050321826,
      "grad_norm": 0.5021116733551025,
      "learning_rate": 0.00019906323185011707,
      "loss": 0.1382,
      "step": 1720
    },
    {
      "epoch": 4.028086600351083,
      "grad_norm": 0.8751382231712341,
      "learning_rate": 0.00019898516783762685,
      "loss": 0.129,
      "step": 1721
    },
    {
      "epoch": 4.03042715038034,
      "grad_norm": 0.6568372249603271,
      "learning_rate": 0.0001989071038251366,
      "loss": 0.1445,
      "step": 1722
    },
    {
      "epoch": 4.032767700409596,
      "grad_norm": 0.6911223530769348,
      "learning_rate": 0.00019882903981264635,
      "loss": 0.2081,
      "step": 1723
    },
    {
      "epoch": 4.035108250438853,
      "grad_norm": 0.8266787528991699,
      "learning_rate": 0.00019875097580015612,
      "loss": 0.1806,
      "step": 1724
    },
    {
      "epoch": 4.03744880046811,
      "grad_norm": 0.5344163775444031,
      "learning_rate": 0.00019867291178766588,
      "loss": 0.1336,
      "step": 1725
    },
    {
      "epoch": 4.039789350497367,
      "grad_norm": 0.7219160795211792,
      "learning_rate": 0.00019859484777517563,
      "loss": 0.1473,
      "step": 1726
    },
    {
      "epoch": 4.042129900526624,
      "grad_norm": 0.47825244069099426,
      "learning_rate": 0.0001985167837626854,
      "loss": 0.1216,
      "step": 1727
    },
    {
      "epoch": 4.044470450555881,
      "grad_norm": 0.5668443441390991,
      "learning_rate": 0.00019843871975019515,
      "loss": 0.1233,
      "step": 1728
    },
    {
      "epoch": 4.046811000585137,
      "grad_norm": 0.6481621861457825,
      "learning_rate": 0.00019836065573770488,
      "loss": 0.1379,
      "step": 1729
    },
    {
      "epoch": 4.049151550614394,
      "grad_norm": 0.7749913930892944,
      "learning_rate": 0.00019828259172521468,
      "loss": 0.1164,
      "step": 1730
    },
    {
      "epoch": 4.051492100643651,
      "grad_norm": 0.7257227897644043,
      "learning_rate": 0.0001982045277127244,
      "loss": 0.1414,
      "step": 1731
    },
    {
      "epoch": 4.053832650672908,
      "grad_norm": 0.8701662421226501,
      "learning_rate": 0.00019812646370023416,
      "loss": 0.1742,
      "step": 1732
    },
    {
      "epoch": 4.056173200702165,
      "grad_norm": 0.695015549659729,
      "learning_rate": 0.00019804839968774393,
      "loss": 0.2104,
      "step": 1733
    },
    {
      "epoch": 4.0585137507314215,
      "grad_norm": 0.7151007652282715,
      "learning_rate": 0.00019797033567525368,
      "loss": 0.1495,
      "step": 1734
    },
    {
      "epoch": 4.0608543007606785,
      "grad_norm": 0.601492166519165,
      "learning_rate": 0.00019789227166276343,
      "loss": 0.1166,
      "step": 1735
    },
    {
      "epoch": 4.063194850789936,
      "grad_norm": 0.7483110427856445,
      "learning_rate": 0.0001978142076502732,
      "loss": 0.1554,
      "step": 1736
    },
    {
      "epoch": 4.065535400819193,
      "grad_norm": 0.6213191747665405,
      "learning_rate": 0.00019773614363778296,
      "loss": 0.1153,
      "step": 1737
    },
    {
      "epoch": 4.06787595084845,
      "grad_norm": 0.6498849391937256,
      "learning_rate": 0.0001976580796252927,
      "loss": 0.1629,
      "step": 1738
    },
    {
      "epoch": 4.070216500877706,
      "grad_norm": 0.7597633004188538,
      "learning_rate": 0.0001975800156128025,
      "loss": 0.1338,
      "step": 1739
    },
    {
      "epoch": 4.072557050906963,
      "grad_norm": 0.5680312514305115,
      "learning_rate": 0.00019750195160031224,
      "loss": 0.1215,
      "step": 1740
    },
    {
      "epoch": 4.07489760093622,
      "grad_norm": 0.6965517997741699,
      "learning_rate": 0.000197423887587822,
      "loss": 0.1512,
      "step": 1741
    },
    {
      "epoch": 4.077238150965477,
      "grad_norm": 0.6326507925987244,
      "learning_rate": 0.00019734582357533177,
      "loss": 0.1471,
      "step": 1742
    },
    {
      "epoch": 4.079578700994734,
      "grad_norm": 0.6528981924057007,
      "learning_rate": 0.00019726775956284152,
      "loss": 0.1419,
      "step": 1743
    },
    {
      "epoch": 4.081919251023991,
      "grad_norm": 0.7803091406822205,
      "learning_rate": 0.00019718969555035127,
      "loss": 0.1509,
      "step": 1744
    },
    {
      "epoch": 4.084259801053247,
      "grad_norm": 0.5860611796379089,
      "learning_rate": 0.00019711163153786105,
      "loss": 0.1362,
      "step": 1745
    },
    {
      "epoch": 4.086600351082504,
      "grad_norm": 0.9320594668388367,
      "learning_rate": 0.0001970335675253708,
      "loss": 0.1531,
      "step": 1746
    },
    {
      "epoch": 4.088940901111761,
      "grad_norm": 0.5580152869224548,
      "learning_rate": 0.00019695550351288052,
      "loss": 0.1005,
      "step": 1747
    },
    {
      "epoch": 4.091281451141018,
      "grad_norm": 0.8207828998565674,
      "learning_rate": 0.0001968774395003903,
      "loss": 0.1693,
      "step": 1748
    },
    {
      "epoch": 4.093622001170275,
      "grad_norm": 2.289133310317993,
      "learning_rate": 0.00019679937548790005,
      "loss": 0.1598,
      "step": 1749
    },
    {
      "epoch": 4.0959625511995315,
      "grad_norm": 0.8682012557983398,
      "learning_rate": 0.0001967213114754098,
      "loss": 0.1533,
      "step": 1750
    },
    {
      "epoch": 4.0983031012287885,
      "grad_norm": 0.6836218237876892,
      "learning_rate": 0.00019664324746291958,
      "loss": 0.1189,
      "step": 1751
    },
    {
      "epoch": 4.1006436512580455,
      "grad_norm": 0.947311520576477,
      "learning_rate": 0.00019656518345042933,
      "loss": 0.1987,
      "step": 1752
    },
    {
      "epoch": 4.102984201287303,
      "grad_norm": 0.7349843978881836,
      "learning_rate": 0.00019648711943793908,
      "loss": 0.1576,
      "step": 1753
    },
    {
      "epoch": 4.10532475131656,
      "grad_norm": 0.959129273891449,
      "learning_rate": 0.00019640905542544885,
      "loss": 0.1622,
      "step": 1754
    },
    {
      "epoch": 4.107665301345817,
      "grad_norm": 0.6665233373641968,
      "learning_rate": 0.0001963309914129586,
      "loss": 0.142,
      "step": 1755
    },
    {
      "epoch": 4.110005851375073,
      "grad_norm": 0.766828715801239,
      "learning_rate": 0.00019625292740046835,
      "loss": 0.1515,
      "step": 1756
    },
    {
      "epoch": 4.11234640140433,
      "grad_norm": 0.5277089476585388,
      "learning_rate": 0.00019617486338797813,
      "loss": 0.1203,
      "step": 1757
    },
    {
      "epoch": 4.114686951433587,
      "grad_norm": 1.1097087860107422,
      "learning_rate": 0.00019609679937548788,
      "loss": 0.1213,
      "step": 1758
    },
    {
      "epoch": 4.117027501462844,
      "grad_norm": 0.6643620729446411,
      "learning_rate": 0.00019601873536299763,
      "loss": 0.142,
      "step": 1759
    },
    {
      "epoch": 4.119368051492101,
      "grad_norm": 0.8011826276779175,
      "learning_rate": 0.0001959406713505074,
      "loss": 0.1421,
      "step": 1760
    },
    {
      "epoch": 4.121708601521357,
      "grad_norm": 0.762505292892456,
      "learning_rate": 0.00019586260733801716,
      "loss": 0.1776,
      "step": 1761
    },
    {
      "epoch": 4.124049151550614,
      "grad_norm": 0.9324020743370056,
      "learning_rate": 0.0001957845433255269,
      "loss": 0.1576,
      "step": 1762
    },
    {
      "epoch": 4.126389701579871,
      "grad_norm": 0.8485443592071533,
      "learning_rate": 0.0001957064793130367,
      "loss": 0.1785,
      "step": 1763
    },
    {
      "epoch": 4.128730251609128,
      "grad_norm": 0.5052439570426941,
      "learning_rate": 0.00019562841530054644,
      "loss": 0.122,
      "step": 1764
    },
    {
      "epoch": 4.131070801638385,
      "grad_norm": 0.9395536780357361,
      "learning_rate": 0.00019555035128805616,
      "loss": 0.1776,
      "step": 1765
    },
    {
      "epoch": 4.133411351667642,
      "grad_norm": 0.6415286064147949,
      "learning_rate": 0.00019547228727556594,
      "loss": 0.1508,
      "step": 1766
    },
    {
      "epoch": 4.135751901696898,
      "grad_norm": 0.6399238705635071,
      "learning_rate": 0.0001953942232630757,
      "loss": 0.0968,
      "step": 1767
    },
    {
      "epoch": 4.1380924517261555,
      "grad_norm": 0.733486533164978,
      "learning_rate": 0.00019531615925058544,
      "loss": 0.165,
      "step": 1768
    },
    {
      "epoch": 4.1404330017554125,
      "grad_norm": 0.5698510408401489,
      "learning_rate": 0.00019523809523809522,
      "loss": 0.0935,
      "step": 1769
    },
    {
      "epoch": 4.1427735517846696,
      "grad_norm": 0.8576094508171082,
      "learning_rate": 0.00019516003122560497,
      "loss": 0.1436,
      "step": 1770
    },
    {
      "epoch": 4.145114101813927,
      "grad_norm": 0.5845741629600525,
      "learning_rate": 0.00019508196721311472,
      "loss": 0.1232,
      "step": 1771
    },
    {
      "epoch": 4.147454651843183,
      "grad_norm": 0.7457246780395508,
      "learning_rate": 0.0001950039032006245,
      "loss": 0.1305,
      "step": 1772
    },
    {
      "epoch": 4.14979520187244,
      "grad_norm": 0.8672595620155334,
      "learning_rate": 0.00019492583918813425,
      "loss": 0.122,
      "step": 1773
    },
    {
      "epoch": 4.152135751901697,
      "grad_norm": 1.1413955688476562,
      "learning_rate": 0.00019484777517564402,
      "loss": 0.1554,
      "step": 1774
    },
    {
      "epoch": 4.154476301930954,
      "grad_norm": 0.6901500225067139,
      "learning_rate": 0.00019476971116315378,
      "loss": 0.1367,
      "step": 1775
    },
    {
      "epoch": 4.156816851960211,
      "grad_norm": 0.8172840476036072,
      "learning_rate": 0.00019469164715066353,
      "loss": 0.172,
      "step": 1776
    },
    {
      "epoch": 4.159157401989468,
      "grad_norm": 0.7077823281288147,
      "learning_rate": 0.0001946135831381733,
      "loss": 0.1249,
      "step": 1777
    },
    {
      "epoch": 4.161497952018724,
      "grad_norm": 0.8125540614128113,
      "learning_rate": 0.00019453551912568305,
      "loss": 0.1605,
      "step": 1778
    },
    {
      "epoch": 4.163838502047981,
      "grad_norm": 0.5695638656616211,
      "learning_rate": 0.0001944574551131928,
      "loss": 0.117,
      "step": 1779
    },
    {
      "epoch": 4.166179052077238,
      "grad_norm": 0.6021895408630371,
      "learning_rate": 0.00019437939110070258,
      "loss": 0.1513,
      "step": 1780
    },
    {
      "epoch": 4.168519602106495,
      "grad_norm": 0.8208651542663574,
      "learning_rate": 0.00019430132708821233,
      "loss": 0.1702,
      "step": 1781
    },
    {
      "epoch": 4.170860152135752,
      "grad_norm": 1.2076791524887085,
      "learning_rate": 0.00019422326307572208,
      "loss": 0.1678,
      "step": 1782
    },
    {
      "epoch": 4.173200702165008,
      "grad_norm": 0.4994677007198334,
      "learning_rate": 0.00019414519906323186,
      "loss": 0.1178,
      "step": 1783
    },
    {
      "epoch": 4.175541252194265,
      "grad_norm": 0.4884163737297058,
      "learning_rate": 0.00019406713505074158,
      "loss": 0.1054,
      "step": 1784
    },
    {
      "epoch": 4.1778818022235225,
      "grad_norm": 0.5710026025772095,
      "learning_rate": 0.00019398907103825133,
      "loss": 0.1385,
      "step": 1785
    },
    {
      "epoch": 4.1802223522527795,
      "grad_norm": 0.6580292582511902,
      "learning_rate": 0.0001939110070257611,
      "loss": 0.1215,
      "step": 1786
    },
    {
      "epoch": 4.1825629022820365,
      "grad_norm": 0.6298270225524902,
      "learning_rate": 0.00019383294301327086,
      "loss": 0.1422,
      "step": 1787
    },
    {
      "epoch": 4.184903452311294,
      "grad_norm": 0.7200627326965332,
      "learning_rate": 0.0001937548790007806,
      "loss": 0.1759,
      "step": 1788
    },
    {
      "epoch": 4.18724400234055,
      "grad_norm": 0.7708375453948975,
      "learning_rate": 0.0001936768149882904,
      "loss": 0.1971,
      "step": 1789
    },
    {
      "epoch": 4.189584552369807,
      "grad_norm": 0.7057085633277893,
      "learning_rate": 0.00019359875097580014,
      "loss": 0.1781,
      "step": 1790
    },
    {
      "epoch": 4.191925102399064,
      "grad_norm": 0.6953216791152954,
      "learning_rate": 0.0001935206869633099,
      "loss": 0.1555,
      "step": 1791
    },
    {
      "epoch": 4.194265652428321,
      "grad_norm": 0.6555821299552917,
      "learning_rate": 0.00019344262295081967,
      "loss": 0.1379,
      "step": 1792
    },
    {
      "epoch": 4.196606202457578,
      "grad_norm": 0.5612399578094482,
      "learning_rate": 0.00019336455893832942,
      "loss": 0.1369,
      "step": 1793
    },
    {
      "epoch": 4.198946752486834,
      "grad_norm": 0.711033046245575,
      "learning_rate": 0.00019328649492583917,
      "loss": 0.1506,
      "step": 1794
    },
    {
      "epoch": 4.201287302516091,
      "grad_norm": 0.5210107564926147,
      "learning_rate": 0.00019320843091334895,
      "loss": 0.1233,
      "step": 1795
    },
    {
      "epoch": 4.203627852545348,
      "grad_norm": 0.6328303217887878,
      "learning_rate": 0.0001931303669008587,
      "loss": 0.1416,
      "step": 1796
    },
    {
      "epoch": 4.205968402574605,
      "grad_norm": 0.5534288287162781,
      "learning_rate": 0.00019305230288836845,
      "loss": 0.1468,
      "step": 1797
    },
    {
      "epoch": 4.208308952603862,
      "grad_norm": 0.7562413215637207,
      "learning_rate": 0.00019297423887587822,
      "loss": 0.155,
      "step": 1798
    },
    {
      "epoch": 4.210649502633119,
      "grad_norm": 0.8558638095855713,
      "learning_rate": 0.00019289617486338797,
      "loss": 0.1786,
      "step": 1799
    },
    {
      "epoch": 4.212990052662375,
      "grad_norm": 1.101816177368164,
      "learning_rate": 0.0001928181108508977,
      "loss": 0.2401,
      "step": 1800
    },
    {
      "epoch": 4.215330602691632,
      "grad_norm": 0.6708752512931824,
      "learning_rate": 0.0001927400468384075,
      "loss": 0.1298,
      "step": 1801
    },
    {
      "epoch": 4.217671152720889,
      "grad_norm": 0.6223053336143494,
      "learning_rate": 0.00019266198282591723,
      "loss": 0.1365,
      "step": 1802
    },
    {
      "epoch": 4.2200117027501465,
      "grad_norm": 0.6675848960876465,
      "learning_rate": 0.00019258391881342698,
      "loss": 0.1184,
      "step": 1803
    },
    {
      "epoch": 4.2223522527794035,
      "grad_norm": 0.6607059240341187,
      "learning_rate": 0.00019250585480093675,
      "loss": 0.1419,
      "step": 1804
    },
    {
      "epoch": 4.22469280280866,
      "grad_norm": 0.7375609278678894,
      "learning_rate": 0.0001924277907884465,
      "loss": 0.1424,
      "step": 1805
    },
    {
      "epoch": 4.227033352837917,
      "grad_norm": 0.7433579564094543,
      "learning_rate": 0.00019234972677595625,
      "loss": 0.208,
      "step": 1806
    },
    {
      "epoch": 4.229373902867174,
      "grad_norm": 0.6439061164855957,
      "learning_rate": 0.00019227166276346603,
      "loss": 0.185,
      "step": 1807
    },
    {
      "epoch": 4.231714452896431,
      "grad_norm": 0.6850075125694275,
      "learning_rate": 0.00019219359875097578,
      "loss": 0.1442,
      "step": 1808
    },
    {
      "epoch": 4.234055002925688,
      "grad_norm": 0.8280903697013855,
      "learning_rate": 0.00019211553473848553,
      "loss": 0.1647,
      "step": 1809
    },
    {
      "epoch": 4.236395552954944,
      "grad_norm": 0.759894847869873,
      "learning_rate": 0.0001920374707259953,
      "loss": 0.1845,
      "step": 1810
    },
    {
      "epoch": 4.238736102984201,
      "grad_norm": 0.5290737152099609,
      "learning_rate": 0.00019195940671350506,
      "loss": 0.0955,
      "step": 1811
    },
    {
      "epoch": 4.241076653013458,
      "grad_norm": 0.6204432249069214,
      "learning_rate": 0.0001918813427010148,
      "loss": 0.1676,
      "step": 1812
    },
    {
      "epoch": 4.243417203042715,
      "grad_norm": 0.6306606531143188,
      "learning_rate": 0.0001918032786885246,
      "loss": 0.1392,
      "step": 1813
    },
    {
      "epoch": 4.245757753071972,
      "grad_norm": 0.7927477359771729,
      "learning_rate": 0.00019172521467603434,
      "loss": 0.1734,
      "step": 1814
    },
    {
      "epoch": 4.248098303101229,
      "grad_norm": 0.5753564238548279,
      "learning_rate": 0.0001916471506635441,
      "loss": 0.1264,
      "step": 1815
    },
    {
      "epoch": 4.250438853130485,
      "grad_norm": 0.8081679940223694,
      "learning_rate": 0.00019156908665105387,
      "loss": 0.1628,
      "step": 1816
    },
    {
      "epoch": 4.252779403159742,
      "grad_norm": 0.6126545071601868,
      "learning_rate": 0.00019149102263856362,
      "loss": 0.1352,
      "step": 1817
    },
    {
      "epoch": 4.255119953188999,
      "grad_norm": 0.5398722290992737,
      "learning_rate": 0.00019141295862607334,
      "loss": 0.149,
      "step": 1818
    },
    {
      "epoch": 4.257460503218256,
      "grad_norm": 0.5043925642967224,
      "learning_rate": 0.00019133489461358315,
      "loss": 0.1067,
      "step": 1819
    },
    {
      "epoch": 4.2598010532475135,
      "grad_norm": 0.6181203126907349,
      "learning_rate": 0.00019125683060109287,
      "loss": 0.1801,
      "step": 1820
    },
    {
      "epoch": 4.2621416032767705,
      "grad_norm": 0.7015630006790161,
      "learning_rate": 0.00019117876658860262,
      "loss": 0.1001,
      "step": 1821
    },
    {
      "epoch": 4.264482153306027,
      "grad_norm": 0.6787303686141968,
      "learning_rate": 0.0001911007025761124,
      "loss": 0.2105,
      "step": 1822
    },
    {
      "epoch": 4.266822703335284,
      "grad_norm": 0.6693812012672424,
      "learning_rate": 0.00019102263856362215,
      "loss": 0.161,
      "step": 1823
    },
    {
      "epoch": 4.269163253364541,
      "grad_norm": 0.7397106289863586,
      "learning_rate": 0.0001909445745511319,
      "loss": 0.176,
      "step": 1824
    },
    {
      "epoch": 4.271503803393798,
      "grad_norm": 0.6957216262817383,
      "learning_rate": 0.00019086651053864168,
      "loss": 0.1528,
      "step": 1825
    },
    {
      "epoch": 4.273844353423055,
      "grad_norm": 0.621731162071228,
      "learning_rate": 0.00019078844652615143,
      "loss": 0.1554,
      "step": 1826
    },
    {
      "epoch": 4.276184903452311,
      "grad_norm": 0.6572393774986267,
      "learning_rate": 0.00019071038251366118,
      "loss": 0.1266,
      "step": 1827
    },
    {
      "epoch": 4.278525453481568,
      "grad_norm": 0.6036361455917358,
      "learning_rate": 0.00019063231850117095,
      "loss": 0.1455,
      "step": 1828
    },
    {
      "epoch": 4.280866003510825,
      "grad_norm": 0.4691389799118042,
      "learning_rate": 0.0001905542544886807,
      "loss": 0.1086,
      "step": 1829
    },
    {
      "epoch": 4.283206553540082,
      "grad_norm": 0.6794605255126953,
      "learning_rate": 0.00019047619047619045,
      "loss": 0.1585,
      "step": 1830
    },
    {
      "epoch": 4.285547103569339,
      "grad_norm": 0.5609448552131653,
      "learning_rate": 0.00019039812646370023,
      "loss": 0.1405,
      "step": 1831
    },
    {
      "epoch": 4.287887653598595,
      "grad_norm": 0.7699596881866455,
      "learning_rate": 0.00019032006245120998,
      "loss": 0.2018,
      "step": 1832
    },
    {
      "epoch": 4.290228203627852,
      "grad_norm": 0.8140062093734741,
      "learning_rate": 0.00019024199843871973,
      "loss": 0.1658,
      "step": 1833
    },
    {
      "epoch": 4.292568753657109,
      "grad_norm": 0.6166318655014038,
      "learning_rate": 0.0001901639344262295,
      "loss": 0.1371,
      "step": 1834
    },
    {
      "epoch": 4.294909303686366,
      "grad_norm": 0.8357102870941162,
      "learning_rate": 0.00019008587041373926,
      "loss": 0.1459,
      "step": 1835
    },
    {
      "epoch": 4.297249853715623,
      "grad_norm": 0.7914184927940369,
      "learning_rate": 0.00019000780640124898,
      "loss": 0.1658,
      "step": 1836
    },
    {
      "epoch": 4.29959040374488,
      "grad_norm": 0.5640776753425598,
      "learning_rate": 0.0001899297423887588,
      "loss": 0.1039,
      "step": 1837
    },
    {
      "epoch": 4.301930953774137,
      "grad_norm": 0.6817331910133362,
      "learning_rate": 0.0001898516783762685,
      "loss": 0.1401,
      "step": 1838
    },
    {
      "epoch": 4.304271503803394,
      "grad_norm": 0.6312854886054993,
      "learning_rate": 0.00018977361436377826,
      "loss": 0.1475,
      "step": 1839
    },
    {
      "epoch": 4.306612053832651,
      "grad_norm": 0.6121812462806702,
      "learning_rate": 0.00018969555035128804,
      "loss": 0.1273,
      "step": 1840
    },
    {
      "epoch": 4.308952603861908,
      "grad_norm": 0.5955795049667358,
      "learning_rate": 0.0001896174863387978,
      "loss": 0.1498,
      "step": 1841
    },
    {
      "epoch": 4.311293153891165,
      "grad_norm": 0.7778139710426331,
      "learning_rate": 0.00018953942232630754,
      "loss": 0.1462,
      "step": 1842
    },
    {
      "epoch": 4.313633703920421,
      "grad_norm": 0.6614415645599365,
      "learning_rate": 0.00018946135831381732,
      "loss": 0.1824,
      "step": 1843
    },
    {
      "epoch": 4.315974253949678,
      "grad_norm": 0.7197617888450623,
      "learning_rate": 0.00018938329430132707,
      "loss": 0.1775,
      "step": 1844
    },
    {
      "epoch": 4.318314803978935,
      "grad_norm": 0.7729789614677429,
      "learning_rate": 0.00018930523028883682,
      "loss": 0.1088,
      "step": 1845
    },
    {
      "epoch": 4.320655354008192,
      "grad_norm": 0.5506161451339722,
      "learning_rate": 0.0001892271662763466,
      "loss": 0.1267,
      "step": 1846
    },
    {
      "epoch": 4.322995904037449,
      "grad_norm": 0.651040256023407,
      "learning_rate": 0.00018914910226385635,
      "loss": 0.1524,
      "step": 1847
    },
    {
      "epoch": 4.325336454066706,
      "grad_norm": 0.9929510951042175,
      "learning_rate": 0.0001890710382513661,
      "loss": 0.1949,
      "step": 1848
    },
    {
      "epoch": 4.327677004095962,
      "grad_norm": 0.57872474193573,
      "learning_rate": 0.00018899297423887587,
      "loss": 0.129,
      "step": 1849
    },
    {
      "epoch": 4.330017554125219,
      "grad_norm": 0.7679044604301453,
      "learning_rate": 0.00018891491022638563,
      "loss": 0.1799,
      "step": 1850
    },
    {
      "epoch": 4.332358104154476,
      "grad_norm": 0.7477898001670837,
      "learning_rate": 0.00018883684621389538,
      "loss": 0.1871,
      "step": 1851
    },
    {
      "epoch": 4.334698654183733,
      "grad_norm": 0.4990279972553253,
      "learning_rate": 0.00018875878220140515,
      "loss": 0.1272,
      "step": 1852
    },
    {
      "epoch": 4.33703920421299,
      "grad_norm": 0.6542421579360962,
      "learning_rate": 0.0001886807181889149,
      "loss": 0.1387,
      "step": 1853
    },
    {
      "epoch": 4.3393797542422465,
      "grad_norm": 0.7303916215896606,
      "learning_rate": 0.00018860265417642463,
      "loss": 0.1251,
      "step": 1854
    },
    {
      "epoch": 4.341720304271504,
      "grad_norm": 0.812468945980072,
      "learning_rate": 0.00018852459016393443,
      "loss": 0.2257,
      "step": 1855
    },
    {
      "epoch": 4.344060854300761,
      "grad_norm": 0.8738968372344971,
      "learning_rate": 0.00018844652615144415,
      "loss": 0.1708,
      "step": 1856
    },
    {
      "epoch": 4.346401404330018,
      "grad_norm": 0.8243931531906128,
      "learning_rate": 0.0001883684621389539,
      "loss": 0.1719,
      "step": 1857
    },
    {
      "epoch": 4.348741954359275,
      "grad_norm": 0.9854154586791992,
      "learning_rate": 0.00018829039812646368,
      "loss": 0.1847,
      "step": 1858
    },
    {
      "epoch": 4.351082504388531,
      "grad_norm": 0.8128676414489746,
      "learning_rate": 0.00018821233411397343,
      "loss": 0.1712,
      "step": 1859
    },
    {
      "epoch": 4.353423054417788,
      "grad_norm": 0.7376213669776917,
      "learning_rate": 0.00018813427010148318,
      "loss": 0.1337,
      "step": 1860
    },
    {
      "epoch": 4.355763604447045,
      "grad_norm": 0.5967316031455994,
      "learning_rate": 0.00018805620608899296,
      "loss": 0.1791,
      "step": 1861
    },
    {
      "epoch": 4.358104154476302,
      "grad_norm": 0.5463809370994568,
      "learning_rate": 0.0001879781420765027,
      "loss": 0.1393,
      "step": 1862
    },
    {
      "epoch": 4.360444704505559,
      "grad_norm": 0.6153857707977295,
      "learning_rate": 0.00018790007806401246,
      "loss": 0.1227,
      "step": 1863
    },
    {
      "epoch": 4.362785254534816,
      "grad_norm": 0.6292644143104553,
      "learning_rate": 0.00018782201405152224,
      "loss": 0.1437,
      "step": 1864
    },
    {
      "epoch": 4.365125804564072,
      "grad_norm": 0.566677451133728,
      "learning_rate": 0.000187743950039032,
      "loss": 0.1562,
      "step": 1865
    },
    {
      "epoch": 4.367466354593329,
      "grad_norm": 0.7518686056137085,
      "learning_rate": 0.00018766588602654174,
      "loss": 0.1604,
      "step": 1866
    },
    {
      "epoch": 4.369806904622586,
      "grad_norm": 0.6581715941429138,
      "learning_rate": 0.00018758782201405152,
      "loss": 0.1844,
      "step": 1867
    },
    {
      "epoch": 4.372147454651843,
      "grad_norm": 0.7687103748321533,
      "learning_rate": 0.00018750975800156127,
      "loss": 0.1241,
      "step": 1868
    },
    {
      "epoch": 4.3744880046811,
      "grad_norm": 0.6024690866470337,
      "learning_rate": 0.00018743169398907102,
      "loss": 0.1686,
      "step": 1869
    },
    {
      "epoch": 4.376828554710357,
      "grad_norm": 0.6393648982048035,
      "learning_rate": 0.0001873536299765808,
      "loss": 0.1475,
      "step": 1870
    },
    {
      "epoch": 4.3791691047396135,
      "grad_norm": 0.5909847021102905,
      "learning_rate": 0.00018727556596409055,
      "loss": 0.1125,
      "step": 1871
    },
    {
      "epoch": 4.3815096547688706,
      "grad_norm": 0.9156430959701538,
      "learning_rate": 0.00018719750195160027,
      "loss": 0.211,
      "step": 1872
    },
    {
      "epoch": 4.383850204798128,
      "grad_norm": 0.945854663848877,
      "learning_rate": 0.00018711943793911007,
      "loss": 0.1978,
      "step": 1873
    },
    {
      "epoch": 4.386190754827385,
      "grad_norm": 0.8146758079528809,
      "learning_rate": 0.0001870413739266198,
      "loss": 0.1175,
      "step": 1874
    },
    {
      "epoch": 4.388531304856642,
      "grad_norm": 0.6700084209442139,
      "learning_rate": 0.00018696330991412955,
      "loss": 0.1284,
      "step": 1875
    },
    {
      "epoch": 4.390871854885898,
      "grad_norm": 0.8443002104759216,
      "learning_rate": 0.00018688524590163933,
      "loss": 0.1772,
      "step": 1876
    },
    {
      "epoch": 4.393212404915155,
      "grad_norm": 1.04275381565094,
      "learning_rate": 0.00018680718188914908,
      "loss": 0.1249,
      "step": 1877
    },
    {
      "epoch": 4.395552954944412,
      "grad_norm": 0.6317563652992249,
      "learning_rate": 0.00018672911787665883,
      "loss": 0.1085,
      "step": 1878
    },
    {
      "epoch": 4.397893504973669,
      "grad_norm": 0.8001025319099426,
      "learning_rate": 0.0001866510538641686,
      "loss": 0.1586,
      "step": 1879
    },
    {
      "epoch": 4.400234055002926,
      "grad_norm": 0.5789204239845276,
      "learning_rate": 0.00018657298985167835,
      "loss": 0.141,
      "step": 1880
    },
    {
      "epoch": 4.402574605032182,
      "grad_norm": 0.4793316423892975,
      "learning_rate": 0.0001864949258391881,
      "loss": 0.1149,
      "step": 1881
    },
    {
      "epoch": 4.404915155061439,
      "grad_norm": 0.8176646828651428,
      "learning_rate": 0.00018641686182669788,
      "loss": 0.1794,
      "step": 1882
    },
    {
      "epoch": 4.407255705090696,
      "grad_norm": 0.5411889553070068,
      "learning_rate": 0.00018633879781420763,
      "loss": 0.1064,
      "step": 1883
    },
    {
      "epoch": 4.409596255119953,
      "grad_norm": 0.6509284377098083,
      "learning_rate": 0.00018626073380171738,
      "loss": 0.1378,
      "step": 1884
    },
    {
      "epoch": 4.41193680514921,
      "grad_norm": 0.8041638731956482,
      "learning_rate": 0.00018618266978922716,
      "loss": 0.1572,
      "step": 1885
    },
    {
      "epoch": 4.414277355178467,
      "grad_norm": 0.5938758850097656,
      "learning_rate": 0.0001861046057767369,
      "loss": 0.1282,
      "step": 1886
    },
    {
      "epoch": 4.4166179052077235,
      "grad_norm": 0.7067198753356934,
      "learning_rate": 0.00018602654176424666,
      "loss": 0.1619,
      "step": 1887
    },
    {
      "epoch": 4.4189584552369805,
      "grad_norm": 0.6333790421485901,
      "learning_rate": 0.00018594847775175644,
      "loss": 0.1417,
      "step": 1888
    },
    {
      "epoch": 4.4212990052662375,
      "grad_norm": 0.5950586199760437,
      "learning_rate": 0.0001858704137392662,
      "loss": 0.1173,
      "step": 1889
    },
    {
      "epoch": 4.423639555295495,
      "grad_norm": 0.6132529377937317,
      "learning_rate": 0.0001857923497267759,
      "loss": 0.1566,
      "step": 1890
    },
    {
      "epoch": 4.425980105324752,
      "grad_norm": 0.8204821348190308,
      "learning_rate": 0.00018571428571428572,
      "loss": 0.1496,
      "step": 1891
    },
    {
      "epoch": 4.428320655354009,
      "grad_norm": 0.6130139827728271,
      "learning_rate": 0.00018563622170179544,
      "loss": 0.145,
      "step": 1892
    },
    {
      "epoch": 4.430661205383265,
      "grad_norm": 0.7158376574516296,
      "learning_rate": 0.0001855581576893052,
      "loss": 0.1388,
      "step": 1893
    },
    {
      "epoch": 4.433001755412522,
      "grad_norm": 0.6508030891418457,
      "learning_rate": 0.00018548009367681497,
      "loss": 0.1724,
      "step": 1894
    },
    {
      "epoch": 4.435342305441779,
      "grad_norm": 0.6326659321784973,
      "learning_rate": 0.00018540202966432472,
      "loss": 0.1603,
      "step": 1895
    },
    {
      "epoch": 4.437682855471036,
      "grad_norm": 0.5771428942680359,
      "learning_rate": 0.00018532396565183447,
      "loss": 0.115,
      "step": 1896
    },
    {
      "epoch": 4.440023405500293,
      "grad_norm": 0.6921930909156799,
      "learning_rate": 0.00018524590163934425,
      "loss": 0.1469,
      "step": 1897
    },
    {
      "epoch": 4.442363955529549,
      "grad_norm": 0.8974649310112,
      "learning_rate": 0.000185167837626854,
      "loss": 0.1538,
      "step": 1898
    },
    {
      "epoch": 4.444704505558806,
      "grad_norm": 0.6341091990470886,
      "learning_rate": 0.00018508977361436375,
      "loss": 0.0975,
      "step": 1899
    },
    {
      "epoch": 4.447045055588063,
      "grad_norm": 0.5688852071762085,
      "learning_rate": 0.00018501170960187353,
      "loss": 0.1346,
      "step": 1900
    },
    {
      "epoch": 4.44938560561732,
      "grad_norm": 0.6938167810440063,
      "learning_rate": 0.00018493364558938328,
      "loss": 0.1454,
      "step": 1901
    },
    {
      "epoch": 4.451726155646577,
      "grad_norm": 0.7541831731796265,
      "learning_rate": 0.00018485558157689305,
      "loss": 0.1733,
      "step": 1902
    },
    {
      "epoch": 4.454066705675833,
      "grad_norm": 0.8770627379417419,
      "learning_rate": 0.0001847775175644028,
      "loss": 0.1593,
      "step": 1903
    },
    {
      "epoch": 4.45640725570509,
      "grad_norm": 1.173012614250183,
      "learning_rate": 0.00018469945355191255,
      "loss": 0.1772,
      "step": 1904
    },
    {
      "epoch": 4.4587478057343475,
      "grad_norm": 0.7861271500587463,
      "learning_rate": 0.00018462138953942233,
      "loss": 0.174,
      "step": 1905
    },
    {
      "epoch": 4.4610883557636045,
      "grad_norm": 0.8689467906951904,
      "learning_rate": 0.00018454332552693208,
      "loss": 0.1667,
      "step": 1906
    },
    {
      "epoch": 4.463428905792862,
      "grad_norm": 0.7456481456756592,
      "learning_rate": 0.00018446526151444183,
      "loss": 0.1538,
      "step": 1907
    },
    {
      "epoch": 4.465769455822119,
      "grad_norm": 0.6519671082496643,
      "learning_rate": 0.0001843871975019516,
      "loss": 0.1337,
      "step": 1908
    },
    {
      "epoch": 4.468110005851375,
      "grad_norm": 0.5853421092033386,
      "learning_rate": 0.00018430913348946136,
      "loss": 0.1467,
      "step": 1909
    },
    {
      "epoch": 4.470450555880632,
      "grad_norm": 0.8078714609146118,
      "learning_rate": 0.00018423106947697108,
      "loss": 0.1393,
      "step": 1910
    },
    {
      "epoch": 4.472791105909889,
      "grad_norm": 1.3151562213897705,
      "learning_rate": 0.00018415300546448086,
      "loss": 0.2049,
      "step": 1911
    },
    {
      "epoch": 4.475131655939146,
      "grad_norm": 0.8046008348464966,
      "learning_rate": 0.0001840749414519906,
      "loss": 0.1396,
      "step": 1912
    },
    {
      "epoch": 4.477472205968403,
      "grad_norm": 0.694299042224884,
      "learning_rate": 0.00018399687743950036,
      "loss": 0.1532,
      "step": 1913
    },
    {
      "epoch": 4.479812755997659,
      "grad_norm": 0.5581992268562317,
      "learning_rate": 0.00018391881342701014,
      "loss": 0.1512,
      "step": 1914
    },
    {
      "epoch": 4.482153306026916,
      "grad_norm": 0.6181902885437012,
      "learning_rate": 0.0001838407494145199,
      "loss": 0.1455,
      "step": 1915
    },
    {
      "epoch": 4.484493856056173,
      "grad_norm": 0.668755054473877,
      "learning_rate": 0.00018376268540202964,
      "loss": 0.1554,
      "step": 1916
    },
    {
      "epoch": 4.48683440608543,
      "grad_norm": 0.8011254072189331,
      "learning_rate": 0.00018368462138953942,
      "loss": 0.1637,
      "step": 1917
    },
    {
      "epoch": 4.489174956114687,
      "grad_norm": 0.6899452805519104,
      "learning_rate": 0.00018360655737704917,
      "loss": 0.1179,
      "step": 1918
    },
    {
      "epoch": 4.491515506143944,
      "grad_norm": 0.6292478442192078,
      "learning_rate": 0.00018352849336455892,
      "loss": 0.1323,
      "step": 1919
    },
    {
      "epoch": 4.4938560561732,
      "grad_norm": 0.6863187551498413,
      "learning_rate": 0.0001834504293520687,
      "loss": 0.1632,
      "step": 1920
    },
    {
      "epoch": 4.496196606202457,
      "grad_norm": 0.8276425004005432,
      "learning_rate": 0.00018337236533957845,
      "loss": 0.1579,
      "step": 1921
    },
    {
      "epoch": 4.4985371562317145,
      "grad_norm": 0.792519748210907,
      "learning_rate": 0.0001832943013270882,
      "loss": 0.1755,
      "step": 1922
    },
    {
      "epoch": 4.5008777062609715,
      "grad_norm": 0.7766165733337402,
      "learning_rate": 0.00018321623731459797,
      "loss": 0.1633,
      "step": 1923
    },
    {
      "epoch": 4.5032182562902285,
      "grad_norm": 0.5395877361297607,
      "learning_rate": 0.00018313817330210772,
      "loss": 0.1479,
      "step": 1924
    },
    {
      "epoch": 4.505558806319485,
      "grad_norm": 0.7743718028068542,
      "learning_rate": 0.00018306010928961748,
      "loss": 0.1494,
      "step": 1925
    },
    {
      "epoch": 4.507899356348742,
      "grad_norm": 0.6842054128646851,
      "learning_rate": 0.00018298204527712725,
      "loss": 0.1587,
      "step": 1926
    },
    {
      "epoch": 4.510239906377999,
      "grad_norm": 0.767167866230011,
      "learning_rate": 0.00018290398126463698,
      "loss": 0.1391,
      "step": 1927
    },
    {
      "epoch": 4.512580456407256,
      "grad_norm": 0.570317268371582,
      "learning_rate": 0.00018282591725214673,
      "loss": 0.1512,
      "step": 1928
    },
    {
      "epoch": 4.514921006436513,
      "grad_norm": 0.7238123416900635,
      "learning_rate": 0.0001827478532396565,
      "loss": 0.1418,
      "step": 1929
    },
    {
      "epoch": 4.517261556465769,
      "grad_norm": 0.4239139258861542,
      "learning_rate": 0.00018266978922716625,
      "loss": 0.1101,
      "step": 1930
    },
    {
      "epoch": 4.519602106495026,
      "grad_norm": 0.5561056137084961,
      "learning_rate": 0.000182591725214676,
      "loss": 0.1502,
      "step": 1931
    },
    {
      "epoch": 4.521942656524283,
      "grad_norm": 1.2868908643722534,
      "learning_rate": 0.00018251366120218578,
      "loss": 0.173,
      "step": 1932
    },
    {
      "epoch": 4.52428320655354,
      "grad_norm": 0.5814483165740967,
      "learning_rate": 0.00018243559718969553,
      "loss": 0.1392,
      "step": 1933
    },
    {
      "epoch": 4.526623756582797,
      "grad_norm": 0.5037684440612793,
      "learning_rate": 0.00018235753317720528,
      "loss": 0.1349,
      "step": 1934
    },
    {
      "epoch": 4.528964306612054,
      "grad_norm": 0.4781155586242676,
      "learning_rate": 0.00018227946916471506,
      "loss": 0.0988,
      "step": 1935
    },
    {
      "epoch": 4.53130485664131,
      "grad_norm": 0.5105490684509277,
      "learning_rate": 0.0001822014051522248,
      "loss": 0.1074,
      "step": 1936
    },
    {
      "epoch": 4.533645406670567,
      "grad_norm": 0.7463979721069336,
      "learning_rate": 0.00018212334113973456,
      "loss": 0.2001,
      "step": 1937
    },
    {
      "epoch": 4.535985956699824,
      "grad_norm": 0.5882770419120789,
      "learning_rate": 0.00018204527712724434,
      "loss": 0.1299,
      "step": 1938
    },
    {
      "epoch": 4.5383265067290814,
      "grad_norm": 0.5505688190460205,
      "learning_rate": 0.0001819672131147541,
      "loss": 0.1198,
      "step": 1939
    },
    {
      "epoch": 4.5406670567583385,
      "grad_norm": 0.6006383299827576,
      "learning_rate": 0.00018188914910226384,
      "loss": 0.1283,
      "step": 1940
    },
    {
      "epoch": 4.5430076067875955,
      "grad_norm": 0.8111487627029419,
      "learning_rate": 0.00018181108508977362,
      "loss": 0.1881,
      "step": 1941
    },
    {
      "epoch": 4.545348156816852,
      "grad_norm": 0.6074774265289307,
      "learning_rate": 0.00018173302107728337,
      "loss": 0.1381,
      "step": 1942
    },
    {
      "epoch": 4.547688706846109,
      "grad_norm": 1.0033446550369263,
      "learning_rate": 0.00018165495706479312,
      "loss": 0.1389,
      "step": 1943
    },
    {
      "epoch": 4.550029256875366,
      "grad_norm": 1.3386938571929932,
      "learning_rate": 0.0001815768930523029,
      "loss": 0.2038,
      "step": 1944
    },
    {
      "epoch": 4.552369806904623,
      "grad_norm": 0.8190494179725647,
      "learning_rate": 0.00018149882903981262,
      "loss": 0.1693,
      "step": 1945
    },
    {
      "epoch": 4.55471035693388,
      "grad_norm": 0.5356841087341309,
      "learning_rate": 0.00018142076502732237,
      "loss": 0.1401,
      "step": 1946
    },
    {
      "epoch": 4.557050906963136,
      "grad_norm": 0.6040247082710266,
      "learning_rate": 0.00018134270101483215,
      "loss": 0.0981,
      "step": 1947
    },
    {
      "epoch": 4.559391456992393,
      "grad_norm": 0.9683418273925781,
      "learning_rate": 0.0001812646370023419,
      "loss": 0.1788,
      "step": 1948
    },
    {
      "epoch": 4.56173200702165,
      "grad_norm": 0.7729719877243042,
      "learning_rate": 0.00018118657298985165,
      "loss": 0.1376,
      "step": 1949
    },
    {
      "epoch": 4.564072557050907,
      "grad_norm": 0.7611526846885681,
      "learning_rate": 0.00018110850897736143,
      "loss": 0.1389,
      "step": 1950
    },
    {
      "epoch": 4.566413107080164,
      "grad_norm": 0.7618671655654907,
      "learning_rate": 0.00018103044496487118,
      "loss": 0.146,
      "step": 1951
    },
    {
      "epoch": 4.56875365710942,
      "grad_norm": 0.8869189023971558,
      "learning_rate": 0.00018095238095238093,
      "loss": 0.1946,
      "step": 1952
    },
    {
      "epoch": 4.571094207138677,
      "grad_norm": 0.6371983289718628,
      "learning_rate": 0.0001808743169398907,
      "loss": 0.1494,
      "step": 1953
    },
    {
      "epoch": 4.573434757167934,
      "grad_norm": 0.8092763423919678,
      "learning_rate": 0.00018079625292740045,
      "loss": 0.1756,
      "step": 1954
    },
    {
      "epoch": 4.575775307197191,
      "grad_norm": 0.5741297006607056,
      "learning_rate": 0.0001807181889149102,
      "loss": 0.1047,
      "step": 1955
    },
    {
      "epoch": 4.578115857226448,
      "grad_norm": 0.5044792294502258,
      "learning_rate": 0.00018064012490241998,
      "loss": 0.1193,
      "step": 1956
    },
    {
      "epoch": 4.5804564072557055,
      "grad_norm": 0.7887789011001587,
      "learning_rate": 0.00018056206088992973,
      "loss": 0.14,
      "step": 1957
    },
    {
      "epoch": 4.582796957284962,
      "grad_norm": 0.8975586295127869,
      "learning_rate": 0.00018048399687743948,
      "loss": 0.2055,
      "step": 1958
    },
    {
      "epoch": 4.585137507314219,
      "grad_norm": 0.7729550004005432,
      "learning_rate": 0.00018040593286494926,
      "loss": 0.1394,
      "step": 1959
    },
    {
      "epoch": 4.587478057343476,
      "grad_norm": 0.6999059319496155,
      "learning_rate": 0.000180327868852459,
      "loss": 0.1505,
      "step": 1960
    },
    {
      "epoch": 4.589818607372733,
      "grad_norm": 0.6003915071487427,
      "learning_rate": 0.00018024980483996876,
      "loss": 0.1391,
      "step": 1961
    },
    {
      "epoch": 4.59215915740199,
      "grad_norm": 0.6999074220657349,
      "learning_rate": 0.00018017174082747854,
      "loss": 0.1377,
      "step": 1962
    },
    {
      "epoch": 4.594499707431247,
      "grad_norm": 0.5209636688232422,
      "learning_rate": 0.00018009367681498826,
      "loss": 0.1335,
      "step": 1963
    },
    {
      "epoch": 4.596840257460503,
      "grad_norm": 0.622645378112793,
      "learning_rate": 0.000180015612802498,
      "loss": 0.1313,
      "step": 1964
    },
    {
      "epoch": 4.59918080748976,
      "grad_norm": 0.6232007145881653,
      "learning_rate": 0.0001799375487900078,
      "loss": 0.107,
      "step": 1965
    },
    {
      "epoch": 4.601521357519017,
      "grad_norm": 0.7079991698265076,
      "learning_rate": 0.00017985948477751754,
      "loss": 0.1411,
      "step": 1966
    },
    {
      "epoch": 4.603861907548274,
      "grad_norm": 0.7496300935745239,
      "learning_rate": 0.0001797814207650273,
      "loss": 0.1667,
      "step": 1967
    },
    {
      "epoch": 4.606202457577531,
      "grad_norm": 0.7682214379310608,
      "learning_rate": 0.00017970335675253707,
      "loss": 0.1866,
      "step": 1968
    },
    {
      "epoch": 4.608543007606787,
      "grad_norm": 0.7145346999168396,
      "learning_rate": 0.00017962529274004682,
      "loss": 0.119,
      "step": 1969
    },
    {
      "epoch": 4.610883557636044,
      "grad_norm": 0.7321184277534485,
      "learning_rate": 0.00017954722872755657,
      "loss": 0.1963,
      "step": 1970
    },
    {
      "epoch": 4.613224107665301,
      "grad_norm": 0.6365939378738403,
      "learning_rate": 0.00017946916471506635,
      "loss": 0.1221,
      "step": 1971
    },
    {
      "epoch": 4.615564657694558,
      "grad_norm": 0.6446269154548645,
      "learning_rate": 0.0001793911007025761,
      "loss": 0.1342,
      "step": 1972
    },
    {
      "epoch": 4.617905207723815,
      "grad_norm": 0.7978159189224243,
      "learning_rate": 0.00017931303669008585,
      "loss": 0.1497,
      "step": 1973
    },
    {
      "epoch": 4.620245757753072,
      "grad_norm": 0.7529513239860535,
      "learning_rate": 0.00017923497267759563,
      "loss": 0.1914,
      "step": 1974
    },
    {
      "epoch": 4.622586307782329,
      "grad_norm": 0.5743211507797241,
      "learning_rate": 0.00017915690866510538,
      "loss": 0.1193,
      "step": 1975
    },
    {
      "epoch": 4.624926857811586,
      "grad_norm": 0.7463330626487732,
      "learning_rate": 0.00017907884465261513,
      "loss": 0.1268,
      "step": 1976
    },
    {
      "epoch": 4.627267407840843,
      "grad_norm": 0.8864316344261169,
      "learning_rate": 0.0001790007806401249,
      "loss": 0.1661,
      "step": 1977
    },
    {
      "epoch": 4.6296079578701,
      "grad_norm": 0.7224182486534119,
      "learning_rate": 0.00017892271662763465,
      "loss": 0.1423,
      "step": 1978
    },
    {
      "epoch": 4.631948507899356,
      "grad_norm": 0.7662861943244934,
      "learning_rate": 0.00017884465261514438,
      "loss": 0.1791,
      "step": 1979
    },
    {
      "epoch": 4.634289057928613,
      "grad_norm": 0.5390246510505676,
      "learning_rate": 0.00017876658860265418,
      "loss": 0.1501,
      "step": 1980
    },
    {
      "epoch": 4.63662960795787,
      "grad_norm": 0.747387170791626,
      "learning_rate": 0.0001786885245901639,
      "loss": 0.1814,
      "step": 1981
    },
    {
      "epoch": 4.638970157987127,
      "grad_norm": 0.5472539067268372,
      "learning_rate": 0.00017861046057767366,
      "loss": 0.1631,
      "step": 1982
    },
    {
      "epoch": 4.641310708016384,
      "grad_norm": 0.709801435470581,
      "learning_rate": 0.00017853239656518343,
      "loss": 0.1584,
      "step": 1983
    },
    {
      "epoch": 4.643651258045641,
      "grad_norm": 0.5918972492218018,
      "learning_rate": 0.00017845433255269318,
      "loss": 0.142,
      "step": 1984
    },
    {
      "epoch": 4.645991808074898,
      "grad_norm": 0.8385117650032043,
      "learning_rate": 0.00017837626854020293,
      "loss": 0.2084,
      "step": 1985
    },
    {
      "epoch": 4.648332358104154,
      "grad_norm": 0.5648414492607117,
      "learning_rate": 0.0001782982045277127,
      "loss": 0.1213,
      "step": 1986
    },
    {
      "epoch": 4.650672908133411,
      "grad_norm": 0.5199500918388367,
      "learning_rate": 0.00017822014051522246,
      "loss": 0.1083,
      "step": 1987
    },
    {
      "epoch": 4.653013458162668,
      "grad_norm": 0.641265332698822,
      "learning_rate": 0.0001781420765027322,
      "loss": 0.1586,
      "step": 1988
    },
    {
      "epoch": 4.655354008191925,
      "grad_norm": 0.7346384525299072,
      "learning_rate": 0.000178064012490242,
      "loss": 0.1696,
      "step": 1989
    },
    {
      "epoch": 4.657694558221182,
      "grad_norm": 0.5660353899002075,
      "learning_rate": 0.00017798594847775174,
      "loss": 0.1074,
      "step": 1990
    },
    {
      "epoch": 4.6600351082504385,
      "grad_norm": 0.7141843438148499,
      "learning_rate": 0.0001779078844652615,
      "loss": 0.1769,
      "step": 1991
    },
    {
      "epoch": 4.662375658279696,
      "grad_norm": 0.601913571357727,
      "learning_rate": 0.00017782982045277127,
      "loss": 0.1307,
      "step": 1992
    },
    {
      "epoch": 4.664716208308953,
      "grad_norm": 0.598227858543396,
      "learning_rate": 0.00017775175644028102,
      "loss": 0.1568,
      "step": 1993
    },
    {
      "epoch": 4.66705675833821,
      "grad_norm": 0.822580873966217,
      "learning_rate": 0.00017767369242779077,
      "loss": 0.132,
      "step": 1994
    },
    {
      "epoch": 4.669397308367467,
      "grad_norm": 0.666307270526886,
      "learning_rate": 0.00017759562841530055,
      "loss": 0.1844,
      "step": 1995
    },
    {
      "epoch": 4.671737858396723,
      "grad_norm": 0.7011789083480835,
      "learning_rate": 0.0001775175644028103,
      "loss": 0.1241,
      "step": 1996
    },
    {
      "epoch": 4.67407840842598,
      "grad_norm": 0.7857585549354553,
      "learning_rate": 0.00017743950039032002,
      "loss": 0.144,
      "step": 1997
    },
    {
      "epoch": 4.676418958455237,
      "grad_norm": 0.6273255944252014,
      "learning_rate": 0.00017736143637782982,
      "loss": 0.1301,
      "step": 1998
    },
    {
      "epoch": 4.678759508484494,
      "grad_norm": 0.6558715105056763,
      "learning_rate": 0.00017728337236533955,
      "loss": 0.1431,
      "step": 1999
    },
    {
      "epoch": 4.681100058513751,
      "grad_norm": 0.7418802976608276,
      "learning_rate": 0.0001772053083528493,
      "loss": 0.1581,
      "step": 2000
    },
    {
      "epoch": 4.683440608543007,
      "grad_norm": 0.89745032787323,
      "learning_rate": 0.00017712724434035908,
      "loss": 0.14,
      "step": 2001
    },
    {
      "epoch": 4.685781158572264,
      "grad_norm": 0.7110739946365356,
      "learning_rate": 0.00017704918032786883,
      "loss": 0.1424,
      "step": 2002
    },
    {
      "epoch": 4.688121708601521,
      "grad_norm": 0.5523712635040283,
      "learning_rate": 0.00017697111631537858,
      "loss": 0.1267,
      "step": 2003
    },
    {
      "epoch": 4.690462258630778,
      "grad_norm": 0.8778390288352966,
      "learning_rate": 0.00017689305230288835,
      "loss": 0.1656,
      "step": 2004
    },
    {
      "epoch": 4.692802808660035,
      "grad_norm": 0.48064860701560974,
      "learning_rate": 0.0001768149882903981,
      "loss": 0.1316,
      "step": 2005
    },
    {
      "epoch": 4.695143358689292,
      "grad_norm": 0.492931604385376,
      "learning_rate": 0.00017673692427790786,
      "loss": 0.1151,
      "step": 2006
    },
    {
      "epoch": 4.6974839087185485,
      "grad_norm": 0.8768459558486938,
      "learning_rate": 0.00017665886026541763,
      "loss": 0.2032,
      "step": 2007
    },
    {
      "epoch": 4.6998244587478055,
      "grad_norm": 0.6515156626701355,
      "learning_rate": 0.00017658079625292738,
      "loss": 0.1201,
      "step": 2008
    },
    {
      "epoch": 4.702165008777063,
      "grad_norm": 0.4286341369152069,
      "learning_rate": 0.00017650273224043713,
      "loss": 0.1254,
      "step": 2009
    },
    {
      "epoch": 4.70450555880632,
      "grad_norm": 0.6823873519897461,
      "learning_rate": 0.0001764246682279469,
      "loss": 0.1664,
      "step": 2010
    },
    {
      "epoch": 4.706846108835577,
      "grad_norm": 0.5382854342460632,
      "learning_rate": 0.00017634660421545666,
      "loss": 0.1171,
      "step": 2011
    },
    {
      "epoch": 4.709186658864834,
      "grad_norm": 0.836345911026001,
      "learning_rate": 0.0001762685402029664,
      "loss": 0.1165,
      "step": 2012
    },
    {
      "epoch": 4.71152720889409,
      "grad_norm": 0.9646338224411011,
      "learning_rate": 0.0001761904761904762,
      "loss": 0.1435,
      "step": 2013
    },
    {
      "epoch": 4.713867758923347,
      "grad_norm": 0.7010369300842285,
      "learning_rate": 0.00017611241217798594,
      "loss": 0.1435,
      "step": 2014
    },
    {
      "epoch": 4.716208308952604,
      "grad_norm": 0.6432908177375793,
      "learning_rate": 0.00017603434816549566,
      "loss": 0.1626,
      "step": 2015
    },
    {
      "epoch": 4.718548858981861,
      "grad_norm": 0.8818672299385071,
      "learning_rate": 0.00017595628415300547,
      "loss": 0.2012,
      "step": 2016
    },
    {
      "epoch": 4.720889409011118,
      "grad_norm": 0.6863499283790588,
      "learning_rate": 0.0001758782201405152,
      "loss": 0.1433,
      "step": 2017
    },
    {
      "epoch": 4.723229959040374,
      "grad_norm": 0.8482297658920288,
      "learning_rate": 0.00017580015612802494,
      "loss": 0.183,
      "step": 2018
    },
    {
      "epoch": 4.725570509069631,
      "grad_norm": 0.8808090090751648,
      "learning_rate": 0.00017572209211553472,
      "loss": 0.1554,
      "step": 2019
    },
    {
      "epoch": 4.727911059098888,
      "grad_norm": 0.824496328830719,
      "learning_rate": 0.00017564402810304447,
      "loss": 0.1725,
      "step": 2020
    },
    {
      "epoch": 4.730251609128145,
      "grad_norm": 0.7712833285331726,
      "learning_rate": 0.00017556596409055422,
      "loss": 0.1507,
      "step": 2021
    },
    {
      "epoch": 4.732592159157402,
      "grad_norm": 0.8484505414962769,
      "learning_rate": 0.000175487900078064,
      "loss": 0.1484,
      "step": 2022
    },
    {
      "epoch": 4.734932709186658,
      "grad_norm": 0.5908847451210022,
      "learning_rate": 0.00017540983606557375,
      "loss": 0.1696,
      "step": 2023
    },
    {
      "epoch": 4.7372732592159155,
      "grad_norm": 0.6113643646240234,
      "learning_rate": 0.0001753317720530835,
      "loss": 0.1422,
      "step": 2024
    },
    {
      "epoch": 4.7396138092451725,
      "grad_norm": 0.5108547806739807,
      "learning_rate": 0.00017525370804059328,
      "loss": 0.101,
      "step": 2025
    },
    {
      "epoch": 4.7419543592744295,
      "grad_norm": 0.5495480895042419,
      "learning_rate": 0.00017517564402810303,
      "loss": 0.1855,
      "step": 2026
    },
    {
      "epoch": 4.744294909303687,
      "grad_norm": 0.6622937321662903,
      "learning_rate": 0.00017509758001561278,
      "loss": 0.1243,
      "step": 2027
    },
    {
      "epoch": 4.746635459332944,
      "grad_norm": 0.9540312886238098,
      "learning_rate": 0.00017501951600312255,
      "loss": 0.1856,
      "step": 2028
    },
    {
      "epoch": 4.7489760093622,
      "grad_norm": 0.8623123168945312,
      "learning_rate": 0.0001749414519906323,
      "loss": 0.2107,
      "step": 2029
    },
    {
      "epoch": 4.751316559391457,
      "grad_norm": 0.7688409090042114,
      "learning_rate": 0.00017486338797814208,
      "loss": 0.1545,
      "step": 2030
    },
    {
      "epoch": 4.753657109420714,
      "grad_norm": 0.589767575263977,
      "learning_rate": 0.00017478532396565183,
      "loss": 0.1466,
      "step": 2031
    },
    {
      "epoch": 4.755997659449971,
      "grad_norm": 0.6835980415344238,
      "learning_rate": 0.00017470725995316158,
      "loss": 0.1401,
      "step": 2032
    },
    {
      "epoch": 4.758338209479228,
      "grad_norm": 0.597828209400177,
      "learning_rate": 0.00017462919594067136,
      "loss": 0.1455,
      "step": 2033
    },
    {
      "epoch": 4.760678759508485,
      "grad_norm": 0.5893029570579529,
      "learning_rate": 0.0001745511319281811,
      "loss": 0.0998,
      "step": 2034
    },
    {
      "epoch": 4.763019309537741,
      "grad_norm": 0.5804377198219299,
      "learning_rate": 0.00017447306791569083,
      "loss": 0.155,
      "step": 2035
    },
    {
      "epoch": 4.765359859566998,
      "grad_norm": 0.7224408984184265,
      "learning_rate": 0.00017439500390320064,
      "loss": 0.1807,
      "step": 2036
    },
    {
      "epoch": 4.767700409596255,
      "grad_norm": 0.774082362651825,
      "learning_rate": 0.00017431693989071036,
      "loss": 0.1576,
      "step": 2037
    },
    {
      "epoch": 4.770040959625512,
      "grad_norm": 0.6161869764328003,
      "learning_rate": 0.0001742388758782201,
      "loss": 0.1394,
      "step": 2038
    },
    {
      "epoch": 4.772381509654769,
      "grad_norm": 0.6894835829734802,
      "learning_rate": 0.0001741608118657299,
      "loss": 0.1717,
      "step": 2039
    },
    {
      "epoch": 4.774722059684025,
      "grad_norm": 0.5287206172943115,
      "learning_rate": 0.00017408274785323964,
      "loss": 0.1073,
      "step": 2040
    },
    {
      "epoch": 4.7770626097132824,
      "grad_norm": 0.7004848122596741,
      "learning_rate": 0.0001740046838407494,
      "loss": 0.1577,
      "step": 2041
    },
    {
      "epoch": 4.7794031597425395,
      "grad_norm": 0.7199407815933228,
      "learning_rate": 0.00017392661982825917,
      "loss": 0.1712,
      "step": 2042
    },
    {
      "epoch": 4.7817437097717965,
      "grad_norm": 0.6166281700134277,
      "learning_rate": 0.00017384855581576892,
      "loss": 0.1461,
      "step": 2043
    },
    {
      "epoch": 4.784084259801054,
      "grad_norm": 0.6181027293205261,
      "learning_rate": 0.00017377049180327867,
      "loss": 0.1123,
      "step": 2044
    },
    {
      "epoch": 4.78642480983031,
      "grad_norm": 0.6615619659423828,
      "learning_rate": 0.00017369242779078845,
      "loss": 0.136,
      "step": 2045
    },
    {
      "epoch": 4.788765359859567,
      "grad_norm": 0.7566392421722412,
      "learning_rate": 0.0001736143637782982,
      "loss": 0.1591,
      "step": 2046
    },
    {
      "epoch": 4.791105909888824,
      "grad_norm": 0.557097315788269,
      "learning_rate": 0.00017353629976580795,
      "loss": 0.1054,
      "step": 2047
    },
    {
      "epoch": 4.793446459918081,
      "grad_norm": 0.8105932474136353,
      "learning_rate": 0.00017345823575331772,
      "loss": 0.1349,
      "step": 2048
    },
    {
      "epoch": 4.795787009947338,
      "grad_norm": 0.553986132144928,
      "learning_rate": 0.00017338017174082748,
      "loss": 0.1511,
      "step": 2049
    },
    {
      "epoch": 4.798127559976594,
      "grad_norm": 0.4735310971736908,
      "learning_rate": 0.00017330210772833723,
      "loss": 0.099,
      "step": 2050
    },
    {
      "epoch": 4.800468110005851,
      "grad_norm": 0.696692943572998,
      "learning_rate": 0.000173224043715847,
      "loss": 0.1123,
      "step": 2051
    },
    {
      "epoch": 4.802808660035108,
      "grad_norm": 0.5932154059410095,
      "learning_rate": 0.00017314597970335675,
      "loss": 0.1364,
      "step": 2052
    },
    {
      "epoch": 4.805149210064365,
      "grad_norm": 0.6074979901313782,
      "learning_rate": 0.00017306791569086648,
      "loss": 0.1344,
      "step": 2053
    },
    {
      "epoch": 4.807489760093622,
      "grad_norm": 0.4925973117351532,
      "learning_rate": 0.00017298985167837625,
      "loss": 0.1372,
      "step": 2054
    },
    {
      "epoch": 4.809830310122879,
      "grad_norm": 0.7950984239578247,
      "learning_rate": 0.000172911787665886,
      "loss": 0.1575,
      "step": 2055
    },
    {
      "epoch": 4.812170860152136,
      "grad_norm": 0.4115891754627228,
      "learning_rate": 0.00017283372365339576,
      "loss": 0.0502,
      "step": 2056
    },
    {
      "epoch": 4.814511410181392,
      "grad_norm": 0.6513320803642273,
      "learning_rate": 0.00017275565964090553,
      "loss": 0.1324,
      "step": 2057
    },
    {
      "epoch": 4.816851960210649,
      "grad_norm": 0.7562972903251648,
      "learning_rate": 0.00017267759562841528,
      "loss": 0.192,
      "step": 2058
    },
    {
      "epoch": 4.8191925102399065,
      "grad_norm": 0.9576177000999451,
      "learning_rate": 0.00017259953161592503,
      "loss": 0.188,
      "step": 2059
    },
    {
      "epoch": 4.8215330602691635,
      "grad_norm": 0.6840614676475525,
      "learning_rate": 0.0001725214676034348,
      "loss": 0.1868,
      "step": 2060
    },
    {
      "epoch": 4.8238736102984205,
      "grad_norm": 0.707913339138031,
      "learning_rate": 0.00017244340359094456,
      "loss": 0.1802,
      "step": 2061
    },
    {
      "epoch": 4.826214160327677,
      "grad_norm": 0.5331506133079529,
      "learning_rate": 0.0001723653395784543,
      "loss": 0.1065,
      "step": 2062
    },
    {
      "epoch": 4.828554710356934,
      "grad_norm": 0.7496339678764343,
      "learning_rate": 0.0001722872755659641,
      "loss": 0.1276,
      "step": 2063
    },
    {
      "epoch": 4.830895260386191,
      "grad_norm": 0.7057371735572815,
      "learning_rate": 0.00017220921155347384,
      "loss": 0.1724,
      "step": 2064
    },
    {
      "epoch": 4.833235810415448,
      "grad_norm": 0.6893234252929688,
      "learning_rate": 0.0001721311475409836,
      "loss": 0.141,
      "step": 2065
    },
    {
      "epoch": 4.835576360444705,
      "grad_norm": 0.6989895701408386,
      "learning_rate": 0.00017205308352849337,
      "loss": 0.1236,
      "step": 2066
    },
    {
      "epoch": 4.837916910473961,
      "grad_norm": 0.7248987555503845,
      "learning_rate": 0.00017197501951600312,
      "loss": 0.1541,
      "step": 2067
    },
    {
      "epoch": 4.840257460503218,
      "grad_norm": 0.7627169489860535,
      "learning_rate": 0.00017189695550351287,
      "loss": 0.1527,
      "step": 2068
    },
    {
      "epoch": 4.842598010532475,
      "grad_norm": 0.6567290425300598,
      "learning_rate": 0.00017181889149102265,
      "loss": 0.1896,
      "step": 2069
    },
    {
      "epoch": 4.844938560561732,
      "grad_norm": 0.7037553191184998,
      "learning_rate": 0.0001717408274785324,
      "loss": 0.1427,
      "step": 2070
    },
    {
      "epoch": 4.847279110590989,
      "grad_norm": 0.5769292712211609,
      "learning_rate": 0.00017166276346604212,
      "loss": 0.125,
      "step": 2071
    },
    {
      "epoch": 4.849619660620245,
      "grad_norm": 0.760389506816864,
      "learning_rate": 0.0001715846994535519,
      "loss": 0.1108,
      "step": 2072
    },
    {
      "epoch": 4.851960210649502,
      "grad_norm": 0.6445094347000122,
      "learning_rate": 0.00017150663544106165,
      "loss": 0.1499,
      "step": 2073
    },
    {
      "epoch": 4.854300760678759,
      "grad_norm": 1.421909213066101,
      "learning_rate": 0.0001714285714285714,
      "loss": 0.1668,
      "step": 2074
    },
    {
      "epoch": 4.856641310708016,
      "grad_norm": 0.8196819424629211,
      "learning_rate": 0.00017135050741608118,
      "loss": 0.1605,
      "step": 2075
    },
    {
      "epoch": 4.8589818607372735,
      "grad_norm": 0.6239237785339355,
      "learning_rate": 0.00017127244340359093,
      "loss": 0.1665,
      "step": 2076
    },
    {
      "epoch": 4.8613224107665305,
      "grad_norm": 0.6118910908699036,
      "learning_rate": 0.00017119437939110068,
      "loss": 0.1144,
      "step": 2077
    },
    {
      "epoch": 4.863662960795787,
      "grad_norm": 1.383621096611023,
      "learning_rate": 0.00017111631537861045,
      "loss": 0.1443,
      "step": 2078
    },
    {
      "epoch": 4.866003510825044,
      "grad_norm": 0.7481427788734436,
      "learning_rate": 0.0001710382513661202,
      "loss": 0.1291,
      "step": 2079
    },
    {
      "epoch": 4.868344060854301,
      "grad_norm": 0.5497840642929077,
      "learning_rate": 0.00017096018735362995,
      "loss": 0.1499,
      "step": 2080
    },
    {
      "epoch": 4.870684610883558,
      "grad_norm": 0.6682730913162231,
      "learning_rate": 0.00017088212334113973,
      "loss": 0.1652,
      "step": 2081
    },
    {
      "epoch": 4.873025160912815,
      "grad_norm": 0.7420730590820312,
      "learning_rate": 0.00017080405932864948,
      "loss": 0.1686,
      "step": 2082
    },
    {
      "epoch": 4.875365710942072,
      "grad_norm": 0.5284426212310791,
      "learning_rate": 0.00017072599531615923,
      "loss": 0.1081,
      "step": 2083
    },
    {
      "epoch": 4.877706260971328,
      "grad_norm": 0.5361320972442627,
      "learning_rate": 0.000170647931303669,
      "loss": 0.178,
      "step": 2084
    },
    {
      "epoch": 4.880046811000585,
      "grad_norm": 0.7357752919197083,
      "learning_rate": 0.00017056986729117876,
      "loss": 0.1406,
      "step": 2085
    },
    {
      "epoch": 4.882387361029842,
      "grad_norm": 0.5755234956741333,
      "learning_rate": 0.0001704918032786885,
      "loss": 0.1316,
      "step": 2086
    },
    {
      "epoch": 4.884727911059099,
      "grad_norm": 0.8997892737388611,
      "learning_rate": 0.0001704137392661983,
      "loss": 0.1454,
      "step": 2087
    },
    {
      "epoch": 4.887068461088356,
      "grad_norm": 0.5566614866256714,
      "learning_rate": 0.00017033567525370804,
      "loss": 0.0904,
      "step": 2088
    },
    {
      "epoch": 4.889409011117612,
      "grad_norm": 0.8556140661239624,
      "learning_rate": 0.00017025761124121776,
      "loss": 0.2037,
      "step": 2089
    },
    {
      "epoch": 4.891749561146869,
      "grad_norm": 1.1295950412750244,
      "learning_rate": 0.00017017954722872754,
      "loss": 0.1676,
      "step": 2090
    },
    {
      "epoch": 4.894090111176126,
      "grad_norm": 0.8345170021057129,
      "learning_rate": 0.0001701014832162373,
      "loss": 0.195,
      "step": 2091
    },
    {
      "epoch": 4.896430661205383,
      "grad_norm": 0.6548200249671936,
      "learning_rate": 0.00017002341920374704,
      "loss": 0.1411,
      "step": 2092
    },
    {
      "epoch": 4.89877121123464,
      "grad_norm": 0.6547970771789551,
      "learning_rate": 0.00016994535519125682,
      "loss": 0.1399,
      "step": 2093
    },
    {
      "epoch": 4.901111761263897,
      "grad_norm": 0.6748490333557129,
      "learning_rate": 0.00016986729117876657,
      "loss": 0.1466,
      "step": 2094
    },
    {
      "epoch": 4.903452311293154,
      "grad_norm": 0.560424268245697,
      "learning_rate": 0.00016978922716627632,
      "loss": 0.1495,
      "step": 2095
    },
    {
      "epoch": 4.905792861322411,
      "grad_norm": 0.5676383972167969,
      "learning_rate": 0.0001697111631537861,
      "loss": 0.1356,
      "step": 2096
    },
    {
      "epoch": 4.908133411351668,
      "grad_norm": 0.6781577467918396,
      "learning_rate": 0.00016963309914129585,
      "loss": 0.149,
      "step": 2097
    },
    {
      "epoch": 4.910473961380925,
      "grad_norm": 0.656710147857666,
      "learning_rate": 0.0001695550351288056,
      "loss": 0.1701,
      "step": 2098
    },
    {
      "epoch": 4.912814511410182,
      "grad_norm": 0.7254874110221863,
      "learning_rate": 0.00016947697111631538,
      "loss": 0.1484,
      "step": 2099
    },
    {
      "epoch": 4.915155061439438,
      "grad_norm": 0.555854320526123,
      "learning_rate": 0.00016939890710382513,
      "loss": 0.1241,
      "step": 2100
    },
    {
      "epoch": 4.917495611468695,
      "grad_norm": 0.7191785573959351,
      "learning_rate": 0.00016932084309133488,
      "loss": 0.1189,
      "step": 2101
    },
    {
      "epoch": 4.919836161497952,
      "grad_norm": 0.7609049677848816,
      "learning_rate": 0.00016924277907884465,
      "loss": 0.1565,
      "step": 2102
    },
    {
      "epoch": 4.922176711527209,
      "grad_norm": 0.7343191504478455,
      "learning_rate": 0.0001691647150663544,
      "loss": 0.1405,
      "step": 2103
    },
    {
      "epoch": 4.924517261556466,
      "grad_norm": 0.6648486852645874,
      "learning_rate": 0.00016908665105386415,
      "loss": 0.1321,
      "step": 2104
    },
    {
      "epoch": 4.926857811585723,
      "grad_norm": 0.8275675177574158,
      "learning_rate": 0.00016900858704137393,
      "loss": 0.1796,
      "step": 2105
    },
    {
      "epoch": 4.929198361614979,
      "grad_norm": 0.7329282760620117,
      "learning_rate": 0.00016893052302888366,
      "loss": 0.1205,
      "step": 2106
    },
    {
      "epoch": 4.931538911644236,
      "grad_norm": 0.7810617685317993,
      "learning_rate": 0.0001688524590163934,
      "loss": 0.1846,
      "step": 2107
    },
    {
      "epoch": 4.933879461673493,
      "grad_norm": 0.7172843217849731,
      "learning_rate": 0.00016877439500390318,
      "loss": 0.1318,
      "step": 2108
    },
    {
      "epoch": 4.93622001170275,
      "grad_norm": 0.6712007522583008,
      "learning_rate": 0.00016869633099141293,
      "loss": 0.1674,
      "step": 2109
    },
    {
      "epoch": 4.938560561732007,
      "grad_norm": 0.5519238114356995,
      "learning_rate": 0.00016861826697892268,
      "loss": 0.118,
      "step": 2110
    },
    {
      "epoch": 4.940901111761264,
      "grad_norm": 0.559536874294281,
      "learning_rate": 0.00016854020296643246,
      "loss": 0.1193,
      "step": 2111
    },
    {
      "epoch": 4.943241661790521,
      "grad_norm": 0.6205847263336182,
      "learning_rate": 0.0001684621389539422,
      "loss": 0.1569,
      "step": 2112
    },
    {
      "epoch": 4.945582211819778,
      "grad_norm": 0.7370690703392029,
      "learning_rate": 0.00016838407494145196,
      "loss": 0.1454,
      "step": 2113
    },
    {
      "epoch": 4.947922761849035,
      "grad_norm": 0.6731003522872925,
      "learning_rate": 0.00016830601092896174,
      "loss": 0.1741,
      "step": 2114
    },
    {
      "epoch": 4.950263311878292,
      "grad_norm": 0.49542465806007385,
      "learning_rate": 0.0001682279469164715,
      "loss": 0.127,
      "step": 2115
    },
    {
      "epoch": 4.952603861907548,
      "grad_norm": 0.6055631041526794,
      "learning_rate": 0.00016814988290398124,
      "loss": 0.122,
      "step": 2116
    },
    {
      "epoch": 4.954944411936805,
      "grad_norm": 0.7374733686447144,
      "learning_rate": 0.00016807181889149102,
      "loss": 0.1644,
      "step": 2117
    },
    {
      "epoch": 4.957284961966062,
      "grad_norm": 0.637874960899353,
      "learning_rate": 0.00016799375487900077,
      "loss": 0.1251,
      "step": 2118
    },
    {
      "epoch": 4.959625511995319,
      "grad_norm": 0.7358796000480652,
      "learning_rate": 0.00016791569086651052,
      "loss": 0.154,
      "step": 2119
    },
    {
      "epoch": 4.961966062024576,
      "grad_norm": 0.675049364566803,
      "learning_rate": 0.0001678376268540203,
      "loss": 0.1417,
      "step": 2120
    },
    {
      "epoch": 4.964306612053832,
      "grad_norm": 1.1033676862716675,
      "learning_rate": 0.00016775956284153005,
      "loss": 0.1502,
      "step": 2121
    },
    {
      "epoch": 4.966647162083089,
      "grad_norm": 0.7076665163040161,
      "learning_rate": 0.0001676814988290398,
      "loss": 0.177,
      "step": 2122
    },
    {
      "epoch": 4.968987712112346,
      "grad_norm": 0.7101672291755676,
      "learning_rate": 0.00016760343481654957,
      "loss": 0.1169,
      "step": 2123
    },
    {
      "epoch": 4.971328262141603,
      "grad_norm": 0.7499718070030212,
      "learning_rate": 0.0001675253708040593,
      "loss": 0.1564,
      "step": 2124
    },
    {
      "epoch": 4.97366881217086,
      "grad_norm": 1.1275033950805664,
      "learning_rate": 0.00016744730679156905,
      "loss": 0.1526,
      "step": 2125
    },
    {
      "epoch": 4.976009362200117,
      "grad_norm": 0.9951605200767517,
      "learning_rate": 0.00016736924277907883,
      "loss": 0.1917,
      "step": 2126
    },
    {
      "epoch": 4.978349912229374,
      "grad_norm": 0.6025328040122986,
      "learning_rate": 0.00016729117876658858,
      "loss": 0.1693,
      "step": 2127
    },
    {
      "epoch": 4.9806904622586305,
      "grad_norm": 0.6423271894454956,
      "learning_rate": 0.00016721311475409833,
      "loss": 0.1165,
      "step": 2128
    },
    {
      "epoch": 4.983031012287888,
      "grad_norm": 0.605718731880188,
      "learning_rate": 0.0001671350507416081,
      "loss": 0.1134,
      "step": 2129
    },
    {
      "epoch": 4.985371562317145,
      "grad_norm": 0.7145519256591797,
      "learning_rate": 0.00016705698672911785,
      "loss": 0.1756,
      "step": 2130
    },
    {
      "epoch": 4.987712112346402,
      "grad_norm": 1.0498416423797607,
      "learning_rate": 0.0001669789227166276,
      "loss": 0.1966,
      "step": 2131
    },
    {
      "epoch": 4.990052662375659,
      "grad_norm": 0.5802971720695496,
      "learning_rate": 0.00016690085870413738,
      "loss": 0.1264,
      "step": 2132
    },
    {
      "epoch": 4.992393212404915,
      "grad_norm": 0.7830216884613037,
      "learning_rate": 0.00016682279469164713,
      "loss": 0.168,
      "step": 2133
    },
    {
      "epoch": 4.994733762434172,
      "grad_norm": 0.5826237797737122,
      "learning_rate": 0.00016674473067915688,
      "loss": 0.1024,
      "step": 2134
    },
    {
      "epoch": 4.997074312463429,
      "grad_norm": 0.7499382495880127,
      "learning_rate": 0.00016666666666666666,
      "loss": 0.1702,
      "step": 2135
    },
    {
      "epoch": 4.999414862492686,
      "grad_norm": 0.6311134099960327,
      "learning_rate": 0.0001665886026541764,
      "loss": 0.1717,
      "step": 2136
    },
    {
      "epoch": 4.999414862492686,
      "eval_loss": 0.25438231229782104,
      "eval_runtime": 128.6039,
      "eval_samples_per_second": 4.292,
      "eval_steps_per_second": 0.537,
      "step": 2136
    }
  ],
  "logging_steps": 1,
  "max_steps": 4270,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 6.203668775648625e+17,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
