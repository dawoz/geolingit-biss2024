{
  "best_metric": 0.2214934378862381,
  "best_model_checkpoint": "LLaMinerva/GeoLingIt/LLaMA/checkpoint-1709",
  "epoch": 4.0,
  "eval_steps": 500,
  "global_step": 1709,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0023405500292568754,
      "grad_norm": 1.3075157403945923,
      "learning_rate": 7.025761124121779e-07,
      "loss": 2.3907,
      "step": 1
    },
    {
      "epoch": 0.004681100058513751,
      "grad_norm": 1.2638753652572632,
      "learning_rate": 1.4051522248243558e-06,
      "loss": 2.3386,
      "step": 2
    },
    {
      "epoch": 0.007021650087770626,
      "grad_norm": 1.4088300466537476,
      "learning_rate": 2.107728337236534e-06,
      "loss": 2.4335,
      "step": 3
    },
    {
      "epoch": 0.009362200117027502,
      "grad_norm": 1.335129737854004,
      "learning_rate": 2.8103044496487116e-06,
      "loss": 2.4183,
      "step": 4
    },
    {
      "epoch": 0.011702750146284377,
      "grad_norm": 1.413881778717041,
      "learning_rate": 3.5128805620608897e-06,
      "loss": 2.3846,
      "step": 5
    },
    {
      "epoch": 0.014043300175541252,
      "grad_norm": 1.3626625537872314,
      "learning_rate": 4.215456674473068e-06,
      "loss": 2.3963,
      "step": 6
    },
    {
      "epoch": 0.016383850204798128,
      "grad_norm": 1.4106955528259277,
      "learning_rate": 4.9180327868852455e-06,
      "loss": 2.3278,
      "step": 7
    },
    {
      "epoch": 0.018724400234055003,
      "grad_norm": 1.3657892942428589,
      "learning_rate": 5.620608899297423e-06,
      "loss": 2.3689,
      "step": 8
    },
    {
      "epoch": 0.021064950263311878,
      "grad_norm": 1.3119163513183594,
      "learning_rate": 6.323185011709601e-06,
      "loss": 2.4207,
      "step": 9
    },
    {
      "epoch": 0.023405500292568753,
      "grad_norm": 1.4543135166168213,
      "learning_rate": 7.025761124121779e-06,
      "loss": 2.3838,
      "step": 10
    },
    {
      "epoch": 0.025746050321825628,
      "grad_norm": 1.3380881547927856,
      "learning_rate": 7.728337236533957e-06,
      "loss": 2.3302,
      "step": 11
    },
    {
      "epoch": 0.028086600351082503,
      "grad_norm": 1.3970142602920532,
      "learning_rate": 8.430913348946136e-06,
      "loss": 2.3158,
      "step": 12
    },
    {
      "epoch": 0.030427150380339378,
      "grad_norm": 1.3762916326522827,
      "learning_rate": 9.133489461358312e-06,
      "loss": 2.2586,
      "step": 13
    },
    {
      "epoch": 0.032767700409596257,
      "grad_norm": 1.3718304634094238,
      "learning_rate": 9.836065573770491e-06,
      "loss": 2.3311,
      "step": 14
    },
    {
      "epoch": 0.03510825043885313,
      "grad_norm": 1.3810392618179321,
      "learning_rate": 1.0538641686182668e-05,
      "loss": 2.3057,
      "step": 15
    },
    {
      "epoch": 0.037448800468110006,
      "grad_norm": 1.4270919561386108,
      "learning_rate": 1.1241217798594846e-05,
      "loss": 2.3265,
      "step": 16
    },
    {
      "epoch": 0.03978935049736688,
      "grad_norm": 1.4527926445007324,
      "learning_rate": 1.1943793911007025e-05,
      "loss": 2.2598,
      "step": 17
    },
    {
      "epoch": 0.042129900526623756,
      "grad_norm": 1.4354729652404785,
      "learning_rate": 1.2646370023419202e-05,
      "loss": 2.2339,
      "step": 18
    },
    {
      "epoch": 0.044470450555880635,
      "grad_norm": 1.3947365283966064,
      "learning_rate": 1.334894613583138e-05,
      "loss": 2.2605,
      "step": 19
    },
    {
      "epoch": 0.046811000585137506,
      "grad_norm": 1.5118610858917236,
      "learning_rate": 1.4051522248243559e-05,
      "loss": 2.2179,
      "step": 20
    },
    {
      "epoch": 0.049151550614394385,
      "grad_norm": 1.449415683746338,
      "learning_rate": 1.4754098360655736e-05,
      "loss": 2.1592,
      "step": 21
    },
    {
      "epoch": 0.051492100643651256,
      "grad_norm": 1.4515163898468018,
      "learning_rate": 1.5456674473067914e-05,
      "loss": 2.1279,
      "step": 22
    },
    {
      "epoch": 0.053832650672908135,
      "grad_norm": 1.4177380800247192,
      "learning_rate": 1.6159250585480093e-05,
      "loss": 2.117,
      "step": 23
    },
    {
      "epoch": 0.056173200702165006,
      "grad_norm": 1.4473860263824463,
      "learning_rate": 1.686182669789227e-05,
      "loss": 2.0375,
      "step": 24
    },
    {
      "epoch": 0.058513750731421885,
      "grad_norm": 1.4893802404403687,
      "learning_rate": 1.756440281030445e-05,
      "loss": 2.0756,
      "step": 25
    },
    {
      "epoch": 0.060854300760678756,
      "grad_norm": 1.6317111253738403,
      "learning_rate": 1.8266978922716625e-05,
      "loss": 2.0063,
      "step": 26
    },
    {
      "epoch": 0.06319485078993564,
      "grad_norm": 1.6238042116165161,
      "learning_rate": 1.8969555035128803e-05,
      "loss": 2.067,
      "step": 27
    },
    {
      "epoch": 0.06553540081919251,
      "grad_norm": 1.556071400642395,
      "learning_rate": 1.9672131147540982e-05,
      "loss": 1.9891,
      "step": 28
    },
    {
      "epoch": 0.06787595084844938,
      "grad_norm": 1.5013676881790161,
      "learning_rate": 2.037470725995316e-05,
      "loss": 1.8896,
      "step": 29
    },
    {
      "epoch": 0.07021650087770626,
      "grad_norm": 1.5984901189804077,
      "learning_rate": 2.1077283372365335e-05,
      "loss": 1.8764,
      "step": 30
    },
    {
      "epoch": 0.07255705090696314,
      "grad_norm": 1.654853105545044,
      "learning_rate": 2.1779859484777514e-05,
      "loss": 1.8326,
      "step": 31
    },
    {
      "epoch": 0.07489760093622001,
      "grad_norm": 1.6410921812057495,
      "learning_rate": 2.2482435597189693e-05,
      "loss": 1.8229,
      "step": 32
    },
    {
      "epoch": 0.07723815096547688,
      "grad_norm": 1.7152082920074463,
      "learning_rate": 2.318501170960187e-05,
      "loss": 1.82,
      "step": 33
    },
    {
      "epoch": 0.07957870099473376,
      "grad_norm": 1.8206788301467896,
      "learning_rate": 2.388758782201405e-05,
      "loss": 1.7143,
      "step": 34
    },
    {
      "epoch": 0.08191925102399064,
      "grad_norm": 1.8192237615585327,
      "learning_rate": 2.4590163934426225e-05,
      "loss": 1.6404,
      "step": 35
    },
    {
      "epoch": 0.08425980105324751,
      "grad_norm": 1.7882306575775146,
      "learning_rate": 2.5292740046838403e-05,
      "loss": 1.6149,
      "step": 36
    },
    {
      "epoch": 0.08660035108250438,
      "grad_norm": 1.879793643951416,
      "learning_rate": 2.5995316159250582e-05,
      "loss": 1.5511,
      "step": 37
    },
    {
      "epoch": 0.08894090111176127,
      "grad_norm": 1.827930212020874,
      "learning_rate": 2.669789227166276e-05,
      "loss": 1.4331,
      "step": 38
    },
    {
      "epoch": 0.09128145114101814,
      "grad_norm": 1.8847341537475586,
      "learning_rate": 2.740046838407494e-05,
      "loss": 1.3803,
      "step": 39
    },
    {
      "epoch": 0.09362200117027501,
      "grad_norm": 1.893025517463684,
      "learning_rate": 2.8103044496487117e-05,
      "loss": 1.3089,
      "step": 40
    },
    {
      "epoch": 0.09596255119953188,
      "grad_norm": 1.9126771688461304,
      "learning_rate": 2.8805620608899293e-05,
      "loss": 1.2513,
      "step": 41
    },
    {
      "epoch": 0.09830310122878877,
      "grad_norm": 1.8810867071151733,
      "learning_rate": 2.950819672131147e-05,
      "loss": 1.2097,
      "step": 42
    },
    {
      "epoch": 0.10064365125804564,
      "grad_norm": 1.8834633827209473,
      "learning_rate": 3.021077283372365e-05,
      "loss": 1.1476,
      "step": 43
    },
    {
      "epoch": 0.10298420128730251,
      "grad_norm": 1.7569632530212402,
      "learning_rate": 3.091334894613583e-05,
      "loss": 0.9943,
      "step": 44
    },
    {
      "epoch": 0.1053247513165594,
      "grad_norm": 1.7583837509155273,
      "learning_rate": 3.161592505854801e-05,
      "loss": 0.9463,
      "step": 45
    },
    {
      "epoch": 0.10766530134581627,
      "grad_norm": 1.558616042137146,
      "learning_rate": 3.2318501170960185e-05,
      "loss": 0.8803,
      "step": 46
    },
    {
      "epoch": 0.11000585137507314,
      "grad_norm": 1.4375554323196411,
      "learning_rate": 3.3021077283372364e-05,
      "loss": 0.8555,
      "step": 47
    },
    {
      "epoch": 0.11234640140433001,
      "grad_norm": 1.4197192192077637,
      "learning_rate": 3.372365339578454e-05,
      "loss": 0.7229,
      "step": 48
    },
    {
      "epoch": 0.1146869514335869,
      "grad_norm": 1.1976360082626343,
      "learning_rate": 3.442622950819672e-05,
      "loss": 0.7232,
      "step": 49
    },
    {
      "epoch": 0.11702750146284377,
      "grad_norm": 1.138659954071045,
      "learning_rate": 3.51288056206089e-05,
      "loss": 0.6816,
      "step": 50
    },
    {
      "epoch": 0.11936805149210064,
      "grad_norm": 0.8948962688446045,
      "learning_rate": 3.583138173302107e-05,
      "loss": 0.6153,
      "step": 51
    },
    {
      "epoch": 0.12170860152135751,
      "grad_norm": 0.8759509921073914,
      "learning_rate": 3.653395784543325e-05,
      "loss": 0.543,
      "step": 52
    },
    {
      "epoch": 0.1240491515506144,
      "grad_norm": 1.0210652351379395,
      "learning_rate": 3.723653395784543e-05,
      "loss": 0.5398,
      "step": 53
    },
    {
      "epoch": 0.12638970157987128,
      "grad_norm": 0.8832714557647705,
      "learning_rate": 3.7939110070257607e-05,
      "loss": 0.52,
      "step": 54
    },
    {
      "epoch": 0.12873025160912815,
      "grad_norm": 0.6473265886306763,
      "learning_rate": 3.8641686182669785e-05,
      "loss": 0.5449,
      "step": 55
    },
    {
      "epoch": 0.13107080163838503,
      "grad_norm": 0.727357804775238,
      "learning_rate": 3.9344262295081964e-05,
      "loss": 0.4973,
      "step": 56
    },
    {
      "epoch": 0.1334113516676419,
      "grad_norm": 0.6498393416404724,
      "learning_rate": 4.004683840749414e-05,
      "loss": 0.492,
      "step": 57
    },
    {
      "epoch": 0.13575190169689877,
      "grad_norm": 0.6647109389305115,
      "learning_rate": 4.074941451990632e-05,
      "loss": 0.4816,
      "step": 58
    },
    {
      "epoch": 0.13809245172615564,
      "grad_norm": 0.7038798332214355,
      "learning_rate": 4.145199063231849e-05,
      "loss": 0.439,
      "step": 59
    },
    {
      "epoch": 0.1404330017554125,
      "grad_norm": 0.658500075340271,
      "learning_rate": 4.215456674473067e-05,
      "loss": 0.4334,
      "step": 60
    },
    {
      "epoch": 0.14277355178466938,
      "grad_norm": 0.6784713864326477,
      "learning_rate": 4.285714285714285e-05,
      "loss": 0.4813,
      "step": 61
    },
    {
      "epoch": 0.14511410181392628,
      "grad_norm": 0.772458553314209,
      "learning_rate": 4.355971896955503e-05,
      "loss": 0.4412,
      "step": 62
    },
    {
      "epoch": 0.14745465184318315,
      "grad_norm": 1.0415784120559692,
      "learning_rate": 4.4262295081967207e-05,
      "loss": 0.5058,
      "step": 63
    },
    {
      "epoch": 0.14979520187244003,
      "grad_norm": 0.6722142696380615,
      "learning_rate": 4.4964871194379385e-05,
      "loss": 0.3914,
      "step": 64
    },
    {
      "epoch": 0.1521357519016969,
      "grad_norm": 0.5692099928855896,
      "learning_rate": 4.5667447306791564e-05,
      "loss": 0.3273,
      "step": 65
    },
    {
      "epoch": 0.15447630193095377,
      "grad_norm": 0.7278772592544556,
      "learning_rate": 4.637002341920374e-05,
      "loss": 0.4953,
      "step": 66
    },
    {
      "epoch": 0.15681685196021064,
      "grad_norm": 0.8645195960998535,
      "learning_rate": 4.707259953161592e-05,
      "loss": 0.4258,
      "step": 67
    },
    {
      "epoch": 0.1591574019894675,
      "grad_norm": 0.9355226755142212,
      "learning_rate": 4.77751756440281e-05,
      "loss": 0.4609,
      "step": 68
    },
    {
      "epoch": 0.1614979520187244,
      "grad_norm": 0.6526535153388977,
      "learning_rate": 4.847775175644028e-05,
      "loss": 0.415,
      "step": 69
    },
    {
      "epoch": 0.16383850204798128,
      "grad_norm": 0.5073607563972473,
      "learning_rate": 4.918032786885245e-05,
      "loss": 0.4054,
      "step": 70
    },
    {
      "epoch": 0.16617905207723815,
      "grad_norm": 0.6290750503540039,
      "learning_rate": 4.988290398126463e-05,
      "loss": 0.4395,
      "step": 71
    },
    {
      "epoch": 0.16851960210649503,
      "grad_norm": 0.7252535223960876,
      "learning_rate": 5.0585480093676807e-05,
      "loss": 0.4413,
      "step": 72
    },
    {
      "epoch": 0.1708601521357519,
      "grad_norm": 0.6600777506828308,
      "learning_rate": 5.1288056206088985e-05,
      "loss": 0.4525,
      "step": 73
    },
    {
      "epoch": 0.17320070216500877,
      "grad_norm": 0.6701820492744446,
      "learning_rate": 5.1990632318501164e-05,
      "loss": 0.3869,
      "step": 74
    },
    {
      "epoch": 0.17554125219426564,
      "grad_norm": 0.8461474776268005,
      "learning_rate": 5.269320843091334e-05,
      "loss": 0.4009,
      "step": 75
    },
    {
      "epoch": 0.17788180222352254,
      "grad_norm": 0.8573521971702576,
      "learning_rate": 5.339578454332552e-05,
      "loss": 0.3054,
      "step": 76
    },
    {
      "epoch": 0.1802223522527794,
      "grad_norm": 0.7761601805686951,
      "learning_rate": 5.40983606557377e-05,
      "loss": 0.4069,
      "step": 77
    },
    {
      "epoch": 0.18256290228203628,
      "grad_norm": 0.5159727334976196,
      "learning_rate": 5.480093676814988e-05,
      "loss": 0.3387,
      "step": 78
    },
    {
      "epoch": 0.18490345231129315,
      "grad_norm": 0.5012618899345398,
      "learning_rate": 5.5503512880562056e-05,
      "loss": 0.3528,
      "step": 79
    },
    {
      "epoch": 0.18724400234055003,
      "grad_norm": 0.6105583906173706,
      "learning_rate": 5.6206088992974235e-05,
      "loss": 0.3479,
      "step": 80
    },
    {
      "epoch": 0.1895845523698069,
      "grad_norm": 0.8292217254638672,
      "learning_rate": 5.6908665105386407e-05,
      "loss": 0.3231,
      "step": 81
    },
    {
      "epoch": 0.19192510239906377,
      "grad_norm": 0.6521927714347839,
      "learning_rate": 5.7611241217798585e-05,
      "loss": 0.3633,
      "step": 82
    },
    {
      "epoch": 0.19426565242832067,
      "grad_norm": 0.7296267747879028,
      "learning_rate": 5.8313817330210764e-05,
      "loss": 0.3061,
      "step": 83
    },
    {
      "epoch": 0.19660620245757754,
      "grad_norm": 0.6201072931289673,
      "learning_rate": 5.901639344262294e-05,
      "loss": 0.3486,
      "step": 84
    },
    {
      "epoch": 0.1989467524868344,
      "grad_norm": 0.4525306224822998,
      "learning_rate": 5.971896955503512e-05,
      "loss": 0.3119,
      "step": 85
    },
    {
      "epoch": 0.20128730251609128,
      "grad_norm": 0.656587541103363,
      "learning_rate": 6.04215456674473e-05,
      "loss": 0.4219,
      "step": 86
    },
    {
      "epoch": 0.20362785254534815,
      "grad_norm": 0.7000443339347839,
      "learning_rate": 6.112412177985948e-05,
      "loss": 0.3345,
      "step": 87
    },
    {
      "epoch": 0.20596840257460503,
      "grad_norm": 0.5518720746040344,
      "learning_rate": 6.182669789227166e-05,
      "loss": 0.3404,
      "step": 88
    },
    {
      "epoch": 0.2083089526038619,
      "grad_norm": 0.4767035245895386,
      "learning_rate": 6.252927400468383e-05,
      "loss": 0.3631,
      "step": 89
    },
    {
      "epoch": 0.2106495026331188,
      "grad_norm": 0.4379793405532837,
      "learning_rate": 6.323185011709601e-05,
      "loss": 0.3906,
      "step": 90
    },
    {
      "epoch": 0.21299005266237567,
      "grad_norm": 0.4530552327632904,
      "learning_rate": 6.393442622950819e-05,
      "loss": 0.2991,
      "step": 91
    },
    {
      "epoch": 0.21533060269163254,
      "grad_norm": 0.7493224143981934,
      "learning_rate": 6.463700234192037e-05,
      "loss": 0.1767,
      "step": 92
    },
    {
      "epoch": 0.2176711527208894,
      "grad_norm": 0.37507402896881104,
      "learning_rate": 6.533957845433255e-05,
      "loss": 0.3769,
      "step": 93
    },
    {
      "epoch": 0.22001170275014628,
      "grad_norm": 0.41083166003227234,
      "learning_rate": 6.604215456674473e-05,
      "loss": 0.4079,
      "step": 94
    },
    {
      "epoch": 0.22235225277940315,
      "grad_norm": 0.5730900764465332,
      "learning_rate": 6.67447306791569e-05,
      "loss": 0.4228,
      "step": 95
    },
    {
      "epoch": 0.22469280280866002,
      "grad_norm": 0.41166698932647705,
      "learning_rate": 6.744730679156908e-05,
      "loss": 0.2941,
      "step": 96
    },
    {
      "epoch": 0.22703335283791692,
      "grad_norm": 0.3606652021408081,
      "learning_rate": 6.814988290398126e-05,
      "loss": 0.2823,
      "step": 97
    },
    {
      "epoch": 0.2293739028671738,
      "grad_norm": 0.32912683486938477,
      "learning_rate": 6.885245901639344e-05,
      "loss": 0.3522,
      "step": 98
    },
    {
      "epoch": 0.23171445289643067,
      "grad_norm": 0.42570698261260986,
      "learning_rate": 6.955503512880562e-05,
      "loss": 0.2897,
      "step": 99
    },
    {
      "epoch": 0.23405500292568754,
      "grad_norm": 0.35717108845710754,
      "learning_rate": 7.02576112412178e-05,
      "loss": 0.2657,
      "step": 100
    },
    {
      "epoch": 0.2363955529549444,
      "grad_norm": 0.4132005572319031,
      "learning_rate": 7.096018735362998e-05,
      "loss": 0.334,
      "step": 101
    },
    {
      "epoch": 0.23873610298420128,
      "grad_norm": 0.4048447906970978,
      "learning_rate": 7.166276346604214e-05,
      "loss": 0.3219,
      "step": 102
    },
    {
      "epoch": 0.24107665301345815,
      "grad_norm": 0.3967728912830353,
      "learning_rate": 7.236533957845432e-05,
      "loss": 0.3156,
      "step": 103
    },
    {
      "epoch": 0.24341720304271502,
      "grad_norm": 0.4340967535972595,
      "learning_rate": 7.30679156908665e-05,
      "loss": 0.3611,
      "step": 104
    },
    {
      "epoch": 0.24575775307197192,
      "grad_norm": 0.34062108397483826,
      "learning_rate": 7.377049180327868e-05,
      "loss": 0.3307,
      "step": 105
    },
    {
      "epoch": 0.2480983031012288,
      "grad_norm": 0.39451971650123596,
      "learning_rate": 7.447306791569086e-05,
      "loss": 0.2904,
      "step": 106
    },
    {
      "epoch": 0.25043885313048564,
      "grad_norm": 0.3907928168773651,
      "learning_rate": 7.517564402810303e-05,
      "loss": 0.3478,
      "step": 107
    },
    {
      "epoch": 0.25277940315974257,
      "grad_norm": 0.37488511204719543,
      "learning_rate": 7.587822014051521e-05,
      "loss": 0.2547,
      "step": 108
    },
    {
      "epoch": 0.25511995318899944,
      "grad_norm": 0.4519350826740265,
      "learning_rate": 7.658079625292739e-05,
      "loss": 0.3288,
      "step": 109
    },
    {
      "epoch": 0.2574605032182563,
      "grad_norm": 0.519005298614502,
      "learning_rate": 7.728337236533957e-05,
      "loss": 0.3092,
      "step": 110
    },
    {
      "epoch": 0.2598010532475132,
      "grad_norm": 0.3383859395980835,
      "learning_rate": 7.798594847775175e-05,
      "loss": 0.3265,
      "step": 111
    },
    {
      "epoch": 0.26214160327677005,
      "grad_norm": 0.28597521781921387,
      "learning_rate": 7.868852459016393e-05,
      "loss": 0.3544,
      "step": 112
    },
    {
      "epoch": 0.2644821533060269,
      "grad_norm": 0.4308033287525177,
      "learning_rate": 7.93911007025761e-05,
      "loss": 0.3082,
      "step": 113
    },
    {
      "epoch": 0.2668227033352838,
      "grad_norm": 0.35500022768974304,
      "learning_rate": 8.009367681498828e-05,
      "loss": 0.2846,
      "step": 114
    },
    {
      "epoch": 0.26916325336454067,
      "grad_norm": 0.3331556022167206,
      "learning_rate": 8.079625292740046e-05,
      "loss": 0.3191,
      "step": 115
    },
    {
      "epoch": 0.27150380339379754,
      "grad_norm": 0.3788938522338867,
      "learning_rate": 8.149882903981264e-05,
      "loss": 0.2298,
      "step": 116
    },
    {
      "epoch": 0.2738443534230544,
      "grad_norm": 0.3549221456050873,
      "learning_rate": 8.220140515222482e-05,
      "loss": 0.2837,
      "step": 117
    },
    {
      "epoch": 0.2761849034523113,
      "grad_norm": 0.4072129428386688,
      "learning_rate": 8.290398126463698e-05,
      "loss": 0.3064,
      "step": 118
    },
    {
      "epoch": 0.27852545348156815,
      "grad_norm": 0.42125144600868225,
      "learning_rate": 8.360655737704916e-05,
      "loss": 0.3631,
      "step": 119
    },
    {
      "epoch": 0.280866003510825,
      "grad_norm": 0.36516016721725464,
      "learning_rate": 8.430913348946134e-05,
      "loss": 0.3505,
      "step": 120
    },
    {
      "epoch": 0.2832065535400819,
      "grad_norm": 0.33953580260276794,
      "learning_rate": 8.501170960187352e-05,
      "loss": 0.2541,
      "step": 121
    },
    {
      "epoch": 0.28554710356933877,
      "grad_norm": 0.3969985544681549,
      "learning_rate": 8.57142857142857e-05,
      "loss": 0.2485,
      "step": 122
    },
    {
      "epoch": 0.2878876535985957,
      "grad_norm": 0.3505416810512543,
      "learning_rate": 8.641686182669788e-05,
      "loss": 0.3162,
      "step": 123
    },
    {
      "epoch": 0.29022820362785257,
      "grad_norm": 0.46451449394226074,
      "learning_rate": 8.711943793911006e-05,
      "loss": 0.3466,
      "step": 124
    },
    {
      "epoch": 0.29256875365710944,
      "grad_norm": 0.3464169502258301,
      "learning_rate": 8.782201405152223e-05,
      "loss": 0.3135,
      "step": 125
    },
    {
      "epoch": 0.2949093036863663,
      "grad_norm": 0.35586991906166077,
      "learning_rate": 8.852459016393441e-05,
      "loss": 0.2725,
      "step": 126
    },
    {
      "epoch": 0.2972498537156232,
      "grad_norm": 0.42192724347114563,
      "learning_rate": 8.922716627634659e-05,
      "loss": 0.3221,
      "step": 127
    },
    {
      "epoch": 0.29959040374488005,
      "grad_norm": 0.47556284070014954,
      "learning_rate": 8.992974238875877e-05,
      "loss": 0.3256,
      "step": 128
    },
    {
      "epoch": 0.3019309537741369,
      "grad_norm": 0.3432627022266388,
      "learning_rate": 9.063231850117095e-05,
      "loss": 0.304,
      "step": 129
    },
    {
      "epoch": 0.3042715038033938,
      "grad_norm": 0.5835237503051758,
      "learning_rate": 9.133489461358313e-05,
      "loss": 0.3413,
      "step": 130
    },
    {
      "epoch": 0.30661205383265067,
      "grad_norm": 0.5011448264122009,
      "learning_rate": 9.20374707259953e-05,
      "loss": 0.3575,
      "step": 131
    },
    {
      "epoch": 0.30895260386190754,
      "grad_norm": 0.5656663179397583,
      "learning_rate": 9.274004683840748e-05,
      "loss": 0.3643,
      "step": 132
    },
    {
      "epoch": 0.3112931538911644,
      "grad_norm": 0.349385142326355,
      "learning_rate": 9.344262295081966e-05,
      "loss": 0.3268,
      "step": 133
    },
    {
      "epoch": 0.3136337039204213,
      "grad_norm": 0.35471081733703613,
      "learning_rate": 9.414519906323184e-05,
      "loss": 0.3016,
      "step": 134
    },
    {
      "epoch": 0.31597425394967815,
      "grad_norm": 0.4162161350250244,
      "learning_rate": 9.484777517564402e-05,
      "loss": 0.3683,
      "step": 135
    },
    {
      "epoch": 0.318314803978935,
      "grad_norm": 0.5241917371749878,
      "learning_rate": 9.55503512880562e-05,
      "loss": 0.2819,
      "step": 136
    },
    {
      "epoch": 0.32065535400819195,
      "grad_norm": 0.4248739182949066,
      "learning_rate": 9.625292740046838e-05,
      "loss": 0.3298,
      "step": 137
    },
    {
      "epoch": 0.3229959040374488,
      "grad_norm": 0.41428589820861816,
      "learning_rate": 9.695550351288056e-05,
      "loss": 0.2721,
      "step": 138
    },
    {
      "epoch": 0.3253364540667057,
      "grad_norm": 0.4609375298023224,
      "learning_rate": 9.765807962529272e-05,
      "loss": 0.3593,
      "step": 139
    },
    {
      "epoch": 0.32767700409596257,
      "grad_norm": 0.378097802400589,
      "learning_rate": 9.83606557377049e-05,
      "loss": 0.2293,
      "step": 140
    },
    {
      "epoch": 0.33001755412521944,
      "grad_norm": 0.39688512682914734,
      "learning_rate": 9.906323185011708e-05,
      "loss": 0.3297,
      "step": 141
    },
    {
      "epoch": 0.3323581041544763,
      "grad_norm": 0.44131508469581604,
      "learning_rate": 9.976580796252926e-05,
      "loss": 0.4097,
      "step": 142
    },
    {
      "epoch": 0.3346986541837332,
      "grad_norm": 0.32726970314979553,
      "learning_rate": 0.00010046838407494143,
      "loss": 0.3178,
      "step": 143
    },
    {
      "epoch": 0.33703920421299005,
      "grad_norm": 0.4096187651157379,
      "learning_rate": 0.00010117096018735361,
      "loss": 0.3235,
      "step": 144
    },
    {
      "epoch": 0.3393797542422469,
      "grad_norm": 0.4314582049846649,
      "learning_rate": 0.00010187353629976579,
      "loss": 0.3545,
      "step": 145
    },
    {
      "epoch": 0.3417203042715038,
      "grad_norm": 0.3780286908149719,
      "learning_rate": 0.00010257611241217797,
      "loss": 0.3235,
      "step": 146
    },
    {
      "epoch": 0.34406085430076067,
      "grad_norm": 0.45858243107795715,
      "learning_rate": 0.00010327868852459015,
      "loss": 0.3243,
      "step": 147
    },
    {
      "epoch": 0.34640140433001754,
      "grad_norm": 0.40075913071632385,
      "learning_rate": 0.00010398126463700233,
      "loss": 0.3205,
      "step": 148
    },
    {
      "epoch": 0.3487419543592744,
      "grad_norm": 0.42145836353302,
      "learning_rate": 0.0001046838407494145,
      "loss": 0.2874,
      "step": 149
    },
    {
      "epoch": 0.3510825043885313,
      "grad_norm": 0.4258599579334259,
      "learning_rate": 0.00010538641686182668,
      "loss": 0.3536,
      "step": 150
    },
    {
      "epoch": 0.3534230544177882,
      "grad_norm": 0.475663959980011,
      "learning_rate": 0.00010608899297423886,
      "loss": 0.3461,
      "step": 151
    },
    {
      "epoch": 0.3557636044470451,
      "grad_norm": 0.45754384994506836,
      "learning_rate": 0.00010679156908665104,
      "loss": 0.2744,
      "step": 152
    },
    {
      "epoch": 0.35810415447630195,
      "grad_norm": 0.37627583742141724,
      "learning_rate": 0.00010749414519906322,
      "loss": 0.2669,
      "step": 153
    },
    {
      "epoch": 0.3604447045055588,
      "grad_norm": 0.4004940688610077,
      "learning_rate": 0.0001081967213114754,
      "loss": 0.3019,
      "step": 154
    },
    {
      "epoch": 0.3627852545348157,
      "grad_norm": 0.471226304769516,
      "learning_rate": 0.00010889929742388758,
      "loss": 0.3276,
      "step": 155
    },
    {
      "epoch": 0.36512580456407256,
      "grad_norm": 0.6340135335922241,
      "learning_rate": 0.00010960187353629976,
      "loss": 0.3387,
      "step": 156
    },
    {
      "epoch": 0.36746635459332944,
      "grad_norm": 0.4948498010635376,
      "learning_rate": 0.00011030444964871193,
      "loss": 0.2905,
      "step": 157
    },
    {
      "epoch": 0.3698069046225863,
      "grad_norm": 0.5159802436828613,
      "learning_rate": 0.00011100702576112411,
      "loss": 0.3346,
      "step": 158
    },
    {
      "epoch": 0.3721474546518432,
      "grad_norm": 0.4786936044692993,
      "learning_rate": 0.00011170960187353629,
      "loss": 0.304,
      "step": 159
    },
    {
      "epoch": 0.37448800468110005,
      "grad_norm": 0.42688825726509094,
      "learning_rate": 0.00011241217798594847,
      "loss": 0.3595,
      "step": 160
    },
    {
      "epoch": 0.3768285547103569,
      "grad_norm": 0.6631761789321899,
      "learning_rate": 0.00011311475409836063,
      "loss": 0.3004,
      "step": 161
    },
    {
      "epoch": 0.3791691047396138,
      "grad_norm": 0.8345941305160522,
      "learning_rate": 0.00011381733021077281,
      "loss": 0.2626,
      "step": 162
    },
    {
      "epoch": 0.38150965476887067,
      "grad_norm": 0.5626307725906372,
      "learning_rate": 0.00011451990632318499,
      "loss": 0.3906,
      "step": 163
    },
    {
      "epoch": 0.38385020479812754,
      "grad_norm": 0.42218294739723206,
      "learning_rate": 0.00011522248243559717,
      "loss": 0.2345,
      "step": 164
    },
    {
      "epoch": 0.3861907548273844,
      "grad_norm": 0.3799636960029602,
      "learning_rate": 0.00011592505854800935,
      "loss": 0.2837,
      "step": 165
    },
    {
      "epoch": 0.38853130485664134,
      "grad_norm": 0.6572702527046204,
      "learning_rate": 0.00011662763466042153,
      "loss": 0.3847,
      "step": 166
    },
    {
      "epoch": 0.3908718548858982,
      "grad_norm": 0.4690800905227661,
      "learning_rate": 0.0001173302107728337,
      "loss": 0.3143,
      "step": 167
    },
    {
      "epoch": 0.3932124049151551,
      "grad_norm": 0.5363594889640808,
      "learning_rate": 0.00011803278688524588,
      "loss": 0.305,
      "step": 168
    },
    {
      "epoch": 0.39555295494441195,
      "grad_norm": 0.44413185119628906,
      "learning_rate": 0.00011873536299765806,
      "loss": 0.3052,
      "step": 169
    },
    {
      "epoch": 0.3978935049736688,
      "grad_norm": 0.4704926013946533,
      "learning_rate": 0.00011943793911007024,
      "loss": 0.2104,
      "step": 170
    },
    {
      "epoch": 0.4002340550029257,
      "grad_norm": 0.33276844024658203,
      "learning_rate": 0.00012014051522248242,
      "loss": 0.3335,
      "step": 171
    },
    {
      "epoch": 0.40257460503218256,
      "grad_norm": 0.7700223326683044,
      "learning_rate": 0.0001208430913348946,
      "loss": 0.3206,
      "step": 172
    },
    {
      "epoch": 0.40491515506143944,
      "grad_norm": 0.4257805347442627,
      "learning_rate": 0.00012154566744730678,
      "loss": 0.3277,
      "step": 173
    },
    {
      "epoch": 0.4072557050906963,
      "grad_norm": 0.6086185574531555,
      "learning_rate": 0.00012224824355971896,
      "loss": 0.3386,
      "step": 174
    },
    {
      "epoch": 0.4095962551199532,
      "grad_norm": 0.3652767539024353,
      "learning_rate": 0.00012295081967213115,
      "loss": 0.1992,
      "step": 175
    },
    {
      "epoch": 0.41193680514921005,
      "grad_norm": 0.5759636163711548,
      "learning_rate": 0.0001236533957845433,
      "loss": 0.3509,
      "step": 176
    },
    {
      "epoch": 0.4142773551784669,
      "grad_norm": 0.5895202159881592,
      "learning_rate": 0.0001243559718969555,
      "loss": 0.4208,
      "step": 177
    },
    {
      "epoch": 0.4166179052077238,
      "grad_norm": 0.4766615927219391,
      "learning_rate": 0.00012505854800936767,
      "loss": 0.2675,
      "step": 178
    },
    {
      "epoch": 0.41895845523698066,
      "grad_norm": 0.3585280776023865,
      "learning_rate": 0.00012576112412177986,
      "loss": 0.2386,
      "step": 179
    },
    {
      "epoch": 0.4212990052662376,
      "grad_norm": 0.4672686457633972,
      "learning_rate": 0.00012646370023419203,
      "loss": 0.377,
      "step": 180
    },
    {
      "epoch": 0.42363955529549446,
      "grad_norm": 0.5058435201644897,
      "learning_rate": 0.00012716627634660422,
      "loss": 0.352,
      "step": 181
    },
    {
      "epoch": 0.42598010532475133,
      "grad_norm": 0.4492247998714447,
      "learning_rate": 0.00012786885245901638,
      "loss": 0.3474,
      "step": 182
    },
    {
      "epoch": 0.4283206553540082,
      "grad_norm": 0.3723186254501343,
      "learning_rate": 0.00012857142857142855,
      "loss": 0.3398,
      "step": 183
    },
    {
      "epoch": 0.4306612053832651,
      "grad_norm": 0.4762703478336334,
      "learning_rate": 0.00012927400468384074,
      "loss": 0.3026,
      "step": 184
    },
    {
      "epoch": 0.43300175541252195,
      "grad_norm": 0.4432997405529022,
      "learning_rate": 0.0001299765807962529,
      "loss": 0.3314,
      "step": 185
    },
    {
      "epoch": 0.4353423054417788,
      "grad_norm": 0.5557265877723694,
      "learning_rate": 0.0001306791569086651,
      "loss": 0.2704,
      "step": 186
    },
    {
      "epoch": 0.4376828554710357,
      "grad_norm": 0.35001376271247864,
      "learning_rate": 0.00013138173302107726,
      "loss": 0.2694,
      "step": 187
    },
    {
      "epoch": 0.44002340550029256,
      "grad_norm": 0.41710108518600464,
      "learning_rate": 0.00013208430913348945,
      "loss": 0.348,
      "step": 188
    },
    {
      "epoch": 0.44236395552954944,
      "grad_norm": 0.3899070918560028,
      "learning_rate": 0.00013278688524590162,
      "loss": 0.2804,
      "step": 189
    },
    {
      "epoch": 0.4447045055588063,
      "grad_norm": 0.34398430585861206,
      "learning_rate": 0.0001334894613583138,
      "loss": 0.2654,
      "step": 190
    },
    {
      "epoch": 0.4470450555880632,
      "grad_norm": 0.46375027298927307,
      "learning_rate": 0.00013419203747072598,
      "loss": 0.3369,
      "step": 191
    },
    {
      "epoch": 0.44938560561732005,
      "grad_norm": 0.5009866952896118,
      "learning_rate": 0.00013489461358313817,
      "loss": 0.294,
      "step": 192
    },
    {
      "epoch": 0.4517261556465769,
      "grad_norm": 0.4422433078289032,
      "learning_rate": 0.00013559718969555033,
      "loss": 0.2255,
      "step": 193
    },
    {
      "epoch": 0.45406670567583385,
      "grad_norm": 0.3943370580673218,
      "learning_rate": 0.00013629976580796253,
      "loss": 0.3283,
      "step": 194
    },
    {
      "epoch": 0.4564072557050907,
      "grad_norm": 0.40654951333999634,
      "learning_rate": 0.0001370023419203747,
      "loss": 0.2877,
      "step": 195
    },
    {
      "epoch": 0.4587478057343476,
      "grad_norm": 0.3880634307861328,
      "learning_rate": 0.00013770491803278688,
      "loss": 0.2576,
      "step": 196
    },
    {
      "epoch": 0.46108835576360446,
      "grad_norm": 0.4527228772640228,
      "learning_rate": 0.00013840749414519905,
      "loss": 0.2375,
      "step": 197
    },
    {
      "epoch": 0.46342890579286133,
      "grad_norm": 0.4566499888896942,
      "learning_rate": 0.00013911007025761124,
      "loss": 0.3566,
      "step": 198
    },
    {
      "epoch": 0.4657694558221182,
      "grad_norm": 0.38687166571617126,
      "learning_rate": 0.0001398126463700234,
      "loss": 0.3426,
      "step": 199
    },
    {
      "epoch": 0.4681100058513751,
      "grad_norm": 0.5023470520973206,
      "learning_rate": 0.0001405152224824356,
      "loss": 0.348,
      "step": 200
    },
    {
      "epoch": 0.47045055588063195,
      "grad_norm": 0.38558581471443176,
      "learning_rate": 0.00014121779859484776,
      "loss": 0.2692,
      "step": 201
    },
    {
      "epoch": 0.4727911059098888,
      "grad_norm": 0.3736518323421478,
      "learning_rate": 0.00014192037470725995,
      "loss": 0.3332,
      "step": 202
    },
    {
      "epoch": 0.4751316559391457,
      "grad_norm": 0.45000818371772766,
      "learning_rate": 0.00014262295081967212,
      "loss": 0.2965,
      "step": 203
    },
    {
      "epoch": 0.47747220596840256,
      "grad_norm": 0.5682644844055176,
      "learning_rate": 0.00014332552693208428,
      "loss": 0.287,
      "step": 204
    },
    {
      "epoch": 0.47981275599765943,
      "grad_norm": 0.3084190785884857,
      "learning_rate": 0.00014402810304449648,
      "loss": 0.3372,
      "step": 205
    },
    {
      "epoch": 0.4821533060269163,
      "grad_norm": 0.39794057607650757,
      "learning_rate": 0.00014473067915690864,
      "loss": 0.3694,
      "step": 206
    },
    {
      "epoch": 0.4844938560561732,
      "grad_norm": 0.39560580253601074,
      "learning_rate": 0.00014543325526932083,
      "loss": 0.3257,
      "step": 207
    },
    {
      "epoch": 0.48683440608543005,
      "grad_norm": 0.5171453952789307,
      "learning_rate": 0.000146135831381733,
      "loss": 0.3795,
      "step": 208
    },
    {
      "epoch": 0.489174956114687,
      "grad_norm": 0.46178826689720154,
      "learning_rate": 0.0001468384074941452,
      "loss": 0.3912,
      "step": 209
    },
    {
      "epoch": 0.49151550614394385,
      "grad_norm": 0.41249439120292664,
      "learning_rate": 0.00014754098360655736,
      "loss": 0.3287,
      "step": 210
    },
    {
      "epoch": 0.4938560561732007,
      "grad_norm": 0.3391643166542053,
      "learning_rate": 0.00014824355971896955,
      "loss": 0.2798,
      "step": 211
    },
    {
      "epoch": 0.4961966062024576,
      "grad_norm": 0.3883397579193115,
      "learning_rate": 0.0001489461358313817,
      "loss": 0.3136,
      "step": 212
    },
    {
      "epoch": 0.49853715623171446,
      "grad_norm": 0.39075160026550293,
      "learning_rate": 0.0001496487119437939,
      "loss": 0.2793,
      "step": 213
    },
    {
      "epoch": 0.5008777062609713,
      "grad_norm": 0.4008817970752716,
      "learning_rate": 0.00015035128805620607,
      "loss": 0.2929,
      "step": 214
    },
    {
      "epoch": 0.5032182562902282,
      "grad_norm": 0.4347364008426666,
      "learning_rate": 0.00015105386416861826,
      "loss": 0.3364,
      "step": 215
    },
    {
      "epoch": 0.5055588063194851,
      "grad_norm": 0.38719114661216736,
      "learning_rate": 0.00015175644028103043,
      "loss": 0.3018,
      "step": 216
    },
    {
      "epoch": 0.507899356348742,
      "grad_norm": 0.4263725280761719,
      "learning_rate": 0.00015245901639344262,
      "loss": 0.3055,
      "step": 217
    },
    {
      "epoch": 0.5102399063779989,
      "grad_norm": 0.46486231684684753,
      "learning_rate": 0.00015316159250585478,
      "loss": 0.2855,
      "step": 218
    },
    {
      "epoch": 0.5125804564072557,
      "grad_norm": 0.4554624855518341,
      "learning_rate": 0.00015386416861826698,
      "loss": 0.3516,
      "step": 219
    },
    {
      "epoch": 0.5149210064365126,
      "grad_norm": 0.3457655906677246,
      "learning_rate": 0.00015456674473067914,
      "loss": 0.2569,
      "step": 220
    },
    {
      "epoch": 0.5172615564657694,
      "grad_norm": 0.3987472951412201,
      "learning_rate": 0.00015526932084309133,
      "loss": 0.3449,
      "step": 221
    },
    {
      "epoch": 0.5196021064950264,
      "grad_norm": 0.4908752739429474,
      "learning_rate": 0.0001559718969555035,
      "loss": 0.3812,
      "step": 222
    },
    {
      "epoch": 0.5219426565242832,
      "grad_norm": 0.35612326860427856,
      "learning_rate": 0.0001566744730679157,
      "loss": 0.321,
      "step": 223
    },
    {
      "epoch": 0.5242832065535401,
      "grad_norm": 0.3147023916244507,
      "learning_rate": 0.00015737704918032785,
      "loss": 0.3153,
      "step": 224
    },
    {
      "epoch": 0.5266237565827969,
      "grad_norm": 0.4080836772918701,
      "learning_rate": 0.00015807962529274005,
      "loss": 0.294,
      "step": 225
    },
    {
      "epoch": 0.5289643066120538,
      "grad_norm": 0.405121773481369,
      "learning_rate": 0.0001587822014051522,
      "loss": 0.3175,
      "step": 226
    },
    {
      "epoch": 0.5313048566413107,
      "grad_norm": 0.5383305549621582,
      "learning_rate": 0.0001594847775175644,
      "loss": 0.3323,
      "step": 227
    },
    {
      "epoch": 0.5336454066705676,
      "grad_norm": 0.378863126039505,
      "learning_rate": 0.00016018735362997657,
      "loss": 0.3013,
      "step": 228
    },
    {
      "epoch": 0.5359859566998244,
      "grad_norm": 0.32974377274513245,
      "learning_rate": 0.00016088992974238876,
      "loss": 0.3133,
      "step": 229
    },
    {
      "epoch": 0.5383265067290813,
      "grad_norm": 0.40917810797691345,
      "learning_rate": 0.00016159250585480093,
      "loss": 0.2566,
      "step": 230
    },
    {
      "epoch": 0.5406670567583383,
      "grad_norm": 0.36830049753189087,
      "learning_rate": 0.00016229508196721312,
      "loss": 0.2591,
      "step": 231
    },
    {
      "epoch": 0.5430076067875951,
      "grad_norm": 0.5981646180152893,
      "learning_rate": 0.00016299765807962528,
      "loss": 0.3651,
      "step": 232
    },
    {
      "epoch": 0.545348156816852,
      "grad_norm": 0.366531103849411,
      "learning_rate": 0.00016370023419203747,
      "loss": 0.2681,
      "step": 233
    },
    {
      "epoch": 0.5476887068461088,
      "grad_norm": 0.3491288125514984,
      "learning_rate": 0.00016440281030444964,
      "loss": 0.2461,
      "step": 234
    },
    {
      "epoch": 0.5500292568753657,
      "grad_norm": 0.3897869884967804,
      "learning_rate": 0.0001651053864168618,
      "loss": 0.2809,
      "step": 235
    },
    {
      "epoch": 0.5523698069046226,
      "grad_norm": 0.32702428102493286,
      "learning_rate": 0.00016580796252927397,
      "loss": 0.2705,
      "step": 236
    },
    {
      "epoch": 0.5547103569338795,
      "grad_norm": 0.39718085527420044,
      "learning_rate": 0.00016651053864168616,
      "loss": 0.258,
      "step": 237
    },
    {
      "epoch": 0.5570509069631363,
      "grad_norm": 0.36009645462036133,
      "learning_rate": 0.00016721311475409833,
      "loss": 0.3289,
      "step": 238
    },
    {
      "epoch": 0.5593914569923932,
      "grad_norm": 0.4100712239742279,
      "learning_rate": 0.00016791569086651052,
      "loss": 0.2997,
      "step": 239
    },
    {
      "epoch": 0.56173200702165,
      "grad_norm": 0.25334590673446655,
      "learning_rate": 0.00016861826697892268,
      "loss": 0.2341,
      "step": 240
    },
    {
      "epoch": 0.564072557050907,
      "grad_norm": 0.3853577971458435,
      "learning_rate": 0.00016932084309133488,
      "loss": 0.3264,
      "step": 241
    },
    {
      "epoch": 0.5664131070801638,
      "grad_norm": 0.6100504994392395,
      "learning_rate": 0.00017002341920374704,
      "loss": 0.3542,
      "step": 242
    },
    {
      "epoch": 0.5687536571094207,
      "grad_norm": 0.352998822927475,
      "learning_rate": 0.00017072599531615923,
      "loss": 0.3044,
      "step": 243
    },
    {
      "epoch": 0.5710942071386775,
      "grad_norm": 0.3676944971084595,
      "learning_rate": 0.0001714285714285714,
      "loss": 0.2271,
      "step": 244
    },
    {
      "epoch": 0.5734347571679345,
      "grad_norm": 0.42094510793685913,
      "learning_rate": 0.0001721311475409836,
      "loss": 0.3266,
      "step": 245
    },
    {
      "epoch": 0.5757753071971914,
      "grad_norm": 0.34197545051574707,
      "learning_rate": 0.00017283372365339576,
      "loss": 0.2905,
      "step": 246
    },
    {
      "epoch": 0.5781158572264482,
      "grad_norm": 0.29929041862487793,
      "learning_rate": 0.00017353629976580795,
      "loss": 0.2618,
      "step": 247
    },
    {
      "epoch": 0.5804564072557051,
      "grad_norm": 0.3467540442943573,
      "learning_rate": 0.0001742388758782201,
      "loss": 0.3081,
      "step": 248
    },
    {
      "epoch": 0.582796957284962,
      "grad_norm": 0.3524278700351715,
      "learning_rate": 0.0001749414519906323,
      "loss": 0.2841,
      "step": 249
    },
    {
      "epoch": 0.5851375073142189,
      "grad_norm": 0.3742045760154724,
      "learning_rate": 0.00017564402810304447,
      "loss": 0.3105,
      "step": 250
    },
    {
      "epoch": 0.5874780573434757,
      "grad_norm": 0.5120700597763062,
      "learning_rate": 0.00017634660421545666,
      "loss": 0.372,
      "step": 251
    },
    {
      "epoch": 0.5898186073727326,
      "grad_norm": 0.4536421298980713,
      "learning_rate": 0.00017704918032786883,
      "loss": 0.2628,
      "step": 252
    },
    {
      "epoch": 0.5921591574019894,
      "grad_norm": 0.37460607290267944,
      "learning_rate": 0.00017775175644028102,
      "loss": 0.2532,
      "step": 253
    },
    {
      "epoch": 0.5944997074312464,
      "grad_norm": 0.48460525274276733,
      "learning_rate": 0.00017845433255269318,
      "loss": 0.3402,
      "step": 254
    },
    {
      "epoch": 0.5968402574605032,
      "grad_norm": 0.36584585905075073,
      "learning_rate": 0.00017915690866510538,
      "loss": 0.2657,
      "step": 255
    },
    {
      "epoch": 0.5991808074897601,
      "grad_norm": 0.34767401218414307,
      "learning_rate": 0.00017985948477751754,
      "loss": 0.2601,
      "step": 256
    },
    {
      "epoch": 0.6015213575190169,
      "grad_norm": 0.3745335638523102,
      "learning_rate": 0.00018056206088992973,
      "loss": 0.2646,
      "step": 257
    },
    {
      "epoch": 0.6038619075482738,
      "grad_norm": 0.5099627375602722,
      "learning_rate": 0.0001812646370023419,
      "loss": 0.3283,
      "step": 258
    },
    {
      "epoch": 0.6062024575775308,
      "grad_norm": 0.5888233780860901,
      "learning_rate": 0.0001819672131147541,
      "loss": 0.2675,
      "step": 259
    },
    {
      "epoch": 0.6085430076067876,
      "grad_norm": 0.3951157033443451,
      "learning_rate": 0.00018266978922716625,
      "loss": 0.2397,
      "step": 260
    },
    {
      "epoch": 0.6108835576360445,
      "grad_norm": 0.414898157119751,
      "learning_rate": 0.00018337236533957845,
      "loss": 0.2969,
      "step": 261
    },
    {
      "epoch": 0.6132241076653013,
      "grad_norm": 0.3515671491622925,
      "learning_rate": 0.0001840749414519906,
      "loss": 0.254,
      "step": 262
    },
    {
      "epoch": 0.6155646576945583,
      "grad_norm": 0.5317211747169495,
      "learning_rate": 0.0001847775175644028,
      "loss": 0.2412,
      "step": 263
    },
    {
      "epoch": 0.6179052077238151,
      "grad_norm": 0.4360625743865967,
      "learning_rate": 0.00018548009367681497,
      "loss": 0.2975,
      "step": 264
    },
    {
      "epoch": 0.620245757753072,
      "grad_norm": 0.33789050579071045,
      "learning_rate": 0.00018618266978922716,
      "loss": 0.2285,
      "step": 265
    },
    {
      "epoch": 0.6225863077823288,
      "grad_norm": 0.43190962076187134,
      "learning_rate": 0.00018688524590163933,
      "loss": 0.2453,
      "step": 266
    },
    {
      "epoch": 0.6249268578115857,
      "grad_norm": 0.6414533257484436,
      "learning_rate": 0.00018758782201405152,
      "loss": 0.2763,
      "step": 267
    },
    {
      "epoch": 0.6272674078408426,
      "grad_norm": 0.6795402765274048,
      "learning_rate": 0.00018829039812646368,
      "loss": 0.3464,
      "step": 268
    },
    {
      "epoch": 0.6296079578700995,
      "grad_norm": 0.6600128412246704,
      "learning_rate": 0.00018899297423887587,
      "loss": 0.306,
      "step": 269
    },
    {
      "epoch": 0.6319485078993563,
      "grad_norm": 0.37166088819503784,
      "learning_rate": 0.00018969555035128804,
      "loss": 0.2167,
      "step": 270
    },
    {
      "epoch": 0.6342890579286132,
      "grad_norm": 0.6442379355430603,
      "learning_rate": 0.00019039812646370023,
      "loss": 0.3074,
      "step": 271
    },
    {
      "epoch": 0.63662960795787,
      "grad_norm": 0.5272722244262695,
      "learning_rate": 0.0001911007025761124,
      "loss": 0.2735,
      "step": 272
    },
    {
      "epoch": 0.638970157987127,
      "grad_norm": 0.546147346496582,
      "learning_rate": 0.0001918032786885246,
      "loss": 0.2555,
      "step": 273
    },
    {
      "epoch": 0.6413107080163839,
      "grad_norm": 0.4218522310256958,
      "learning_rate": 0.00019250585480093675,
      "loss": 0.3639,
      "step": 274
    },
    {
      "epoch": 0.6436512580456407,
      "grad_norm": 0.2951143980026245,
      "learning_rate": 0.00019320843091334895,
      "loss": 0.3057,
      "step": 275
    },
    {
      "epoch": 0.6459918080748976,
      "grad_norm": 0.4417000114917755,
      "learning_rate": 0.0001939110070257611,
      "loss": 0.3779,
      "step": 276
    },
    {
      "epoch": 0.6483323581041545,
      "grad_norm": 0.41680708527565,
      "learning_rate": 0.0001946135831381733,
      "loss": 0.236,
      "step": 277
    },
    {
      "epoch": 0.6506729081334114,
      "grad_norm": 0.39889705181121826,
      "learning_rate": 0.00019531615925058544,
      "loss": 0.2287,
      "step": 278
    },
    {
      "epoch": 0.6530134581626682,
      "grad_norm": 0.575021505355835,
      "learning_rate": 0.00019601873536299763,
      "loss": 0.3047,
      "step": 279
    },
    {
      "epoch": 0.6553540081919251,
      "grad_norm": 0.39922842383384705,
      "learning_rate": 0.0001967213114754098,
      "loss": 0.2597,
      "step": 280
    },
    {
      "epoch": 0.657694558221182,
      "grad_norm": 0.7218676209449768,
      "learning_rate": 0.000197423887587822,
      "loss": 0.3434,
      "step": 281
    },
    {
      "epoch": 0.6600351082504389,
      "grad_norm": 0.44743847846984863,
      "learning_rate": 0.00019812646370023416,
      "loss": 0.335,
      "step": 282
    },
    {
      "epoch": 0.6623756582796957,
      "grad_norm": 0.39200618863105774,
      "learning_rate": 0.00019882903981264635,
      "loss": 0.2529,
      "step": 283
    },
    {
      "epoch": 0.6647162083089526,
      "grad_norm": 0.5832295417785645,
      "learning_rate": 0.0001995316159250585,
      "loss": 0.3386,
      "step": 284
    },
    {
      "epoch": 0.6670567583382094,
      "grad_norm": 0.5218175649642944,
      "learning_rate": 0.0002002341920374707,
      "loss": 0.2376,
      "step": 285
    },
    {
      "epoch": 0.6693973083674664,
      "grad_norm": 0.80935138463974,
      "learning_rate": 0.00020093676814988287,
      "loss": 0.2586,
      "step": 286
    },
    {
      "epoch": 0.6717378583967232,
      "grad_norm": 0.4719852805137634,
      "learning_rate": 0.00020163934426229506,
      "loss": 0.2433,
      "step": 287
    },
    {
      "epoch": 0.6740784084259801,
      "grad_norm": 0.4429175853729248,
      "learning_rate": 0.00020234192037470723,
      "loss": 0.195,
      "step": 288
    },
    {
      "epoch": 0.676418958455237,
      "grad_norm": 0.4681704342365265,
      "learning_rate": 0.00020304449648711942,
      "loss": 0.2713,
      "step": 289
    },
    {
      "epoch": 0.6787595084844938,
      "grad_norm": 0.5616859197616577,
      "learning_rate": 0.00020374707259953158,
      "loss": 0.2669,
      "step": 290
    },
    {
      "epoch": 0.6811000585137508,
      "grad_norm": 0.5205119252204895,
      "learning_rate": 0.00020444964871194378,
      "loss": 0.3156,
      "step": 291
    },
    {
      "epoch": 0.6834406085430076,
      "grad_norm": 0.44303277134895325,
      "learning_rate": 0.00020515222482435594,
      "loss": 0.2482,
      "step": 292
    },
    {
      "epoch": 0.6857811585722645,
      "grad_norm": 0.3461930453777313,
      "learning_rate": 0.00020585480093676813,
      "loss": 0.2927,
      "step": 293
    },
    {
      "epoch": 0.6881217086015213,
      "grad_norm": 0.41374853253364563,
      "learning_rate": 0.0002065573770491803,
      "loss": 0.2681,
      "step": 294
    },
    {
      "epoch": 0.6904622586307783,
      "grad_norm": 0.4854588508605957,
      "learning_rate": 0.0002072599531615925,
      "loss": 0.2469,
      "step": 295
    },
    {
      "epoch": 0.6928028086600351,
      "grad_norm": 0.3651812970638275,
      "learning_rate": 0.00020796252927400465,
      "loss": 0.3598,
      "step": 296
    },
    {
      "epoch": 0.695143358689292,
      "grad_norm": 0.36730217933654785,
      "learning_rate": 0.00020866510538641685,
      "loss": 0.3176,
      "step": 297
    },
    {
      "epoch": 0.6974839087185488,
      "grad_norm": 0.2987474203109741,
      "learning_rate": 0.000209367681498829,
      "loss": 0.2319,
      "step": 298
    },
    {
      "epoch": 0.6998244587478057,
      "grad_norm": 0.42185357213020325,
      "learning_rate": 0.0002100702576112412,
      "loss": 0.241,
      "step": 299
    },
    {
      "epoch": 0.7021650087770626,
      "grad_norm": 0.4072744846343994,
      "learning_rate": 0.00021077283372365337,
      "loss": 0.2894,
      "step": 300
    },
    {
      "epoch": 0.7045055588063195,
      "grad_norm": 0.4353775382041931,
      "learning_rate": 0.00021147540983606556,
      "loss": 0.3003,
      "step": 301
    },
    {
      "epoch": 0.7068461088355764,
      "grad_norm": 0.3208424150943756,
      "learning_rate": 0.00021217798594847773,
      "loss": 0.2601,
      "step": 302
    },
    {
      "epoch": 0.7091866588648332,
      "grad_norm": 0.39292702078819275,
      "learning_rate": 0.00021288056206088992,
      "loss": 0.2941,
      "step": 303
    },
    {
      "epoch": 0.7115272088940902,
      "grad_norm": 0.5000929832458496,
      "learning_rate": 0.00021358313817330208,
      "loss": 0.2717,
      "step": 304
    },
    {
      "epoch": 0.713867758923347,
      "grad_norm": 0.4399562478065491,
      "learning_rate": 0.00021428571428571427,
      "loss": 0.2832,
      "step": 305
    },
    {
      "epoch": 0.7162083089526039,
      "grad_norm": 0.405272513628006,
      "learning_rate": 0.00021498829039812644,
      "loss": 0.3214,
      "step": 306
    },
    {
      "epoch": 0.7185488589818607,
      "grad_norm": 0.6314390301704407,
      "learning_rate": 0.00021569086651053863,
      "loss": 0.3395,
      "step": 307
    },
    {
      "epoch": 0.7208894090111176,
      "grad_norm": 0.37299758195877075,
      "learning_rate": 0.0002163934426229508,
      "loss": 0.3099,
      "step": 308
    },
    {
      "epoch": 0.7232299590403745,
      "grad_norm": 0.41166532039642334,
      "learning_rate": 0.000217096018735363,
      "loss": 0.3691,
      "step": 309
    },
    {
      "epoch": 0.7255705090696314,
      "grad_norm": 0.4873071014881134,
      "learning_rate": 0.00021779859484777515,
      "loss": 0.2391,
      "step": 310
    },
    {
      "epoch": 0.7279110590988882,
      "grad_norm": 0.5248721837997437,
      "learning_rate": 0.00021850117096018735,
      "loss": 0.2374,
      "step": 311
    },
    {
      "epoch": 0.7302516091281451,
      "grad_norm": 0.33734607696533203,
      "learning_rate": 0.0002192037470725995,
      "loss": 0.2469,
      "step": 312
    },
    {
      "epoch": 0.732592159157402,
      "grad_norm": 0.43550512194633484,
      "learning_rate": 0.0002199063231850117,
      "loss": 0.2452,
      "step": 313
    },
    {
      "epoch": 0.7349327091866589,
      "grad_norm": 0.5444730520248413,
      "learning_rate": 0.00022060889929742387,
      "loss": 0.2977,
      "step": 314
    },
    {
      "epoch": 0.7372732592159157,
      "grad_norm": 0.4749915897846222,
      "learning_rate": 0.00022131147540983606,
      "loss": 0.3085,
      "step": 315
    },
    {
      "epoch": 0.7396138092451726,
      "grad_norm": 0.42601290345191956,
      "learning_rate": 0.00022201405152224822,
      "loss": 0.2428,
      "step": 316
    },
    {
      "epoch": 0.7419543592744295,
      "grad_norm": 0.536464512348175,
      "learning_rate": 0.00022271662763466042,
      "loss": 0.3569,
      "step": 317
    },
    {
      "epoch": 0.7442949093036864,
      "grad_norm": 0.47525110840797424,
      "learning_rate": 0.00022341920374707258,
      "loss": 0.261,
      "step": 318
    },
    {
      "epoch": 0.7466354593329433,
      "grad_norm": 0.4029865562915802,
      "learning_rate": 0.00022412177985948477,
      "loss": 0.2285,
      "step": 319
    },
    {
      "epoch": 0.7489760093622001,
      "grad_norm": 0.4423048496246338,
      "learning_rate": 0.00022482435597189694,
      "loss": 0.2591,
      "step": 320
    },
    {
      "epoch": 0.751316559391457,
      "grad_norm": 0.3998928666114807,
      "learning_rate": 0.0002255269320843091,
      "loss": 0.2505,
      "step": 321
    },
    {
      "epoch": 0.7536571094207138,
      "grad_norm": 0.4694056808948517,
      "learning_rate": 0.00022622950819672127,
      "loss": 0.2809,
      "step": 322
    },
    {
      "epoch": 0.7559976594499708,
      "grad_norm": 0.47633808851242065,
      "learning_rate": 0.00022693208430913346,
      "loss": 0.3062,
      "step": 323
    },
    {
      "epoch": 0.7583382094792276,
      "grad_norm": 0.42647451162338257,
      "learning_rate": 0.00022763466042154563,
      "loss": 0.237,
      "step": 324
    },
    {
      "epoch": 0.7606787595084845,
      "grad_norm": 0.35358795523643494,
      "learning_rate": 0.00022833723653395782,
      "loss": 0.2135,
      "step": 325
    },
    {
      "epoch": 0.7630193095377413,
      "grad_norm": 0.44420087337493896,
      "learning_rate": 0.00022903981264636998,
      "loss": 0.2555,
      "step": 326
    },
    {
      "epoch": 0.7653598595669983,
      "grad_norm": 0.39493831992149353,
      "learning_rate": 0.00022974238875878218,
      "loss": 0.2786,
      "step": 327
    },
    {
      "epoch": 0.7677004095962551,
      "grad_norm": 0.46084216237068176,
      "learning_rate": 0.00023044496487119434,
      "loss": 0.2744,
      "step": 328
    },
    {
      "epoch": 0.770040959625512,
      "grad_norm": 0.427609920501709,
      "learning_rate": 0.00023114754098360653,
      "loss": 0.2948,
      "step": 329
    },
    {
      "epoch": 0.7723815096547688,
      "grad_norm": 0.3578585684299469,
      "learning_rate": 0.0002318501170960187,
      "loss": 0.2897,
      "step": 330
    },
    {
      "epoch": 0.7747220596840257,
      "grad_norm": 0.3936671316623688,
      "learning_rate": 0.0002325526932084309,
      "loss": 0.2116,
      "step": 331
    },
    {
      "epoch": 0.7770626097132827,
      "grad_norm": 0.44199153780937195,
      "learning_rate": 0.00023325526932084305,
      "loss": 0.2665,
      "step": 332
    },
    {
      "epoch": 0.7794031597425395,
      "grad_norm": 0.37447234988212585,
      "learning_rate": 0.00023395784543325525,
      "loss": 0.2049,
      "step": 333
    },
    {
      "epoch": 0.7817437097717964,
      "grad_norm": 0.4720609486103058,
      "learning_rate": 0.0002346604215456674,
      "loss": 0.3054,
      "step": 334
    },
    {
      "epoch": 0.7840842598010532,
      "grad_norm": 0.35800817608833313,
      "learning_rate": 0.0002353629976580796,
      "loss": 0.2478,
      "step": 335
    },
    {
      "epoch": 0.7864248098303102,
      "grad_norm": 0.38648104667663574,
      "learning_rate": 0.00023606557377049177,
      "loss": 0.2835,
      "step": 336
    },
    {
      "epoch": 0.788765359859567,
      "grad_norm": 0.3742077052593231,
      "learning_rate": 0.00023676814988290396,
      "loss": 0.2766,
      "step": 337
    },
    {
      "epoch": 0.7911059098888239,
      "grad_norm": 0.3673323392868042,
      "learning_rate": 0.00023747072599531613,
      "loss": 0.2732,
      "step": 338
    },
    {
      "epoch": 0.7934464599180807,
      "grad_norm": 0.3965262174606323,
      "learning_rate": 0.00023817330210772832,
      "loss": 0.2773,
      "step": 339
    },
    {
      "epoch": 0.7957870099473376,
      "grad_norm": 0.29859086871147156,
      "learning_rate": 0.00023887587822014048,
      "loss": 0.1971,
      "step": 340
    },
    {
      "epoch": 0.7981275599765945,
      "grad_norm": 0.3934193551540375,
      "learning_rate": 0.00023957845433255267,
      "loss": 0.2453,
      "step": 341
    },
    {
      "epoch": 0.8004681100058514,
      "grad_norm": 0.3170946538448334,
      "learning_rate": 0.00024028103044496484,
      "loss": 0.1919,
      "step": 342
    },
    {
      "epoch": 0.8028086600351082,
      "grad_norm": 0.34541070461273193,
      "learning_rate": 0.00024098360655737703,
      "loss": 0.2976,
      "step": 343
    },
    {
      "epoch": 0.8051492100643651,
      "grad_norm": 0.38161662220954895,
      "learning_rate": 0.0002416861826697892,
      "loss": 0.2649,
      "step": 344
    },
    {
      "epoch": 0.807489760093622,
      "grad_norm": 0.45019060373306274,
      "learning_rate": 0.0002423887587822014,
      "loss": 0.2719,
      "step": 345
    },
    {
      "epoch": 0.8098303101228789,
      "grad_norm": 0.40406933426856995,
      "learning_rate": 0.00024309133489461355,
      "loss": 0.265,
      "step": 346
    },
    {
      "epoch": 0.8121708601521358,
      "grad_norm": 0.38106852769851685,
      "learning_rate": 0.00024379391100702575,
      "loss": 0.3052,
      "step": 347
    },
    {
      "epoch": 0.8145114101813926,
      "grad_norm": 0.36872902512550354,
      "learning_rate": 0.0002444964871194379,
      "loss": 0.3113,
      "step": 348
    },
    {
      "epoch": 0.8168519602106495,
      "grad_norm": 0.3373385965824127,
      "learning_rate": 0.0002451990632318501,
      "loss": 0.2419,
      "step": 349
    },
    {
      "epoch": 0.8191925102399064,
      "grad_norm": 0.4044322371482849,
      "learning_rate": 0.0002459016393442623,
      "loss": 0.3056,
      "step": 350
    },
    {
      "epoch": 0.8215330602691633,
      "grad_norm": 0.3286450505256653,
      "learning_rate": 0.00024660421545667446,
      "loss": 0.2229,
      "step": 351
    },
    {
      "epoch": 0.8238736102984201,
      "grad_norm": 0.420375794172287,
      "learning_rate": 0.0002473067915690866,
      "loss": 0.215,
      "step": 352
    },
    {
      "epoch": 0.826214160327677,
      "grad_norm": 0.5897389054298401,
      "learning_rate": 0.0002480093676814988,
      "loss": 0.3105,
      "step": 353
    },
    {
      "epoch": 0.8285547103569338,
      "grad_norm": 0.3642483353614807,
      "learning_rate": 0.000248711943793911,
      "loss": 0.2744,
      "step": 354
    },
    {
      "epoch": 0.8308952603861908,
      "grad_norm": 0.4525600075721741,
      "learning_rate": 0.0002494145199063232,
      "loss": 0.284,
      "step": 355
    },
    {
      "epoch": 0.8332358104154476,
      "grad_norm": 0.3039945065975189,
      "learning_rate": 0.00025011709601873534,
      "loss": 0.2468,
      "step": 356
    },
    {
      "epoch": 0.8355763604447045,
      "grad_norm": 0.4910363256931305,
      "learning_rate": 0.00025081967213114756,
      "loss": 0.2774,
      "step": 357
    },
    {
      "epoch": 0.8379169104739613,
      "grad_norm": 0.37527766823768616,
      "learning_rate": 0.0002515222482435597,
      "loss": 0.3014,
      "step": 358
    },
    {
      "epoch": 0.8402574605032183,
      "grad_norm": 0.3917308747768402,
      "learning_rate": 0.0002522248243559719,
      "loss": 0.194,
      "step": 359
    },
    {
      "epoch": 0.8425980105324752,
      "grad_norm": 0.32863131165504456,
      "learning_rate": 0.00025292740046838405,
      "loss": 0.2115,
      "step": 360
    },
    {
      "epoch": 0.844938560561732,
      "grad_norm": 0.492628812789917,
      "learning_rate": 0.00025362997658079627,
      "loss": 0.2997,
      "step": 361
    },
    {
      "epoch": 0.8472791105909889,
      "grad_norm": 0.35402220487594604,
      "learning_rate": 0.00025433255269320844,
      "loss": 0.2254,
      "step": 362
    },
    {
      "epoch": 0.8496196606202457,
      "grad_norm": 0.3125516474246979,
      "learning_rate": 0.00025503512880562055,
      "loss": 0.2035,
      "step": 363
    },
    {
      "epoch": 0.8519602106495027,
      "grad_norm": 0.43826255202293396,
      "learning_rate": 0.00025573770491803277,
      "loss": 0.2246,
      "step": 364
    },
    {
      "epoch": 0.8543007606787595,
      "grad_norm": 0.3962307870388031,
      "learning_rate": 0.00025644028103044493,
      "loss": 0.2513,
      "step": 365
    },
    {
      "epoch": 0.8566413107080164,
      "grad_norm": 0.34235239028930664,
      "learning_rate": 0.0002571428571428571,
      "loss": 0.2049,
      "step": 366
    },
    {
      "epoch": 0.8589818607372732,
      "grad_norm": 0.4512186348438263,
      "learning_rate": 0.00025784543325526926,
      "loss": 0.3016,
      "step": 367
    },
    {
      "epoch": 0.8613224107665302,
      "grad_norm": 0.39583444595336914,
      "learning_rate": 0.0002585480093676815,
      "loss": 0.2594,
      "step": 368
    },
    {
      "epoch": 0.863662960795787,
      "grad_norm": 0.3655487596988678,
      "learning_rate": 0.00025925058548009365,
      "loss": 0.315,
      "step": 369
    },
    {
      "epoch": 0.8660035108250439,
      "grad_norm": 0.41854533553123474,
      "learning_rate": 0.0002599531615925058,
      "loss": 0.177,
      "step": 370
    },
    {
      "epoch": 0.8683440608543007,
      "grad_norm": 0.39044007658958435,
      "learning_rate": 0.000260655737704918,
      "loss": 0.2559,
      "step": 371
    },
    {
      "epoch": 0.8706846108835576,
      "grad_norm": 0.3711216449737549,
      "learning_rate": 0.0002613583138173302,
      "loss": 0.188,
      "step": 372
    },
    {
      "epoch": 0.8730251609128145,
      "grad_norm": 0.3371989130973816,
      "learning_rate": 0.00026206088992974236,
      "loss": 0.2296,
      "step": 373
    },
    {
      "epoch": 0.8753657109420714,
      "grad_norm": 0.4017578065395355,
      "learning_rate": 0.0002627634660421545,
      "loss": 0.2063,
      "step": 374
    },
    {
      "epoch": 0.8777062609713283,
      "grad_norm": 0.4437357485294342,
      "learning_rate": 0.0002634660421545667,
      "loss": 0.3124,
      "step": 375
    },
    {
      "epoch": 0.8800468110005851,
      "grad_norm": 0.4194537103176117,
      "learning_rate": 0.0002641686182669789,
      "loss": 0.2321,
      "step": 376
    },
    {
      "epoch": 0.882387361029842,
      "grad_norm": 0.2880573570728302,
      "learning_rate": 0.0002648711943793911,
      "loss": 0.2137,
      "step": 377
    },
    {
      "epoch": 0.8847279110590989,
      "grad_norm": 0.3752621114253998,
      "learning_rate": 0.00026557377049180324,
      "loss": 0.2603,
      "step": 378
    },
    {
      "epoch": 0.8870684610883558,
      "grad_norm": 0.48997342586517334,
      "learning_rate": 0.0002662763466042154,
      "loss": 0.273,
      "step": 379
    },
    {
      "epoch": 0.8894090111176126,
      "grad_norm": 0.41157767176628113,
      "learning_rate": 0.0002669789227166276,
      "loss": 0.2178,
      "step": 380
    },
    {
      "epoch": 0.8917495611468695,
      "grad_norm": 0.3840894103050232,
      "learning_rate": 0.0002676814988290398,
      "loss": 0.3491,
      "step": 381
    },
    {
      "epoch": 0.8940901111761264,
      "grad_norm": 0.37293869256973267,
      "learning_rate": 0.00026838407494145195,
      "loss": 0.285,
      "step": 382
    },
    {
      "epoch": 0.8964306612053833,
      "grad_norm": 0.3183538615703583,
      "learning_rate": 0.0002690866510538641,
      "loss": 0.2579,
      "step": 383
    },
    {
      "epoch": 0.8987712112346401,
      "grad_norm": 0.3398457169532776,
      "learning_rate": 0.00026978922716627634,
      "loss": 0.26,
      "step": 384
    },
    {
      "epoch": 0.901111761263897,
      "grad_norm": 0.43311041593551636,
      "learning_rate": 0.0002704918032786885,
      "loss": 0.263,
      "step": 385
    },
    {
      "epoch": 0.9034523112931538,
      "grad_norm": 0.3216579258441925,
      "learning_rate": 0.00027119437939110067,
      "loss": 0.2701,
      "step": 386
    },
    {
      "epoch": 0.9057928613224108,
      "grad_norm": 0.5514270067214966,
      "learning_rate": 0.00027189695550351283,
      "loss": 0.3382,
      "step": 387
    },
    {
      "epoch": 0.9081334113516677,
      "grad_norm": 0.33583810925483704,
      "learning_rate": 0.00027259953161592505,
      "loss": 0.2377,
      "step": 388
    },
    {
      "epoch": 0.9104739613809245,
      "grad_norm": 0.4220353066921234,
      "learning_rate": 0.0002733021077283372,
      "loss": 0.307,
      "step": 389
    },
    {
      "epoch": 0.9128145114101814,
      "grad_norm": 0.46817120909690857,
      "learning_rate": 0.0002740046838407494,
      "loss": 0.3189,
      "step": 390
    },
    {
      "epoch": 0.9151550614394383,
      "grad_norm": 0.3621525168418884,
      "learning_rate": 0.00027470725995316155,
      "loss": 0.2904,
      "step": 391
    },
    {
      "epoch": 0.9174956114686952,
      "grad_norm": 0.37541812658309937,
      "learning_rate": 0.00027540983606557377,
      "loss": 0.3021,
      "step": 392
    },
    {
      "epoch": 0.919836161497952,
      "grad_norm": 0.46484610438346863,
      "learning_rate": 0.00027611241217798593,
      "loss": 0.3632,
      "step": 393
    },
    {
      "epoch": 0.9221767115272089,
      "grad_norm": 0.3325311243534088,
      "learning_rate": 0.0002768149882903981,
      "loss": 0.2263,
      "step": 394
    },
    {
      "epoch": 0.9245172615564657,
      "grad_norm": 0.38745784759521484,
      "learning_rate": 0.0002775175644028103,
      "loss": 0.2053,
      "step": 395
    },
    {
      "epoch": 0.9268578115857227,
      "grad_norm": 0.3102709650993347,
      "learning_rate": 0.0002782201405152225,
      "loss": 0.2606,
      "step": 396
    },
    {
      "epoch": 0.9291983616149795,
      "grad_norm": 0.3954944908618927,
      "learning_rate": 0.00027892271662763465,
      "loss": 0.2606,
      "step": 397
    },
    {
      "epoch": 0.9315389116442364,
      "grad_norm": 0.40081116557121277,
      "learning_rate": 0.0002796252927400468,
      "loss": 0.2504,
      "step": 398
    },
    {
      "epoch": 0.9338794616734932,
      "grad_norm": 0.46179652214050293,
      "learning_rate": 0.00028032786885245903,
      "loss": 0.3375,
      "step": 399
    },
    {
      "epoch": 0.9362200117027502,
      "grad_norm": 0.36163631081581116,
      "learning_rate": 0.0002810304449648712,
      "loss": 0.2165,
      "step": 400
    },
    {
      "epoch": 0.938560561732007,
      "grad_norm": 0.34917357563972473,
      "learning_rate": 0.00028173302107728336,
      "loss": 0.2213,
      "step": 401
    },
    {
      "epoch": 0.9409011117612639,
      "grad_norm": 0.40621939301490784,
      "learning_rate": 0.0002824355971896955,
      "loss": 0.3247,
      "step": 402
    },
    {
      "epoch": 0.9432416617905208,
      "grad_norm": 0.388895183801651,
      "learning_rate": 0.00028313817330210774,
      "loss": 0.2415,
      "step": 403
    },
    {
      "epoch": 0.9455822118197776,
      "grad_norm": 0.38514840602874756,
      "learning_rate": 0.0002838407494145199,
      "loss": 0.272,
      "step": 404
    },
    {
      "epoch": 0.9479227618490346,
      "grad_norm": 0.3957756757736206,
      "learning_rate": 0.0002845433255269321,
      "loss": 0.2247,
      "step": 405
    },
    {
      "epoch": 0.9502633118782914,
      "grad_norm": 0.42563581466674805,
      "learning_rate": 0.00028524590163934424,
      "loss": 0.2892,
      "step": 406
    },
    {
      "epoch": 0.9526038619075483,
      "grad_norm": 0.30120307207107544,
      "learning_rate": 0.0002859484777517564,
      "loss": 0.1504,
      "step": 407
    },
    {
      "epoch": 0.9549444119368051,
      "grad_norm": 0.3571068346500397,
      "learning_rate": 0.00028665105386416857,
      "loss": 0.2456,
      "step": 408
    },
    {
      "epoch": 0.957284961966062,
      "grad_norm": 0.37554773688316345,
      "learning_rate": 0.00028735362997658073,
      "loss": 0.2285,
      "step": 409
    },
    {
      "epoch": 0.9596255119953189,
      "grad_norm": 0.424096941947937,
      "learning_rate": 0.00028805620608899295,
      "loss": 0.2714,
      "step": 410
    },
    {
      "epoch": 0.9619660620245758,
      "grad_norm": 0.4512574374675751,
      "learning_rate": 0.0002887587822014051,
      "loss": 0.2798,
      "step": 411
    },
    {
      "epoch": 0.9643066120538326,
      "grad_norm": 0.355158269405365,
      "learning_rate": 0.0002894613583138173,
      "loss": 0.2042,
      "step": 412
    },
    {
      "epoch": 0.9666471620830895,
      "grad_norm": 0.40620964765548706,
      "learning_rate": 0.00029016393442622945,
      "loss": 0.2211,
      "step": 413
    },
    {
      "epoch": 0.9689877121123464,
      "grad_norm": 0.5447445511817932,
      "learning_rate": 0.00029086651053864167,
      "loss": 0.2817,
      "step": 414
    },
    {
      "epoch": 0.9713282621416033,
      "grad_norm": 0.3055792450904846,
      "learning_rate": 0.00029156908665105383,
      "loss": 0.2158,
      "step": 415
    },
    {
      "epoch": 0.9736688121708601,
      "grad_norm": 0.4253140389919281,
      "learning_rate": 0.000292271662763466,
      "loss": 0.2933,
      "step": 416
    },
    {
      "epoch": 0.976009362200117,
      "grad_norm": 0.4404054582118988,
      "learning_rate": 0.00029297423887587816,
      "loss": 0.2439,
      "step": 417
    },
    {
      "epoch": 0.978349912229374,
      "grad_norm": 0.2973087430000305,
      "learning_rate": 0.0002936768149882904,
      "loss": 0.1706,
      "step": 418
    },
    {
      "epoch": 0.9806904622586308,
      "grad_norm": 0.32032081484794617,
      "learning_rate": 0.00029437939110070255,
      "loss": 0.2112,
      "step": 419
    },
    {
      "epoch": 0.9830310122878877,
      "grad_norm": 0.34091997146606445,
      "learning_rate": 0.0002950819672131147,
      "loss": 0.2544,
      "step": 420
    },
    {
      "epoch": 0.9853715623171445,
      "grad_norm": 0.359714537858963,
      "learning_rate": 0.0002957845433255269,
      "loss": 0.1985,
      "step": 421
    },
    {
      "epoch": 0.9877121123464014,
      "grad_norm": 0.36689451336860657,
      "learning_rate": 0.0002964871194379391,
      "loss": 0.2554,
      "step": 422
    },
    {
      "epoch": 0.9900526623756583,
      "grad_norm": 0.39814266562461853,
      "learning_rate": 0.00029718969555035126,
      "loss": 0.2583,
      "step": 423
    },
    {
      "epoch": 0.9923932124049152,
      "grad_norm": 0.49227386713027954,
      "learning_rate": 0.0002978922716627634,
      "loss": 0.3029,
      "step": 424
    },
    {
      "epoch": 0.994733762434172,
      "grad_norm": 0.34719353914260864,
      "learning_rate": 0.0002985948477751756,
      "loss": 0.2161,
      "step": 425
    },
    {
      "epoch": 0.9970743124634289,
      "grad_norm": 0.31449782848358154,
      "learning_rate": 0.0002992974238875878,
      "loss": 0.199,
      "step": 426
    },
    {
      "epoch": 0.9994148624926857,
      "grad_norm": 0.3642027974128723,
      "learning_rate": 0.0003,
      "loss": 0.2778,
      "step": 427
    },
    {
      "epoch": 0.9994148624926857,
      "eval_loss": 0.3240148425102234,
      "eval_runtime": 128.3802,
      "eval_samples_per_second": 4.3,
      "eval_steps_per_second": 0.537,
      "step": 427
    },
    {
      "epoch": 1.0017554125219426,
      "grad_norm": 0.35868000984191895,
      "learning_rate": 0.0002999219359875097,
      "loss": 0.243,
      "step": 428
    },
    {
      "epoch": 1.0040959625511996,
      "grad_norm": 0.37849172949790955,
      "learning_rate": 0.0002998438719750195,
      "loss": 0.1999,
      "step": 429
    },
    {
      "epoch": 1.0064365125804564,
      "grad_norm": 0.3515348434448242,
      "learning_rate": 0.00029976580796252925,
      "loss": 0.2259,
      "step": 430
    },
    {
      "epoch": 1.0087770626097132,
      "grad_norm": 0.31324321031570435,
      "learning_rate": 0.000299687743950039,
      "loss": 0.198,
      "step": 431
    },
    {
      "epoch": 1.0111176126389703,
      "grad_norm": 0.5365183353424072,
      "learning_rate": 0.00029960967993754875,
      "loss": 0.3121,
      "step": 432
    },
    {
      "epoch": 1.013458162668227,
      "grad_norm": 0.34239834547042847,
      "learning_rate": 0.00029953161592505853,
      "loss": 0.2206,
      "step": 433
    },
    {
      "epoch": 1.015798712697484,
      "grad_norm": 0.42858439683914185,
      "learning_rate": 0.00029945355191256825,
      "loss": 0.2422,
      "step": 434
    },
    {
      "epoch": 1.0181392627267407,
      "grad_norm": 0.36915886402130127,
      "learning_rate": 0.00029937548790007803,
      "loss": 0.2045,
      "step": 435
    },
    {
      "epoch": 1.0204798127559978,
      "grad_norm": 0.43000346422195435,
      "learning_rate": 0.0002992974238875878,
      "loss": 0.2846,
      "step": 436
    },
    {
      "epoch": 1.0228203627852546,
      "grad_norm": 0.2894876003265381,
      "learning_rate": 0.00029921935987509753,
      "loss": 0.2029,
      "step": 437
    },
    {
      "epoch": 1.0251609128145114,
      "grad_norm": 0.3497278392314911,
      "learning_rate": 0.0002991412958626073,
      "loss": 0.2134,
      "step": 438
    },
    {
      "epoch": 1.0275014628437682,
      "grad_norm": 0.4114449620246887,
      "learning_rate": 0.0002990632318501171,
      "loss": 0.2892,
      "step": 439
    },
    {
      "epoch": 1.0298420128730252,
      "grad_norm": 0.36412760615348816,
      "learning_rate": 0.0002989851678376268,
      "loss": 0.2636,
      "step": 440
    },
    {
      "epoch": 1.032182562902282,
      "grad_norm": 0.3419552445411682,
      "learning_rate": 0.0002989071038251366,
      "loss": 0.2228,
      "step": 441
    },
    {
      "epoch": 1.0345231129315389,
      "grad_norm": 0.4519004821777344,
      "learning_rate": 0.00029882903981264637,
      "loss": 0.2279,
      "step": 442
    },
    {
      "epoch": 1.0368636629607957,
      "grad_norm": 0.4055764675140381,
      "learning_rate": 0.0002987509758001561,
      "loss": 0.2682,
      "step": 443
    },
    {
      "epoch": 1.0392042129900527,
      "grad_norm": 0.3292044401168823,
      "learning_rate": 0.00029867291178766587,
      "loss": 0.2496,
      "step": 444
    },
    {
      "epoch": 1.0415447630193095,
      "grad_norm": 0.3747800886631012,
      "learning_rate": 0.0002985948477751756,
      "loss": 0.2479,
      "step": 445
    },
    {
      "epoch": 1.0438853130485664,
      "grad_norm": 0.35649219155311584,
      "learning_rate": 0.00029851678376268537,
      "loss": 0.2019,
      "step": 446
    },
    {
      "epoch": 1.0462258630778234,
      "grad_norm": 0.37540408968925476,
      "learning_rate": 0.00029843871975019514,
      "loss": 0.2234,
      "step": 447
    },
    {
      "epoch": 1.0485664131070802,
      "grad_norm": 0.3183968961238861,
      "learning_rate": 0.00029836065573770487,
      "loss": 0.1329,
      "step": 448
    },
    {
      "epoch": 1.050906963136337,
      "grad_norm": 0.5112112164497375,
      "learning_rate": 0.00029828259172521465,
      "loss": 0.3007,
      "step": 449
    },
    {
      "epoch": 1.0532475131655938,
      "grad_norm": 0.31417039036750793,
      "learning_rate": 0.0002982045277127244,
      "loss": 0.1834,
      "step": 450
    },
    {
      "epoch": 1.0555880631948509,
      "grad_norm": 0.6221544742584229,
      "learning_rate": 0.00029812646370023415,
      "loss": 0.3682,
      "step": 451
    },
    {
      "epoch": 1.0579286132241077,
      "grad_norm": 0.5530638694763184,
      "learning_rate": 0.0002980483996877439,
      "loss": 0.2406,
      "step": 452
    },
    {
      "epoch": 1.0602691632533645,
      "grad_norm": 0.33839765191078186,
      "learning_rate": 0.0002979703356752537,
      "loss": 0.2392,
      "step": 453
    },
    {
      "epoch": 1.0626097132826213,
      "grad_norm": 0.497271865606308,
      "learning_rate": 0.0002978922716627634,
      "loss": 0.3118,
      "step": 454
    },
    {
      "epoch": 1.0649502633118784,
      "grad_norm": 0.4276483654975891,
      "learning_rate": 0.0002978142076502732,
      "loss": 0.1631,
      "step": 455
    },
    {
      "epoch": 1.0672908133411352,
      "grad_norm": 0.3787417411804199,
      "learning_rate": 0.000297736143637783,
      "loss": 0.2024,
      "step": 456
    },
    {
      "epoch": 1.069631363370392,
      "grad_norm": 0.3081357777118683,
      "learning_rate": 0.0002976580796252927,
      "loss": 0.2445,
      "step": 457
    },
    {
      "epoch": 1.0719719133996488,
      "grad_norm": 0.30737727880477905,
      "learning_rate": 0.0002975800156128025,
      "loss": 0.1795,
      "step": 458
    },
    {
      "epoch": 1.0743124634289059,
      "grad_norm": 0.3524869978427887,
      "learning_rate": 0.00029750195160031226,
      "loss": 0.2556,
      "step": 459
    },
    {
      "epoch": 1.0766530134581627,
      "grad_norm": 0.43919864296913147,
      "learning_rate": 0.000297423887587822,
      "loss": 0.1757,
      "step": 460
    },
    {
      "epoch": 1.0789935634874195,
      "grad_norm": 0.30108562111854553,
      "learning_rate": 0.00029734582357533176,
      "loss": 0.2178,
      "step": 461
    },
    {
      "epoch": 1.0813341135166765,
      "grad_norm": 0.3473600149154663,
      "learning_rate": 0.00029726775956284154,
      "loss": 0.2182,
      "step": 462
    },
    {
      "epoch": 1.0836746635459333,
      "grad_norm": 0.5704877972602844,
      "learning_rate": 0.00029718969555035126,
      "loss": 0.2472,
      "step": 463
    },
    {
      "epoch": 1.0860152135751902,
      "grad_norm": 0.3572012186050415,
      "learning_rate": 0.000297111631537861,
      "loss": 0.205,
      "step": 464
    },
    {
      "epoch": 1.088355763604447,
      "grad_norm": 0.3214288651943207,
      "learning_rate": 0.00029703356752537076,
      "loss": 0.2167,
      "step": 465
    },
    {
      "epoch": 1.090696313633704,
      "grad_norm": 0.3139094412326813,
      "learning_rate": 0.00029695550351288054,
      "loss": 0.2085,
      "step": 466
    },
    {
      "epoch": 1.0930368636629608,
      "grad_norm": 0.31500646471977234,
      "learning_rate": 0.00029687743950039026,
      "loss": 0.247,
      "step": 467
    },
    {
      "epoch": 1.0953774136922176,
      "grad_norm": 0.34293505549430847,
      "learning_rate": 0.00029679937548790004,
      "loss": 0.1829,
      "step": 468
    },
    {
      "epoch": 1.0977179637214745,
      "grad_norm": 0.6190766096115112,
      "learning_rate": 0.0002967213114754098,
      "loss": 0.3005,
      "step": 469
    },
    {
      "epoch": 1.1000585137507315,
      "grad_norm": 0.39027920365333557,
      "learning_rate": 0.00029664324746291954,
      "loss": 0.3184,
      "step": 470
    },
    {
      "epoch": 1.1023990637799883,
      "grad_norm": 0.39208781719207764,
      "learning_rate": 0.0002965651834504293,
      "loss": 0.2528,
      "step": 471
    },
    {
      "epoch": 1.1047396138092451,
      "grad_norm": 0.3418152630329132,
      "learning_rate": 0.0002964871194379391,
      "loss": 0.242,
      "step": 472
    },
    {
      "epoch": 1.1070801638385022,
      "grad_norm": 0.37912988662719727,
      "learning_rate": 0.0002964090554254488,
      "loss": 0.2214,
      "step": 473
    },
    {
      "epoch": 1.109420713867759,
      "grad_norm": 0.32849711179733276,
      "learning_rate": 0.0002963309914129586,
      "loss": 0.2187,
      "step": 474
    },
    {
      "epoch": 1.1117612638970158,
      "grad_norm": 0.32733243703842163,
      "learning_rate": 0.0002962529274004684,
      "loss": 0.2668,
      "step": 475
    },
    {
      "epoch": 1.1141018139262726,
      "grad_norm": 0.3390064835548401,
      "learning_rate": 0.0002961748633879781,
      "loss": 0.204,
      "step": 476
    },
    {
      "epoch": 1.1164423639555296,
      "grad_norm": 0.3791615962982178,
      "learning_rate": 0.0002960967993754879,
      "loss": 0.2744,
      "step": 477
    },
    {
      "epoch": 1.1187829139847865,
      "grad_norm": 0.3992796838283539,
      "learning_rate": 0.00029601873536299765,
      "loss": 0.2368,
      "step": 478
    },
    {
      "epoch": 1.1211234640140433,
      "grad_norm": 0.2987007200717926,
      "learning_rate": 0.0002959406713505074,
      "loss": 0.1695,
      "step": 479
    },
    {
      "epoch": 1.1234640140433,
      "grad_norm": 0.33412817120552063,
      "learning_rate": 0.00029586260733801715,
      "loss": 0.1507,
      "step": 480
    },
    {
      "epoch": 1.1258045640725571,
      "grad_norm": 0.49685779213905334,
      "learning_rate": 0.0002957845433255269,
      "loss": 0.3479,
      "step": 481
    },
    {
      "epoch": 1.128145114101814,
      "grad_norm": 0.42629334330558777,
      "learning_rate": 0.00029570647931303665,
      "loss": 0.224,
      "step": 482
    },
    {
      "epoch": 1.1304856641310708,
      "grad_norm": 0.45074760913848877,
      "learning_rate": 0.00029562841530054643,
      "loss": 0.2503,
      "step": 483
    },
    {
      "epoch": 1.1328262141603276,
      "grad_norm": 0.3584100604057312,
      "learning_rate": 0.00029555035128805615,
      "loss": 0.2381,
      "step": 484
    },
    {
      "epoch": 1.1351667641895846,
      "grad_norm": 0.2969689667224884,
      "learning_rate": 0.00029547228727556593,
      "loss": 0.1702,
      "step": 485
    },
    {
      "epoch": 1.1375073142188414,
      "grad_norm": 0.36709466576576233,
      "learning_rate": 0.0002953942232630757,
      "loss": 0.2605,
      "step": 486
    },
    {
      "epoch": 1.1398478642480983,
      "grad_norm": 0.32862457633018494,
      "learning_rate": 0.00029531615925058543,
      "loss": 0.2058,
      "step": 487
    },
    {
      "epoch": 1.142188414277355,
      "grad_norm": 0.4035586416721344,
      "learning_rate": 0.0002952380952380952,
      "loss": 0.2943,
      "step": 488
    },
    {
      "epoch": 1.144528964306612,
      "grad_norm": 0.33856385946273804,
      "learning_rate": 0.000295160031225605,
      "loss": 0.1826,
      "step": 489
    },
    {
      "epoch": 1.146869514335869,
      "grad_norm": 0.3134371042251587,
      "learning_rate": 0.0002950819672131147,
      "loss": 0.2149,
      "step": 490
    },
    {
      "epoch": 1.1492100643651257,
      "grad_norm": 0.2872786521911621,
      "learning_rate": 0.0002950039032006245,
      "loss": 0.2176,
      "step": 491
    },
    {
      "epoch": 1.1515506143943828,
      "grad_norm": 0.32190820574760437,
      "learning_rate": 0.00029492583918813427,
      "loss": 0.1828,
      "step": 492
    },
    {
      "epoch": 1.1538911644236396,
      "grad_norm": 0.5025395750999451,
      "learning_rate": 0.000294847775175644,
      "loss": 0.3047,
      "step": 493
    },
    {
      "epoch": 1.1562317144528964,
      "grad_norm": 0.27709275484085083,
      "learning_rate": 0.00029476971116315377,
      "loss": 0.1559,
      "step": 494
    },
    {
      "epoch": 1.1585722644821532,
      "grad_norm": 0.30188319087028503,
      "learning_rate": 0.00029469164715066354,
      "loss": 0.19,
      "step": 495
    },
    {
      "epoch": 1.1609128145114103,
      "grad_norm": 0.3307143449783325,
      "learning_rate": 0.00029461358313817327,
      "loss": 0.1934,
      "step": 496
    },
    {
      "epoch": 1.163253364540667,
      "grad_norm": 0.32950085401535034,
      "learning_rate": 0.00029453551912568304,
      "loss": 0.2346,
      "step": 497
    },
    {
      "epoch": 1.165593914569924,
      "grad_norm": 0.4335446357727051,
      "learning_rate": 0.0002944574551131928,
      "loss": 0.2521,
      "step": 498
    },
    {
      "epoch": 1.1679344645991807,
      "grad_norm": 0.28891274333000183,
      "learning_rate": 0.00029437939110070255,
      "loss": 0.1722,
      "step": 499
    },
    {
      "epoch": 1.1702750146284377,
      "grad_norm": 0.39655497670173645,
      "learning_rate": 0.0002943013270882123,
      "loss": 0.2568,
      "step": 500
    },
    {
      "epoch": 1.1726155646576946,
      "grad_norm": 0.3329543471336365,
      "learning_rate": 0.00029422326307572205,
      "loss": 0.2148,
      "step": 501
    },
    {
      "epoch": 1.1749561146869514,
      "grad_norm": 0.40363991260528564,
      "learning_rate": 0.0002941451990632318,
      "loss": 0.2693,
      "step": 502
    },
    {
      "epoch": 1.1772966647162084,
      "grad_norm": 0.41776034235954285,
      "learning_rate": 0.0002940671350507416,
      "loss": 0.2785,
      "step": 503
    },
    {
      "epoch": 1.1796372147454652,
      "grad_norm": 0.39950940012931824,
      "learning_rate": 0.0002939890710382513,
      "loss": 0.2543,
      "step": 504
    },
    {
      "epoch": 1.181977764774722,
      "grad_norm": 0.3628886044025421,
      "learning_rate": 0.0002939110070257611,
      "loss": 0.2567,
      "step": 505
    },
    {
      "epoch": 1.1843183148039789,
      "grad_norm": 0.3283146023750305,
      "learning_rate": 0.0002938329430132709,
      "loss": 0.2081,
      "step": 506
    },
    {
      "epoch": 1.186658864833236,
      "grad_norm": 0.3302691876888275,
      "learning_rate": 0.0002937548790007806,
      "loss": 0.2419,
      "step": 507
    },
    {
      "epoch": 1.1889994148624927,
      "grad_norm": 0.38889047503471375,
      "learning_rate": 0.0002936768149882904,
      "loss": 0.2263,
      "step": 508
    },
    {
      "epoch": 1.1913399648917495,
      "grad_norm": 0.31901815533638,
      "learning_rate": 0.00029359875097580016,
      "loss": 0.1889,
      "step": 509
    },
    {
      "epoch": 1.1936805149210064,
      "grad_norm": 0.2872103154659271,
      "learning_rate": 0.0002935206869633099,
      "loss": 0.2013,
      "step": 510
    },
    {
      "epoch": 1.1960210649502634,
      "grad_norm": 0.2848789691925049,
      "learning_rate": 0.00029344262295081966,
      "loss": 0.1979,
      "step": 511
    },
    {
      "epoch": 1.1983616149795202,
      "grad_norm": 0.39344221353530884,
      "learning_rate": 0.00029336455893832944,
      "loss": 0.2723,
      "step": 512
    },
    {
      "epoch": 1.200702165008777,
      "grad_norm": 0.3612116873264313,
      "learning_rate": 0.00029328649492583916,
      "loss": 0.2737,
      "step": 513
    },
    {
      "epoch": 1.203042715038034,
      "grad_norm": 0.39685919880867004,
      "learning_rate": 0.00029320843091334894,
      "loss": 0.2849,
      "step": 514
    },
    {
      "epoch": 1.2053832650672909,
      "grad_norm": 0.46929970383644104,
      "learning_rate": 0.0002931303669008587,
      "loss": 0.2263,
      "step": 515
    },
    {
      "epoch": 1.2077238150965477,
      "grad_norm": 0.47178545594215393,
      "learning_rate": 0.00029305230288836844,
      "loss": 0.2192,
      "step": 516
    },
    {
      "epoch": 1.2100643651258045,
      "grad_norm": 0.3613191545009613,
      "learning_rate": 0.00029297423887587816,
      "loss": 0.1754,
      "step": 517
    },
    {
      "epoch": 1.2124049151550613,
      "grad_norm": 0.41678643226623535,
      "learning_rate": 0.000292896174863388,
      "loss": 0.2853,
      "step": 518
    },
    {
      "epoch": 1.2147454651843184,
      "grad_norm": 0.3156086504459381,
      "learning_rate": 0.0002928181108508977,
      "loss": 0.1863,
      "step": 519
    },
    {
      "epoch": 1.2170860152135752,
      "grad_norm": 0.2783469259738922,
      "learning_rate": 0.00029274004683840744,
      "loss": 0.1577,
      "step": 520
    },
    {
      "epoch": 1.219426565242832,
      "grad_norm": 0.29908424615859985,
      "learning_rate": 0.0002926619828259172,
      "loss": 0.2132,
      "step": 521
    },
    {
      "epoch": 1.221767115272089,
      "grad_norm": 0.3472006022930145,
      "learning_rate": 0.000292583918813427,
      "loss": 0.1813,
      "step": 522
    },
    {
      "epoch": 1.2241076653013458,
      "grad_norm": 0.32345259189605713,
      "learning_rate": 0.0002925058548009367,
      "loss": 0.2443,
      "step": 523
    },
    {
      "epoch": 1.2264482153306027,
      "grad_norm": 0.2639113664627075,
      "learning_rate": 0.0002924277907884465,
      "loss": 0.1618,
      "step": 524
    },
    {
      "epoch": 1.2287887653598595,
      "grad_norm": 0.4400657117366791,
      "learning_rate": 0.0002923497267759563,
      "loss": 0.3437,
      "step": 525
    },
    {
      "epoch": 1.2311293153891165,
      "grad_norm": 0.4773165285587311,
      "learning_rate": 0.000292271662763466,
      "loss": 0.2555,
      "step": 526
    },
    {
      "epoch": 1.2334698654183733,
      "grad_norm": 0.4645727574825287,
      "learning_rate": 0.0002921935987509758,
      "loss": 0.2892,
      "step": 527
    },
    {
      "epoch": 1.2358104154476302,
      "grad_norm": 0.3669758439064026,
      "learning_rate": 0.00029211553473848555,
      "loss": 0.2222,
      "step": 528
    },
    {
      "epoch": 1.238150965476887,
      "grad_norm": 0.36810919642448425,
      "learning_rate": 0.0002920374707259953,
      "loss": 0.2361,
      "step": 529
    },
    {
      "epoch": 1.240491515506144,
      "grad_norm": 0.34565141797065735,
      "learning_rate": 0.00029195940671350505,
      "loss": 0.2333,
      "step": 530
    },
    {
      "epoch": 1.2428320655354008,
      "grad_norm": 0.27749285101890564,
      "learning_rate": 0.00029188134270101483,
      "loss": 0.1676,
      "step": 531
    },
    {
      "epoch": 1.2451726155646576,
      "grad_norm": 0.3701418340206146,
      "learning_rate": 0.00029180327868852455,
      "loss": 0.2297,
      "step": 532
    },
    {
      "epoch": 1.2475131655939147,
      "grad_norm": 0.34345725178718567,
      "learning_rate": 0.00029172521467603433,
      "loss": 0.2894,
      "step": 533
    },
    {
      "epoch": 1.2498537156231715,
      "grad_norm": 0.3035535514354706,
      "learning_rate": 0.0002916471506635441,
      "loss": 0.2109,
      "step": 534
    },
    {
      "epoch": 1.2521942656524283,
      "grad_norm": 0.2699846029281616,
      "learning_rate": 0.00029156908665105383,
      "loss": 0.1522,
      "step": 535
    },
    {
      "epoch": 1.2545348156816851,
      "grad_norm": 0.3647426962852478,
      "learning_rate": 0.0002914910226385636,
      "loss": 0.2248,
      "step": 536
    },
    {
      "epoch": 1.256875365710942,
      "grad_norm": 0.2945650517940521,
      "learning_rate": 0.00029141295862607333,
      "loss": 0.228,
      "step": 537
    },
    {
      "epoch": 1.259215915740199,
      "grad_norm": 0.27823537588119507,
      "learning_rate": 0.0002913348946135831,
      "loss": 0.2021,
      "step": 538
    },
    {
      "epoch": 1.2615564657694558,
      "grad_norm": 0.3803277909755707,
      "learning_rate": 0.0002912568306010929,
      "loss": 0.2584,
      "step": 539
    },
    {
      "epoch": 1.2638970157987126,
      "grad_norm": 0.3632267415523529,
      "learning_rate": 0.0002911787665886026,
      "loss": 0.2201,
      "step": 540
    },
    {
      "epoch": 1.2662375658279696,
      "grad_norm": 0.30861198902130127,
      "learning_rate": 0.0002911007025761124,
      "loss": 0.2392,
      "step": 541
    },
    {
      "epoch": 1.2685781158572265,
      "grad_norm": 0.31754979491233826,
      "learning_rate": 0.00029102263856362217,
      "loss": 0.2157,
      "step": 542
    },
    {
      "epoch": 1.2709186658864833,
      "grad_norm": 0.34109577536582947,
      "learning_rate": 0.0002909445745511319,
      "loss": 0.2387,
      "step": 543
    },
    {
      "epoch": 1.2732592159157403,
      "grad_norm": 0.3438536822795868,
      "learning_rate": 0.00029086651053864167,
      "loss": 0.2191,
      "step": 544
    },
    {
      "epoch": 1.2755997659449971,
      "grad_norm": 0.4242745637893677,
      "learning_rate": 0.00029078844652615144,
      "loss": 0.2295,
      "step": 545
    },
    {
      "epoch": 1.277940315974254,
      "grad_norm": 0.3676247298717499,
      "learning_rate": 0.00029071038251366117,
      "loss": 0.2036,
      "step": 546
    },
    {
      "epoch": 1.2802808660035108,
      "grad_norm": 0.4062911570072174,
      "learning_rate": 0.00029063231850117094,
      "loss": 0.2128,
      "step": 547
    },
    {
      "epoch": 1.2826214160327676,
      "grad_norm": 0.2718835473060608,
      "learning_rate": 0.0002905542544886807,
      "loss": 0.2042,
      "step": 548
    },
    {
      "epoch": 1.2849619660620246,
      "grad_norm": 0.28009623289108276,
      "learning_rate": 0.00029047619047619045,
      "loss": 0.2319,
      "step": 549
    },
    {
      "epoch": 1.2873025160912814,
      "grad_norm": 0.30903881788253784,
      "learning_rate": 0.0002903981264637002,
      "loss": 0.1904,
      "step": 550
    },
    {
      "epoch": 1.2896430661205383,
      "grad_norm": 0.35623642802238464,
      "learning_rate": 0.00029032006245121,
      "loss": 0.2152,
      "step": 551
    },
    {
      "epoch": 1.2919836161497953,
      "grad_norm": 0.32813313603401184,
      "learning_rate": 0.0002902419984387197,
      "loss": 0.1544,
      "step": 552
    },
    {
      "epoch": 1.294324166179052,
      "grad_norm": 0.3381251096725464,
      "learning_rate": 0.00029016393442622945,
      "loss": 0.1989,
      "step": 553
    },
    {
      "epoch": 1.296664716208309,
      "grad_norm": 0.33374956250190735,
      "learning_rate": 0.0002900858704137393,
      "loss": 0.2169,
      "step": 554
    },
    {
      "epoch": 1.299005266237566,
      "grad_norm": 0.3800491988658905,
      "learning_rate": 0.000290007806401249,
      "loss": 0.2039,
      "step": 555
    },
    {
      "epoch": 1.3013458162668228,
      "grad_norm": 0.30818381905555725,
      "learning_rate": 0.0002899297423887587,
      "loss": 0.1806,
      "step": 556
    },
    {
      "epoch": 1.3036863662960796,
      "grad_norm": 0.38177791237831116,
      "learning_rate": 0.0002898516783762685,
      "loss": 0.2752,
      "step": 557
    },
    {
      "epoch": 1.3060269163253364,
      "grad_norm": 0.3360247015953064,
      "learning_rate": 0.0002897736143637783,
      "loss": 0.226,
      "step": 558
    },
    {
      "epoch": 1.3083674663545932,
      "grad_norm": 0.3380598723888397,
      "learning_rate": 0.000289695550351288,
      "loss": 0.2387,
      "step": 559
    },
    {
      "epoch": 1.3107080163838503,
      "grad_norm": 0.4063241481781006,
      "learning_rate": 0.0002896174863387978,
      "loss": 0.2537,
      "step": 560
    },
    {
      "epoch": 1.313048566413107,
      "grad_norm": 0.31129831075668335,
      "learning_rate": 0.00028953942232630756,
      "loss": 0.1852,
      "step": 561
    },
    {
      "epoch": 1.315389116442364,
      "grad_norm": 0.3023993670940399,
      "learning_rate": 0.0002894613583138173,
      "loss": 0.217,
      "step": 562
    },
    {
      "epoch": 1.317729666471621,
      "grad_norm": 0.3601124584674835,
      "learning_rate": 0.00028938329430132706,
      "loss": 0.266,
      "step": 563
    },
    {
      "epoch": 1.3200702165008777,
      "grad_norm": 0.331790030002594,
      "learning_rate": 0.00028930523028883684,
      "loss": 0.2663,
      "step": 564
    },
    {
      "epoch": 1.3224107665301346,
      "grad_norm": 0.3702603280544281,
      "learning_rate": 0.00028922716627634656,
      "loss": 0.2467,
      "step": 565
    },
    {
      "epoch": 1.3247513165593914,
      "grad_norm": 0.36749395728111267,
      "learning_rate": 0.00028914910226385634,
      "loss": 0.2246,
      "step": 566
    },
    {
      "epoch": 1.3270918665886482,
      "grad_norm": 0.4085153043270111,
      "learning_rate": 0.0002890710382513661,
      "loss": 0.2269,
      "step": 567
    },
    {
      "epoch": 1.3294324166179052,
      "grad_norm": 0.2979714572429657,
      "learning_rate": 0.00028899297423887584,
      "loss": 0.2033,
      "step": 568
    },
    {
      "epoch": 1.331772966647162,
      "grad_norm": 0.3950141668319702,
      "learning_rate": 0.0002889149102263856,
      "loss": 0.1951,
      "step": 569
    },
    {
      "epoch": 1.3341135166764189,
      "grad_norm": 0.3665187656879425,
      "learning_rate": 0.0002888368462138954,
      "loss": 0.2207,
      "step": 570
    },
    {
      "epoch": 1.336454066705676,
      "grad_norm": 0.5917483568191528,
      "learning_rate": 0.0002887587822014051,
      "loss": 0.2486,
      "step": 571
    },
    {
      "epoch": 1.3387946167349327,
      "grad_norm": 0.5261374711990356,
      "learning_rate": 0.0002886807181889149,
      "loss": 0.1816,
      "step": 572
    },
    {
      "epoch": 1.3411351667641895,
      "grad_norm": 0.3089171350002289,
      "learning_rate": 0.0002886026541764246,
      "loss": 0.1734,
      "step": 573
    },
    {
      "epoch": 1.3434757167934466,
      "grad_norm": 0.5920963883399963,
      "learning_rate": 0.0002885245901639344,
      "loss": 0.3375,
      "step": 574
    },
    {
      "epoch": 1.3458162668227034,
      "grad_norm": 1.636335015296936,
      "learning_rate": 0.0002884465261514442,
      "loss": 0.2379,
      "step": 575
    },
    {
      "epoch": 1.3481568168519602,
      "grad_norm": 0.4673244059085846,
      "learning_rate": 0.0002883684621389539,
      "loss": 0.2219,
      "step": 576
    },
    {
      "epoch": 1.350497366881217,
      "grad_norm": 0.4045529067516327,
      "learning_rate": 0.0002882903981264637,
      "loss": 0.317,
      "step": 577
    },
    {
      "epoch": 1.3528379169104738,
      "grad_norm": 0.33127009868621826,
      "learning_rate": 0.00028821233411397345,
      "loss": 0.2082,
      "step": 578
    },
    {
      "epoch": 1.3551784669397309,
      "grad_norm": 0.45555728673934937,
      "learning_rate": 0.0002881342701014832,
      "loss": 0.2545,
      "step": 579
    },
    {
      "epoch": 1.3575190169689877,
      "grad_norm": 0.32501599192619324,
      "learning_rate": 0.00028805620608899295,
      "loss": 0.239,
      "step": 580
    },
    {
      "epoch": 1.3598595669982445,
      "grad_norm": 0.2518259286880493,
      "learning_rate": 0.00028797814207650273,
      "loss": 0.1689,
      "step": 581
    },
    {
      "epoch": 1.3622001170275015,
      "grad_norm": 0.39093849062919617,
      "learning_rate": 0.00028790007806401245,
      "loss": 0.2201,
      "step": 582
    },
    {
      "epoch": 1.3645406670567584,
      "grad_norm": 0.2697904706001282,
      "learning_rate": 0.00028782201405152223,
      "loss": 0.1736,
      "step": 583
    },
    {
      "epoch": 1.3668812170860152,
      "grad_norm": 0.24396872520446777,
      "learning_rate": 0.000287743950039032,
      "loss": 0.2111,
      "step": 584
    },
    {
      "epoch": 1.3692217671152722,
      "grad_norm": 0.25910523533821106,
      "learning_rate": 0.00028766588602654173,
      "loss": 0.2198,
      "step": 585
    },
    {
      "epoch": 1.371562317144529,
      "grad_norm": 0.3263148069381714,
      "learning_rate": 0.0002875878220140515,
      "loss": 0.2155,
      "step": 586
    },
    {
      "epoch": 1.3739028671737858,
      "grad_norm": 0.2445160299539566,
      "learning_rate": 0.0002875097580015613,
      "loss": 0.2082,
      "step": 587
    },
    {
      "epoch": 1.3762434172030427,
      "grad_norm": 0.37740275263786316,
      "learning_rate": 0.000287431693989071,
      "loss": 0.2272,
      "step": 588
    },
    {
      "epoch": 1.3785839672322995,
      "grad_norm": 0.2560933530330658,
      "learning_rate": 0.00028735362997658073,
      "loss": 0.1821,
      "step": 589
    },
    {
      "epoch": 1.3809245172615565,
      "grad_norm": 0.3093729019165039,
      "learning_rate": 0.0002872755659640905,
      "loss": 0.2032,
      "step": 590
    },
    {
      "epoch": 1.3832650672908133,
      "grad_norm": 0.3193407654762268,
      "learning_rate": 0.0002871975019516003,
      "loss": 0.1781,
      "step": 591
    },
    {
      "epoch": 1.3856056173200701,
      "grad_norm": 0.4070664644241333,
      "learning_rate": 0.00028711943793911,
      "loss": 0.2441,
      "step": 592
    },
    {
      "epoch": 1.3879461673493272,
      "grad_norm": 0.33400508761405945,
      "learning_rate": 0.0002870413739266198,
      "loss": 0.224,
      "step": 593
    },
    {
      "epoch": 1.390286717378584,
      "grad_norm": 0.4253562092781067,
      "learning_rate": 0.00028696330991412957,
      "loss": 0.2864,
      "step": 594
    },
    {
      "epoch": 1.3926272674078408,
      "grad_norm": 0.3457549214363098,
      "learning_rate": 0.0002868852459016393,
      "loss": 0.2148,
      "step": 595
    },
    {
      "epoch": 1.3949678174370979,
      "grad_norm": 0.4003278613090515,
      "learning_rate": 0.00028680718188914907,
      "loss": 0.2458,
      "step": 596
    },
    {
      "epoch": 1.3973083674663547,
      "grad_norm": 0.2835473418235779,
      "learning_rate": 0.00028672911787665884,
      "loss": 0.1739,
      "step": 597
    },
    {
      "epoch": 1.3996489174956115,
      "grad_norm": 0.3546636700630188,
      "learning_rate": 0.00028665105386416857,
      "loss": 0.2319,
      "step": 598
    },
    {
      "epoch": 1.4019894675248683,
      "grad_norm": 0.48376134037971497,
      "learning_rate": 0.00028657298985167835,
      "loss": 0.3234,
      "step": 599
    },
    {
      "epoch": 1.4043300175541251,
      "grad_norm": 0.3754653334617615,
      "learning_rate": 0.0002864949258391881,
      "loss": 0.1972,
      "step": 600
    },
    {
      "epoch": 1.4066705675833822,
      "grad_norm": 0.38960516452789307,
      "learning_rate": 0.00028641686182669785,
      "loss": 0.1897,
      "step": 601
    },
    {
      "epoch": 1.409011117612639,
      "grad_norm": 0.34523069858551025,
      "learning_rate": 0.0002863387978142076,
      "loss": 0.2194,
      "step": 602
    },
    {
      "epoch": 1.4113516676418958,
      "grad_norm": 0.38412976264953613,
      "learning_rate": 0.0002862607338017174,
      "loss": 0.2011,
      "step": 603
    },
    {
      "epoch": 1.4136922176711528,
      "grad_norm": 0.2904255986213684,
      "learning_rate": 0.0002861826697892271,
      "loss": 0.2005,
      "step": 604
    },
    {
      "epoch": 1.4160327677004096,
      "grad_norm": 0.355954110622406,
      "learning_rate": 0.0002861046057767369,
      "loss": 0.1848,
      "step": 605
    },
    {
      "epoch": 1.4183733177296665,
      "grad_norm": 0.2867383658885956,
      "learning_rate": 0.0002860265417642467,
      "loss": 0.1351,
      "step": 606
    },
    {
      "epoch": 1.4207138677589233,
      "grad_norm": 0.4362402558326721,
      "learning_rate": 0.0002859484777517564,
      "loss": 0.2504,
      "step": 607
    },
    {
      "epoch": 1.42305441778818,
      "grad_norm": 0.34849825501441956,
      "learning_rate": 0.0002858704137392662,
      "loss": 0.2395,
      "step": 608
    },
    {
      "epoch": 1.4253949678174371,
      "grad_norm": 0.37655362486839294,
      "learning_rate": 0.0002857923497267759,
      "loss": 0.1735,
      "step": 609
    },
    {
      "epoch": 1.427735517846694,
      "grad_norm": 0.45097336173057556,
      "learning_rate": 0.0002857142857142857,
      "loss": 0.2215,
      "step": 610
    },
    {
      "epoch": 1.4300760678759508,
      "grad_norm": 0.31924760341644287,
      "learning_rate": 0.00028563622170179546,
      "loss": 0.1816,
      "step": 611
    },
    {
      "epoch": 1.4324166179052078,
      "grad_norm": 0.374929279088974,
      "learning_rate": 0.0002855581576893052,
      "loss": 0.2453,
      "step": 612
    },
    {
      "epoch": 1.4347571679344646,
      "grad_norm": 0.34809255599975586,
      "learning_rate": 0.00028548009367681496,
      "loss": 0.2168,
      "step": 613
    },
    {
      "epoch": 1.4370977179637214,
      "grad_norm": 0.2872608006000519,
      "learning_rate": 0.00028540202966432474,
      "loss": 0.2008,
      "step": 614
    },
    {
      "epoch": 1.4394382679929785,
      "grad_norm": 0.3408019244670868,
      "learning_rate": 0.00028532396565183446,
      "loss": 0.2215,
      "step": 615
    },
    {
      "epoch": 1.4417788180222353,
      "grad_norm": 0.3628350794315338,
      "learning_rate": 0.00028524590163934424,
      "loss": 0.2574,
      "step": 616
    },
    {
      "epoch": 1.444119368051492,
      "grad_norm": 0.2906443476676941,
      "learning_rate": 0.000285167837626854,
      "loss": 0.1962,
      "step": 617
    },
    {
      "epoch": 1.446459918080749,
      "grad_norm": 0.2992312014102936,
      "learning_rate": 0.00028508977361436374,
      "loss": 0.2092,
      "step": 618
    },
    {
      "epoch": 1.4488004681100057,
      "grad_norm": 0.30560675263404846,
      "learning_rate": 0.0002850117096018735,
      "loss": 0.2013,
      "step": 619
    },
    {
      "epoch": 1.4511410181392628,
      "grad_norm": 0.27567511796951294,
      "learning_rate": 0.0002849336455893833,
      "loss": 0.1527,
      "step": 620
    },
    {
      "epoch": 1.4534815681685196,
      "grad_norm": 0.3048168122768402,
      "learning_rate": 0.000284855581576893,
      "loss": 0.2186,
      "step": 621
    },
    {
      "epoch": 1.4558221181977764,
      "grad_norm": 0.3265182673931122,
      "learning_rate": 0.0002847775175644028,
      "loss": 0.2098,
      "step": 622
    },
    {
      "epoch": 1.4581626682270334,
      "grad_norm": 0.26593708992004395,
      "learning_rate": 0.00028469945355191257,
      "loss": 0.2006,
      "step": 623
    },
    {
      "epoch": 1.4605032182562903,
      "grad_norm": 0.34364232420921326,
      "learning_rate": 0.0002846213895394223,
      "loss": 0.2786,
      "step": 624
    },
    {
      "epoch": 1.462843768285547,
      "grad_norm": 0.323724627494812,
      "learning_rate": 0.0002845433255269321,
      "loss": 0.2676,
      "step": 625
    },
    {
      "epoch": 1.4651843183148041,
      "grad_norm": 0.36947664618492126,
      "learning_rate": 0.0002844652615144418,
      "loss": 0.2722,
      "step": 626
    },
    {
      "epoch": 1.467524868344061,
      "grad_norm": 0.3757578134536743,
      "learning_rate": 0.0002843871975019516,
      "loss": 0.2602,
      "step": 627
    },
    {
      "epoch": 1.4698654183733177,
      "grad_norm": 0.31827855110168457,
      "learning_rate": 0.00028430913348946135,
      "loss": 0.1856,
      "step": 628
    },
    {
      "epoch": 1.4722059684025746,
      "grad_norm": 0.3080314099788666,
      "learning_rate": 0.0002842310694769711,
      "loss": 0.2204,
      "step": 629
    },
    {
      "epoch": 1.4745465184318314,
      "grad_norm": 0.3152318298816681,
      "learning_rate": 0.00028415300546448085,
      "loss": 0.1667,
      "step": 630
    },
    {
      "epoch": 1.4768870684610884,
      "grad_norm": 0.2530961036682129,
      "learning_rate": 0.00028407494145199063,
      "loss": 0.1364,
      "step": 631
    },
    {
      "epoch": 1.4792276184903452,
      "grad_norm": 0.3050868511199951,
      "learning_rate": 0.00028399687743950035,
      "loss": 0.1665,
      "step": 632
    },
    {
      "epoch": 1.481568168519602,
      "grad_norm": 0.32038167119026184,
      "learning_rate": 0.00028391881342701013,
      "loss": 0.1629,
      "step": 633
    },
    {
      "epoch": 1.483908718548859,
      "grad_norm": 0.3830728530883789,
      "learning_rate": 0.0002838407494145199,
      "loss": 0.204,
      "step": 634
    },
    {
      "epoch": 1.486249268578116,
      "grad_norm": 0.4260599911212921,
      "learning_rate": 0.00028376268540202963,
      "loss": 0.2276,
      "step": 635
    },
    {
      "epoch": 1.4885898186073727,
      "grad_norm": 0.4755648970603943,
      "learning_rate": 0.0002836846213895394,
      "loss": 0.1999,
      "step": 636
    },
    {
      "epoch": 1.4909303686366295,
      "grad_norm": 0.3421362042427063,
      "learning_rate": 0.0002836065573770492,
      "loss": 0.2249,
      "step": 637
    },
    {
      "epoch": 1.4932709186658863,
      "grad_norm": 0.49516382813453674,
      "learning_rate": 0.0002835284933645589,
      "loss": 0.2307,
      "step": 638
    },
    {
      "epoch": 1.4956114686951434,
      "grad_norm": 0.49636054039001465,
      "learning_rate": 0.0002834504293520687,
      "loss": 0.2368,
      "step": 639
    },
    {
      "epoch": 1.4979520187244002,
      "grad_norm": 0.3493986427783966,
      "learning_rate": 0.00028337236533957846,
      "loss": 0.1914,
      "step": 640
    },
    {
      "epoch": 1.500292568753657,
      "grad_norm": 0.33007222414016724,
      "learning_rate": 0.0002832943013270882,
      "loss": 0.1846,
      "step": 641
    },
    {
      "epoch": 1.502633118782914,
      "grad_norm": 0.3669058680534363,
      "learning_rate": 0.0002832162373145979,
      "loss": 0.3141,
      "step": 642
    },
    {
      "epoch": 1.5049736688121709,
      "grad_norm": 0.2580389678478241,
      "learning_rate": 0.00028313817330210774,
      "loss": 0.1581,
      "step": 643
    },
    {
      "epoch": 1.5073142188414277,
      "grad_norm": 0.3369505703449249,
      "learning_rate": 0.00028306010928961747,
      "loss": 0.1708,
      "step": 644
    },
    {
      "epoch": 1.5096547688706847,
      "grad_norm": 0.2634880542755127,
      "learning_rate": 0.0002829820452771272,
      "loss": 0.1625,
      "step": 645
    },
    {
      "epoch": 1.5119953188999413,
      "grad_norm": 0.24198414385318756,
      "learning_rate": 0.00028290398126463697,
      "loss": 0.1653,
      "step": 646
    },
    {
      "epoch": 1.5143358689291984,
      "grad_norm": 0.2934982478618622,
      "learning_rate": 0.00028282591725214674,
      "loss": 0.2218,
      "step": 647
    },
    {
      "epoch": 1.5166764189584554,
      "grad_norm": 0.299027681350708,
      "learning_rate": 0.00028274785323965647,
      "loss": 0.1729,
      "step": 648
    },
    {
      "epoch": 1.519016968987712,
      "grad_norm": 0.2953488230705261,
      "learning_rate": 0.00028266978922716625,
      "loss": 0.2213,
      "step": 649
    },
    {
      "epoch": 1.521357519016969,
      "grad_norm": 0.32412299513816833,
      "learning_rate": 0.000282591725214676,
      "loss": 0.1935,
      "step": 650
    },
    {
      "epoch": 1.5236980690462258,
      "grad_norm": 0.37124258279800415,
      "learning_rate": 0.00028251366120218575,
      "loss": 0.2952,
      "step": 651
    },
    {
      "epoch": 1.5260386190754827,
      "grad_norm": 0.31013110280036926,
      "learning_rate": 0.0002824355971896955,
      "loss": 0.2231,
      "step": 652
    },
    {
      "epoch": 1.5283791691047397,
      "grad_norm": 0.40142881870269775,
      "learning_rate": 0.0002823575331772053,
      "loss": 0.2143,
      "step": 653
    },
    {
      "epoch": 1.5307197191339965,
      "grad_norm": 0.26094239950180054,
      "learning_rate": 0.000282279469164715,
      "loss": 0.1555,
      "step": 654
    },
    {
      "epoch": 1.5330602691632533,
      "grad_norm": 0.3947512209415436,
      "learning_rate": 0.0002822014051522248,
      "loss": 0.2728,
      "step": 655
    },
    {
      "epoch": 1.5354008191925104,
      "grad_norm": 0.29523733258247375,
      "learning_rate": 0.0002821233411397346,
      "loss": 0.1912,
      "step": 656
    },
    {
      "epoch": 1.537741369221767,
      "grad_norm": 0.36016812920570374,
      "learning_rate": 0.0002820452771272443,
      "loss": 0.2515,
      "step": 657
    },
    {
      "epoch": 1.540081919251024,
      "grad_norm": 0.275814026594162,
      "learning_rate": 0.0002819672131147541,
      "loss": 0.1727,
      "step": 658
    },
    {
      "epoch": 1.5424224692802808,
      "grad_norm": 0.3179207444190979,
      "learning_rate": 0.00028188914910226386,
      "loss": 0.236,
      "step": 659
    },
    {
      "epoch": 1.5447630193095376,
      "grad_norm": 0.34900549054145813,
      "learning_rate": 0.0002818110850897736,
      "loss": 0.2069,
      "step": 660
    },
    {
      "epoch": 1.5471035693387947,
      "grad_norm": 0.5385758876800537,
      "learning_rate": 0.00028173302107728336,
      "loss": 0.3097,
      "step": 661
    },
    {
      "epoch": 1.5494441193680515,
      "grad_norm": 0.26023170351982117,
      "learning_rate": 0.0002816549570647931,
      "loss": 0.1249,
      "step": 662
    },
    {
      "epoch": 1.5517846693973083,
      "grad_norm": 0.3608771860599518,
      "learning_rate": 0.00028157689305230286,
      "loss": 0.2323,
      "step": 663
    },
    {
      "epoch": 1.5541252194265653,
      "grad_norm": 0.417236864566803,
      "learning_rate": 0.00028149882903981264,
      "loss": 0.2501,
      "step": 664
    },
    {
      "epoch": 1.5564657694558222,
      "grad_norm": 0.5118183493614197,
      "learning_rate": 0.00028142076502732236,
      "loss": 0.321,
      "step": 665
    },
    {
      "epoch": 1.558806319485079,
      "grad_norm": 0.3430100977420807,
      "learning_rate": 0.00028134270101483214,
      "loss": 0.1681,
      "step": 666
    },
    {
      "epoch": 1.561146869514336,
      "grad_norm": 0.3656028211116791,
      "learning_rate": 0.0002812646370023419,
      "loss": 0.1943,
      "step": 667
    },
    {
      "epoch": 1.5634874195435926,
      "grad_norm": 0.3827469050884247,
      "learning_rate": 0.00028118657298985164,
      "loss": 0.1948,
      "step": 668
    },
    {
      "epoch": 1.5658279695728496,
      "grad_norm": 0.2910468876361847,
      "learning_rate": 0.0002811085089773614,
      "loss": 0.1806,
      "step": 669
    },
    {
      "epoch": 1.5681685196021065,
      "grad_norm": 0.33613812923431396,
      "learning_rate": 0.0002810304449648712,
      "loss": 0.2108,
      "step": 670
    },
    {
      "epoch": 1.5705090696313633,
      "grad_norm": 0.35266318917274475,
      "learning_rate": 0.0002809523809523809,
      "loss": 0.213,
      "step": 671
    },
    {
      "epoch": 1.5728496196606203,
      "grad_norm": 0.2479061484336853,
      "learning_rate": 0.0002808743169398907,
      "loss": 0.1707,
      "step": 672
    },
    {
      "epoch": 1.5751901696898771,
      "grad_norm": 0.2746613919734955,
      "learning_rate": 0.00028079625292740047,
      "loss": 0.177,
      "step": 673
    },
    {
      "epoch": 1.577530719719134,
      "grad_norm": 0.2695852518081665,
      "learning_rate": 0.0002807181889149102,
      "loss": 0.1858,
      "step": 674
    },
    {
      "epoch": 1.579871269748391,
      "grad_norm": 0.23547974228858948,
      "learning_rate": 0.00028064012490242,
      "loss": 0.1272,
      "step": 675
    },
    {
      "epoch": 1.5822118197776478,
      "grad_norm": 0.2872800827026367,
      "learning_rate": 0.00028056206088992975,
      "loss": 0.2073,
      "step": 676
    },
    {
      "epoch": 1.5845523698069046,
      "grad_norm": 0.3389231264591217,
      "learning_rate": 0.0002804839968774395,
      "loss": 0.2374,
      "step": 677
    },
    {
      "epoch": 1.5868929198361617,
      "grad_norm": 0.34046846628189087,
      "learning_rate": 0.0002804059328649492,
      "loss": 0.1924,
      "step": 678
    },
    {
      "epoch": 1.5892334698654182,
      "grad_norm": 0.2676577568054199,
      "learning_rate": 0.00028032786885245903,
      "loss": 0.151,
      "step": 679
    },
    {
      "epoch": 1.5915740198946753,
      "grad_norm": 0.41356706619262695,
      "learning_rate": 0.00028024980483996875,
      "loss": 0.2684,
      "step": 680
    },
    {
      "epoch": 1.593914569923932,
      "grad_norm": 0.3587040603160858,
      "learning_rate": 0.0002801717408274785,
      "loss": 0.2125,
      "step": 681
    },
    {
      "epoch": 1.596255119953189,
      "grad_norm": 0.36658838391304016,
      "learning_rate": 0.00028009367681498825,
      "loss": 0.2352,
      "step": 682
    },
    {
      "epoch": 1.598595669982446,
      "grad_norm": 0.29990288615226746,
      "learning_rate": 0.00028001561280249803,
      "loss": 0.2325,
      "step": 683
    },
    {
      "epoch": 1.6009362200117028,
      "grad_norm": 0.34757378697395325,
      "learning_rate": 0.00027993754879000775,
      "loss": 0.1836,
      "step": 684
    },
    {
      "epoch": 1.6032767700409596,
      "grad_norm": 0.275863915681839,
      "learning_rate": 0.00027985948477751753,
      "loss": 0.1308,
      "step": 685
    },
    {
      "epoch": 1.6056173200702166,
      "grad_norm": 0.27052098512649536,
      "learning_rate": 0.0002797814207650273,
      "loss": 0.2213,
      "step": 686
    },
    {
      "epoch": 1.6079578700994732,
      "grad_norm": 0.2650897800922394,
      "learning_rate": 0.00027970335675253703,
      "loss": 0.1853,
      "step": 687
    },
    {
      "epoch": 1.6102984201287303,
      "grad_norm": 0.4102299213409424,
      "learning_rate": 0.0002796252927400468,
      "loss": 0.2911,
      "step": 688
    },
    {
      "epoch": 1.612638970157987,
      "grad_norm": 0.3213823437690735,
      "learning_rate": 0.0002795472287275566,
      "loss": 0.1969,
      "step": 689
    },
    {
      "epoch": 1.6149795201872439,
      "grad_norm": 0.2487405240535736,
      "learning_rate": 0.0002794691647150663,
      "loss": 0.1578,
      "step": 690
    },
    {
      "epoch": 1.617320070216501,
      "grad_norm": 0.24649584293365479,
      "learning_rate": 0.0002793911007025761,
      "loss": 0.138,
      "step": 691
    },
    {
      "epoch": 1.6196606202457577,
      "grad_norm": 0.4570413827896118,
      "learning_rate": 0.00027931303669008587,
      "loss": 0.3017,
      "step": 692
    },
    {
      "epoch": 1.6220011702750146,
      "grad_norm": 0.37284839153289795,
      "learning_rate": 0.0002792349726775956,
      "loss": 0.2063,
      "step": 693
    },
    {
      "epoch": 1.6243417203042716,
      "grad_norm": 0.34004178643226624,
      "learning_rate": 0.00027915690866510537,
      "loss": 0.2215,
      "step": 694
    },
    {
      "epoch": 1.6266822703335284,
      "grad_norm": 0.3680613934993744,
      "learning_rate": 0.00027907884465261514,
      "loss": 0.2359,
      "step": 695
    },
    {
      "epoch": 1.6290228203627852,
      "grad_norm": 0.4062170088291168,
      "learning_rate": 0.00027900078064012487,
      "loss": 0.2576,
      "step": 696
    },
    {
      "epoch": 1.6313633703920423,
      "grad_norm": 0.2656954526901245,
      "learning_rate": 0.00027892271662763465,
      "loss": 0.1306,
      "step": 697
    },
    {
      "epoch": 1.6337039204212989,
      "grad_norm": 0.28851380944252014,
      "learning_rate": 0.00027884465261514437,
      "loss": 0.1624,
      "step": 698
    },
    {
      "epoch": 1.636044470450556,
      "grad_norm": 0.40471136569976807,
      "learning_rate": 0.00027876658860265415,
      "loss": 0.2098,
      "step": 699
    },
    {
      "epoch": 1.6383850204798127,
      "grad_norm": 0.2631782293319702,
      "learning_rate": 0.0002786885245901639,
      "loss": 0.1819,
      "step": 700
    },
    {
      "epoch": 1.6407255705090695,
      "grad_norm": 0.42636269330978394,
      "learning_rate": 0.00027861046057767365,
      "loss": 0.2326,
      "step": 701
    },
    {
      "epoch": 1.6430661205383266,
      "grad_norm": 0.37621068954467773,
      "learning_rate": 0.0002785323965651834,
      "loss": 0.2455,
      "step": 702
    },
    {
      "epoch": 1.6454066705675834,
      "grad_norm": 0.30132007598876953,
      "learning_rate": 0.0002784543325526932,
      "loss": 0.1921,
      "step": 703
    },
    {
      "epoch": 1.6477472205968402,
      "grad_norm": 0.33700624108314514,
      "learning_rate": 0.0002783762685402029,
      "loss": 0.1801,
      "step": 704
    },
    {
      "epoch": 1.6500877706260972,
      "grad_norm": 0.27502331137657166,
      "learning_rate": 0.0002782982045277127,
      "loss": 0.2287,
      "step": 705
    },
    {
      "epoch": 1.652428320655354,
      "grad_norm": 0.28909412026405334,
      "learning_rate": 0.0002782201405152225,
      "loss": 0.1846,
      "step": 706
    },
    {
      "epoch": 1.6547688706846109,
      "grad_norm": 0.29930850863456726,
      "learning_rate": 0.0002781420765027322,
      "loss": 0.1908,
      "step": 707
    },
    {
      "epoch": 1.657109420713868,
      "grad_norm": 0.33316731452941895,
      "learning_rate": 0.000278064012490242,
      "loss": 0.1563,
      "step": 708
    },
    {
      "epoch": 1.6594499707431245,
      "grad_norm": 0.3277682363986969,
      "learning_rate": 0.00027798594847775176,
      "loss": 0.2346,
      "step": 709
    },
    {
      "epoch": 1.6617905207723815,
      "grad_norm": 0.3185054361820221,
      "learning_rate": 0.0002779078844652615,
      "loss": 0.1999,
      "step": 710
    },
    {
      "epoch": 1.6641310708016384,
      "grad_norm": 0.23381628096103668,
      "learning_rate": 0.00027782982045277126,
      "loss": 0.1434,
      "step": 711
    },
    {
      "epoch": 1.6664716208308952,
      "grad_norm": 0.2571001648902893,
      "learning_rate": 0.00027775175644028104,
      "loss": 0.1658,
      "step": 712
    },
    {
      "epoch": 1.6688121708601522,
      "grad_norm": 0.25351276993751526,
      "learning_rate": 0.00027767369242779076,
      "loss": 0.1625,
      "step": 713
    },
    {
      "epoch": 1.671152720889409,
      "grad_norm": 0.28124362230300903,
      "learning_rate": 0.0002775956284153005,
      "loss": 0.1979,
      "step": 714
    },
    {
      "epoch": 1.6734932709186658,
      "grad_norm": 0.28992336988449097,
      "learning_rate": 0.0002775175644028103,
      "loss": 0.2104,
      "step": 715
    },
    {
      "epoch": 1.6758338209479229,
      "grad_norm": 0.318783164024353,
      "learning_rate": 0.00027743950039032004,
      "loss": 0.2259,
      "step": 716
    },
    {
      "epoch": 1.6781743709771795,
      "grad_norm": 0.2933308482170105,
      "learning_rate": 0.00027736143637782976,
      "loss": 0.2016,
      "step": 717
    },
    {
      "epoch": 1.6805149210064365,
      "grad_norm": 0.3367266058921814,
      "learning_rate": 0.00027728337236533954,
      "loss": 0.1895,
      "step": 718
    },
    {
      "epoch": 1.6828554710356936,
      "grad_norm": 0.29948675632476807,
      "learning_rate": 0.0002772053083528493,
      "loss": 0.1713,
      "step": 719
    },
    {
      "epoch": 1.6851960210649501,
      "grad_norm": 0.3150722086429596,
      "learning_rate": 0.00027712724434035904,
      "loss": 0.2065,
      "step": 720
    },
    {
      "epoch": 1.6875365710942072,
      "grad_norm": 0.3241063058376312,
      "learning_rate": 0.0002770491803278688,
      "loss": 0.2219,
      "step": 721
    },
    {
      "epoch": 1.689877121123464,
      "grad_norm": 0.35768646001815796,
      "learning_rate": 0.0002769711163153786,
      "loss": 0.1934,
      "step": 722
    },
    {
      "epoch": 1.6922176711527208,
      "grad_norm": 0.3337737023830414,
      "learning_rate": 0.0002768930523028883,
      "loss": 0.2131,
      "step": 723
    },
    {
      "epoch": 1.6945582211819779,
      "grad_norm": 0.2970288395881653,
      "learning_rate": 0.0002768149882903981,
      "loss": 0.1774,
      "step": 724
    },
    {
      "epoch": 1.6968987712112347,
      "grad_norm": 0.36246952414512634,
      "learning_rate": 0.0002767369242779079,
      "loss": 0.2237,
      "step": 725
    },
    {
      "epoch": 1.6992393212404915,
      "grad_norm": 0.2933058738708496,
      "learning_rate": 0.0002766588602654176,
      "loss": 0.2063,
      "step": 726
    },
    {
      "epoch": 1.7015798712697485,
      "grad_norm": 0.37409722805023193,
      "learning_rate": 0.0002765807962529274,
      "loss": 0.2201,
      "step": 727
    },
    {
      "epoch": 1.7039204212990051,
      "grad_norm": 0.38956189155578613,
      "learning_rate": 0.00027650273224043715,
      "loss": 0.2511,
      "step": 728
    },
    {
      "epoch": 1.7062609713282622,
      "grad_norm": 0.3959403932094574,
      "learning_rate": 0.0002764246682279469,
      "loss": 0.2304,
      "step": 729
    },
    {
      "epoch": 1.708601521357519,
      "grad_norm": 0.3121550679206848,
      "learning_rate": 0.00027634660421545665,
      "loss": 0.204,
      "step": 730
    },
    {
      "epoch": 1.7109420713867758,
      "grad_norm": 0.3501099646091461,
      "learning_rate": 0.00027626854020296643,
      "loss": 0.1732,
      "step": 731
    },
    {
      "epoch": 1.7132826214160328,
      "grad_norm": 0.4078333377838135,
      "learning_rate": 0.00027619047619047615,
      "loss": 0.237,
      "step": 732
    },
    {
      "epoch": 1.7156231714452896,
      "grad_norm": 0.39231228828430176,
      "learning_rate": 0.00027611241217798593,
      "loss": 0.2222,
      "step": 733
    },
    {
      "epoch": 1.7179637214745465,
      "grad_norm": 0.35942280292510986,
      "learning_rate": 0.00027603434816549565,
      "loss": 0.1927,
      "step": 734
    },
    {
      "epoch": 1.7203042715038035,
      "grad_norm": 0.3128317594528198,
      "learning_rate": 0.00027595628415300543,
      "loss": 0.2121,
      "step": 735
    },
    {
      "epoch": 1.7226448215330603,
      "grad_norm": 0.34764373302459717,
      "learning_rate": 0.0002758782201405152,
      "loss": 0.2263,
      "step": 736
    },
    {
      "epoch": 1.7249853715623171,
      "grad_norm": 0.3363564610481262,
      "learning_rate": 0.00027580015612802493,
      "loss": 0.2049,
      "step": 737
    },
    {
      "epoch": 1.7273259215915742,
      "grad_norm": 0.30895933508872986,
      "learning_rate": 0.0002757220921155347,
      "loss": 0.1913,
      "step": 738
    },
    {
      "epoch": 1.7296664716208308,
      "grad_norm": 0.3019641935825348,
      "learning_rate": 0.0002756440281030445,
      "loss": 0.2171,
      "step": 739
    },
    {
      "epoch": 1.7320070216500878,
      "grad_norm": 0.336574524641037,
      "learning_rate": 0.0002755659640905542,
      "loss": 0.1996,
      "step": 740
    },
    {
      "epoch": 1.7343475716793446,
      "grad_norm": 0.3207061290740967,
      "learning_rate": 0.000275487900078064,
      "loss": 0.2212,
      "step": 741
    },
    {
      "epoch": 1.7366881217086014,
      "grad_norm": 0.37616121768951416,
      "learning_rate": 0.00027540983606557377,
      "loss": 0.1716,
      "step": 742
    },
    {
      "epoch": 1.7390286717378585,
      "grad_norm": 0.3559929132461548,
      "learning_rate": 0.0002753317720530835,
      "loss": 0.2544,
      "step": 743
    },
    {
      "epoch": 1.7413692217671153,
      "grad_norm": 0.33340340852737427,
      "learning_rate": 0.00027525370804059327,
      "loss": 0.1991,
      "step": 744
    },
    {
      "epoch": 1.743709771796372,
      "grad_norm": 0.41877877712249756,
      "learning_rate": 0.00027517564402810304,
      "loss": 0.2007,
      "step": 745
    },
    {
      "epoch": 1.7460503218256291,
      "grad_norm": 0.2780206501483917,
      "learning_rate": 0.00027509758001561277,
      "loss": 0.2063,
      "step": 746
    },
    {
      "epoch": 1.748390871854886,
      "grad_norm": 0.3815860450267792,
      "learning_rate": 0.00027501951600312255,
      "loss": 0.2329,
      "step": 747
    },
    {
      "epoch": 1.7507314218841428,
      "grad_norm": 0.3626578450202942,
      "learning_rate": 0.0002749414519906323,
      "loss": 0.2456,
      "step": 748
    },
    {
      "epoch": 1.7530719719133998,
      "grad_norm": 0.3095487058162689,
      "learning_rate": 0.00027486338797814205,
      "loss": 0.1661,
      "step": 749
    },
    {
      "epoch": 1.7554125219426564,
      "grad_norm": 0.29246652126312256,
      "learning_rate": 0.0002747853239656518,
      "loss": 0.2004,
      "step": 750
    },
    {
      "epoch": 1.7577530719719134,
      "grad_norm": 0.27082958817481995,
      "learning_rate": 0.00027470725995316155,
      "loss": 0.1608,
      "step": 751
    },
    {
      "epoch": 1.7600936220011703,
      "grad_norm": 0.2537321150302887,
      "learning_rate": 0.0002746291959406713,
      "loss": 0.1622,
      "step": 752
    },
    {
      "epoch": 1.762434172030427,
      "grad_norm": 0.3164644241333008,
      "learning_rate": 0.0002745511319281811,
      "loss": 0.2275,
      "step": 753
    },
    {
      "epoch": 1.764774722059684,
      "grad_norm": 0.2595629394054413,
      "learning_rate": 0.0002744730679156908,
      "loss": 0.1788,
      "step": 754
    },
    {
      "epoch": 1.767115272088941,
      "grad_norm": 0.28124523162841797,
      "learning_rate": 0.0002743950039032006,
      "loss": 0.2102,
      "step": 755
    },
    {
      "epoch": 1.7694558221181977,
      "grad_norm": 0.311725378036499,
      "learning_rate": 0.0002743169398907104,
      "loss": 0.2063,
      "step": 756
    },
    {
      "epoch": 1.7717963721474548,
      "grad_norm": 0.27824777364730835,
      "learning_rate": 0.0002742388758782201,
      "loss": 0.1985,
      "step": 757
    },
    {
      "epoch": 1.7741369221767114,
      "grad_norm": 0.2755049467086792,
      "learning_rate": 0.0002741608118657299,
      "loss": 0.1703,
      "step": 758
    },
    {
      "epoch": 1.7764774722059684,
      "grad_norm": 0.26313266158103943,
      "learning_rate": 0.00027408274785323966,
      "loss": 0.2231,
      "step": 759
    },
    {
      "epoch": 1.7788180222352252,
      "grad_norm": 0.21430489420890808,
      "learning_rate": 0.0002740046838407494,
      "loss": 0.1476,
      "step": 760
    },
    {
      "epoch": 1.781158572264482,
      "grad_norm": 0.2996794283390045,
      "learning_rate": 0.00027392661982825916,
      "loss": 0.2244,
      "step": 761
    },
    {
      "epoch": 1.783499122293739,
      "grad_norm": 0.3476964831352234,
      "learning_rate": 0.00027384855581576894,
      "loss": 0.2261,
      "step": 762
    },
    {
      "epoch": 1.785839672322996,
      "grad_norm": 0.2522297203540802,
      "learning_rate": 0.00027377049180327866,
      "loss": 0.1341,
      "step": 763
    },
    {
      "epoch": 1.7881802223522527,
      "grad_norm": 0.30872052907943726,
      "learning_rate": 0.00027369242779078844,
      "loss": 0.1656,
      "step": 764
    },
    {
      "epoch": 1.7905207723815098,
      "grad_norm": 0.3135488033294678,
      "learning_rate": 0.0002736143637782982,
      "loss": 0.1765,
      "step": 765
    },
    {
      "epoch": 1.7928613224107666,
      "grad_norm": 0.3079582154750824,
      "learning_rate": 0.00027353629976580794,
      "loss": 0.169,
      "step": 766
    },
    {
      "epoch": 1.7952018724400234,
      "grad_norm": 0.38309407234191895,
      "learning_rate": 0.0002734582357533177,
      "loss": 0.1747,
      "step": 767
    },
    {
      "epoch": 1.7975424224692804,
      "grad_norm": 0.4180060625076294,
      "learning_rate": 0.0002733801717408275,
      "loss": 0.2618,
      "step": 768
    },
    {
      "epoch": 1.799882972498537,
      "grad_norm": 0.3046138882637024,
      "learning_rate": 0.0002733021077283372,
      "loss": 0.1552,
      "step": 769
    },
    {
      "epoch": 1.802223522527794,
      "grad_norm": 0.2962167263031006,
      "learning_rate": 0.00027322404371584694,
      "loss": 0.1807,
      "step": 770
    },
    {
      "epoch": 1.8045640725570509,
      "grad_norm": 0.41454893350601196,
      "learning_rate": 0.0002731459797033567,
      "loss": 0.2101,
      "step": 771
    },
    {
      "epoch": 1.8069046225863077,
      "grad_norm": 0.3375609815120697,
      "learning_rate": 0.0002730679156908665,
      "loss": 0.2457,
      "step": 772
    },
    {
      "epoch": 1.8092451726155647,
      "grad_norm": 0.33527836203575134,
      "learning_rate": 0.0002729898516783762,
      "loss": 0.2158,
      "step": 773
    },
    {
      "epoch": 1.8115857226448215,
      "grad_norm": 0.2842245101928711,
      "learning_rate": 0.000272911787665886,
      "loss": 0.2049,
      "step": 774
    },
    {
      "epoch": 1.8139262726740784,
      "grad_norm": 0.25781524181365967,
      "learning_rate": 0.0002728337236533958,
      "loss": 0.1861,
      "step": 775
    },
    {
      "epoch": 1.8162668227033354,
      "grad_norm": 0.2895190119743347,
      "learning_rate": 0.0002727556596409055,
      "loss": 0.1924,
      "step": 776
    },
    {
      "epoch": 1.8186073727325922,
      "grad_norm": 0.24752095341682434,
      "learning_rate": 0.0002726775956284153,
      "loss": 0.1523,
      "step": 777
    },
    {
      "epoch": 1.820947922761849,
      "grad_norm": 0.2406727820634842,
      "learning_rate": 0.00027259953161592505,
      "loss": 0.1414,
      "step": 778
    },
    {
      "epoch": 1.823288472791106,
      "grad_norm": 0.2803957164287567,
      "learning_rate": 0.0002725214676034348,
      "loss": 0.2425,
      "step": 779
    },
    {
      "epoch": 1.8256290228203627,
      "grad_norm": 0.32550591230392456,
      "learning_rate": 0.00027244340359094455,
      "loss": 0.2528,
      "step": 780
    },
    {
      "epoch": 1.8279695728496197,
      "grad_norm": 0.2836322486400604,
      "learning_rate": 0.00027236533957845433,
      "loss": 0.1839,
      "step": 781
    },
    {
      "epoch": 1.8303101228788765,
      "grad_norm": 0.38176968693733215,
      "learning_rate": 0.00027228727556596405,
      "loss": 0.2655,
      "step": 782
    },
    {
      "epoch": 1.8326506729081333,
      "grad_norm": 0.26983118057250977,
      "learning_rate": 0.00027220921155347383,
      "loss": 0.1871,
      "step": 783
    },
    {
      "epoch": 1.8349912229373904,
      "grad_norm": 0.24438995122909546,
      "learning_rate": 0.0002721311475409836,
      "loss": 0.1877,
      "step": 784
    },
    {
      "epoch": 1.8373317729666472,
      "grad_norm": 0.2669437527656555,
      "learning_rate": 0.00027205308352849333,
      "loss": 0.1727,
      "step": 785
    },
    {
      "epoch": 1.839672322995904,
      "grad_norm": 0.3395662307739258,
      "learning_rate": 0.0002719750195160031,
      "loss": 0.2501,
      "step": 786
    },
    {
      "epoch": 1.842012873025161,
      "grad_norm": 0.2781743109226227,
      "learning_rate": 0.00027189695550351283,
      "loss": 0.2153,
      "step": 787
    },
    {
      "epoch": 1.8443534230544176,
      "grad_norm": 0.24481795728206635,
      "learning_rate": 0.0002718188914910226,
      "loss": 0.1421,
      "step": 788
    },
    {
      "epoch": 1.8466939730836747,
      "grad_norm": 0.25075048208236694,
      "learning_rate": 0.0002717408274785324,
      "loss": 0.169,
      "step": 789
    },
    {
      "epoch": 1.8490345231129317,
      "grad_norm": 0.29981595277786255,
      "learning_rate": 0.0002716627634660421,
      "loss": 0.2037,
      "step": 790
    },
    {
      "epoch": 1.8513750731421883,
      "grad_norm": 0.3044470548629761,
      "learning_rate": 0.0002715846994535519,
      "loss": 0.2029,
      "step": 791
    },
    {
      "epoch": 1.8537156231714453,
      "grad_norm": 0.331672340631485,
      "learning_rate": 0.00027150663544106167,
      "loss": 0.1545,
      "step": 792
    },
    {
      "epoch": 1.8560561732007022,
      "grad_norm": 0.23508821427822113,
      "learning_rate": 0.0002714285714285714,
      "loss": 0.1492,
      "step": 793
    },
    {
      "epoch": 1.858396723229959,
      "grad_norm": 0.3904685974121094,
      "learning_rate": 0.00027135050741608117,
      "loss": 0.2227,
      "step": 794
    },
    {
      "epoch": 1.860737273259216,
      "grad_norm": 0.2382720708847046,
      "learning_rate": 0.00027127244340359094,
      "loss": 0.1734,
      "step": 795
    },
    {
      "epoch": 1.8630778232884728,
      "grad_norm": 0.3315110206604004,
      "learning_rate": 0.00027119437939110067,
      "loss": 0.2283,
      "step": 796
    },
    {
      "epoch": 1.8654183733177296,
      "grad_norm": 0.4099690616130829,
      "learning_rate": 0.00027111631537861045,
      "loss": 0.1997,
      "step": 797
    },
    {
      "epoch": 1.8677589233469867,
      "grad_norm": 0.3083561658859253,
      "learning_rate": 0.0002710382513661202,
      "loss": 0.19,
      "step": 798
    },
    {
      "epoch": 1.8700994733762433,
      "grad_norm": 0.2566165626049042,
      "learning_rate": 0.00027096018735362995,
      "loss": 0.164,
      "step": 799
    },
    {
      "epoch": 1.8724400234055003,
      "grad_norm": 0.38149431347846985,
      "learning_rate": 0.0002708821233411397,
      "loss": 0.2281,
      "step": 800
    },
    {
      "epoch": 1.8747805734347571,
      "grad_norm": 0.28524088859558105,
      "learning_rate": 0.0002708040593286495,
      "loss": 0.2029,
      "step": 801
    },
    {
      "epoch": 1.877121123464014,
      "grad_norm": 0.32112887501716614,
      "learning_rate": 0.0002707259953161592,
      "loss": 0.1815,
      "step": 802
    },
    {
      "epoch": 1.879461673493271,
      "grad_norm": 0.3594035804271698,
      "learning_rate": 0.00027064793130366895,
      "loss": 0.1912,
      "step": 803
    },
    {
      "epoch": 1.8818022235225278,
      "grad_norm": 0.35098037123680115,
      "learning_rate": 0.0002705698672911788,
      "loss": 0.1862,
      "step": 804
    },
    {
      "epoch": 1.8841427735517846,
      "grad_norm": 0.29431402683258057,
      "learning_rate": 0.0002704918032786885,
      "loss": 0.1839,
      "step": 805
    },
    {
      "epoch": 1.8864833235810416,
      "grad_norm": 0.33203426003456116,
      "learning_rate": 0.0002704137392661982,
      "loss": 0.1854,
      "step": 806
    },
    {
      "epoch": 1.8888238736102985,
      "grad_norm": 0.32278379797935486,
      "learning_rate": 0.000270335675253708,
      "loss": 0.1727,
      "step": 807
    },
    {
      "epoch": 1.8911644236395553,
      "grad_norm": 0.36259153485298157,
      "learning_rate": 0.0002702576112412178,
      "loss": 0.217,
      "step": 808
    },
    {
      "epoch": 1.8935049736688123,
      "grad_norm": 0.32697930932044983,
      "learning_rate": 0.0002701795472287275,
      "loss": 0.1834,
      "step": 809
    },
    {
      "epoch": 1.895845523698069,
      "grad_norm": 0.30650678277015686,
      "learning_rate": 0.0002701014832162373,
      "loss": 0.1737,
      "step": 810
    },
    {
      "epoch": 1.898186073727326,
      "grad_norm": 0.3182119131088257,
      "learning_rate": 0.00027002341920374706,
      "loss": 0.1788,
      "step": 811
    },
    {
      "epoch": 1.9005266237565828,
      "grad_norm": 0.31716006994247437,
      "learning_rate": 0.0002699453551912568,
      "loss": 0.2393,
      "step": 812
    },
    {
      "epoch": 1.9028671737858396,
      "grad_norm": 0.3864515423774719,
      "learning_rate": 0.00026986729117876656,
      "loss": 0.2285,
      "step": 813
    },
    {
      "epoch": 1.9052077238150966,
      "grad_norm": 0.2878820300102234,
      "learning_rate": 0.00026978922716627634,
      "loss": 0.1636,
      "step": 814
    },
    {
      "epoch": 1.9075482738443534,
      "grad_norm": 0.3321031332015991,
      "learning_rate": 0.00026971116315378606,
      "loss": 0.2,
      "step": 815
    },
    {
      "epoch": 1.9098888238736103,
      "grad_norm": 0.22870180010795593,
      "learning_rate": 0.00026963309914129584,
      "loss": 0.1448,
      "step": 816
    },
    {
      "epoch": 1.9122293739028673,
      "grad_norm": 0.32753080129623413,
      "learning_rate": 0.0002695550351288056,
      "loss": 0.2377,
      "step": 817
    },
    {
      "epoch": 1.9145699239321239,
      "grad_norm": 0.2629968225955963,
      "learning_rate": 0.00026947697111631534,
      "loss": 0.1643,
      "step": 818
    },
    {
      "epoch": 1.916910473961381,
      "grad_norm": 0.26641467213630676,
      "learning_rate": 0.0002693989071038251,
      "loss": 0.175,
      "step": 819
    },
    {
      "epoch": 1.919251023990638,
      "grad_norm": 0.2898910343647003,
      "learning_rate": 0.0002693208430913349,
      "loss": 0.2007,
      "step": 820
    },
    {
      "epoch": 1.9215915740198946,
      "grad_norm": 0.31254905462265015,
      "learning_rate": 0.0002692427790788446,
      "loss": 0.212,
      "step": 821
    },
    {
      "epoch": 1.9239321240491516,
      "grad_norm": 0.29360660910606384,
      "learning_rate": 0.0002691647150663544,
      "loss": 0.1951,
      "step": 822
    },
    {
      "epoch": 1.9262726740784084,
      "grad_norm": 0.21054381132125854,
      "learning_rate": 0.0002690866510538641,
      "loss": 0.1309,
      "step": 823
    },
    {
      "epoch": 1.9286132241076652,
      "grad_norm": 0.27638280391693115,
      "learning_rate": 0.0002690085870413739,
      "loss": 0.207,
      "step": 824
    },
    {
      "epoch": 1.9309537741369223,
      "grad_norm": 0.33025020360946655,
      "learning_rate": 0.0002689305230288837,
      "loss": 0.2129,
      "step": 825
    },
    {
      "epoch": 1.933294324166179,
      "grad_norm": 0.3237188458442688,
      "learning_rate": 0.0002688524590163934,
      "loss": 0.1899,
      "step": 826
    },
    {
      "epoch": 1.935634874195436,
      "grad_norm": 0.2821224629878998,
      "learning_rate": 0.0002687743950039032,
      "loss": 0.1543,
      "step": 827
    },
    {
      "epoch": 1.937975424224693,
      "grad_norm": 0.2209140956401825,
      "learning_rate": 0.00026869633099141295,
      "loss": 0.1307,
      "step": 828
    },
    {
      "epoch": 1.9403159742539495,
      "grad_norm": 0.35687050223350525,
      "learning_rate": 0.0002686182669789227,
      "loss": 0.2168,
      "step": 829
    },
    {
      "epoch": 1.9426565242832066,
      "grad_norm": 0.3639281988143921,
      "learning_rate": 0.00026854020296643245,
      "loss": 0.2117,
      "step": 830
    },
    {
      "epoch": 1.9449970743124634,
      "grad_norm": 0.31904658675193787,
      "learning_rate": 0.00026846213895394223,
      "loss": 0.1709,
      "step": 831
    },
    {
      "epoch": 1.9473376243417202,
      "grad_norm": 0.3887806534767151,
      "learning_rate": 0.00026838407494145195,
      "loss": 0.2165,
      "step": 832
    },
    {
      "epoch": 1.9496781743709772,
      "grad_norm": 0.32850193977355957,
      "learning_rate": 0.00026830601092896173,
      "loss": 0.1945,
      "step": 833
    },
    {
      "epoch": 1.952018724400234,
      "grad_norm": 0.42566972970962524,
      "learning_rate": 0.0002682279469164715,
      "loss": 0.2795,
      "step": 834
    },
    {
      "epoch": 1.9543592744294909,
      "grad_norm": 0.3658904731273651,
      "learning_rate": 0.00026814988290398123,
      "loss": 0.1412,
      "step": 835
    },
    {
      "epoch": 1.956699824458748,
      "grad_norm": 0.371171236038208,
      "learning_rate": 0.000268071818891491,
      "loss": 0.217,
      "step": 836
    },
    {
      "epoch": 1.9590403744880047,
      "grad_norm": 0.34556224942207336,
      "learning_rate": 0.0002679937548790008,
      "loss": 0.1895,
      "step": 837
    },
    {
      "epoch": 1.9613809245172615,
      "grad_norm": 0.39836978912353516,
      "learning_rate": 0.0002679156908665105,
      "loss": 0.1971,
      "step": 838
    },
    {
      "epoch": 1.9637214745465186,
      "grad_norm": 0.319089412689209,
      "learning_rate": 0.00026783762685402023,
      "loss": 0.1838,
      "step": 839
    },
    {
      "epoch": 1.9660620245757752,
      "grad_norm": 0.29996177554130554,
      "learning_rate": 0.00026775956284153007,
      "loss": 0.166,
      "step": 840
    },
    {
      "epoch": 1.9684025746050322,
      "grad_norm": 0.3832019567489624,
      "learning_rate": 0.0002676814988290398,
      "loss": 0.1879,
      "step": 841
    },
    {
      "epoch": 1.970743124634289,
      "grad_norm": 0.3202957510948181,
      "learning_rate": 0.0002676034348165495,
      "loss": 0.1791,
      "step": 842
    },
    {
      "epoch": 1.9730836746635458,
      "grad_norm": 0.35362330079078674,
      "learning_rate": 0.0002675253708040593,
      "loss": 0.1935,
      "step": 843
    },
    {
      "epoch": 1.9754242246928029,
      "grad_norm": 0.325778603553772,
      "learning_rate": 0.00026744730679156907,
      "loss": 0.193,
      "step": 844
    },
    {
      "epoch": 1.9777647747220597,
      "grad_norm": 0.4400867521762848,
      "learning_rate": 0.0002673692427790788,
      "loss": 0.2948,
      "step": 845
    },
    {
      "epoch": 1.9801053247513165,
      "grad_norm": 0.25739210844039917,
      "learning_rate": 0.00026729117876658857,
      "loss": 0.1668,
      "step": 846
    },
    {
      "epoch": 1.9824458747805735,
      "grad_norm": 0.42293471097946167,
      "learning_rate": 0.00026721311475409835,
      "loss": 0.2211,
      "step": 847
    },
    {
      "epoch": 1.9847864248098304,
      "grad_norm": 0.46145099401474,
      "learning_rate": 0.00026713505074160807,
      "loss": 0.2336,
      "step": 848
    },
    {
      "epoch": 1.9871269748390872,
      "grad_norm": 0.31090739369392395,
      "learning_rate": 0.00026705698672911785,
      "loss": 0.1876,
      "step": 849
    },
    {
      "epoch": 1.9894675248683442,
      "grad_norm": 0.37207916378974915,
      "learning_rate": 0.0002669789227166276,
      "loss": 0.2017,
      "step": 850
    },
    {
      "epoch": 1.9918080748976008,
      "grad_norm": 0.43121951818466187,
      "learning_rate": 0.00026690085870413735,
      "loss": 0.2445,
      "step": 851
    },
    {
      "epoch": 1.9941486249268578,
      "grad_norm": 0.39299827814102173,
      "learning_rate": 0.0002668227946916471,
      "loss": 0.262,
      "step": 852
    },
    {
      "epoch": 1.9964891749561147,
      "grad_norm": 0.34877169132232666,
      "learning_rate": 0.0002667447306791569,
      "loss": 0.2377,
      "step": 853
    },
    {
      "epoch": 1.9988297249853715,
      "grad_norm": 0.37346068024635315,
      "learning_rate": 0.0002666666666666666,
      "loss": 0.2404,
      "step": 854
    },
    {
      "epoch": 1.9988297249853715,
      "eval_loss": 0.25982460379600525,
      "eval_runtime": 128.3395,
      "eval_samples_per_second": 4.301,
      "eval_steps_per_second": 0.538,
      "step": 854
    },
    {
      "epoch": 2.0011702750146285,
      "grad_norm": 0.35930198431015015,
      "learning_rate": 0.0002665886026541764,
      "loss": 0.2321,
      "step": 855
    },
    {
      "epoch": 2.003510825043885,
      "grad_norm": 0.284156858921051,
      "learning_rate": 0.0002665105386416862,
      "loss": 0.1523,
      "step": 856
    },
    {
      "epoch": 2.005851375073142,
      "grad_norm": 0.29822221398353577,
      "learning_rate": 0.0002664324746291959,
      "loss": 0.1766,
      "step": 857
    },
    {
      "epoch": 2.008191925102399,
      "grad_norm": 0.33128851652145386,
      "learning_rate": 0.0002663544106167057,
      "loss": 0.1854,
      "step": 858
    },
    {
      "epoch": 2.010532475131656,
      "grad_norm": 0.39603567123413086,
      "learning_rate": 0.0002662763466042154,
      "loss": 0.2007,
      "step": 859
    },
    {
      "epoch": 2.012873025160913,
      "grad_norm": 0.24690014123916626,
      "learning_rate": 0.0002661982825917252,
      "loss": 0.1801,
      "step": 860
    },
    {
      "epoch": 2.01521357519017,
      "grad_norm": 0.2733466923236847,
      "learning_rate": 0.00026612021857923496,
      "loss": 0.1691,
      "step": 861
    },
    {
      "epoch": 2.0175541252194265,
      "grad_norm": 0.2545069754123688,
      "learning_rate": 0.0002660421545667447,
      "loss": 0.1515,
      "step": 862
    },
    {
      "epoch": 2.0198946752486835,
      "grad_norm": 0.21923018991947174,
      "learning_rate": 0.00026596409055425446,
      "loss": 0.1032,
      "step": 863
    },
    {
      "epoch": 2.0222352252779405,
      "grad_norm": 0.3391037583351135,
      "learning_rate": 0.00026588602654176424,
      "loss": 0.1872,
      "step": 864
    },
    {
      "epoch": 2.024575775307197,
      "grad_norm": 0.27064675092697144,
      "learning_rate": 0.00026580796252927396,
      "loss": 0.1456,
      "step": 865
    },
    {
      "epoch": 2.026916325336454,
      "grad_norm": 0.343076229095459,
      "learning_rate": 0.00026572989851678374,
      "loss": 0.2084,
      "step": 866
    },
    {
      "epoch": 2.0292568753657108,
      "grad_norm": 0.29053109884262085,
      "learning_rate": 0.0002656518345042935,
      "loss": 0.1615,
      "step": 867
    },
    {
      "epoch": 2.031597425394968,
      "grad_norm": 0.4002304971218109,
      "learning_rate": 0.00026557377049180324,
      "loss": 0.2279,
      "step": 868
    },
    {
      "epoch": 2.033937975424225,
      "grad_norm": 0.237699493765831,
      "learning_rate": 0.000265495706479313,
      "loss": 0.1444,
      "step": 869
    },
    {
      "epoch": 2.0362785254534814,
      "grad_norm": 0.24895817041397095,
      "learning_rate": 0.0002654176424668228,
      "loss": 0.1157,
      "step": 870
    },
    {
      "epoch": 2.0386190754827385,
      "grad_norm": 0.2813095450401306,
      "learning_rate": 0.0002653395784543325,
      "loss": 0.1647,
      "step": 871
    },
    {
      "epoch": 2.0409596255119955,
      "grad_norm": 0.39256879687309265,
      "learning_rate": 0.0002652615144418423,
      "loss": 0.2001,
      "step": 872
    },
    {
      "epoch": 2.043300175541252,
      "grad_norm": 0.3058397173881531,
      "learning_rate": 0.00026518345042935207,
      "loss": 0.1731,
      "step": 873
    },
    {
      "epoch": 2.045640725570509,
      "grad_norm": 0.32277634739875793,
      "learning_rate": 0.0002651053864168618,
      "loss": 0.1723,
      "step": 874
    },
    {
      "epoch": 2.0479812755997657,
      "grad_norm": 0.3552807867527008,
      "learning_rate": 0.0002650273224043715,
      "loss": 0.2332,
      "step": 875
    },
    {
      "epoch": 2.0503218256290228,
      "grad_norm": 0.3894519507884979,
      "learning_rate": 0.00026494925839188135,
      "loss": 0.1772,
      "step": 876
    },
    {
      "epoch": 2.05266237565828,
      "grad_norm": 0.33097225427627563,
      "learning_rate": 0.0002648711943793911,
      "loss": 0.1957,
      "step": 877
    },
    {
      "epoch": 2.0550029256875364,
      "grad_norm": 0.3254542052745819,
      "learning_rate": 0.00026479313036690085,
      "loss": 0.1982,
      "step": 878
    },
    {
      "epoch": 2.0573434757167934,
      "grad_norm": 0.27329766750335693,
      "learning_rate": 0.0002647150663544106,
      "loss": 0.1797,
      "step": 879
    },
    {
      "epoch": 2.0596840257460505,
      "grad_norm": 0.2808730900287628,
      "learning_rate": 0.00026463700234192035,
      "loss": 0.136,
      "step": 880
    },
    {
      "epoch": 2.062024575775307,
      "grad_norm": 0.24524900317192078,
      "learning_rate": 0.00026455893832943013,
      "loss": 0.1553,
      "step": 881
    },
    {
      "epoch": 2.064365125804564,
      "grad_norm": 0.3326427936553955,
      "learning_rate": 0.00026448087431693985,
      "loss": 0.2398,
      "step": 882
    },
    {
      "epoch": 2.066705675833821,
      "grad_norm": 0.3494943082332611,
      "learning_rate": 0.00026440281030444963,
      "loss": 0.1673,
      "step": 883
    },
    {
      "epoch": 2.0690462258630777,
      "grad_norm": 0.3896464705467224,
      "learning_rate": 0.0002643247462919594,
      "loss": 0.1973,
      "step": 884
    },
    {
      "epoch": 2.0713867758923348,
      "grad_norm": 0.353990763425827,
      "learning_rate": 0.00026424668227946913,
      "loss": 0.2253,
      "step": 885
    },
    {
      "epoch": 2.0737273259215914,
      "grad_norm": 0.30974099040031433,
      "learning_rate": 0.0002641686182669789,
      "loss": 0.2024,
      "step": 886
    },
    {
      "epoch": 2.0760678759508484,
      "grad_norm": 0.487555593252182,
      "learning_rate": 0.0002640905542544887,
      "loss": 0.2603,
      "step": 887
    },
    {
      "epoch": 2.0784084259801054,
      "grad_norm": 0.3303842842578888,
      "learning_rate": 0.0002640124902419984,
      "loss": 0.2087,
      "step": 888
    },
    {
      "epoch": 2.080748976009362,
      "grad_norm": 0.43161508440971375,
      "learning_rate": 0.0002639344262295082,
      "loss": 0.1641,
      "step": 889
    },
    {
      "epoch": 2.083089526038619,
      "grad_norm": 0.28939324617385864,
      "learning_rate": 0.00026385636221701797,
      "loss": 0.1675,
      "step": 890
    },
    {
      "epoch": 2.085430076067876,
      "grad_norm": 0.3865431547164917,
      "learning_rate": 0.0002637782982045277,
      "loss": 0.1799,
      "step": 891
    },
    {
      "epoch": 2.0877706260971327,
      "grad_norm": 0.3005649745464325,
      "learning_rate": 0.00026370023419203747,
      "loss": 0.1935,
      "step": 892
    },
    {
      "epoch": 2.0901111761263897,
      "grad_norm": 0.2903026044368744,
      "learning_rate": 0.00026362217017954724,
      "loss": 0.1923,
      "step": 893
    },
    {
      "epoch": 2.092451726155647,
      "grad_norm": 0.33777934312820435,
      "learning_rate": 0.00026354410616705697,
      "loss": 0.1908,
      "step": 894
    },
    {
      "epoch": 2.0947922761849034,
      "grad_norm": 0.34100642800331116,
      "learning_rate": 0.0002634660421545667,
      "loss": 0.1857,
      "step": 895
    },
    {
      "epoch": 2.0971328262141604,
      "grad_norm": 0.3157937228679657,
      "learning_rate": 0.00026338797814207647,
      "loss": 0.1768,
      "step": 896
    },
    {
      "epoch": 2.099473376243417,
      "grad_norm": 0.2552745044231415,
      "learning_rate": 0.00026330991412958625,
      "loss": 0.1458,
      "step": 897
    },
    {
      "epoch": 2.101813926272674,
      "grad_norm": 0.2589850425720215,
      "learning_rate": 0.00026323185011709597,
      "loss": 0.1804,
      "step": 898
    },
    {
      "epoch": 2.104154476301931,
      "grad_norm": 0.37693265080451965,
      "learning_rate": 0.00026315378610460575,
      "loss": 0.2153,
      "step": 899
    },
    {
      "epoch": 2.1064950263311877,
      "grad_norm": 0.3293280601501465,
      "learning_rate": 0.0002630757220921155,
      "loss": 0.1672,
      "step": 900
    },
    {
      "epoch": 2.1088355763604447,
      "grad_norm": 0.4657180607318878,
      "learning_rate": 0.00026299765807962525,
      "loss": 0.2377,
      "step": 901
    },
    {
      "epoch": 2.1111761263897018,
      "grad_norm": 0.5887523293495178,
      "learning_rate": 0.000262919594067135,
      "loss": 0.2516,
      "step": 902
    },
    {
      "epoch": 2.1135166764189584,
      "grad_norm": 0.3459973633289337,
      "learning_rate": 0.0002628415300546448,
      "loss": 0.1736,
      "step": 903
    },
    {
      "epoch": 2.1158572264482154,
      "grad_norm": 0.40329229831695557,
      "learning_rate": 0.0002627634660421545,
      "loss": 0.1796,
      "step": 904
    },
    {
      "epoch": 2.118197776477472,
      "grad_norm": 0.5319773554801941,
      "learning_rate": 0.0002626854020296643,
      "loss": 0.2329,
      "step": 905
    },
    {
      "epoch": 2.120538326506729,
      "grad_norm": 0.21299853920936584,
      "learning_rate": 0.0002626073380171741,
      "loss": 0.1477,
      "step": 906
    },
    {
      "epoch": 2.122878876535986,
      "grad_norm": 0.3222864270210266,
      "learning_rate": 0.0002625292740046838,
      "loss": 0.2176,
      "step": 907
    },
    {
      "epoch": 2.1252194265652427,
      "grad_norm": 0.31886565685272217,
      "learning_rate": 0.0002624512099921936,
      "loss": 0.1991,
      "step": 908
    },
    {
      "epoch": 2.1275599765944997,
      "grad_norm": 0.4253339171409607,
      "learning_rate": 0.00026237314597970336,
      "loss": 0.2022,
      "step": 909
    },
    {
      "epoch": 2.1299005266237567,
      "grad_norm": 0.35734981298446655,
      "learning_rate": 0.0002622950819672131,
      "loss": 0.1846,
      "step": 910
    },
    {
      "epoch": 2.1322410766530133,
      "grad_norm": 0.3316683769226074,
      "learning_rate": 0.00026221701795472286,
      "loss": 0.1972,
      "step": 911
    },
    {
      "epoch": 2.1345816266822704,
      "grad_norm": 0.3542555868625641,
      "learning_rate": 0.00026213895394223264,
      "loss": 0.2318,
      "step": 912
    },
    {
      "epoch": 2.1369221767115274,
      "grad_norm": 0.35247108340263367,
      "learning_rate": 0.00026206088992974236,
      "loss": 0.2015,
      "step": 913
    },
    {
      "epoch": 2.139262726740784,
      "grad_norm": 0.3015539050102234,
      "learning_rate": 0.00026198282591725214,
      "loss": 0.172,
      "step": 914
    },
    {
      "epoch": 2.141603276770041,
      "grad_norm": 0.3170155882835388,
      "learning_rate": 0.00026190476190476186,
      "loss": 0.2252,
      "step": 915
    },
    {
      "epoch": 2.1439438267992976,
      "grad_norm": 0.36161336302757263,
      "learning_rate": 0.00026182669789227164,
      "loss": 0.2298,
      "step": 916
    },
    {
      "epoch": 2.1462843768285547,
      "grad_norm": 0.311583936214447,
      "learning_rate": 0.0002617486338797814,
      "loss": 0.1878,
      "step": 917
    },
    {
      "epoch": 2.1486249268578117,
      "grad_norm": 0.3783981502056122,
      "learning_rate": 0.00026167056986729114,
      "loss": 0.2033,
      "step": 918
    },
    {
      "epoch": 2.1509654768870683,
      "grad_norm": 0.2851240038871765,
      "learning_rate": 0.0002615925058548009,
      "loss": 0.187,
      "step": 919
    },
    {
      "epoch": 2.1533060269163253,
      "grad_norm": 0.390820175409317,
      "learning_rate": 0.0002615144418423107,
      "loss": 0.2575,
      "step": 920
    },
    {
      "epoch": 2.1556465769455824,
      "grad_norm": 0.32015612721443176,
      "learning_rate": 0.0002614363778298204,
      "loss": 0.1964,
      "step": 921
    },
    {
      "epoch": 2.157987126974839,
      "grad_norm": 0.30993926525115967,
      "learning_rate": 0.0002613583138173302,
      "loss": 0.1948,
      "step": 922
    },
    {
      "epoch": 2.160327677004096,
      "grad_norm": 0.30915430188179016,
      "learning_rate": 0.00026128024980483997,
      "loss": 0.1812,
      "step": 923
    },
    {
      "epoch": 2.162668227033353,
      "grad_norm": 0.23894652724266052,
      "learning_rate": 0.0002612021857923497,
      "loss": 0.1205,
      "step": 924
    },
    {
      "epoch": 2.1650087770626096,
      "grad_norm": 0.29254150390625,
      "learning_rate": 0.0002611241217798595,
      "loss": 0.1849,
      "step": 925
    },
    {
      "epoch": 2.1673493270918667,
      "grad_norm": 0.4023430049419403,
      "learning_rate": 0.00026104605776736925,
      "loss": 0.1942,
      "step": 926
    },
    {
      "epoch": 2.1696898771211233,
      "grad_norm": 0.21995021402835846,
      "learning_rate": 0.000260967993754879,
      "loss": 0.1111,
      "step": 927
    },
    {
      "epoch": 2.1720304271503803,
      "grad_norm": 0.3116229772567749,
      "learning_rate": 0.00026088992974238875,
      "loss": 0.2059,
      "step": 928
    },
    {
      "epoch": 2.1743709771796373,
      "grad_norm": 0.2945775091648102,
      "learning_rate": 0.00026081186572989853,
      "loss": 0.161,
      "step": 929
    },
    {
      "epoch": 2.176711527208894,
      "grad_norm": 0.2530854344367981,
      "learning_rate": 0.00026073380171740825,
      "loss": 0.1422,
      "step": 930
    },
    {
      "epoch": 2.179052077238151,
      "grad_norm": 0.4264177978038788,
      "learning_rate": 0.000260655737704918,
      "loss": 0.2205,
      "step": 931
    },
    {
      "epoch": 2.181392627267408,
      "grad_norm": 0.38003894686698914,
      "learning_rate": 0.00026057767369242775,
      "loss": 0.2318,
      "step": 932
    },
    {
      "epoch": 2.1837331772966646,
      "grad_norm": 0.28656747937202454,
      "learning_rate": 0.00026049960967993753,
      "loss": 0.1226,
      "step": 933
    },
    {
      "epoch": 2.1860737273259216,
      "grad_norm": 0.30607229471206665,
      "learning_rate": 0.00026042154566744725,
      "loss": 0.1607,
      "step": 934
    },
    {
      "epoch": 2.1884142773551787,
      "grad_norm": 0.2908457815647125,
      "learning_rate": 0.00026034348165495703,
      "loss": 0.1654,
      "step": 935
    },
    {
      "epoch": 2.1907548273844353,
      "grad_norm": 0.35889244079589844,
      "learning_rate": 0.0002602654176424668,
      "loss": 0.1611,
      "step": 936
    },
    {
      "epoch": 2.1930953774136923,
      "grad_norm": 0.2762083411216736,
      "learning_rate": 0.00026018735362997653,
      "loss": 0.1531,
      "step": 937
    },
    {
      "epoch": 2.195435927442949,
      "grad_norm": 0.3740366995334625,
      "learning_rate": 0.0002601092896174863,
      "loss": 0.2107,
      "step": 938
    },
    {
      "epoch": 2.197776477472206,
      "grad_norm": 0.2865069806575775,
      "learning_rate": 0.0002600312256049961,
      "loss": 0.1635,
      "step": 939
    },
    {
      "epoch": 2.200117027501463,
      "grad_norm": 0.2695469856262207,
      "learning_rate": 0.0002599531615925058,
      "loss": 0.1562,
      "step": 940
    },
    {
      "epoch": 2.2024575775307196,
      "grad_norm": 0.29390257596969604,
      "learning_rate": 0.0002598750975800156,
      "loss": 0.2018,
      "step": 941
    },
    {
      "epoch": 2.2047981275599766,
      "grad_norm": 0.25414687395095825,
      "learning_rate": 0.00025979703356752537,
      "loss": 0.1451,
      "step": 942
    },
    {
      "epoch": 2.2071386775892337,
      "grad_norm": 0.3665079176425934,
      "learning_rate": 0.0002597189695550351,
      "loss": 0.2087,
      "step": 943
    },
    {
      "epoch": 2.2094792276184902,
      "grad_norm": 0.282009482383728,
      "learning_rate": 0.00025964090554254487,
      "loss": 0.1386,
      "step": 944
    },
    {
      "epoch": 2.2118197776477473,
      "grad_norm": 0.29750314354896545,
      "learning_rate": 0.00025956284153005464,
      "loss": 0.1653,
      "step": 945
    },
    {
      "epoch": 2.2141603276770043,
      "grad_norm": 0.2950228750705719,
      "learning_rate": 0.00025948477751756437,
      "loss": 0.1842,
      "step": 946
    },
    {
      "epoch": 2.216500877706261,
      "grad_norm": 0.30982211232185364,
      "learning_rate": 0.00025940671350507415,
      "loss": 0.2156,
      "step": 947
    },
    {
      "epoch": 2.218841427735518,
      "grad_norm": 0.35334306955337524,
      "learning_rate": 0.00025932864949258387,
      "loss": 0.1971,
      "step": 948
    },
    {
      "epoch": 2.2211819777647746,
      "grad_norm": 0.32419082522392273,
      "learning_rate": 0.00025925058548009365,
      "loss": 0.229,
      "step": 949
    },
    {
      "epoch": 2.2235225277940316,
      "grad_norm": 0.25703251361846924,
      "learning_rate": 0.0002591725214676034,
      "loss": 0.1457,
      "step": 950
    },
    {
      "epoch": 2.2258630778232886,
      "grad_norm": 0.2659605145454407,
      "learning_rate": 0.00025909445745511315,
      "loss": 0.1559,
      "step": 951
    },
    {
      "epoch": 2.228203627852545,
      "grad_norm": 0.3644069731235504,
      "learning_rate": 0.0002590163934426229,
      "loss": 0.2447,
      "step": 952
    },
    {
      "epoch": 2.2305441778818023,
      "grad_norm": 0.26070424914360046,
      "learning_rate": 0.0002589383294301327,
      "loss": 0.1534,
      "step": 953
    },
    {
      "epoch": 2.2328847279110593,
      "grad_norm": 0.3367753326892853,
      "learning_rate": 0.0002588602654176424,
      "loss": 0.198,
      "step": 954
    },
    {
      "epoch": 2.235225277940316,
      "grad_norm": 0.29483383893966675,
      "learning_rate": 0.0002587822014051522,
      "loss": 0.1451,
      "step": 955
    },
    {
      "epoch": 2.237565827969573,
      "grad_norm": 0.27977609634399414,
      "learning_rate": 0.000258704137392662,
      "loss": 0.1576,
      "step": 956
    },
    {
      "epoch": 2.2399063779988295,
      "grad_norm": 0.34729617834091187,
      "learning_rate": 0.0002586260733801717,
      "loss": 0.1953,
      "step": 957
    },
    {
      "epoch": 2.2422469280280866,
      "grad_norm": 0.5001634359359741,
      "learning_rate": 0.0002585480093676815,
      "loss": 0.2296,
      "step": 958
    },
    {
      "epoch": 2.2445874780573436,
      "grad_norm": 0.38090619444847107,
      "learning_rate": 0.00025846994535519126,
      "loss": 0.2133,
      "step": 959
    },
    {
      "epoch": 2.2469280280866,
      "grad_norm": 0.35359546542167664,
      "learning_rate": 0.000258391881342701,
      "loss": 0.1951,
      "step": 960
    },
    {
      "epoch": 2.2492685781158572,
      "grad_norm": 0.33407682180404663,
      "learning_rate": 0.00025831381733021076,
      "loss": 0.1468,
      "step": 961
    },
    {
      "epoch": 2.2516091281451143,
      "grad_norm": 0.28716275095939636,
      "learning_rate": 0.00025823575331772054,
      "loss": 0.1805,
      "step": 962
    },
    {
      "epoch": 2.253949678174371,
      "grad_norm": 0.32937175035476685,
      "learning_rate": 0.00025815768930523026,
      "loss": 0.1713,
      "step": 963
    },
    {
      "epoch": 2.256290228203628,
      "grad_norm": 0.27033665776252747,
      "learning_rate": 0.00025807962529274004,
      "loss": 0.1366,
      "step": 964
    },
    {
      "epoch": 2.2586307782328845,
      "grad_norm": 0.24602074921131134,
      "learning_rate": 0.0002580015612802498,
      "loss": 0.1375,
      "step": 965
    },
    {
      "epoch": 2.2609713282621415,
      "grad_norm": 0.27123159170150757,
      "learning_rate": 0.00025792349726775954,
      "loss": 0.1283,
      "step": 966
    },
    {
      "epoch": 2.2633118782913986,
      "grad_norm": 0.25450998544692993,
      "learning_rate": 0.00025784543325526926,
      "loss": 0.1429,
      "step": 967
    },
    {
      "epoch": 2.265652428320655,
      "grad_norm": 0.29490840435028076,
      "learning_rate": 0.00025776736924277904,
      "loss": 0.1832,
      "step": 968
    },
    {
      "epoch": 2.267992978349912,
      "grad_norm": 0.30212390422821045,
      "learning_rate": 0.0002576893052302888,
      "loss": 0.151,
      "step": 969
    },
    {
      "epoch": 2.2703335283791692,
      "grad_norm": 0.38663914799690247,
      "learning_rate": 0.00025761124121779854,
      "loss": 0.1848,
      "step": 970
    },
    {
      "epoch": 2.272674078408426,
      "grad_norm": 0.6142393946647644,
      "learning_rate": 0.0002575331772053083,
      "loss": 0.1181,
      "step": 971
    },
    {
      "epoch": 2.275014628437683,
      "grad_norm": 0.46846699714660645,
      "learning_rate": 0.0002574551131928181,
      "loss": 0.2683,
      "step": 972
    },
    {
      "epoch": 2.27735517846694,
      "grad_norm": 0.46226489543914795,
      "learning_rate": 0.0002573770491803278,
      "loss": 0.1861,
      "step": 973
    },
    {
      "epoch": 2.2796957284961965,
      "grad_norm": 0.2559327483177185,
      "learning_rate": 0.0002572989851678376,
      "loss": 0.1781,
      "step": 974
    },
    {
      "epoch": 2.2820362785254535,
      "grad_norm": 0.27663925290107727,
      "learning_rate": 0.0002572209211553474,
      "loss": 0.1299,
      "step": 975
    },
    {
      "epoch": 2.28437682855471,
      "grad_norm": 0.34591415524482727,
      "learning_rate": 0.0002571428571428571,
      "loss": 0.1882,
      "step": 976
    },
    {
      "epoch": 2.286717378583967,
      "grad_norm": 0.5534387230873108,
      "learning_rate": 0.0002570647931303669,
      "loss": 0.1731,
      "step": 977
    },
    {
      "epoch": 2.289057928613224,
      "grad_norm": 0.2957712411880493,
      "learning_rate": 0.00025698672911787665,
      "loss": 0.1423,
      "step": 978
    },
    {
      "epoch": 2.291398478642481,
      "grad_norm": 0.4508711099624634,
      "learning_rate": 0.0002569086651053864,
      "loss": 0.1306,
      "step": 979
    },
    {
      "epoch": 2.293739028671738,
      "grad_norm": 0.30748358368873596,
      "learning_rate": 0.00025683060109289615,
      "loss": 0.159,
      "step": 980
    },
    {
      "epoch": 2.296079578700995,
      "grad_norm": 0.39881977438926697,
      "learning_rate": 0.00025675253708040593,
      "loss": 0.2288,
      "step": 981
    },
    {
      "epoch": 2.2984201287302515,
      "grad_norm": 0.35908281803131104,
      "learning_rate": 0.00025667447306791565,
      "loss": 0.1994,
      "step": 982
    },
    {
      "epoch": 2.3007606787595085,
      "grad_norm": 0.36440855264663696,
      "learning_rate": 0.00025659640905542543,
      "loss": 0.1974,
      "step": 983
    },
    {
      "epoch": 2.3031012287887656,
      "grad_norm": 0.2769385874271393,
      "learning_rate": 0.00025651834504293515,
      "loss": 0.183,
      "step": 984
    },
    {
      "epoch": 2.305441778818022,
      "grad_norm": 0.45600852370262146,
      "learning_rate": 0.00025644028103044493,
      "loss": 0.2171,
      "step": 985
    },
    {
      "epoch": 2.307782328847279,
      "grad_norm": 0.3253774344921112,
      "learning_rate": 0.0002563622170179547,
      "loss": 0.1902,
      "step": 986
    },
    {
      "epoch": 2.310122878876536,
      "grad_norm": 0.39331552386283875,
      "learning_rate": 0.00025628415300546443,
      "loss": 0.2271,
      "step": 987
    },
    {
      "epoch": 2.312463428905793,
      "grad_norm": 0.4314499497413635,
      "learning_rate": 0.0002562060889929742,
      "loss": 0.2405,
      "step": 988
    },
    {
      "epoch": 2.31480397893505,
      "grad_norm": 0.30237463116645813,
      "learning_rate": 0.000256128024980484,
      "loss": 0.1511,
      "step": 989
    },
    {
      "epoch": 2.3171445289643064,
      "grad_norm": 0.3237968385219574,
      "learning_rate": 0.0002560499609679937,
      "loss": 0.1841,
      "step": 990
    },
    {
      "epoch": 2.3194850789935635,
      "grad_norm": 0.2572801113128662,
      "learning_rate": 0.0002559718969555035,
      "loss": 0.1531,
      "step": 991
    },
    {
      "epoch": 2.3218256290228205,
      "grad_norm": 0.262531042098999,
      "learning_rate": 0.00025589383294301327,
      "loss": 0.1448,
      "step": 992
    },
    {
      "epoch": 2.324166179052077,
      "grad_norm": 0.34187382459640503,
      "learning_rate": 0.000255815768930523,
      "loss": 0.1823,
      "step": 993
    },
    {
      "epoch": 2.326506729081334,
      "grad_norm": 0.26519113779067993,
      "learning_rate": 0.00025573770491803277,
      "loss": 0.1364,
      "step": 994
    },
    {
      "epoch": 2.328847279110591,
      "grad_norm": 0.47961804270744324,
      "learning_rate": 0.00025565964090554254,
      "loss": 0.2198,
      "step": 995
    },
    {
      "epoch": 2.331187829139848,
      "grad_norm": 0.33145672082901,
      "learning_rate": 0.00025558157689305227,
      "loss": 0.1708,
      "step": 996
    },
    {
      "epoch": 2.333528379169105,
      "grad_norm": 0.38540777564048767,
      "learning_rate": 0.00025550351288056205,
      "loss": 0.2028,
      "step": 997
    },
    {
      "epoch": 2.3358689291983614,
      "grad_norm": 0.41692590713500977,
      "learning_rate": 0.0002554254488680718,
      "loss": 0.2128,
      "step": 998
    },
    {
      "epoch": 2.3382094792276185,
      "grad_norm": 0.27599722146987915,
      "learning_rate": 0.00025534738485558155,
      "loss": 0.1085,
      "step": 999
    },
    {
      "epoch": 2.3405500292568755,
      "grad_norm": 0.42330029606819153,
      "learning_rate": 0.00025526932084309127,
      "loss": 0.2271,
      "step": 1000
    },
    {
      "epoch": 2.342890579286132,
      "grad_norm": 0.41872140765190125,
      "learning_rate": 0.0002551912568306011,
      "loss": 0.1966,
      "step": 1001
    },
    {
      "epoch": 2.345231129315389,
      "grad_norm": 0.3623790442943573,
      "learning_rate": 0.0002551131928181108,
      "loss": 0.1636,
      "step": 1002
    },
    {
      "epoch": 2.347571679344646,
      "grad_norm": 0.31502214074134827,
      "learning_rate": 0.00025503512880562055,
      "loss": 0.1745,
      "step": 1003
    },
    {
      "epoch": 2.3499122293739028,
      "grad_norm": 0.31948551535606384,
      "learning_rate": 0.0002549570647931303,
      "loss": 0.1754,
      "step": 1004
    },
    {
      "epoch": 2.35225277940316,
      "grad_norm": 0.2717357277870178,
      "learning_rate": 0.0002548790007806401,
      "loss": 0.1376,
      "step": 1005
    },
    {
      "epoch": 2.354593329432417,
      "grad_norm": 0.34872356057167053,
      "learning_rate": 0.0002548009367681499,
      "loss": 0.2133,
      "step": 1006
    },
    {
      "epoch": 2.3569338794616734,
      "grad_norm": 0.2933010160923004,
      "learning_rate": 0.0002547228727556596,
      "loss": 0.2021,
      "step": 1007
    },
    {
      "epoch": 2.3592744294909305,
      "grad_norm": 0.23830200731754303,
      "learning_rate": 0.0002546448087431694,
      "loss": 0.1032,
      "step": 1008
    },
    {
      "epoch": 2.361614979520187,
      "grad_norm": 0.3240378201007843,
      "learning_rate": 0.00025456674473067916,
      "loss": 0.1368,
      "step": 1009
    },
    {
      "epoch": 2.363955529549444,
      "grad_norm": 0.3210214674472809,
      "learning_rate": 0.0002544886807181889,
      "loss": 0.2023,
      "step": 1010
    },
    {
      "epoch": 2.366296079578701,
      "grad_norm": 0.28683310747146606,
      "learning_rate": 0.00025441061670569866,
      "loss": 0.1688,
      "step": 1011
    },
    {
      "epoch": 2.3686366296079577,
      "grad_norm": 0.2714800536632538,
      "learning_rate": 0.00025433255269320844,
      "loss": 0.1591,
      "step": 1012
    },
    {
      "epoch": 2.3709771796372148,
      "grad_norm": 0.3413642644882202,
      "learning_rate": 0.00025425448868071816,
      "loss": 0.1592,
      "step": 1013
    },
    {
      "epoch": 2.373317729666472,
      "grad_norm": 0.3428962528705597,
      "learning_rate": 0.00025417642466822794,
      "loss": 0.1635,
      "step": 1014
    },
    {
      "epoch": 2.3756582796957284,
      "grad_norm": 0.37474268674850464,
      "learning_rate": 0.0002540983606557377,
      "loss": 0.1909,
      "step": 1015
    },
    {
      "epoch": 2.3779988297249854,
      "grad_norm": 0.33789148926734924,
      "learning_rate": 0.00025402029664324744,
      "loss": 0.167,
      "step": 1016
    },
    {
      "epoch": 2.3803393797542425,
      "grad_norm": 0.33303654193878174,
      "learning_rate": 0.0002539422326307572,
      "loss": 0.2201,
      "step": 1017
    },
    {
      "epoch": 2.382679929783499,
      "grad_norm": 0.35934996604919434,
      "learning_rate": 0.000253864168618267,
      "loss": 0.1985,
      "step": 1018
    },
    {
      "epoch": 2.385020479812756,
      "grad_norm": 0.2893749475479126,
      "learning_rate": 0.0002537861046057767,
      "loss": 0.2009,
      "step": 1019
    },
    {
      "epoch": 2.3873610298420127,
      "grad_norm": 0.34878137707710266,
      "learning_rate": 0.00025370804059328644,
      "loss": 0.2146,
      "step": 1020
    },
    {
      "epoch": 2.3897015798712697,
      "grad_norm": 0.3704708516597748,
      "learning_rate": 0.00025362997658079627,
      "loss": 0.2278,
      "step": 1021
    },
    {
      "epoch": 2.392042129900527,
      "grad_norm": 0.312545508146286,
      "learning_rate": 0.000253551912568306,
      "loss": 0.1771,
      "step": 1022
    },
    {
      "epoch": 2.3943826799297834,
      "grad_norm": 0.23196564614772797,
      "learning_rate": 0.0002534738485558157,
      "loss": 0.1171,
      "step": 1023
    },
    {
      "epoch": 2.3967232299590404,
      "grad_norm": 0.34735921025276184,
      "learning_rate": 0.0002533957845433255,
      "loss": 0.2033,
      "step": 1024
    },
    {
      "epoch": 2.399063779988297,
      "grad_norm": 0.38568371534347534,
      "learning_rate": 0.0002533177205308353,
      "loss": 0.2219,
      "step": 1025
    },
    {
      "epoch": 2.401404330017554,
      "grad_norm": 0.2922919988632202,
      "learning_rate": 0.000253239656518345,
      "loss": 0.1916,
      "step": 1026
    },
    {
      "epoch": 2.403744880046811,
      "grad_norm": 0.2924385070800781,
      "learning_rate": 0.0002531615925058548,
      "loss": 0.1628,
      "step": 1027
    },
    {
      "epoch": 2.406085430076068,
      "grad_norm": 0.3068244755268097,
      "learning_rate": 0.00025308352849336455,
      "loss": 0.1657,
      "step": 1028
    },
    {
      "epoch": 2.4084259801053247,
      "grad_norm": 0.2510543763637543,
      "learning_rate": 0.0002530054644808743,
      "loss": 0.1423,
      "step": 1029
    },
    {
      "epoch": 2.4107665301345818,
      "grad_norm": 0.29700809717178345,
      "learning_rate": 0.00025292740046838405,
      "loss": 0.1593,
      "step": 1030
    },
    {
      "epoch": 2.4131070801638383,
      "grad_norm": 0.31590473651885986,
      "learning_rate": 0.00025284933645589383,
      "loss": 0.2233,
      "step": 1031
    },
    {
      "epoch": 2.4154476301930954,
      "grad_norm": 0.27875232696533203,
      "learning_rate": 0.00025277127244340355,
      "loss": 0.1421,
      "step": 1032
    },
    {
      "epoch": 2.4177881802223524,
      "grad_norm": 0.33961594104766846,
      "learning_rate": 0.00025269320843091333,
      "loss": 0.1836,
      "step": 1033
    },
    {
      "epoch": 2.420128730251609,
      "grad_norm": 0.2448531687259674,
      "learning_rate": 0.0002526151444184231,
      "loss": 0.1546,
      "step": 1034
    },
    {
      "epoch": 2.422469280280866,
      "grad_norm": 0.33007022738456726,
      "learning_rate": 0.00025253708040593283,
      "loss": 0.1838,
      "step": 1035
    },
    {
      "epoch": 2.4248098303101226,
      "grad_norm": 0.362962007522583,
      "learning_rate": 0.0002524590163934426,
      "loss": 0.181,
      "step": 1036
    },
    {
      "epoch": 2.4271503803393797,
      "grad_norm": 0.4905785918235779,
      "learning_rate": 0.0002523809523809524,
      "loss": 0.1837,
      "step": 1037
    },
    {
      "epoch": 2.4294909303686367,
      "grad_norm": 0.3108205199241638,
      "learning_rate": 0.0002523028883684621,
      "loss": 0.1702,
      "step": 1038
    },
    {
      "epoch": 2.4318314803978933,
      "grad_norm": 0.3578597903251648,
      "learning_rate": 0.0002522248243559719,
      "loss": 0.2015,
      "step": 1039
    },
    {
      "epoch": 2.4341720304271504,
      "grad_norm": 0.3598422110080719,
      "learning_rate": 0.0002521467603434816,
      "loss": 0.2005,
      "step": 1040
    },
    {
      "epoch": 2.4365125804564074,
      "grad_norm": 0.3449280261993408,
      "learning_rate": 0.0002520686963309914,
      "loss": 0.1862,
      "step": 1041
    },
    {
      "epoch": 2.438853130485664,
      "grad_norm": 0.3640083074569702,
      "learning_rate": 0.00025199063231850117,
      "loss": 0.1615,
      "step": 1042
    },
    {
      "epoch": 2.441193680514921,
      "grad_norm": 0.3052462041378021,
      "learning_rate": 0.0002519125683060109,
      "loss": 0.1732,
      "step": 1043
    },
    {
      "epoch": 2.443534230544178,
      "grad_norm": 0.3540273606777191,
      "learning_rate": 0.00025183450429352067,
      "loss": 0.132,
      "step": 1044
    },
    {
      "epoch": 2.4458747805734347,
      "grad_norm": 0.36813119053840637,
      "learning_rate": 0.00025175644028103044,
      "loss": 0.1789,
      "step": 1045
    },
    {
      "epoch": 2.4482153306026917,
      "grad_norm": 0.29302486777305603,
      "learning_rate": 0.00025167837626854017,
      "loss": 0.1275,
      "step": 1046
    },
    {
      "epoch": 2.4505558806319483,
      "grad_norm": 0.3481680452823639,
      "learning_rate": 0.00025160031225604995,
      "loss": 0.1528,
      "step": 1047
    },
    {
      "epoch": 2.4528964306612053,
      "grad_norm": 0.38007792830467224,
      "learning_rate": 0.0002515222482435597,
      "loss": 0.1834,
      "step": 1048
    },
    {
      "epoch": 2.4552369806904624,
      "grad_norm": 0.291143536567688,
      "learning_rate": 0.00025144418423106945,
      "loss": 0.179,
      "step": 1049
    },
    {
      "epoch": 2.457577530719719,
      "grad_norm": 0.3532809615135193,
      "learning_rate": 0.0002513661202185792,
      "loss": 0.1906,
      "step": 1050
    },
    {
      "epoch": 2.459918080748976,
      "grad_norm": 0.4503769874572754,
      "learning_rate": 0.000251288056206089,
      "loss": 0.1814,
      "step": 1051
    },
    {
      "epoch": 2.462258630778233,
      "grad_norm": 0.292989045381546,
      "learning_rate": 0.0002512099921935987,
      "loss": 0.1872,
      "step": 1052
    },
    {
      "epoch": 2.4645991808074896,
      "grad_norm": 0.3393923342227936,
      "learning_rate": 0.0002511319281811085,
      "loss": 0.198,
      "step": 1053
    },
    {
      "epoch": 2.4669397308367467,
      "grad_norm": 0.2732589840888977,
      "learning_rate": 0.0002510538641686183,
      "loss": 0.1606,
      "step": 1054
    },
    {
      "epoch": 2.4692802808660037,
      "grad_norm": 0.2802025079727173,
      "learning_rate": 0.000250975800156128,
      "loss": 0.1618,
      "step": 1055
    },
    {
      "epoch": 2.4716208308952603,
      "grad_norm": 0.28349030017852783,
      "learning_rate": 0.0002508977361436377,
      "loss": 0.1502,
      "step": 1056
    },
    {
      "epoch": 2.4739613809245173,
      "grad_norm": 0.33708974719047546,
      "learning_rate": 0.00025081967213114756,
      "loss": 0.1764,
      "step": 1057
    },
    {
      "epoch": 2.476301930953774,
      "grad_norm": 0.24545074999332428,
      "learning_rate": 0.0002507416081186573,
      "loss": 0.1192,
      "step": 1058
    },
    {
      "epoch": 2.478642480983031,
      "grad_norm": 0.29222941398620605,
      "learning_rate": 0.000250663544106167,
      "loss": 0.1688,
      "step": 1059
    },
    {
      "epoch": 2.480983031012288,
      "grad_norm": 0.3238208591938019,
      "learning_rate": 0.0002505854800936768,
      "loss": 0.1797,
      "step": 1060
    },
    {
      "epoch": 2.4833235810415446,
      "grad_norm": 0.25987496972084045,
      "learning_rate": 0.00025050741608118656,
      "loss": 0.1374,
      "step": 1061
    },
    {
      "epoch": 2.4856641310708016,
      "grad_norm": 0.25617507100105286,
      "learning_rate": 0.0002504293520686963,
      "loss": 0.1481,
      "step": 1062
    },
    {
      "epoch": 2.4880046811000587,
      "grad_norm": 0.36278581619262695,
      "learning_rate": 0.00025035128805620606,
      "loss": 0.2061,
      "step": 1063
    },
    {
      "epoch": 2.4903452311293153,
      "grad_norm": 0.3689759373664856,
      "learning_rate": 0.00025027322404371584,
      "loss": 0.2109,
      "step": 1064
    },
    {
      "epoch": 2.4926857811585723,
      "grad_norm": 0.3412160873413086,
      "learning_rate": 0.00025019516003122556,
      "loss": 0.1844,
      "step": 1065
    },
    {
      "epoch": 2.4950263311878293,
      "grad_norm": 0.26054978370666504,
      "learning_rate": 0.00025011709601873534,
      "loss": 0.1361,
      "step": 1066
    },
    {
      "epoch": 2.497366881217086,
      "grad_norm": 0.3261890709400177,
      "learning_rate": 0.0002500390320062451,
      "loss": 0.168,
      "step": 1067
    },
    {
      "epoch": 2.499707431246343,
      "grad_norm": 0.32310187816619873,
      "learning_rate": 0.00024996096799375484,
      "loss": 0.1631,
      "step": 1068
    },
    {
      "epoch": 2.5020479812755996,
      "grad_norm": 0.294645220041275,
      "learning_rate": 0.0002498829039812646,
      "loss": 0.1541,
      "step": 1069
    },
    {
      "epoch": 2.5043885313048566,
      "grad_norm": 0.3545045852661133,
      "learning_rate": 0.0002498048399687744,
      "loss": 0.1981,
      "step": 1070
    },
    {
      "epoch": 2.5067290813341137,
      "grad_norm": 0.4050980806350708,
      "learning_rate": 0.0002497267759562841,
      "loss": 0.2257,
      "step": 1071
    },
    {
      "epoch": 2.5090696313633702,
      "grad_norm": 0.36340340971946716,
      "learning_rate": 0.0002496487119437939,
      "loss": 0.1932,
      "step": 1072
    },
    {
      "epoch": 2.5114101813926273,
      "grad_norm": 0.37429964542388916,
      "learning_rate": 0.0002495706479313037,
      "loss": 0.1645,
      "step": 1073
    },
    {
      "epoch": 2.513750731421884,
      "grad_norm": 0.3375966250896454,
      "learning_rate": 0.0002494925839188134,
      "loss": 0.1867,
      "step": 1074
    },
    {
      "epoch": 2.516091281451141,
      "grad_norm": 0.3426850736141205,
      "learning_rate": 0.0002494145199063232,
      "loss": 0.1864,
      "step": 1075
    },
    {
      "epoch": 2.518431831480398,
      "grad_norm": 0.34060412645339966,
      "learning_rate": 0.0002493364558938329,
      "loss": 0.1714,
      "step": 1076
    },
    {
      "epoch": 2.520772381509655,
      "grad_norm": 0.23888568580150604,
      "learning_rate": 0.0002492583918813427,
      "loss": 0.1203,
      "step": 1077
    },
    {
      "epoch": 2.5231129315389116,
      "grad_norm": 0.36685532331466675,
      "learning_rate": 0.00024918032786885245,
      "loss": 0.1636,
      "step": 1078
    },
    {
      "epoch": 2.5254534815681686,
      "grad_norm": 0.3963756263256073,
      "learning_rate": 0.0002491022638563622,
      "loss": 0.2304,
      "step": 1079
    },
    {
      "epoch": 2.527794031597425,
      "grad_norm": 0.30992138385772705,
      "learning_rate": 0.00024902419984387195,
      "loss": 0.1848,
      "step": 1080
    },
    {
      "epoch": 2.5301345816266823,
      "grad_norm": 0.3426603674888611,
      "learning_rate": 0.00024894613583138173,
      "loss": 0.1859,
      "step": 1081
    },
    {
      "epoch": 2.5324751316559393,
      "grad_norm": 0.27507010102272034,
      "learning_rate": 0.00024886807181889145,
      "loss": 0.1612,
      "step": 1082
    },
    {
      "epoch": 2.534815681685196,
      "grad_norm": 0.2805071175098419,
      "learning_rate": 0.00024879000780640123,
      "loss": 0.1624,
      "step": 1083
    },
    {
      "epoch": 2.537156231714453,
      "grad_norm": 0.3359098732471466,
      "learning_rate": 0.000248711943793911,
      "loss": 0.2066,
      "step": 1084
    },
    {
      "epoch": 2.5394967817437095,
      "grad_norm": 0.28875651955604553,
      "learning_rate": 0.00024863387978142073,
      "loss": 0.1707,
      "step": 1085
    },
    {
      "epoch": 2.5418373317729666,
      "grad_norm": 0.271894633769989,
      "learning_rate": 0.0002485558157689305,
      "loss": 0.1602,
      "step": 1086
    },
    {
      "epoch": 2.5441778818022236,
      "grad_norm": 0.3575250208377838,
      "learning_rate": 0.0002484777517564403,
      "loss": 0.2002,
      "step": 1087
    },
    {
      "epoch": 2.5465184318314806,
      "grad_norm": 0.3344385027885437,
      "learning_rate": 0.00024839968774395,
      "loss": 0.1483,
      "step": 1088
    },
    {
      "epoch": 2.5488589818607372,
      "grad_norm": 0.2102763056755066,
      "learning_rate": 0.0002483216237314598,
      "loss": 0.1016,
      "step": 1089
    },
    {
      "epoch": 2.5511995318899943,
      "grad_norm": 0.36514154076576233,
      "learning_rate": 0.00024824355971896957,
      "loss": 0.1763,
      "step": 1090
    },
    {
      "epoch": 2.553540081919251,
      "grad_norm": 0.3666400909423828,
      "learning_rate": 0.0002481654957064793,
      "loss": 0.2227,
      "step": 1091
    },
    {
      "epoch": 2.555880631948508,
      "grad_norm": 0.266109824180603,
      "learning_rate": 0.000248087431693989,
      "loss": 0.1408,
      "step": 1092
    },
    {
      "epoch": 2.558221181977765,
      "grad_norm": 0.3773782253265381,
      "learning_rate": 0.0002480093676814988,
      "loss": 0.1801,
      "step": 1093
    },
    {
      "epoch": 2.5605617320070215,
      "grad_norm": 0.4204925298690796,
      "learning_rate": 0.00024793130366900857,
      "loss": 0.174,
      "step": 1094
    },
    {
      "epoch": 2.5629022820362786,
      "grad_norm": 0.2774686813354492,
      "learning_rate": 0.0002478532396565183,
      "loss": 0.1312,
      "step": 1095
    },
    {
      "epoch": 2.565242832065535,
      "grad_norm": 0.28138449788093567,
      "learning_rate": 0.00024777517564402807,
      "loss": 0.1699,
      "step": 1096
    },
    {
      "epoch": 2.567583382094792,
      "grad_norm": 0.37330469489097595,
      "learning_rate": 0.00024769711163153785,
      "loss": 0.2007,
      "step": 1097
    },
    {
      "epoch": 2.5699239321240492,
      "grad_norm": 0.30042827129364014,
      "learning_rate": 0.00024761904761904757,
      "loss": 0.1354,
      "step": 1098
    },
    {
      "epoch": 2.5722644821533063,
      "grad_norm": 0.2575162351131439,
      "learning_rate": 0.00024754098360655735,
      "loss": 0.1594,
      "step": 1099
    },
    {
      "epoch": 2.574605032182563,
      "grad_norm": 0.3091842234134674,
      "learning_rate": 0.0002474629195940671,
      "loss": 0.1625,
      "step": 1100
    },
    {
      "epoch": 2.57694558221182,
      "grad_norm": 0.3681490421295166,
      "learning_rate": 0.00024738485558157685,
      "loss": 0.2403,
      "step": 1101
    },
    {
      "epoch": 2.5792861322410765,
      "grad_norm": 0.28288814425468445,
      "learning_rate": 0.0002473067915690866,
      "loss": 0.1511,
      "step": 1102
    },
    {
      "epoch": 2.5816266822703335,
      "grad_norm": 0.30052992701530457,
      "learning_rate": 0.0002472287275565964,
      "loss": 0.1724,
      "step": 1103
    },
    {
      "epoch": 2.5839672322995906,
      "grad_norm": 0.28216466307640076,
      "learning_rate": 0.0002471506635441061,
      "loss": 0.1614,
      "step": 1104
    },
    {
      "epoch": 2.586307782328847,
      "grad_norm": 0.41923439502716064,
      "learning_rate": 0.0002470725995316159,
      "loss": 0.213,
      "step": 1105
    },
    {
      "epoch": 2.588648332358104,
      "grad_norm": 0.22877167165279388,
      "learning_rate": 0.0002469945355191257,
      "loss": 0.1428,
      "step": 1106
    },
    {
      "epoch": 2.590988882387361,
      "grad_norm": 0.30152684450149536,
      "learning_rate": 0.0002469164715066354,
      "loss": 0.1666,
      "step": 1107
    },
    {
      "epoch": 2.593329432416618,
      "grad_norm": 0.2774517238140106,
      "learning_rate": 0.0002468384074941452,
      "loss": 0.1775,
      "step": 1108
    },
    {
      "epoch": 2.595669982445875,
      "grad_norm": 0.22371476888656616,
      "learning_rate": 0.0002467603434816549,
      "loss": 0.1322,
      "step": 1109
    },
    {
      "epoch": 2.598010532475132,
      "grad_norm": 0.35933688282966614,
      "learning_rate": 0.0002466822794691647,
      "loss": 0.1781,
      "step": 1110
    },
    {
      "epoch": 2.6003510825043885,
      "grad_norm": 0.3887648284435272,
      "learning_rate": 0.00024660421545667446,
      "loss": 0.189,
      "step": 1111
    },
    {
      "epoch": 2.6026916325336455,
      "grad_norm": 0.5264129638671875,
      "learning_rate": 0.0002465261514441842,
      "loss": 0.2432,
      "step": 1112
    },
    {
      "epoch": 2.605032182562902,
      "grad_norm": 0.3427891433238983,
      "learning_rate": 0.00024644808743169396,
      "loss": 0.1254,
      "step": 1113
    },
    {
      "epoch": 2.607372732592159,
      "grad_norm": 0.297514408826828,
      "learning_rate": 0.00024637002341920374,
      "loss": 0.1264,
      "step": 1114
    },
    {
      "epoch": 2.609713282621416,
      "grad_norm": 0.3443466126918793,
      "learning_rate": 0.00024629195940671346,
      "loss": 0.1713,
      "step": 1115
    },
    {
      "epoch": 2.612053832650673,
      "grad_norm": 0.31242835521698,
      "learning_rate": 0.00024621389539422324,
      "loss": 0.1413,
      "step": 1116
    },
    {
      "epoch": 2.61439438267993,
      "grad_norm": 0.29507747292518616,
      "learning_rate": 0.000246135831381733,
      "loss": 0.1844,
      "step": 1117
    },
    {
      "epoch": 2.6167349327091864,
      "grad_norm": 0.34667471051216125,
      "learning_rate": 0.00024605776736924274,
      "loss": 0.2015,
      "step": 1118
    },
    {
      "epoch": 2.6190754827384435,
      "grad_norm": 0.29507675766944885,
      "learning_rate": 0.0002459797033567525,
      "loss": 0.1523,
      "step": 1119
    },
    {
      "epoch": 2.6214160327677005,
      "grad_norm": 0.26178884506225586,
      "learning_rate": 0.0002459016393442623,
      "loss": 0.1535,
      "step": 1120
    },
    {
      "epoch": 2.6237565827969576,
      "grad_norm": 0.32938018441200256,
      "learning_rate": 0.000245823575331772,
      "loss": 0.1841,
      "step": 1121
    },
    {
      "epoch": 2.626097132826214,
      "grad_norm": 0.26314181089401245,
      "learning_rate": 0.0002457455113192818,
      "loss": 0.1553,
      "step": 1122
    },
    {
      "epoch": 2.628437682855471,
      "grad_norm": 0.35542944073677063,
      "learning_rate": 0.0002456674473067916,
      "loss": 0.2115,
      "step": 1123
    },
    {
      "epoch": 2.630778232884728,
      "grad_norm": 0.29191604256629944,
      "learning_rate": 0.0002455893832943013,
      "loss": 0.1757,
      "step": 1124
    },
    {
      "epoch": 2.633118782913985,
      "grad_norm": 0.3071920573711395,
      "learning_rate": 0.0002455113192818111,
      "loss": 0.1786,
      "step": 1125
    },
    {
      "epoch": 2.635459332943242,
      "grad_norm": 0.29151636362075806,
      "learning_rate": 0.00024543325526932085,
      "loss": 0.1828,
      "step": 1126
    },
    {
      "epoch": 2.6377998829724985,
      "grad_norm": 0.28377237915992737,
      "learning_rate": 0.0002453551912568306,
      "loss": 0.1492,
      "step": 1127
    },
    {
      "epoch": 2.6401404330017555,
      "grad_norm": 0.32077640295028687,
      "learning_rate": 0.0002452771272443403,
      "loss": 0.1565,
      "step": 1128
    },
    {
      "epoch": 2.642480983031012,
      "grad_norm": 0.2575078010559082,
      "learning_rate": 0.0002451990632318501,
      "loss": 0.155,
      "step": 1129
    },
    {
      "epoch": 2.644821533060269,
      "grad_norm": 0.30400991439819336,
      "learning_rate": 0.00024512099921935985,
      "loss": 0.1947,
      "step": 1130
    },
    {
      "epoch": 2.647162083089526,
      "grad_norm": 0.2448127269744873,
      "learning_rate": 0.0002450429352068696,
      "loss": 0.131,
      "step": 1131
    },
    {
      "epoch": 2.6495026331187828,
      "grad_norm": 0.34468525648117065,
      "learning_rate": 0.00024496487119437935,
      "loss": 0.1881,
      "step": 1132
    },
    {
      "epoch": 2.65184318314804,
      "grad_norm": 0.401347279548645,
      "learning_rate": 0.00024488680718188913,
      "loss": 0.2015,
      "step": 1133
    },
    {
      "epoch": 2.6541837331772964,
      "grad_norm": 0.252134770154953,
      "learning_rate": 0.0002448087431693989,
      "loss": 0.1668,
      "step": 1134
    },
    {
      "epoch": 2.6565242832065534,
      "grad_norm": 0.2752816677093506,
      "learning_rate": 0.00024473067915690863,
      "loss": 0.1784,
      "step": 1135
    },
    {
      "epoch": 2.6588648332358105,
      "grad_norm": 0.4072990417480469,
      "learning_rate": 0.0002446526151444184,
      "loss": 0.1816,
      "step": 1136
    },
    {
      "epoch": 2.6612053832650675,
      "grad_norm": 0.3685093820095062,
      "learning_rate": 0.0002445745511319282,
      "loss": 0.1835,
      "step": 1137
    },
    {
      "epoch": 2.663545933294324,
      "grad_norm": 0.34478917717933655,
      "learning_rate": 0.0002444964871194379,
      "loss": 0.1682,
      "step": 1138
    },
    {
      "epoch": 2.665886483323581,
      "grad_norm": 0.3048938512802124,
      "learning_rate": 0.0002444184231069477,
      "loss": 0.2062,
      "step": 1139
    },
    {
      "epoch": 2.6682270333528377,
      "grad_norm": 0.39869004487991333,
      "learning_rate": 0.00024434035909445747,
      "loss": 0.2133,
      "step": 1140
    },
    {
      "epoch": 2.6705675833820948,
      "grad_norm": 0.30423641204833984,
      "learning_rate": 0.0002442622950819672,
      "loss": 0.1709,
      "step": 1141
    },
    {
      "epoch": 2.672908133411352,
      "grad_norm": 0.31437093019485474,
      "learning_rate": 0.00024418423106947697,
      "loss": 0.1322,
      "step": 1142
    },
    {
      "epoch": 2.6752486834406084,
      "grad_norm": 0.3882395625114441,
      "learning_rate": 0.00024410616705698672,
      "loss": 0.2521,
      "step": 1143
    },
    {
      "epoch": 2.6775892334698654,
      "grad_norm": 0.27639830112457275,
      "learning_rate": 0.00024402810304449647,
      "loss": 0.151,
      "step": 1144
    },
    {
      "epoch": 2.679929783499122,
      "grad_norm": 0.3947092890739441,
      "learning_rate": 0.00024395003903200622,
      "loss": 0.2092,
      "step": 1145
    },
    {
      "epoch": 2.682270333528379,
      "grad_norm": 0.29292094707489014,
      "learning_rate": 0.000243871975019516,
      "loss": 0.1451,
      "step": 1146
    },
    {
      "epoch": 2.684610883557636,
      "grad_norm": 0.2532867193222046,
      "learning_rate": 0.00024379391100702575,
      "loss": 0.1254,
      "step": 1147
    },
    {
      "epoch": 2.686951433586893,
      "grad_norm": 0.3758658468723297,
      "learning_rate": 0.0002437158469945355,
      "loss": 0.1612,
      "step": 1148
    },
    {
      "epoch": 2.6892919836161497,
      "grad_norm": 0.3262855112552643,
      "learning_rate": 0.00024363778298204527,
      "loss": 0.1362,
      "step": 1149
    },
    {
      "epoch": 2.6916325336454068,
      "grad_norm": 0.4266234338283539,
      "learning_rate": 0.00024355971896955502,
      "loss": 0.1614,
      "step": 1150
    },
    {
      "epoch": 2.6939730836746634,
      "grad_norm": 0.3776741325855255,
      "learning_rate": 0.00024348165495706477,
      "loss": 0.1869,
      "step": 1151
    },
    {
      "epoch": 2.6963136337039204,
      "grad_norm": 0.4243743121623993,
      "learning_rate": 0.00024340359094457455,
      "loss": 0.1774,
      "step": 1152
    },
    {
      "epoch": 2.6986541837331774,
      "grad_norm": 0.389630526304245,
      "learning_rate": 0.0002433255269320843,
      "loss": 0.2067,
      "step": 1153
    },
    {
      "epoch": 2.700994733762434,
      "grad_norm": 0.35648396611213684,
      "learning_rate": 0.00024324746291959403,
      "loss": 0.2437,
      "step": 1154
    },
    {
      "epoch": 2.703335283791691,
      "grad_norm": 0.3803497850894928,
      "learning_rate": 0.00024316939890710383,
      "loss": 0.1977,
      "step": 1155
    },
    {
      "epoch": 2.7056758338209477,
      "grad_norm": 0.3112552762031555,
      "learning_rate": 0.00024309133489461355,
      "loss": 0.1732,
      "step": 1156
    },
    {
      "epoch": 2.7080163838502047,
      "grad_norm": 0.28750181198120117,
      "learning_rate": 0.0002430132708821233,
      "loss": 0.1585,
      "step": 1157
    },
    {
      "epoch": 2.7103569338794617,
      "grad_norm": 0.3551160991191864,
      "learning_rate": 0.00024293520686963308,
      "loss": 0.2017,
      "step": 1158
    },
    {
      "epoch": 2.712697483908719,
      "grad_norm": 0.3776438534259796,
      "learning_rate": 0.00024285714285714283,
      "loss": 0.1445,
      "step": 1159
    },
    {
      "epoch": 2.7150380339379754,
      "grad_norm": 0.4691866636276245,
      "learning_rate": 0.00024277907884465258,
      "loss": 0.2376,
      "step": 1160
    },
    {
      "epoch": 2.7173785839672324,
      "grad_norm": 0.3800452649593353,
      "learning_rate": 0.00024270101483216236,
      "loss": 0.1761,
      "step": 1161
    },
    {
      "epoch": 2.719719133996489,
      "grad_norm": 0.228168323636055,
      "learning_rate": 0.0002426229508196721,
      "loss": 0.1337,
      "step": 1162
    },
    {
      "epoch": 2.722059684025746,
      "grad_norm": 0.27528125047683716,
      "learning_rate": 0.00024254488680718186,
      "loss": 0.1651,
      "step": 1163
    },
    {
      "epoch": 2.724400234055003,
      "grad_norm": 0.35475918650627136,
      "learning_rate": 0.00024246682279469164,
      "loss": 0.1692,
      "step": 1164
    },
    {
      "epoch": 2.7267407840842597,
      "grad_norm": 0.3956649601459503,
      "learning_rate": 0.0002423887587822014,
      "loss": 0.1753,
      "step": 1165
    },
    {
      "epoch": 2.7290813341135167,
      "grad_norm": 0.618962287902832,
      "learning_rate": 0.00024231069476971114,
      "loss": 0.2072,
      "step": 1166
    },
    {
      "epoch": 2.7314218841427733,
      "grad_norm": 0.33281639218330383,
      "learning_rate": 0.00024223263075722092,
      "loss": 0.1816,
      "step": 1167
    },
    {
      "epoch": 2.7337624341720304,
      "grad_norm": 0.227805033326149,
      "learning_rate": 0.00024215456674473067,
      "loss": 0.1174,
      "step": 1168
    },
    {
      "epoch": 2.7361029842012874,
      "grad_norm": 0.22844204306602478,
      "learning_rate": 0.00024207650273224042,
      "loss": 0.129,
      "step": 1169
    },
    {
      "epoch": 2.7384435342305444,
      "grad_norm": 0.2865290641784668,
      "learning_rate": 0.0002419984387197502,
      "loss": 0.18,
      "step": 1170
    },
    {
      "epoch": 2.740784084259801,
      "grad_norm": 0.19401048123836517,
      "learning_rate": 0.00024192037470725995,
      "loss": 0.1114,
      "step": 1171
    },
    {
      "epoch": 2.743124634289058,
      "grad_norm": 0.23678800463676453,
      "learning_rate": 0.00024184231069476967,
      "loss": 0.132,
      "step": 1172
    },
    {
      "epoch": 2.7454651843183147,
      "grad_norm": 0.21011151373386383,
      "learning_rate": 0.00024176424668227947,
      "loss": 0.1086,
      "step": 1173
    },
    {
      "epoch": 2.7478057343475717,
      "grad_norm": 0.3816782832145691,
      "learning_rate": 0.0002416861826697892,
      "loss": 0.2177,
      "step": 1174
    },
    {
      "epoch": 2.7501462843768287,
      "grad_norm": 0.3474077582359314,
      "learning_rate": 0.00024160811865729895,
      "loss": 0.1627,
      "step": 1175
    },
    {
      "epoch": 2.7524868344060853,
      "grad_norm": 0.33529847860336304,
      "learning_rate": 0.00024153005464480872,
      "loss": 0.21,
      "step": 1176
    },
    {
      "epoch": 2.7548273844353424,
      "grad_norm": 0.4496826231479645,
      "learning_rate": 0.00024145199063231847,
      "loss": 0.2472,
      "step": 1177
    },
    {
      "epoch": 2.757167934464599,
      "grad_norm": 0.34272146224975586,
      "learning_rate": 0.00024137392661982823,
      "loss": 0.2042,
      "step": 1178
    },
    {
      "epoch": 2.759508484493856,
      "grad_norm": 0.2794298529624939,
      "learning_rate": 0.000241295862607338,
      "loss": 0.1752,
      "step": 1179
    },
    {
      "epoch": 2.761849034523113,
      "grad_norm": 0.319126158952713,
      "learning_rate": 0.00024121779859484775,
      "loss": 0.168,
      "step": 1180
    },
    {
      "epoch": 2.76418958455237,
      "grad_norm": 0.34795987606048584,
      "learning_rate": 0.0002411397345823575,
      "loss": 0.2133,
      "step": 1181
    },
    {
      "epoch": 2.7665301345816267,
      "grad_norm": 0.3460998833179474,
      "learning_rate": 0.00024106167056986728,
      "loss": 0.181,
      "step": 1182
    },
    {
      "epoch": 2.7688706846108837,
      "grad_norm": 0.28069084882736206,
      "learning_rate": 0.00024098360655737703,
      "loss": 0.1305,
      "step": 1183
    },
    {
      "epoch": 2.7712112346401403,
      "grad_norm": 0.2735007405281067,
      "learning_rate": 0.00024090554254488678,
      "loss": 0.1599,
      "step": 1184
    },
    {
      "epoch": 2.7735517846693973,
      "grad_norm": 0.2692908048629761,
      "learning_rate": 0.00024082747853239656,
      "loss": 0.158,
      "step": 1185
    },
    {
      "epoch": 2.7758923346986544,
      "grad_norm": 0.28999656438827515,
      "learning_rate": 0.0002407494145199063,
      "loss": 0.1898,
      "step": 1186
    },
    {
      "epoch": 2.778232884727911,
      "grad_norm": 0.3543727397918701,
      "learning_rate": 0.00024067135050741606,
      "loss": 0.1895,
      "step": 1187
    },
    {
      "epoch": 2.780573434757168,
      "grad_norm": 0.32572507858276367,
      "learning_rate": 0.00024059328649492584,
      "loss": 0.1743,
      "step": 1188
    },
    {
      "epoch": 2.7829139847864246,
      "grad_norm": 0.1730811595916748,
      "learning_rate": 0.0002405152224824356,
      "loss": 0.0854,
      "step": 1189
    },
    {
      "epoch": 2.7852545348156816,
      "grad_norm": 0.3098467290401459,
      "learning_rate": 0.0002404371584699453,
      "loss": 0.1687,
      "step": 1190
    },
    {
      "epoch": 2.7875950848449387,
      "grad_norm": 0.273145854473114,
      "learning_rate": 0.00024035909445745512,
      "loss": 0.1659,
      "step": 1191
    },
    {
      "epoch": 2.7899356348741957,
      "grad_norm": 0.3492944836616516,
      "learning_rate": 0.00024028103044496484,
      "loss": 0.1591,
      "step": 1192
    },
    {
      "epoch": 2.7922761849034523,
      "grad_norm": 0.21212022006511688,
      "learning_rate": 0.0002402029664324746,
      "loss": 0.1191,
      "step": 1193
    },
    {
      "epoch": 2.7946167349327093,
      "grad_norm": 0.29211708903312683,
      "learning_rate": 0.00024012490241998437,
      "loss": 0.1934,
      "step": 1194
    },
    {
      "epoch": 2.796957284961966,
      "grad_norm": 0.31897836923599243,
      "learning_rate": 0.00024004683840749412,
      "loss": 0.1502,
      "step": 1195
    },
    {
      "epoch": 2.799297834991223,
      "grad_norm": 0.2623014748096466,
      "learning_rate": 0.00023996877439500387,
      "loss": 0.1472,
      "step": 1196
    },
    {
      "epoch": 2.80163838502048,
      "grad_norm": 0.2865090072154999,
      "learning_rate": 0.00023989071038251365,
      "loss": 0.1614,
      "step": 1197
    },
    {
      "epoch": 2.8039789350497366,
      "grad_norm": 0.310093492269516,
      "learning_rate": 0.0002398126463700234,
      "loss": 0.1807,
      "step": 1198
    },
    {
      "epoch": 2.8063194850789936,
      "grad_norm": 0.2496429830789566,
      "learning_rate": 0.00023973458235753315,
      "loss": 0.151,
      "step": 1199
    },
    {
      "epoch": 2.8086600351082502,
      "grad_norm": 0.2808498740196228,
      "learning_rate": 0.00023965651834504292,
      "loss": 0.165,
      "step": 1200
    },
    {
      "epoch": 2.8110005851375073,
      "grad_norm": 0.32336440682411194,
      "learning_rate": 0.00023957845433255267,
      "loss": 0.1696,
      "step": 1201
    },
    {
      "epoch": 2.8133411351667643,
      "grad_norm": 0.28927919268608093,
      "learning_rate": 0.00023950039032006242,
      "loss": 0.1813,
      "step": 1202
    },
    {
      "epoch": 2.815681685196021,
      "grad_norm": 0.22604115307331085,
      "learning_rate": 0.0002394223263075722,
      "loss": 0.1384,
      "step": 1203
    },
    {
      "epoch": 2.818022235225278,
      "grad_norm": 0.251230388879776,
      "learning_rate": 0.00023934426229508195,
      "loss": 0.1348,
      "step": 1204
    },
    {
      "epoch": 2.8203627852545345,
      "grad_norm": 0.3220057785511017,
      "learning_rate": 0.0002392661982825917,
      "loss": 0.1735,
      "step": 1205
    },
    {
      "epoch": 2.8227033352837916,
      "grad_norm": 0.29892730712890625,
      "learning_rate": 0.00023918813427010148,
      "loss": 0.1451,
      "step": 1206
    },
    {
      "epoch": 2.8250438853130486,
      "grad_norm": 0.28679800033569336,
      "learning_rate": 0.00023911007025761123,
      "loss": 0.1495,
      "step": 1207
    },
    {
      "epoch": 2.8273844353423057,
      "grad_norm": 0.2815251648426056,
      "learning_rate": 0.00023903200624512095,
      "loss": 0.1351,
      "step": 1208
    },
    {
      "epoch": 2.8297249853715623,
      "grad_norm": 0.2720579206943512,
      "learning_rate": 0.00023895394223263076,
      "loss": 0.1583,
      "step": 1209
    },
    {
      "epoch": 2.8320655354008193,
      "grad_norm": 0.3311708867549896,
      "learning_rate": 0.00023887587822014048,
      "loss": 0.15,
      "step": 1210
    },
    {
      "epoch": 2.834406085430076,
      "grad_norm": 0.27930983901023865,
      "learning_rate": 0.00023879781420765023,
      "loss": 0.1311,
      "step": 1211
    },
    {
      "epoch": 2.836746635459333,
      "grad_norm": 0.2957720160484314,
      "learning_rate": 0.00023871975019516,
      "loss": 0.1437,
      "step": 1212
    },
    {
      "epoch": 2.83908718548859,
      "grad_norm": 0.3241961896419525,
      "learning_rate": 0.00023864168618266976,
      "loss": 0.159,
      "step": 1213
    },
    {
      "epoch": 2.8414277355178466,
      "grad_norm": 0.3309374153614044,
      "learning_rate": 0.0002385636221701795,
      "loss": 0.151,
      "step": 1214
    },
    {
      "epoch": 2.8437682855471036,
      "grad_norm": 0.34551236033439636,
      "learning_rate": 0.0002384855581576893,
      "loss": 0.1646,
      "step": 1215
    },
    {
      "epoch": 2.84610883557636,
      "grad_norm": 0.2721559703350067,
      "learning_rate": 0.00023840749414519904,
      "loss": 0.1456,
      "step": 1216
    },
    {
      "epoch": 2.8484493856056172,
      "grad_norm": 0.26727965474128723,
      "learning_rate": 0.0002383294301327088,
      "loss": 0.1316,
      "step": 1217
    },
    {
      "epoch": 2.8507899356348743,
      "grad_norm": 0.2765357494354248,
      "learning_rate": 0.00023825136612021857,
      "loss": 0.1695,
      "step": 1218
    },
    {
      "epoch": 2.8531304856641313,
      "grad_norm": 0.23746976256370544,
      "learning_rate": 0.00023817330210772832,
      "loss": 0.159,
      "step": 1219
    },
    {
      "epoch": 2.855471035693388,
      "grad_norm": 0.39223775267601013,
      "learning_rate": 0.00023809523809523807,
      "loss": 0.1848,
      "step": 1220
    },
    {
      "epoch": 2.857811585722645,
      "grad_norm": 0.26708510518074036,
      "learning_rate": 0.00023801717408274785,
      "loss": 0.1474,
      "step": 1221
    },
    {
      "epoch": 2.8601521357519015,
      "grad_norm": 0.28921380639076233,
      "learning_rate": 0.0002379391100702576,
      "loss": 0.1719,
      "step": 1222
    },
    {
      "epoch": 2.8624926857811586,
      "grad_norm": 0.2633598744869232,
      "learning_rate": 0.00023786104605776735,
      "loss": 0.1321,
      "step": 1223
    },
    {
      "epoch": 2.8648332358104156,
      "grad_norm": 0.3160950243473053,
      "learning_rate": 0.00023778298204527712,
      "loss": 0.145,
      "step": 1224
    },
    {
      "epoch": 2.867173785839672,
      "grad_norm": 0.26897764205932617,
      "learning_rate": 0.00023770491803278687,
      "loss": 0.163,
      "step": 1225
    },
    {
      "epoch": 2.8695143358689292,
      "grad_norm": 0.21159513294696808,
      "learning_rate": 0.0002376268540202966,
      "loss": 0.1384,
      "step": 1226
    },
    {
      "epoch": 2.871854885898186,
      "grad_norm": 0.3618415296077728,
      "learning_rate": 0.0002375487900078064,
      "loss": 0.172,
      "step": 1227
    },
    {
      "epoch": 2.874195435927443,
      "grad_norm": 0.33662328124046326,
      "learning_rate": 0.00023747072599531613,
      "loss": 0.1715,
      "step": 1228
    },
    {
      "epoch": 2.8765359859567,
      "grad_norm": 0.3284226953983307,
      "learning_rate": 0.00023739266198282588,
      "loss": 0.1759,
      "step": 1229
    },
    {
      "epoch": 2.878876535985957,
      "grad_norm": 0.34788838028907776,
      "learning_rate": 0.00023731459797033565,
      "loss": 0.1869,
      "step": 1230
    },
    {
      "epoch": 2.8812170860152135,
      "grad_norm": 0.38995635509490967,
      "learning_rate": 0.0002372365339578454,
      "loss": 0.1575,
      "step": 1231
    },
    {
      "epoch": 2.8835576360444706,
      "grad_norm": 0.35465431213378906,
      "learning_rate": 0.00023715846994535515,
      "loss": 0.202,
      "step": 1232
    },
    {
      "epoch": 2.885898186073727,
      "grad_norm": 0.33900144696235657,
      "learning_rate": 0.00023708040593286493,
      "loss": 0.2141,
      "step": 1233
    },
    {
      "epoch": 2.888238736102984,
      "grad_norm": 0.3036213219165802,
      "learning_rate": 0.00023700234192037468,
      "loss": 0.1971,
      "step": 1234
    },
    {
      "epoch": 2.8905792861322412,
      "grad_norm": 0.3093688189983368,
      "learning_rate": 0.00023692427790788443,
      "loss": 0.1829,
      "step": 1235
    },
    {
      "epoch": 2.892919836161498,
      "grad_norm": 0.30680862069129944,
      "learning_rate": 0.0002368462138953942,
      "loss": 0.1654,
      "step": 1236
    },
    {
      "epoch": 2.895260386190755,
      "grad_norm": 0.2861001789569855,
      "learning_rate": 0.00023676814988290396,
      "loss": 0.1698,
      "step": 1237
    },
    {
      "epoch": 2.8976009362200115,
      "grad_norm": 0.2359694242477417,
      "learning_rate": 0.0002366900858704137,
      "loss": 0.135,
      "step": 1238
    },
    {
      "epoch": 2.8999414862492685,
      "grad_norm": 0.31380411982536316,
      "learning_rate": 0.0002366120218579235,
      "loss": 0.2019,
      "step": 1239
    },
    {
      "epoch": 2.9022820362785255,
      "grad_norm": 0.33828356862068176,
      "learning_rate": 0.00023653395784543324,
      "loss": 0.1768,
      "step": 1240
    },
    {
      "epoch": 2.9046225863077826,
      "grad_norm": 0.3626110553741455,
      "learning_rate": 0.000236455893832943,
      "loss": 0.1712,
      "step": 1241
    },
    {
      "epoch": 2.906963136337039,
      "grad_norm": 0.4235614240169525,
      "learning_rate": 0.00023637782982045277,
      "loss": 0.1679,
      "step": 1242
    },
    {
      "epoch": 2.909303686366296,
      "grad_norm": 0.30708202719688416,
      "learning_rate": 0.00023629976580796252,
      "loss": 0.1743,
      "step": 1243
    },
    {
      "epoch": 2.911644236395553,
      "grad_norm": 0.2398475706577301,
      "learning_rate": 0.00023622170179547224,
      "loss": 0.1061,
      "step": 1244
    },
    {
      "epoch": 2.91398478642481,
      "grad_norm": 0.309129923582077,
      "learning_rate": 0.00023614363778298202,
      "loss": 0.1929,
      "step": 1245
    },
    {
      "epoch": 2.916325336454067,
      "grad_norm": 0.3051758408546448,
      "learning_rate": 0.00023606557377049177,
      "loss": 0.1565,
      "step": 1246
    },
    {
      "epoch": 2.9186658864833235,
      "grad_norm": 0.2610016465187073,
      "learning_rate": 0.00023598750975800152,
      "loss": 0.1529,
      "step": 1247
    },
    {
      "epoch": 2.9210064365125805,
      "grad_norm": 0.27308958768844604,
      "learning_rate": 0.0002359094457455113,
      "loss": 0.1565,
      "step": 1248
    },
    {
      "epoch": 2.923346986541837,
      "grad_norm": 0.29975077509880066,
      "learning_rate": 0.00023583138173302105,
      "loss": 0.1618,
      "step": 1249
    },
    {
      "epoch": 2.925687536571094,
      "grad_norm": 0.3173486888408661,
      "learning_rate": 0.0002357533177205308,
      "loss": 0.1571,
      "step": 1250
    },
    {
      "epoch": 2.928028086600351,
      "grad_norm": 0.32444819808006287,
      "learning_rate": 0.00023567525370804057,
      "loss": 0.1515,
      "step": 1251
    },
    {
      "epoch": 2.9303686366296082,
      "grad_norm": 0.28976601362228394,
      "learning_rate": 0.00023559718969555033,
      "loss": 0.1873,
      "step": 1252
    },
    {
      "epoch": 2.932709186658865,
      "grad_norm": 0.4480333924293518,
      "learning_rate": 0.00023551912568306008,
      "loss": 0.2079,
      "step": 1253
    },
    {
      "epoch": 2.935049736688122,
      "grad_norm": 0.33126193284988403,
      "learning_rate": 0.00023544106167056985,
      "loss": 0.1964,
      "step": 1254
    },
    {
      "epoch": 2.9373902867173785,
      "grad_norm": 0.3198870122432709,
      "learning_rate": 0.0002353629976580796,
      "loss": 0.1465,
      "step": 1255
    },
    {
      "epoch": 2.9397308367466355,
      "grad_norm": 0.30928295850753784,
      "learning_rate": 0.00023528493364558935,
      "loss": 0.1957,
      "step": 1256
    },
    {
      "epoch": 2.9420713867758925,
      "grad_norm": 0.38419607281684875,
      "learning_rate": 0.00023520686963309913,
      "loss": 0.1961,
      "step": 1257
    },
    {
      "epoch": 2.944411936805149,
      "grad_norm": 0.3440893590450287,
      "learning_rate": 0.00023512880562060888,
      "loss": 0.1701,
      "step": 1258
    },
    {
      "epoch": 2.946752486834406,
      "grad_norm": 0.36463212966918945,
      "learning_rate": 0.00023505074160811863,
      "loss": 0.2061,
      "step": 1259
    },
    {
      "epoch": 2.9490930368636628,
      "grad_norm": 0.38119998574256897,
      "learning_rate": 0.0002349726775956284,
      "loss": 0.1996,
      "step": 1260
    },
    {
      "epoch": 2.95143358689292,
      "grad_norm": 0.29488295316696167,
      "learning_rate": 0.00023489461358313816,
      "loss": 0.1595,
      "step": 1261
    },
    {
      "epoch": 2.953774136922177,
      "grad_norm": 0.31537941098213196,
      "learning_rate": 0.00023481654957064794,
      "loss": 0.181,
      "step": 1262
    },
    {
      "epoch": 2.9561146869514334,
      "grad_norm": 0.2994385361671448,
      "learning_rate": 0.00023473848555815766,
      "loss": 0.1777,
      "step": 1263
    },
    {
      "epoch": 2.9584552369806905,
      "grad_norm": 0.27745163440704346,
      "learning_rate": 0.0002346604215456674,
      "loss": 0.1337,
      "step": 1264
    },
    {
      "epoch": 2.9607957870099475,
      "grad_norm": 0.3610593378543854,
      "learning_rate": 0.0002345823575331772,
      "loss": 0.1684,
      "step": 1265
    },
    {
      "epoch": 2.963136337039204,
      "grad_norm": 0.4073832929134369,
      "learning_rate": 0.00023450429352068694,
      "loss": 0.221,
      "step": 1266
    },
    {
      "epoch": 2.965476887068461,
      "grad_norm": 0.3103722035884857,
      "learning_rate": 0.0002344262295081967,
      "loss": 0.1903,
      "step": 1267
    },
    {
      "epoch": 2.967817437097718,
      "grad_norm": 0.35135596990585327,
      "learning_rate": 0.00023434816549570647,
      "loss": 0.2258,
      "step": 1268
    },
    {
      "epoch": 2.9701579871269748,
      "grad_norm": 0.3688735365867615,
      "learning_rate": 0.00023427010148321622,
      "loss": 0.2205,
      "step": 1269
    },
    {
      "epoch": 2.972498537156232,
      "grad_norm": 0.3542459011077881,
      "learning_rate": 0.00023419203747072597,
      "loss": 0.2021,
      "step": 1270
    },
    {
      "epoch": 2.9748390871854884,
      "grad_norm": 0.27727505564689636,
      "learning_rate": 0.00023411397345823575,
      "loss": 0.1513,
      "step": 1271
    },
    {
      "epoch": 2.9771796372147454,
      "grad_norm": 0.3229057192802429,
      "learning_rate": 0.0002340359094457455,
      "loss": 0.1394,
      "step": 1272
    },
    {
      "epoch": 2.9795201872440025,
      "grad_norm": 0.4507632255554199,
      "learning_rate": 0.00023395784543325525,
      "loss": 0.2244,
      "step": 1273
    },
    {
      "epoch": 2.981860737273259,
      "grad_norm": 0.30184516310691833,
      "learning_rate": 0.00023387978142076502,
      "loss": 0.1892,
      "step": 1274
    },
    {
      "epoch": 2.984201287302516,
      "grad_norm": 0.32036498188972473,
      "learning_rate": 0.00023380171740827477,
      "loss": 0.1836,
      "step": 1275
    },
    {
      "epoch": 2.9865418373317727,
      "grad_norm": 0.3933064639568329,
      "learning_rate": 0.00023372365339578452,
      "loss": 0.2109,
      "step": 1276
    },
    {
      "epoch": 2.9888823873610297,
      "grad_norm": 0.36496955156326294,
      "learning_rate": 0.0002336455893832943,
      "loss": 0.198,
      "step": 1277
    },
    {
      "epoch": 2.9912229373902868,
      "grad_norm": 0.2913661003112793,
      "learning_rate": 0.00023356752537080405,
      "loss": 0.1437,
      "step": 1278
    },
    {
      "epoch": 2.993563487419544,
      "grad_norm": 0.3039710223674774,
      "learning_rate": 0.0002334894613583138,
      "loss": 0.1362,
      "step": 1279
    },
    {
      "epoch": 2.9959040374488004,
      "grad_norm": 0.37538644671440125,
      "learning_rate": 0.00023341139734582358,
      "loss": 0.1603,
      "step": 1280
    },
    {
      "epoch": 2.9982445874780574,
      "grad_norm": 0.3948788046836853,
      "learning_rate": 0.0002333333333333333,
      "loss": 0.2373,
      "step": 1281
    },
    {
      "epoch": 2.9982445874780574,
      "eval_loss": 0.23794937133789062,
      "eval_runtime": 128.3743,
      "eval_samples_per_second": 4.3,
      "eval_steps_per_second": 0.537,
      "step": 1281
    },
    {
      "epoch": 3.000585137507314,
      "grad_norm": 0.31012433767318726,
      "learning_rate": 0.00023325526932084305,
      "loss": 0.1561,
      "step": 1282
    },
    {
      "epoch": 3.002925687536571,
      "grad_norm": 0.31806331872940063,
      "learning_rate": 0.00023317720530835283,
      "loss": 0.1924,
      "step": 1283
    },
    {
      "epoch": 3.005266237565828,
      "grad_norm": 0.2850344777107239,
      "learning_rate": 0.00023309914129586258,
      "loss": 0.1643,
      "step": 1284
    },
    {
      "epoch": 3.0076067875950847,
      "grad_norm": 0.24132299423217773,
      "learning_rate": 0.00023302107728337233,
      "loss": 0.1403,
      "step": 1285
    },
    {
      "epoch": 3.0099473376243417,
      "grad_norm": 0.3637680113315582,
      "learning_rate": 0.0002329430132708821,
      "loss": 0.1483,
      "step": 1286
    },
    {
      "epoch": 3.012287887653599,
      "grad_norm": 0.3257412016391754,
      "learning_rate": 0.00023286494925839186,
      "loss": 0.1875,
      "step": 1287
    },
    {
      "epoch": 3.0146284376828554,
      "grad_norm": 0.2608408033847809,
      "learning_rate": 0.0002327868852459016,
      "loss": 0.1662,
      "step": 1288
    },
    {
      "epoch": 3.0169689877121124,
      "grad_norm": 0.3082330822944641,
      "learning_rate": 0.0002327088212334114,
      "loss": 0.1875,
      "step": 1289
    },
    {
      "epoch": 3.019309537741369,
      "grad_norm": 0.31005653738975525,
      "learning_rate": 0.00023263075722092114,
      "loss": 0.1836,
      "step": 1290
    },
    {
      "epoch": 3.021650087770626,
      "grad_norm": 0.250647634267807,
      "learning_rate": 0.0002325526932084309,
      "loss": 0.1457,
      "step": 1291
    },
    {
      "epoch": 3.023990637799883,
      "grad_norm": 0.20601460337638855,
      "learning_rate": 0.00023247462919594067,
      "loss": 0.1241,
      "step": 1292
    },
    {
      "epoch": 3.0263311878291397,
      "grad_norm": 0.3587368428707123,
      "learning_rate": 0.00023239656518345042,
      "loss": 0.1619,
      "step": 1293
    },
    {
      "epoch": 3.0286717378583967,
      "grad_norm": 0.3210257291793823,
      "learning_rate": 0.00023231850117096017,
      "loss": 0.1988,
      "step": 1294
    },
    {
      "epoch": 3.0310122878876538,
      "grad_norm": 0.2795647084712982,
      "learning_rate": 0.00023224043715846995,
      "loss": 0.1316,
      "step": 1295
    },
    {
      "epoch": 3.0333528379169103,
      "grad_norm": 0.33471906185150146,
      "learning_rate": 0.0002321623731459797,
      "loss": 0.1895,
      "step": 1296
    },
    {
      "epoch": 3.0356933879461674,
      "grad_norm": 0.3379661738872528,
      "learning_rate": 0.00023208430913348942,
      "loss": 0.1656,
      "step": 1297
    },
    {
      "epoch": 3.0380339379754244,
      "grad_norm": 0.33446386456489563,
      "learning_rate": 0.00023200624512099922,
      "loss": 0.1182,
      "step": 1298
    },
    {
      "epoch": 3.040374488004681,
      "grad_norm": 0.2509579062461853,
      "learning_rate": 0.00023192818110850895,
      "loss": 0.1439,
      "step": 1299
    },
    {
      "epoch": 3.042715038033938,
      "grad_norm": 0.3702952265739441,
      "learning_rate": 0.0002318501170960187,
      "loss": 0.1578,
      "step": 1300
    },
    {
      "epoch": 3.0450555880631947,
      "grad_norm": 0.32653430104255676,
      "learning_rate": 0.00023177205308352847,
      "loss": 0.1682,
      "step": 1301
    },
    {
      "epoch": 3.0473961380924517,
      "grad_norm": 0.3205694556236267,
      "learning_rate": 0.00023169398907103823,
      "loss": 0.135,
      "step": 1302
    },
    {
      "epoch": 3.0497366881217087,
      "grad_norm": 0.34633880853652954,
      "learning_rate": 0.00023161592505854798,
      "loss": 0.1743,
      "step": 1303
    },
    {
      "epoch": 3.0520772381509653,
      "grad_norm": 0.2355489283800125,
      "learning_rate": 0.00023153786104605775,
      "loss": 0.0919,
      "step": 1304
    },
    {
      "epoch": 3.0544177881802224,
      "grad_norm": 0.3806746304035187,
      "learning_rate": 0.0002314597970335675,
      "loss": 0.1595,
      "step": 1305
    },
    {
      "epoch": 3.0567583382094794,
      "grad_norm": 0.3386971354484558,
      "learning_rate": 0.00023138173302107725,
      "loss": 0.1498,
      "step": 1306
    },
    {
      "epoch": 3.059098888238736,
      "grad_norm": 0.374949187040329,
      "learning_rate": 0.00023130366900858703,
      "loss": 0.1527,
      "step": 1307
    },
    {
      "epoch": 3.061439438267993,
      "grad_norm": 0.3171136975288391,
      "learning_rate": 0.00023122560499609678,
      "loss": 0.1451,
      "step": 1308
    },
    {
      "epoch": 3.06377998829725,
      "grad_norm": 0.2570232152938843,
      "learning_rate": 0.00023114754098360653,
      "loss": 0.1467,
      "step": 1309
    },
    {
      "epoch": 3.0661205383265067,
      "grad_norm": 0.2812236547470093,
      "learning_rate": 0.0002310694769711163,
      "loss": 0.1354,
      "step": 1310
    },
    {
      "epoch": 3.0684610883557637,
      "grad_norm": 0.25373417139053345,
      "learning_rate": 0.00023099141295862606,
      "loss": 0.1212,
      "step": 1311
    },
    {
      "epoch": 3.0708016383850203,
      "grad_norm": 0.2661435008049011,
      "learning_rate": 0.0002309133489461358,
      "loss": 0.1279,
      "step": 1312
    },
    {
      "epoch": 3.0731421884142773,
      "grad_norm": 0.4108847379684448,
      "learning_rate": 0.0002308352849336456,
      "loss": 0.2102,
      "step": 1313
    },
    {
      "epoch": 3.0754827384435344,
      "grad_norm": 0.362857460975647,
      "learning_rate": 0.00023075722092115534,
      "loss": 0.1714,
      "step": 1314
    },
    {
      "epoch": 3.077823288472791,
      "grad_norm": 0.24189355969429016,
      "learning_rate": 0.00023067915690866506,
      "loss": 0.1451,
      "step": 1315
    },
    {
      "epoch": 3.080163838502048,
      "grad_norm": 0.2642935812473297,
      "learning_rate": 0.00023060109289617487,
      "loss": 0.1184,
      "step": 1316
    },
    {
      "epoch": 3.082504388531305,
      "grad_norm": 0.2602000832557678,
      "learning_rate": 0.0002305230288836846,
      "loss": 0.1348,
      "step": 1317
    },
    {
      "epoch": 3.0848449385605616,
      "grad_norm": 0.3280637860298157,
      "learning_rate": 0.00023044496487119434,
      "loss": 0.1616,
      "step": 1318
    },
    {
      "epoch": 3.0871854885898187,
      "grad_norm": 0.27732792496681213,
      "learning_rate": 0.00023036690085870412,
      "loss": 0.1451,
      "step": 1319
    },
    {
      "epoch": 3.0895260386190753,
      "grad_norm": 0.27432650327682495,
      "learning_rate": 0.00023028883684621387,
      "loss": 0.1635,
      "step": 1320
    },
    {
      "epoch": 3.0918665886483323,
      "grad_norm": 0.2684182822704315,
      "learning_rate": 0.00023021077283372362,
      "loss": 0.1421,
      "step": 1321
    },
    {
      "epoch": 3.0942071386775893,
      "grad_norm": 0.379245787858963,
      "learning_rate": 0.0002301327088212334,
      "loss": 0.1553,
      "step": 1322
    },
    {
      "epoch": 3.096547688706846,
      "grad_norm": 0.27118632197380066,
      "learning_rate": 0.00023005464480874315,
      "loss": 0.1553,
      "step": 1323
    },
    {
      "epoch": 3.098888238736103,
      "grad_norm": 0.35655078291893005,
      "learning_rate": 0.0002299765807962529,
      "loss": 0.1656,
      "step": 1324
    },
    {
      "epoch": 3.10122878876536,
      "grad_norm": 0.23249849677085876,
      "learning_rate": 0.00022989851678376267,
      "loss": 0.1046,
      "step": 1325
    },
    {
      "epoch": 3.1035693387946166,
      "grad_norm": 0.4175960123538971,
      "learning_rate": 0.00022982045277127242,
      "loss": 0.1836,
      "step": 1326
    },
    {
      "epoch": 3.1059098888238736,
      "grad_norm": 0.3122042715549469,
      "learning_rate": 0.00022974238875878218,
      "loss": 0.1556,
      "step": 1327
    },
    {
      "epoch": 3.1082504388531307,
      "grad_norm": 0.333625465631485,
      "learning_rate": 0.00022966432474629195,
      "loss": 0.1305,
      "step": 1328
    },
    {
      "epoch": 3.1105909888823873,
      "grad_norm": 0.3448735475540161,
      "learning_rate": 0.0002295862607338017,
      "loss": 0.1596,
      "step": 1329
    },
    {
      "epoch": 3.1129315389116443,
      "grad_norm": 0.370393306016922,
      "learning_rate": 0.00022950819672131145,
      "loss": 0.1467,
      "step": 1330
    },
    {
      "epoch": 3.115272088940901,
      "grad_norm": 0.36238211393356323,
      "learning_rate": 0.00022943013270882123,
      "loss": 0.162,
      "step": 1331
    },
    {
      "epoch": 3.117612638970158,
      "grad_norm": 0.31225672364234924,
      "learning_rate": 0.00022935206869633098,
      "loss": 0.1727,
      "step": 1332
    },
    {
      "epoch": 3.119953188999415,
      "grad_norm": 0.2480645328760147,
      "learning_rate": 0.0002292740046838407,
      "loss": 0.0927,
      "step": 1333
    },
    {
      "epoch": 3.1222937390286716,
      "grad_norm": 0.3645078241825104,
      "learning_rate": 0.0002291959406713505,
      "loss": 0.131,
      "step": 1334
    },
    {
      "epoch": 3.1246342890579286,
      "grad_norm": 0.5263814926147461,
      "learning_rate": 0.00022911787665886023,
      "loss": 0.2062,
      "step": 1335
    },
    {
      "epoch": 3.1269748390871857,
      "grad_norm": 0.45461541414260864,
      "learning_rate": 0.00022903981264636998,
      "loss": 0.1948,
      "step": 1336
    },
    {
      "epoch": 3.1293153891164422,
      "grad_norm": 0.40140673518180847,
      "learning_rate": 0.00022896174863387976,
      "loss": 0.1561,
      "step": 1337
    },
    {
      "epoch": 3.1316559391456993,
      "grad_norm": 0.3907777965068817,
      "learning_rate": 0.0002288836846213895,
      "loss": 0.205,
      "step": 1338
    },
    {
      "epoch": 3.1339964891749563,
      "grad_norm": 0.2764192819595337,
      "learning_rate": 0.00022880562060889926,
      "loss": 0.1702,
      "step": 1339
    },
    {
      "epoch": 3.136337039204213,
      "grad_norm": 0.3182956576347351,
      "learning_rate": 0.00022872755659640904,
      "loss": 0.1761,
      "step": 1340
    },
    {
      "epoch": 3.13867758923347,
      "grad_norm": 0.4137844741344452,
      "learning_rate": 0.0002286494925839188,
      "loss": 0.1769,
      "step": 1341
    },
    {
      "epoch": 3.1410181392627265,
      "grad_norm": 0.34937989711761475,
      "learning_rate": 0.00022857142857142854,
      "loss": 0.1527,
      "step": 1342
    },
    {
      "epoch": 3.1433586892919836,
      "grad_norm": 0.30698949098587036,
      "learning_rate": 0.00022849336455893832,
      "loss": 0.1724,
      "step": 1343
    },
    {
      "epoch": 3.1456992393212406,
      "grad_norm": 0.42819923162460327,
      "learning_rate": 0.00022841530054644807,
      "loss": 0.2049,
      "step": 1344
    },
    {
      "epoch": 3.148039789350497,
      "grad_norm": 0.3672041893005371,
      "learning_rate": 0.00022833723653395782,
      "loss": 0.1932,
      "step": 1345
    },
    {
      "epoch": 3.1503803393797543,
      "grad_norm": 0.2658206820487976,
      "learning_rate": 0.0002282591725214676,
      "loss": 0.1257,
      "step": 1346
    },
    {
      "epoch": 3.1527208894090113,
      "grad_norm": 0.3358938694000244,
      "learning_rate": 0.00022818110850897735,
      "loss": 0.1609,
      "step": 1347
    },
    {
      "epoch": 3.155061439438268,
      "grad_norm": 0.40162360668182373,
      "learning_rate": 0.0002281030444964871,
      "loss": 0.1936,
      "step": 1348
    },
    {
      "epoch": 3.157401989467525,
      "grad_norm": 0.36688488721847534,
      "learning_rate": 0.00022802498048399687,
      "loss": 0.1783,
      "step": 1349
    },
    {
      "epoch": 3.159742539496782,
      "grad_norm": 0.34391629695892334,
      "learning_rate": 0.00022794691647150662,
      "loss": 0.167,
      "step": 1350
    },
    {
      "epoch": 3.1620830895260386,
      "grad_norm": 0.44987496733665466,
      "learning_rate": 0.00022786885245901635,
      "loss": 0.2312,
      "step": 1351
    },
    {
      "epoch": 3.1644236395552956,
      "grad_norm": 0.32281622290611267,
      "learning_rate": 0.00022779078844652615,
      "loss": 0.1563,
      "step": 1352
    },
    {
      "epoch": 3.166764189584552,
      "grad_norm": 0.32604101300239563,
      "learning_rate": 0.00022771272443403588,
      "loss": 0.1559,
      "step": 1353
    },
    {
      "epoch": 3.1691047396138092,
      "grad_norm": 0.36884161829948425,
      "learning_rate": 0.00022763466042154563,
      "loss": 0.1516,
      "step": 1354
    },
    {
      "epoch": 3.1714452896430663,
      "grad_norm": 0.332576721906662,
      "learning_rate": 0.0002275565964090554,
      "loss": 0.1463,
      "step": 1355
    },
    {
      "epoch": 3.173785839672323,
      "grad_norm": 0.3232196569442749,
      "learning_rate": 0.00022747853239656515,
      "loss": 0.1428,
      "step": 1356
    },
    {
      "epoch": 3.17612638970158,
      "grad_norm": 0.3194742798805237,
      "learning_rate": 0.0002274004683840749,
      "loss": 0.1594,
      "step": 1357
    },
    {
      "epoch": 3.178466939730837,
      "grad_norm": 0.2999148368835449,
      "learning_rate": 0.00022732240437158468,
      "loss": 0.157,
      "step": 1358
    },
    {
      "epoch": 3.1808074897600935,
      "grad_norm": 0.2449217587709427,
      "learning_rate": 0.00022724434035909443,
      "loss": 0.1057,
      "step": 1359
    },
    {
      "epoch": 3.1831480397893506,
      "grad_norm": 0.32142940163612366,
      "learning_rate": 0.00022716627634660418,
      "loss": 0.1677,
      "step": 1360
    },
    {
      "epoch": 3.1854885898186076,
      "grad_norm": 0.29648029804229736,
      "learning_rate": 0.00022708821233411396,
      "loss": 0.1645,
      "step": 1361
    },
    {
      "epoch": 3.187829139847864,
      "grad_norm": 0.3788374662399292,
      "learning_rate": 0.0002270101483216237,
      "loss": 0.1518,
      "step": 1362
    },
    {
      "epoch": 3.1901696898771212,
      "grad_norm": 0.4369913935661316,
      "learning_rate": 0.00022693208430913346,
      "loss": 0.2157,
      "step": 1363
    },
    {
      "epoch": 3.192510239906378,
      "grad_norm": 0.2747713029384613,
      "learning_rate": 0.00022685402029664324,
      "loss": 0.105,
      "step": 1364
    },
    {
      "epoch": 3.194850789935635,
      "grad_norm": 0.39381393790245056,
      "learning_rate": 0.000226775956284153,
      "loss": 0.1815,
      "step": 1365
    },
    {
      "epoch": 3.197191339964892,
      "grad_norm": 0.33864158391952515,
      "learning_rate": 0.00022669789227166274,
      "loss": 0.172,
      "step": 1366
    },
    {
      "epoch": 3.1995318899941485,
      "grad_norm": 0.4463364779949188,
      "learning_rate": 0.00022661982825917252,
      "loss": 0.1927,
      "step": 1367
    },
    {
      "epoch": 3.2018724400234055,
      "grad_norm": 0.32267409563064575,
      "learning_rate": 0.00022654176424668227,
      "loss": 0.1731,
      "step": 1368
    },
    {
      "epoch": 3.2042129900526626,
      "grad_norm": 0.2522275149822235,
      "learning_rate": 0.000226463700234192,
      "loss": 0.1013,
      "step": 1369
    },
    {
      "epoch": 3.206553540081919,
      "grad_norm": 0.2727472186088562,
      "learning_rate": 0.0002263856362217018,
      "loss": 0.1203,
      "step": 1370
    },
    {
      "epoch": 3.208894090111176,
      "grad_norm": 0.25138548016548157,
      "learning_rate": 0.00022630757220921152,
      "loss": 0.1439,
      "step": 1371
    },
    {
      "epoch": 3.211234640140433,
      "grad_norm": 0.2985876202583313,
      "learning_rate": 0.00022622950819672127,
      "loss": 0.1856,
      "step": 1372
    },
    {
      "epoch": 3.21357519016969,
      "grad_norm": 0.24373047053813934,
      "learning_rate": 0.00022615144418423105,
      "loss": 0.1322,
      "step": 1373
    },
    {
      "epoch": 3.215915740198947,
      "grad_norm": 0.3579215109348297,
      "learning_rate": 0.0002260733801717408,
      "loss": 0.1652,
      "step": 1374
    },
    {
      "epoch": 3.2182562902282035,
      "grad_norm": 0.32513222098350525,
      "learning_rate": 0.00022599531615925055,
      "loss": 0.1385,
      "step": 1375
    },
    {
      "epoch": 3.2205968402574605,
      "grad_norm": 0.4110370874404907,
      "learning_rate": 0.00022591725214676032,
      "loss": 0.1941,
      "step": 1376
    },
    {
      "epoch": 3.2229373902867176,
      "grad_norm": 0.3279185891151428,
      "learning_rate": 0.00022583918813427008,
      "loss": 0.1556,
      "step": 1377
    },
    {
      "epoch": 3.225277940315974,
      "grad_norm": 0.4020979702472687,
      "learning_rate": 0.00022576112412177983,
      "loss": 0.1592,
      "step": 1378
    },
    {
      "epoch": 3.227618490345231,
      "grad_norm": 0.30592191219329834,
      "learning_rate": 0.0002256830601092896,
      "loss": 0.1571,
      "step": 1379
    },
    {
      "epoch": 3.2299590403744878,
      "grad_norm": 0.3308108150959015,
      "learning_rate": 0.00022560499609679935,
      "loss": 0.122,
      "step": 1380
    },
    {
      "epoch": 3.232299590403745,
      "grad_norm": 0.36016616225242615,
      "learning_rate": 0.0002255269320843091,
      "loss": 0.1777,
      "step": 1381
    },
    {
      "epoch": 3.234640140433002,
      "grad_norm": 0.4311821758747101,
      "learning_rate": 0.00022544886807181888,
      "loss": 0.1618,
      "step": 1382
    },
    {
      "epoch": 3.2369806904622584,
      "grad_norm": 0.3568600118160248,
      "learning_rate": 0.00022537080405932863,
      "loss": 0.1529,
      "step": 1383
    },
    {
      "epoch": 3.2393212404915155,
      "grad_norm": 0.2973952889442444,
      "learning_rate": 0.00022529274004683838,
      "loss": 0.1562,
      "step": 1384
    },
    {
      "epoch": 3.2416617905207725,
      "grad_norm": 0.2949390113353729,
      "learning_rate": 0.00022521467603434816,
      "loss": 0.1417,
      "step": 1385
    },
    {
      "epoch": 3.244002340550029,
      "grad_norm": 0.38427940011024475,
      "learning_rate": 0.0002251366120218579,
      "loss": 0.1341,
      "step": 1386
    },
    {
      "epoch": 3.246342890579286,
      "grad_norm": 0.33632007241249084,
      "learning_rate": 0.00022505854800936763,
      "loss": 0.1517,
      "step": 1387
    },
    {
      "epoch": 3.248683440608543,
      "grad_norm": 0.30288439989089966,
      "learning_rate": 0.00022498048399687744,
      "loss": 0.1335,
      "step": 1388
    },
    {
      "epoch": 3.2510239906378,
      "grad_norm": 0.6128953099250793,
      "learning_rate": 0.00022490241998438716,
      "loss": 0.0811,
      "step": 1389
    },
    {
      "epoch": 3.253364540667057,
      "grad_norm": 0.32558757066726685,
      "learning_rate": 0.00022482435597189694,
      "loss": 0.1295,
      "step": 1390
    },
    {
      "epoch": 3.2557050906963134,
      "grad_norm": 0.34884533286094666,
      "learning_rate": 0.0002247462919594067,
      "loss": 0.1663,
      "step": 1391
    },
    {
      "epoch": 3.2580456407255705,
      "grad_norm": 0.36505192518234253,
      "learning_rate": 0.00022466822794691644,
      "loss": 0.121,
      "step": 1392
    },
    {
      "epoch": 3.2603861907548275,
      "grad_norm": 0.46054360270500183,
      "learning_rate": 0.00022459016393442622,
      "loss": 0.1565,
      "step": 1393
    },
    {
      "epoch": 3.262726740784084,
      "grad_norm": 0.3203414976596832,
      "learning_rate": 0.00022451209992193597,
      "loss": 0.1368,
      "step": 1394
    },
    {
      "epoch": 3.265067290813341,
      "grad_norm": 0.3923046588897705,
      "learning_rate": 0.00022443403590944572,
      "loss": 0.1678,
      "step": 1395
    },
    {
      "epoch": 3.267407840842598,
      "grad_norm": 0.28496232628822327,
      "learning_rate": 0.0002243559718969555,
      "loss": 0.1494,
      "step": 1396
    },
    {
      "epoch": 3.2697483908718548,
      "grad_norm": 0.4257170557975769,
      "learning_rate": 0.00022427790788446525,
      "loss": 0.1751,
      "step": 1397
    },
    {
      "epoch": 3.272088940901112,
      "grad_norm": 0.30932700634002686,
      "learning_rate": 0.000224199843871975,
      "loss": 0.1196,
      "step": 1398
    },
    {
      "epoch": 3.274429490930369,
      "grad_norm": 0.38290488719940186,
      "learning_rate": 0.00022412177985948477,
      "loss": 0.1843,
      "step": 1399
    },
    {
      "epoch": 3.2767700409596254,
      "grad_norm": 0.45734307169914246,
      "learning_rate": 0.00022404371584699452,
      "loss": 0.1656,
      "step": 1400
    },
    {
      "epoch": 3.2791105909888825,
      "grad_norm": 0.3817730247974396,
      "learning_rate": 0.00022396565183450427,
      "loss": 0.197,
      "step": 1401
    },
    {
      "epoch": 3.281451141018139,
      "grad_norm": 0.3978230953216553,
      "learning_rate": 0.00022388758782201405,
      "loss": 0.1874,
      "step": 1402
    },
    {
      "epoch": 3.283791691047396,
      "grad_norm": 0.3113473653793335,
      "learning_rate": 0.0002238095238095238,
      "loss": 0.1346,
      "step": 1403
    },
    {
      "epoch": 3.286132241076653,
      "grad_norm": 0.37951165437698364,
      "learning_rate": 0.00022373145979703355,
      "loss": 0.1663,
      "step": 1404
    },
    {
      "epoch": 3.2884727911059097,
      "grad_norm": 0.28934746980667114,
      "learning_rate": 0.00022365339578454333,
      "loss": 0.1394,
      "step": 1405
    },
    {
      "epoch": 3.2908133411351668,
      "grad_norm": 0.41970792412757874,
      "learning_rate": 0.00022357533177205308,
      "loss": 0.1928,
      "step": 1406
    },
    {
      "epoch": 3.293153891164424,
      "grad_norm": 0.3856267035007477,
      "learning_rate": 0.0002234972677595628,
      "loss": 0.1431,
      "step": 1407
    },
    {
      "epoch": 3.2954944411936804,
      "grad_norm": 0.3725024461746216,
      "learning_rate": 0.00022341920374707258,
      "loss": 0.1916,
      "step": 1408
    },
    {
      "epoch": 3.2978349912229374,
      "grad_norm": 0.3222145140171051,
      "learning_rate": 0.00022334113973458233,
      "loss": 0.1484,
      "step": 1409
    },
    {
      "epoch": 3.3001755412521945,
      "grad_norm": 0.26588043570518494,
      "learning_rate": 0.00022326307572209208,
      "loss": 0.1616,
      "step": 1410
    },
    {
      "epoch": 3.302516091281451,
      "grad_norm": 0.28549084067344666,
      "learning_rate": 0.00022318501170960186,
      "loss": 0.1484,
      "step": 1411
    },
    {
      "epoch": 3.304856641310708,
      "grad_norm": 0.2926955223083496,
      "learning_rate": 0.0002231069476971116,
      "loss": 0.1613,
      "step": 1412
    },
    {
      "epoch": 3.3071971913399647,
      "grad_norm": 0.2794033885002136,
      "learning_rate": 0.00022302888368462136,
      "loss": 0.1283,
      "step": 1413
    },
    {
      "epoch": 3.3095377413692217,
      "grad_norm": 0.2892799973487854,
      "learning_rate": 0.00022295081967213114,
      "loss": 0.1344,
      "step": 1414
    },
    {
      "epoch": 3.311878291398479,
      "grad_norm": 0.2872542440891266,
      "learning_rate": 0.0002228727556596409,
      "loss": 0.1385,
      "step": 1415
    },
    {
      "epoch": 3.3142188414277354,
      "grad_norm": 0.3804105222225189,
      "learning_rate": 0.00022279469164715064,
      "loss": 0.1594,
      "step": 1416
    },
    {
      "epoch": 3.3165593914569924,
      "grad_norm": 0.2980175316333771,
      "learning_rate": 0.00022271662763466042,
      "loss": 0.1591,
      "step": 1417
    },
    {
      "epoch": 3.3188999414862494,
      "grad_norm": 0.3288652002811432,
      "learning_rate": 0.00022263856362217017,
      "loss": 0.1541,
      "step": 1418
    },
    {
      "epoch": 3.321240491515506,
      "grad_norm": 0.26520511507987976,
      "learning_rate": 0.00022256049960967992,
      "loss": 0.1293,
      "step": 1419
    },
    {
      "epoch": 3.323581041544763,
      "grad_norm": 0.4790631830692291,
      "learning_rate": 0.0002224824355971897,
      "loss": 0.1842,
      "step": 1420
    },
    {
      "epoch": 3.32592159157402,
      "grad_norm": 0.3155299425125122,
      "learning_rate": 0.00022240437158469945,
      "loss": 0.1822,
      "step": 1421
    },
    {
      "epoch": 3.3282621416032767,
      "grad_norm": 0.3284870982170105,
      "learning_rate": 0.0002223263075722092,
      "loss": 0.1694,
      "step": 1422
    },
    {
      "epoch": 3.3306026916325338,
      "grad_norm": 0.2623620629310608,
      "learning_rate": 0.00022224824355971897,
      "loss": 0.1155,
      "step": 1423
    },
    {
      "epoch": 3.3329432416617903,
      "grad_norm": 0.39647871255874634,
      "learning_rate": 0.0002221701795472287,
      "loss": 0.1816,
      "step": 1424
    },
    {
      "epoch": 3.3352837916910474,
      "grad_norm": 0.3813804090023041,
      "learning_rate": 0.00022209211553473845,
      "loss": 0.1601,
      "step": 1425
    },
    {
      "epoch": 3.3376243417203044,
      "grad_norm": 0.27188318967819214,
      "learning_rate": 0.00022201405152224822,
      "loss": 0.1259,
      "step": 1426
    },
    {
      "epoch": 3.339964891749561,
      "grad_norm": 0.4234720766544342,
      "learning_rate": 0.00022193598750975798,
      "loss": 0.142,
      "step": 1427
    },
    {
      "epoch": 3.342305441778818,
      "grad_norm": 0.38293471932411194,
      "learning_rate": 0.00022185792349726773,
      "loss": 0.1711,
      "step": 1428
    },
    {
      "epoch": 3.344645991808075,
      "grad_norm": 0.3625229597091675,
      "learning_rate": 0.0002217798594847775,
      "loss": 0.1988,
      "step": 1429
    },
    {
      "epoch": 3.3469865418373317,
      "grad_norm": 0.3426458537578583,
      "learning_rate": 0.00022170179547228725,
      "loss": 0.1867,
      "step": 1430
    },
    {
      "epoch": 3.3493270918665887,
      "grad_norm": 0.271878719329834,
      "learning_rate": 0.000221623731459797,
      "loss": 0.147,
      "step": 1431
    },
    {
      "epoch": 3.3516676418958458,
      "grad_norm": 0.3651244044303894,
      "learning_rate": 0.00022154566744730678,
      "loss": 0.1379,
      "step": 1432
    },
    {
      "epoch": 3.3540081919251024,
      "grad_norm": 0.3763253688812256,
      "learning_rate": 0.00022146760343481653,
      "loss": 0.1221,
      "step": 1433
    },
    {
      "epoch": 3.3563487419543594,
      "grad_norm": 0.30609890818595886,
      "learning_rate": 0.00022138953942232628,
      "loss": 0.1518,
      "step": 1434
    },
    {
      "epoch": 3.358689291983616,
      "grad_norm": 0.28277671337127686,
      "learning_rate": 0.00022131147540983606,
      "loss": 0.126,
      "step": 1435
    },
    {
      "epoch": 3.361029842012873,
      "grad_norm": 0.41210293769836426,
      "learning_rate": 0.0002212334113973458,
      "loss": 0.1495,
      "step": 1436
    },
    {
      "epoch": 3.36337039204213,
      "grad_norm": 0.3059065043926239,
      "learning_rate": 0.00022115534738485556,
      "loss": 0.151,
      "step": 1437
    },
    {
      "epoch": 3.3657109420713867,
      "grad_norm": 0.25685814023017883,
      "learning_rate": 0.00022107728337236534,
      "loss": 0.1294,
      "step": 1438
    },
    {
      "epoch": 3.3680514921006437,
      "grad_norm": 0.3061836063861847,
      "learning_rate": 0.0002209992193598751,
      "loss": 0.1481,
      "step": 1439
    },
    {
      "epoch": 3.3703920421299003,
      "grad_norm": 0.2927224934101105,
      "learning_rate": 0.00022092115534738484,
      "loss": 0.1397,
      "step": 1440
    },
    {
      "epoch": 3.3727325921591573,
      "grad_norm": 0.2874574363231659,
      "learning_rate": 0.00022084309133489462,
      "loss": 0.1316,
      "step": 1441
    },
    {
      "epoch": 3.3750731421884144,
      "grad_norm": 0.2748858332633972,
      "learning_rate": 0.00022076502732240434,
      "loss": 0.1213,
      "step": 1442
    },
    {
      "epoch": 3.377413692217671,
      "grad_norm": 0.28158628940582275,
      "learning_rate": 0.0002206869633099141,
      "loss": 0.1362,
      "step": 1443
    },
    {
      "epoch": 3.379754242246928,
      "grad_norm": 0.4776550829410553,
      "learning_rate": 0.00022060889929742387,
      "loss": 0.1554,
      "step": 1444
    },
    {
      "epoch": 3.382094792276185,
      "grad_norm": 0.30350789427757263,
      "learning_rate": 0.00022053083528493362,
      "loss": 0.1237,
      "step": 1445
    },
    {
      "epoch": 3.3844353423054416,
      "grad_norm": 0.3250839412212372,
      "learning_rate": 0.00022045277127244337,
      "loss": 0.1838,
      "step": 1446
    },
    {
      "epoch": 3.3867758923346987,
      "grad_norm": 0.6763313412666321,
      "learning_rate": 0.00022037470725995315,
      "loss": 0.2065,
      "step": 1447
    },
    {
      "epoch": 3.3891164423639557,
      "grad_norm": 0.362170547246933,
      "learning_rate": 0.0002202966432474629,
      "loss": 0.1494,
      "step": 1448
    },
    {
      "epoch": 3.3914569923932123,
      "grad_norm": 0.31655940413475037,
      "learning_rate": 0.00022021857923497265,
      "loss": 0.1245,
      "step": 1449
    },
    {
      "epoch": 3.3937975424224693,
      "grad_norm": 0.29655933380126953,
      "learning_rate": 0.00022014051522248242,
      "loss": 0.1373,
      "step": 1450
    },
    {
      "epoch": 3.396138092451726,
      "grad_norm": 0.2544253170490265,
      "learning_rate": 0.00022006245120999217,
      "loss": 0.078,
      "step": 1451
    },
    {
      "epoch": 3.398478642480983,
      "grad_norm": 0.43688929080963135,
      "learning_rate": 0.00021998438719750193,
      "loss": 0.1571,
      "step": 1452
    },
    {
      "epoch": 3.40081919251024,
      "grad_norm": 0.3151448965072632,
      "learning_rate": 0.0002199063231850117,
      "loss": 0.1457,
      "step": 1453
    },
    {
      "epoch": 3.4031597425394966,
      "grad_norm": 0.34427884221076965,
      "learning_rate": 0.00021982825917252145,
      "loss": 0.1336,
      "step": 1454
    },
    {
      "epoch": 3.4055002925687536,
      "grad_norm": 0.28229066729545593,
      "learning_rate": 0.0002197501951600312,
      "loss": 0.1115,
      "step": 1455
    },
    {
      "epoch": 3.4078408425980107,
      "grad_norm": 0.38048940896987915,
      "learning_rate": 0.00021967213114754098,
      "loss": 0.1205,
      "step": 1456
    },
    {
      "epoch": 3.4101813926272673,
      "grad_norm": 0.4051368534564972,
      "learning_rate": 0.00021959406713505073,
      "loss": 0.1643,
      "step": 1457
    },
    {
      "epoch": 3.4125219426565243,
      "grad_norm": 0.3749653100967407,
      "learning_rate": 0.00021951600312256048,
      "loss": 0.1432,
      "step": 1458
    },
    {
      "epoch": 3.4148624926857813,
      "grad_norm": 0.6256420016288757,
      "learning_rate": 0.00021943793911007026,
      "loss": 0.1648,
      "step": 1459
    },
    {
      "epoch": 3.417203042715038,
      "grad_norm": 0.39910033345222473,
      "learning_rate": 0.00021935987509757998,
      "loss": 0.1533,
      "step": 1460
    },
    {
      "epoch": 3.419543592744295,
      "grad_norm": 0.33810919523239136,
      "learning_rate": 0.00021928181108508973,
      "loss": 0.1752,
      "step": 1461
    },
    {
      "epoch": 3.4218841427735516,
      "grad_norm": 0.39189624786376953,
      "learning_rate": 0.0002192037470725995,
      "loss": 0.1401,
      "step": 1462
    },
    {
      "epoch": 3.4242246928028086,
      "grad_norm": 0.3622637987136841,
      "learning_rate": 0.00021912568306010926,
      "loss": 0.1613,
      "step": 1463
    },
    {
      "epoch": 3.4265652428320656,
      "grad_norm": 0.3147789239883423,
      "learning_rate": 0.000219047619047619,
      "loss": 0.1428,
      "step": 1464
    },
    {
      "epoch": 3.4289057928613222,
      "grad_norm": 0.3906871974468231,
      "learning_rate": 0.0002189695550351288,
      "loss": 0.1506,
      "step": 1465
    },
    {
      "epoch": 3.4312463428905793,
      "grad_norm": 0.4720565974712372,
      "learning_rate": 0.00021889149102263854,
      "loss": 0.1725,
      "step": 1466
    },
    {
      "epoch": 3.4335868929198363,
      "grad_norm": 0.3081052005290985,
      "learning_rate": 0.0002188134270101483,
      "loss": 0.1519,
      "step": 1467
    },
    {
      "epoch": 3.435927442949093,
      "grad_norm": 0.3112882077693939,
      "learning_rate": 0.00021873536299765807,
      "loss": 0.1861,
      "step": 1468
    },
    {
      "epoch": 3.43826799297835,
      "grad_norm": 0.4271341860294342,
      "learning_rate": 0.00021865729898516782,
      "loss": 0.181,
      "step": 1469
    },
    {
      "epoch": 3.440608543007607,
      "grad_norm": 0.3214050531387329,
      "learning_rate": 0.00021857923497267757,
      "loss": 0.1771,
      "step": 1470
    },
    {
      "epoch": 3.4429490930368636,
      "grad_norm": 0.24565353989601135,
      "learning_rate": 0.00021850117096018735,
      "loss": 0.1233,
      "step": 1471
    },
    {
      "epoch": 3.4452896430661206,
      "grad_norm": 0.4261745810508728,
      "learning_rate": 0.0002184231069476971,
      "loss": 0.2164,
      "step": 1472
    },
    {
      "epoch": 3.447630193095377,
      "grad_norm": 0.2955566644668579,
      "learning_rate": 0.00021834504293520685,
      "loss": 0.1602,
      "step": 1473
    },
    {
      "epoch": 3.4499707431246343,
      "grad_norm": 0.3534356355667114,
      "learning_rate": 0.00021826697892271662,
      "loss": 0.1903,
      "step": 1474
    },
    {
      "epoch": 3.4523112931538913,
      "grad_norm": 0.2358357459306717,
      "learning_rate": 0.00021818891491022637,
      "loss": 0.1147,
      "step": 1475
    },
    {
      "epoch": 3.454651843183148,
      "grad_norm": 0.3258298337459564,
      "learning_rate": 0.0002181108508977361,
      "loss": 0.1172,
      "step": 1476
    },
    {
      "epoch": 3.456992393212405,
      "grad_norm": 0.2999716103076935,
      "learning_rate": 0.0002180327868852459,
      "loss": 0.1492,
      "step": 1477
    },
    {
      "epoch": 3.459332943241662,
      "grad_norm": 0.32266849279403687,
      "learning_rate": 0.00021795472287275563,
      "loss": 0.1494,
      "step": 1478
    },
    {
      "epoch": 3.4616734932709186,
      "grad_norm": 0.3282764256000519,
      "learning_rate": 0.00021787665886026538,
      "loss": 0.1229,
      "step": 1479
    },
    {
      "epoch": 3.4640140433001756,
      "grad_norm": 0.3533608019351959,
      "learning_rate": 0.00021779859484777515,
      "loss": 0.1544,
      "step": 1480
    },
    {
      "epoch": 3.4663545933294326,
      "grad_norm": 0.33177220821380615,
      "learning_rate": 0.0002177205308352849,
      "loss": 0.1793,
      "step": 1481
    },
    {
      "epoch": 3.4686951433586892,
      "grad_norm": 0.36665311455726624,
      "learning_rate": 0.00021764246682279465,
      "loss": 0.1843,
      "step": 1482
    },
    {
      "epoch": 3.4710356933879463,
      "grad_norm": 0.29076388478279114,
      "learning_rate": 0.00021756440281030443,
      "loss": 0.1489,
      "step": 1483
    },
    {
      "epoch": 3.473376243417203,
      "grad_norm": 0.3020353615283966,
      "learning_rate": 0.00021748633879781418,
      "loss": 0.1865,
      "step": 1484
    },
    {
      "epoch": 3.47571679344646,
      "grad_norm": 0.25367844104766846,
      "learning_rate": 0.00021740827478532393,
      "loss": 0.1137,
      "step": 1485
    },
    {
      "epoch": 3.478057343475717,
      "grad_norm": 0.21021409332752228,
      "learning_rate": 0.0002173302107728337,
      "loss": 0.0968,
      "step": 1486
    },
    {
      "epoch": 3.4803978935049735,
      "grad_norm": 0.3401426672935486,
      "learning_rate": 0.00021725214676034346,
      "loss": 0.1387,
      "step": 1487
    },
    {
      "epoch": 3.4827384435342306,
      "grad_norm": 0.38352689146995544,
      "learning_rate": 0.0002171740827478532,
      "loss": 0.1826,
      "step": 1488
    },
    {
      "epoch": 3.4850789935634876,
      "grad_norm": 0.3491186797618866,
      "learning_rate": 0.000217096018735363,
      "loss": 0.1754,
      "step": 1489
    },
    {
      "epoch": 3.487419543592744,
      "grad_norm": 0.3538236618041992,
      "learning_rate": 0.00021701795472287274,
      "loss": 0.1772,
      "step": 1490
    },
    {
      "epoch": 3.4897600936220012,
      "grad_norm": 0.2975466847419739,
      "learning_rate": 0.0002169398907103825,
      "loss": 0.1673,
      "step": 1491
    },
    {
      "epoch": 3.4921006436512583,
      "grad_norm": 0.383474200963974,
      "learning_rate": 0.00021686182669789227,
      "loss": 0.1616,
      "step": 1492
    },
    {
      "epoch": 3.494441193680515,
      "grad_norm": 0.22591045498847961,
      "learning_rate": 0.00021678376268540202,
      "loss": 0.11,
      "step": 1493
    },
    {
      "epoch": 3.496781743709772,
      "grad_norm": 0.35237401723861694,
      "learning_rate": 0.00021670569867291174,
      "loss": 0.1472,
      "step": 1494
    },
    {
      "epoch": 3.4991222937390285,
      "grad_norm": 0.33880189061164856,
      "learning_rate": 0.00021662763466042155,
      "loss": 0.1326,
      "step": 1495
    },
    {
      "epoch": 3.5014628437682855,
      "grad_norm": 0.28943267464637756,
      "learning_rate": 0.00021654957064793127,
      "loss": 0.1157,
      "step": 1496
    },
    {
      "epoch": 3.5038033937975426,
      "grad_norm": 0.3464198112487793,
      "learning_rate": 0.00021647150663544102,
      "loss": 0.1406,
      "step": 1497
    },
    {
      "epoch": 3.506143943826799,
      "grad_norm": 0.41926541924476624,
      "learning_rate": 0.0002163934426229508,
      "loss": 0.1556,
      "step": 1498
    },
    {
      "epoch": 3.508484493856056,
      "grad_norm": 0.3902123272418976,
      "learning_rate": 0.00021631537861046055,
      "loss": 0.2045,
      "step": 1499
    },
    {
      "epoch": 3.510825043885313,
      "grad_norm": 0.28664812445640564,
      "learning_rate": 0.0002162373145979703,
      "loss": 0.1194,
      "step": 1500
    },
    {
      "epoch": 3.51316559391457,
      "grad_norm": 0.3930247128009796,
      "learning_rate": 0.00021615925058548008,
      "loss": 0.1686,
      "step": 1501
    },
    {
      "epoch": 3.515506143943827,
      "grad_norm": 0.18170638382434845,
      "learning_rate": 0.00021608118657298983,
      "loss": 0.0905,
      "step": 1502
    },
    {
      "epoch": 3.517846693973084,
      "grad_norm": 0.2458725869655609,
      "learning_rate": 0.00021600312256049958,
      "loss": 0.1133,
      "step": 1503
    },
    {
      "epoch": 3.5201872440023405,
      "grad_norm": 0.25422513484954834,
      "learning_rate": 0.00021592505854800935,
      "loss": 0.0997,
      "step": 1504
    },
    {
      "epoch": 3.5225277940315975,
      "grad_norm": 0.30015671253204346,
      "learning_rate": 0.0002158469945355191,
      "loss": 0.1103,
      "step": 1505
    },
    {
      "epoch": 3.524868344060854,
      "grad_norm": 0.2720787823200226,
      "learning_rate": 0.00021576893052302885,
      "loss": 0.1272,
      "step": 1506
    },
    {
      "epoch": 3.527208894090111,
      "grad_norm": 0.3666893243789673,
      "learning_rate": 0.00021569086651053863,
      "loss": 0.1541,
      "step": 1507
    },
    {
      "epoch": 3.529549444119368,
      "grad_norm": 0.289394348859787,
      "learning_rate": 0.00021561280249804838,
      "loss": 0.1291,
      "step": 1508
    },
    {
      "epoch": 3.531889994148625,
      "grad_norm": 0.28058385848999023,
      "learning_rate": 0.00021553473848555813,
      "loss": 0.0916,
      "step": 1509
    },
    {
      "epoch": 3.534230544177882,
      "grad_norm": 0.4469708204269409,
      "learning_rate": 0.0002154566744730679,
      "loss": 0.2226,
      "step": 1510
    },
    {
      "epoch": 3.5365710942071384,
      "grad_norm": 0.357683390378952,
      "learning_rate": 0.00021537861046057766,
      "loss": 0.1463,
      "step": 1511
    },
    {
      "epoch": 3.5389116442363955,
      "grad_norm": 0.28680163621902466,
      "learning_rate": 0.00021530054644808738,
      "loss": 0.1198,
      "step": 1512
    },
    {
      "epoch": 3.5412521942656525,
      "grad_norm": 0.42510464787483215,
      "learning_rate": 0.0002152224824355972,
      "loss": 0.1826,
      "step": 1513
    },
    {
      "epoch": 3.5435927442949096,
      "grad_norm": 0.34617161750793457,
      "learning_rate": 0.0002151444184231069,
      "loss": 0.1785,
      "step": 1514
    },
    {
      "epoch": 3.545933294324166,
      "grad_norm": 0.3828973174095154,
      "learning_rate": 0.00021506635441061666,
      "loss": 0.1787,
      "step": 1515
    },
    {
      "epoch": 3.548273844353423,
      "grad_norm": 0.3203679025173187,
      "learning_rate": 0.00021498829039812644,
      "loss": 0.1611,
      "step": 1516
    },
    {
      "epoch": 3.55061439438268,
      "grad_norm": 0.38527897000312805,
      "learning_rate": 0.0002149102263856362,
      "loss": 0.1509,
      "step": 1517
    },
    {
      "epoch": 3.552954944411937,
      "grad_norm": 0.3747924864292145,
      "learning_rate": 0.00021483216237314597,
      "loss": 0.1574,
      "step": 1518
    },
    {
      "epoch": 3.555295494441194,
      "grad_norm": 0.24191009998321533,
      "learning_rate": 0.00021475409836065572,
      "loss": 0.1103,
      "step": 1519
    },
    {
      "epoch": 3.5576360444704505,
      "grad_norm": 0.22800368070602417,
      "learning_rate": 0.00021467603434816547,
      "loss": 0.1176,
      "step": 1520
    },
    {
      "epoch": 3.5599765944997075,
      "grad_norm": 0.3614851236343384,
      "learning_rate": 0.00021459797033567525,
      "loss": 0.1608,
      "step": 1521
    },
    {
      "epoch": 3.562317144528964,
      "grad_norm": 0.3779055178165436,
      "learning_rate": 0.000214519906323185,
      "loss": 0.1633,
      "step": 1522
    },
    {
      "epoch": 3.564657694558221,
      "grad_norm": 0.4649943709373474,
      "learning_rate": 0.00021444184231069475,
      "loss": 0.2071,
      "step": 1523
    },
    {
      "epoch": 3.566998244587478,
      "grad_norm": 0.3583996295928955,
      "learning_rate": 0.00021436377829820452,
      "loss": 0.1676,
      "step": 1524
    },
    {
      "epoch": 3.569338794616735,
      "grad_norm": 0.34964457154273987,
      "learning_rate": 0.00021428571428571427,
      "loss": 0.1885,
      "step": 1525
    },
    {
      "epoch": 3.571679344645992,
      "grad_norm": 0.3392292261123657,
      "learning_rate": 0.00021420765027322403,
      "loss": 0.1734,
      "step": 1526
    },
    {
      "epoch": 3.574019894675249,
      "grad_norm": 0.44764837622642517,
      "learning_rate": 0.0002141295862607338,
      "loss": 0.1887,
      "step": 1527
    },
    {
      "epoch": 3.5763604447045054,
      "grad_norm": 0.33358317613601685,
      "learning_rate": 0.00021405152224824355,
      "loss": 0.1247,
      "step": 1528
    },
    {
      "epoch": 3.5787009947337625,
      "grad_norm": 0.36715614795684814,
      "learning_rate": 0.0002139734582357533,
      "loss": 0.174,
      "step": 1529
    },
    {
      "epoch": 3.5810415447630195,
      "grad_norm": 0.40548577904701233,
      "learning_rate": 0.00021389539422326308,
      "loss": 0.1548,
      "step": 1530
    },
    {
      "epoch": 3.583382094792276,
      "grad_norm": 0.2738788425922394,
      "learning_rate": 0.00021381733021077283,
      "loss": 0.143,
      "step": 1531
    },
    {
      "epoch": 3.585722644821533,
      "grad_norm": 0.38196754455566406,
      "learning_rate": 0.00021373926619828255,
      "loss": 0.1727,
      "step": 1532
    },
    {
      "epoch": 3.5880631948507897,
      "grad_norm": 0.3786024749279022,
      "learning_rate": 0.00021366120218579236,
      "loss": 0.1473,
      "step": 1533
    },
    {
      "epoch": 3.5904037448800468,
      "grad_norm": 0.35339781641960144,
      "learning_rate": 0.00021358313817330208,
      "loss": 0.1784,
      "step": 1534
    },
    {
      "epoch": 3.592744294909304,
      "grad_norm": 0.24654072523117065,
      "learning_rate": 0.00021350507416081183,
      "loss": 0.1279,
      "step": 1535
    },
    {
      "epoch": 3.5950848449385604,
      "grad_norm": 0.374888151884079,
      "learning_rate": 0.0002134270101483216,
      "loss": 0.1531,
      "step": 1536
    },
    {
      "epoch": 3.5974253949678174,
      "grad_norm": 0.2579246759414673,
      "learning_rate": 0.00021334894613583136,
      "loss": 0.1485,
      "step": 1537
    },
    {
      "epoch": 3.5997659449970745,
      "grad_norm": 0.3882656991481781,
      "learning_rate": 0.0002132708821233411,
      "loss": 0.1994,
      "step": 1538
    },
    {
      "epoch": 3.602106495026331,
      "grad_norm": 0.28823211789131165,
      "learning_rate": 0.0002131928181108509,
      "loss": 0.113,
      "step": 1539
    },
    {
      "epoch": 3.604447045055588,
      "grad_norm": 0.28291580080986023,
      "learning_rate": 0.00021311475409836064,
      "loss": 0.121,
      "step": 1540
    },
    {
      "epoch": 3.606787595084845,
      "grad_norm": 0.333738774061203,
      "learning_rate": 0.0002130366900858704,
      "loss": 0.1452,
      "step": 1541
    },
    {
      "epoch": 3.6091281451141017,
      "grad_norm": 0.24009394645690918,
      "learning_rate": 0.00021295862607338017,
      "loss": 0.1307,
      "step": 1542
    },
    {
      "epoch": 3.6114686951433588,
      "grad_norm": 0.35157278180122375,
      "learning_rate": 0.00021288056206088992,
      "loss": 0.206,
      "step": 1543
    },
    {
      "epoch": 3.6138092451726154,
      "grad_norm": 0.30370762944221497,
      "learning_rate": 0.00021280249804839967,
      "loss": 0.1481,
      "step": 1544
    },
    {
      "epoch": 3.6161497952018724,
      "grad_norm": 0.2353346347808838,
      "learning_rate": 0.00021272443403590945,
      "loss": 0.1253,
      "step": 1545
    },
    {
      "epoch": 3.6184903452311294,
      "grad_norm": 0.36175063252449036,
      "learning_rate": 0.0002126463700234192,
      "loss": 0.2118,
      "step": 1546
    },
    {
      "epoch": 3.620830895260386,
      "grad_norm": 0.4848809540271759,
      "learning_rate": 0.00021256830601092895,
      "loss": 0.1268,
      "step": 1547
    },
    {
      "epoch": 3.623171445289643,
      "grad_norm": 0.25384587049484253,
      "learning_rate": 0.00021249024199843872,
      "loss": 0.1164,
      "step": 1548
    },
    {
      "epoch": 3.6255119953188997,
      "grad_norm": 0.39441195130348206,
      "learning_rate": 0.00021241217798594847,
      "loss": 0.173,
      "step": 1549
    },
    {
      "epoch": 3.6278525453481567,
      "grad_norm": 0.32943040132522583,
      "learning_rate": 0.0002123341139734582,
      "loss": 0.1585,
      "step": 1550
    },
    {
      "epoch": 3.6301930953774137,
      "grad_norm": 0.31584054231643677,
      "learning_rate": 0.000212256049960968,
      "loss": 0.1469,
      "step": 1551
    },
    {
      "epoch": 3.632533645406671,
      "grad_norm": 0.37478649616241455,
      "learning_rate": 0.00021217798594847773,
      "loss": 0.1275,
      "step": 1552
    },
    {
      "epoch": 3.6348741954359274,
      "grad_norm": 0.3904845118522644,
      "learning_rate": 0.00021209992193598748,
      "loss": 0.1197,
      "step": 1553
    },
    {
      "epoch": 3.6372147454651844,
      "grad_norm": 0.3613046109676361,
      "learning_rate": 0.00021202185792349725,
      "loss": 0.1524,
      "step": 1554
    },
    {
      "epoch": 3.639555295494441,
      "grad_norm": 0.4227735996246338,
      "learning_rate": 0.000211943793911007,
      "loss": 0.1498,
      "step": 1555
    },
    {
      "epoch": 3.641895845523698,
      "grad_norm": 0.3458271622657776,
      "learning_rate": 0.00021186572989851675,
      "loss": 0.1255,
      "step": 1556
    },
    {
      "epoch": 3.644236395552955,
      "grad_norm": 0.3619558811187744,
      "learning_rate": 0.00021178766588602653,
      "loss": 0.1461,
      "step": 1557
    },
    {
      "epoch": 3.6465769455822117,
      "grad_norm": 0.30218344926834106,
      "learning_rate": 0.00021170960187353628,
      "loss": 0.1156,
      "step": 1558
    },
    {
      "epoch": 3.6489174956114687,
      "grad_norm": 0.49854278564453125,
      "learning_rate": 0.00021163153786104603,
      "loss": 0.1806,
      "step": 1559
    },
    {
      "epoch": 3.6512580456407253,
      "grad_norm": 0.41567638516426086,
      "learning_rate": 0.0002115534738485558,
      "loss": 0.163,
      "step": 1560
    },
    {
      "epoch": 3.6535985956699824,
      "grad_norm": 0.4231645464897156,
      "learning_rate": 0.00021147540983606556,
      "loss": 0.1822,
      "step": 1561
    },
    {
      "epoch": 3.6559391456992394,
      "grad_norm": 0.32020094990730286,
      "learning_rate": 0.0002113973458235753,
      "loss": 0.1259,
      "step": 1562
    },
    {
      "epoch": 3.6582796957284964,
      "grad_norm": 0.36149007081985474,
      "learning_rate": 0.0002113192818110851,
      "loss": 0.1895,
      "step": 1563
    },
    {
      "epoch": 3.660620245757753,
      "grad_norm": 0.2909274697303772,
      "learning_rate": 0.00021124121779859484,
      "loss": 0.1357,
      "step": 1564
    },
    {
      "epoch": 3.66296079578701,
      "grad_norm": 0.3504485785961151,
      "learning_rate": 0.0002111631537861046,
      "loss": 0.1651,
      "step": 1565
    },
    {
      "epoch": 3.6653013458162667,
      "grad_norm": 0.35858801007270813,
      "learning_rate": 0.00021108508977361437,
      "loss": 0.1585,
      "step": 1566
    },
    {
      "epoch": 3.6676418958455237,
      "grad_norm": 0.27720993757247925,
      "learning_rate": 0.00021100702576112412,
      "loss": 0.1391,
      "step": 1567
    },
    {
      "epoch": 3.6699824458747807,
      "grad_norm": 0.3762248158454895,
      "learning_rate": 0.00021092896174863384,
      "loss": 0.1731,
      "step": 1568
    },
    {
      "epoch": 3.6723229959040373,
      "grad_norm": 0.3014313578605652,
      "learning_rate": 0.00021085089773614362,
      "loss": 0.153,
      "step": 1569
    },
    {
      "epoch": 3.6746635459332944,
      "grad_norm": 0.25636500120162964,
      "learning_rate": 0.00021077283372365337,
      "loss": 0.1008,
      "step": 1570
    },
    {
      "epoch": 3.677004095962551,
      "grad_norm": 0.24955971539020538,
      "learning_rate": 0.00021069476971116312,
      "loss": 0.1451,
      "step": 1571
    },
    {
      "epoch": 3.679344645991808,
      "grad_norm": 0.27494314312934875,
      "learning_rate": 0.0002106167056986729,
      "loss": 0.1534,
      "step": 1572
    },
    {
      "epoch": 3.681685196021065,
      "grad_norm": 0.3238270580768585,
      "learning_rate": 0.00021053864168618265,
      "loss": 0.1731,
      "step": 1573
    },
    {
      "epoch": 3.684025746050322,
      "grad_norm": 0.3789941370487213,
      "learning_rate": 0.0002104605776736924,
      "loss": 0.2079,
      "step": 1574
    },
    {
      "epoch": 3.6863662960795787,
      "grad_norm": 0.31905049085617065,
      "learning_rate": 0.00021038251366120217,
      "loss": 0.1539,
      "step": 1575
    },
    {
      "epoch": 3.6887068461088357,
      "grad_norm": 0.26101019978523254,
      "learning_rate": 0.00021030444964871193,
      "loss": 0.1303,
      "step": 1576
    },
    {
      "epoch": 3.6910473961380923,
      "grad_norm": 0.29497766494750977,
      "learning_rate": 0.00021022638563622168,
      "loss": 0.1389,
      "step": 1577
    },
    {
      "epoch": 3.6933879461673493,
      "grad_norm": 0.33315256237983704,
      "learning_rate": 0.00021014832162373145,
      "loss": 0.1615,
      "step": 1578
    },
    {
      "epoch": 3.6957284961966064,
      "grad_norm": 0.41421574354171753,
      "learning_rate": 0.0002100702576112412,
      "loss": 0.1923,
      "step": 1579
    },
    {
      "epoch": 3.698069046225863,
      "grad_norm": 0.44090110063552856,
      "learning_rate": 0.00020999219359875095,
      "loss": 0.1839,
      "step": 1580
    },
    {
      "epoch": 3.70040959625512,
      "grad_norm": 0.2770061194896698,
      "learning_rate": 0.00020991412958626073,
      "loss": 0.1531,
      "step": 1581
    },
    {
      "epoch": 3.7027501462843766,
      "grad_norm": 0.4962296187877655,
      "learning_rate": 0.00020983606557377048,
      "loss": 0.1861,
      "step": 1582
    },
    {
      "epoch": 3.7050906963136336,
      "grad_norm": 0.39237603545188904,
      "learning_rate": 0.00020975800156128023,
      "loss": 0.1291,
      "step": 1583
    },
    {
      "epoch": 3.7074312463428907,
      "grad_norm": 0.32527437806129456,
      "learning_rate": 0.00020967993754879,
      "loss": 0.1367,
      "step": 1584
    },
    {
      "epoch": 3.7097717963721477,
      "grad_norm": 0.3557737171649933,
      "learning_rate": 0.00020960187353629976,
      "loss": 0.1459,
      "step": 1585
    },
    {
      "epoch": 3.7121123464014043,
      "grad_norm": 0.26544204354286194,
      "learning_rate": 0.00020952380952380948,
      "loss": 0.131,
      "step": 1586
    },
    {
      "epoch": 3.7144528964306613,
      "grad_norm": 0.33430421352386475,
      "learning_rate": 0.00020944574551131926,
      "loss": 0.1517,
      "step": 1587
    },
    {
      "epoch": 3.716793446459918,
      "grad_norm": 0.35826173424720764,
      "learning_rate": 0.000209367681498829,
      "loss": 0.14,
      "step": 1588
    },
    {
      "epoch": 3.719133996489175,
      "grad_norm": 0.3224731683731079,
      "learning_rate": 0.00020928961748633876,
      "loss": 0.1535,
      "step": 1589
    },
    {
      "epoch": 3.721474546518432,
      "grad_norm": 0.418721467256546,
      "learning_rate": 0.00020921155347384854,
      "loss": 0.1129,
      "step": 1590
    },
    {
      "epoch": 3.7238150965476886,
      "grad_norm": 0.31573134660720825,
      "learning_rate": 0.0002091334894613583,
      "loss": 0.1413,
      "step": 1591
    },
    {
      "epoch": 3.7261556465769456,
      "grad_norm": 0.4024236798286438,
      "learning_rate": 0.00020905542544886804,
      "loss": 0.1941,
      "step": 1592
    },
    {
      "epoch": 3.7284961966062022,
      "grad_norm": 0.35264405608177185,
      "learning_rate": 0.00020897736143637782,
      "loss": 0.1929,
      "step": 1593
    },
    {
      "epoch": 3.7308367466354593,
      "grad_norm": 0.38661080598831177,
      "learning_rate": 0.00020889929742388757,
      "loss": 0.1794,
      "step": 1594
    },
    {
      "epoch": 3.7331772966647163,
      "grad_norm": 0.4579138159751892,
      "learning_rate": 0.00020882123341139732,
      "loss": 0.1598,
      "step": 1595
    },
    {
      "epoch": 3.7355178466939734,
      "grad_norm": 0.35339808464050293,
      "learning_rate": 0.0002087431693989071,
      "loss": 0.1549,
      "step": 1596
    },
    {
      "epoch": 3.73785839672323,
      "grad_norm": 0.3797571063041687,
      "learning_rate": 0.00020866510538641685,
      "loss": 0.1805,
      "step": 1597
    },
    {
      "epoch": 3.740198946752487,
      "grad_norm": 0.28225162625312805,
      "learning_rate": 0.0002085870413739266,
      "loss": 0.1036,
      "step": 1598
    },
    {
      "epoch": 3.7425394967817436,
      "grad_norm": 0.4259050786495209,
      "learning_rate": 0.00020850897736143637,
      "loss": 0.1872,
      "step": 1599
    },
    {
      "epoch": 3.7448800468110006,
      "grad_norm": 0.29583850502967834,
      "learning_rate": 0.00020843091334894612,
      "loss": 0.1486,
      "step": 1600
    },
    {
      "epoch": 3.7472205968402577,
      "grad_norm": 0.43726882338523865,
      "learning_rate": 0.00020835284933645588,
      "loss": 0.1597,
      "step": 1601
    },
    {
      "epoch": 3.7495611468695142,
      "grad_norm": 0.24686822295188904,
      "learning_rate": 0.00020827478532396565,
      "loss": 0.1214,
      "step": 1602
    },
    {
      "epoch": 3.7519016968987713,
      "grad_norm": 0.2543211579322815,
      "learning_rate": 0.00020819672131147538,
      "loss": 0.1056,
      "step": 1603
    },
    {
      "epoch": 3.754242246928028,
      "grad_norm": 0.2354949563741684,
      "learning_rate": 0.00020811865729898513,
      "loss": 0.1048,
      "step": 1604
    },
    {
      "epoch": 3.756582796957285,
      "grad_norm": 0.2587014138698578,
      "learning_rate": 0.0002080405932864949,
      "loss": 0.1381,
      "step": 1605
    },
    {
      "epoch": 3.758923346986542,
      "grad_norm": 0.21573930978775024,
      "learning_rate": 0.00020796252927400465,
      "loss": 0.1184,
      "step": 1606
    },
    {
      "epoch": 3.7612638970157986,
      "grad_norm": 0.2563351094722748,
      "learning_rate": 0.0002078844652615144,
      "loss": 0.1262,
      "step": 1607
    },
    {
      "epoch": 3.7636044470450556,
      "grad_norm": 0.22003257274627686,
      "learning_rate": 0.00020780640124902418,
      "loss": 0.1156,
      "step": 1608
    },
    {
      "epoch": 3.7659449970743126,
      "grad_norm": 0.27185770869255066,
      "learning_rate": 0.00020772833723653393,
      "loss": 0.1212,
      "step": 1609
    },
    {
      "epoch": 3.768285547103569,
      "grad_norm": 0.4021546244621277,
      "learning_rate": 0.00020765027322404368,
      "loss": 0.1656,
      "step": 1610
    },
    {
      "epoch": 3.7706260971328263,
      "grad_norm": 0.37503740191459656,
      "learning_rate": 0.00020757220921155346,
      "loss": 0.1445,
      "step": 1611
    },
    {
      "epoch": 3.7729666471620833,
      "grad_norm": 0.3787154257297516,
      "learning_rate": 0.0002074941451990632,
      "loss": 0.1266,
      "step": 1612
    },
    {
      "epoch": 3.77530719719134,
      "grad_norm": 0.35222718119621277,
      "learning_rate": 0.00020741608118657296,
      "loss": 0.1949,
      "step": 1613
    },
    {
      "epoch": 3.777647747220597,
      "grad_norm": 0.26752740144729614,
      "learning_rate": 0.00020733801717408274,
      "loss": 0.1342,
      "step": 1614
    },
    {
      "epoch": 3.7799882972498535,
      "grad_norm": 0.37965670228004456,
      "learning_rate": 0.0002072599531615925,
      "loss": 0.1539,
      "step": 1615
    },
    {
      "epoch": 3.7823288472791106,
      "grad_norm": 0.34934285283088684,
      "learning_rate": 0.00020718188914910224,
      "loss": 0.1406,
      "step": 1616
    },
    {
      "epoch": 3.7846693973083676,
      "grad_norm": 0.29082104563713074,
      "learning_rate": 0.00020710382513661202,
      "loss": 0.1338,
      "step": 1617
    },
    {
      "epoch": 3.787009947337624,
      "grad_norm": 0.29278624057769775,
      "learning_rate": 0.00020702576112412177,
      "loss": 0.1359,
      "step": 1618
    },
    {
      "epoch": 3.7893504973668812,
      "grad_norm": 0.41471415758132935,
      "learning_rate": 0.00020694769711163152,
      "loss": 0.153,
      "step": 1619
    },
    {
      "epoch": 3.791691047396138,
      "grad_norm": 0.3409372568130493,
      "learning_rate": 0.0002068696330991413,
      "loss": 0.1376,
      "step": 1620
    },
    {
      "epoch": 3.794031597425395,
      "grad_norm": 0.2538366913795471,
      "learning_rate": 0.00020679156908665102,
      "loss": 0.1376,
      "step": 1621
    },
    {
      "epoch": 3.796372147454652,
      "grad_norm": 0.35208213329315186,
      "learning_rate": 0.00020671350507416077,
      "loss": 0.1519,
      "step": 1622
    },
    {
      "epoch": 3.798712697483909,
      "grad_norm": 0.2544875741004944,
      "learning_rate": 0.00020663544106167055,
      "loss": 0.1071,
      "step": 1623
    },
    {
      "epoch": 3.8010532475131655,
      "grad_norm": 0.2880212366580963,
      "learning_rate": 0.0002065573770491803,
      "loss": 0.1231,
      "step": 1624
    },
    {
      "epoch": 3.8033937975424226,
      "grad_norm": 0.3837326765060425,
      "learning_rate": 0.00020647931303669005,
      "loss": 0.1656,
      "step": 1625
    },
    {
      "epoch": 3.805734347571679,
      "grad_norm": 0.34158748388290405,
      "learning_rate": 0.00020640124902419983,
      "loss": 0.1535,
      "step": 1626
    },
    {
      "epoch": 3.808074897600936,
      "grad_norm": 0.42601025104522705,
      "learning_rate": 0.00020632318501170958,
      "loss": 0.1761,
      "step": 1627
    },
    {
      "epoch": 3.8104154476301932,
      "grad_norm": 0.31744515895843506,
      "learning_rate": 0.00020624512099921933,
      "loss": 0.1382,
      "step": 1628
    },
    {
      "epoch": 3.81275599765945,
      "grad_norm": 0.23966741561889648,
      "learning_rate": 0.0002061670569867291,
      "loss": 0.1171,
      "step": 1629
    },
    {
      "epoch": 3.815096547688707,
      "grad_norm": 0.27324166893959045,
      "learning_rate": 0.00020608899297423885,
      "loss": 0.125,
      "step": 1630
    },
    {
      "epoch": 3.8174370977179635,
      "grad_norm": 0.22807331383228302,
      "learning_rate": 0.0002060109289617486,
      "loss": 0.0894,
      "step": 1631
    },
    {
      "epoch": 3.8197776477472205,
      "grad_norm": 0.3745083510875702,
      "learning_rate": 0.00020593286494925838,
      "loss": 0.1387,
      "step": 1632
    },
    {
      "epoch": 3.8221181977764775,
      "grad_norm": 0.384072870016098,
      "learning_rate": 0.00020585480093676813,
      "loss": 0.1819,
      "step": 1633
    },
    {
      "epoch": 3.8244587478057346,
      "grad_norm": 0.2985229194164276,
      "learning_rate": 0.00020577673692427788,
      "loss": 0.1263,
      "step": 1634
    },
    {
      "epoch": 3.826799297834991,
      "grad_norm": 0.3629896938800812,
      "learning_rate": 0.00020569867291178766,
      "loss": 0.1564,
      "step": 1635
    },
    {
      "epoch": 3.829139847864248,
      "grad_norm": 0.41033294796943665,
      "learning_rate": 0.0002056206088992974,
      "loss": 0.1823,
      "step": 1636
    },
    {
      "epoch": 3.831480397893505,
      "grad_norm": 0.40927547216415405,
      "learning_rate": 0.00020554254488680716,
      "loss": 0.1572,
      "step": 1637
    },
    {
      "epoch": 3.833820947922762,
      "grad_norm": 0.3398902416229248,
      "learning_rate": 0.00020546448087431694,
      "loss": 0.1231,
      "step": 1638
    },
    {
      "epoch": 3.836161497952019,
      "grad_norm": 0.3518325388431549,
      "learning_rate": 0.00020538641686182666,
      "loss": 0.1447,
      "step": 1639
    },
    {
      "epoch": 3.8385020479812755,
      "grad_norm": 0.38548147678375244,
      "learning_rate": 0.0002053083528493364,
      "loss": 0.1619,
      "step": 1640
    },
    {
      "epoch": 3.8408425980105325,
      "grad_norm": 0.4068719744682312,
      "learning_rate": 0.0002052302888368462,
      "loss": 0.1848,
      "step": 1641
    },
    {
      "epoch": 3.843183148039789,
      "grad_norm": 0.4162457585334778,
      "learning_rate": 0.00020515222482435594,
      "loss": 0.1756,
      "step": 1642
    },
    {
      "epoch": 3.845523698069046,
      "grad_norm": 0.29736801981925964,
      "learning_rate": 0.0002050741608118657,
      "loss": 0.1305,
      "step": 1643
    },
    {
      "epoch": 3.847864248098303,
      "grad_norm": 0.36129501461982727,
      "learning_rate": 0.00020499609679937547,
      "loss": 0.1765,
      "step": 1644
    },
    {
      "epoch": 3.8502047981275602,
      "grad_norm": 0.3289724290370941,
      "learning_rate": 0.00020491803278688522,
      "loss": 0.1592,
      "step": 1645
    },
    {
      "epoch": 3.852545348156817,
      "grad_norm": 0.39762061834335327,
      "learning_rate": 0.000204839968774395,
      "loss": 0.1744,
      "step": 1646
    },
    {
      "epoch": 3.854885898186074,
      "grad_norm": 0.2252778559923172,
      "learning_rate": 0.00020476190476190475,
      "loss": 0.1373,
      "step": 1647
    },
    {
      "epoch": 3.8572264482153304,
      "grad_norm": 0.3672543466091156,
      "learning_rate": 0.0002046838407494145,
      "loss": 0.176,
      "step": 1648
    },
    {
      "epoch": 3.8595669982445875,
      "grad_norm": 0.2555774450302124,
      "learning_rate": 0.00020460577673692427,
      "loss": 0.1185,
      "step": 1649
    },
    {
      "epoch": 3.8619075482738445,
      "grad_norm": 0.43231070041656494,
      "learning_rate": 0.00020452771272443402,
      "loss": 0.1834,
      "step": 1650
    },
    {
      "epoch": 3.864248098303101,
      "grad_norm": 0.3681057393550873,
      "learning_rate": 0.00020444964871194378,
      "loss": 0.18,
      "step": 1651
    },
    {
      "epoch": 3.866588648332358,
      "grad_norm": 0.2943790555000305,
      "learning_rate": 0.00020437158469945355,
      "loss": 0.1606,
      "step": 1652
    },
    {
      "epoch": 3.8689291983616148,
      "grad_norm": 0.24892200529575348,
      "learning_rate": 0.0002042935206869633,
      "loss": 0.126,
      "step": 1653
    },
    {
      "epoch": 3.871269748390872,
      "grad_norm": 0.30854150652885437,
      "learning_rate": 0.00020421545667447305,
      "loss": 0.1663,
      "step": 1654
    },
    {
      "epoch": 3.873610298420129,
      "grad_norm": 0.2929118573665619,
      "learning_rate": 0.00020413739266198283,
      "loss": 0.1635,
      "step": 1655
    },
    {
      "epoch": 3.875950848449386,
      "grad_norm": 0.31337904930114746,
      "learning_rate": 0.00020405932864949258,
      "loss": 0.1463,
      "step": 1656
    },
    {
      "epoch": 3.8782913984786425,
      "grad_norm": 0.28175491094589233,
      "learning_rate": 0.0002039812646370023,
      "loss": 0.1701,
      "step": 1657
    },
    {
      "epoch": 3.8806319485078995,
      "grad_norm": 0.2398785948753357,
      "learning_rate": 0.0002039032006245121,
      "loss": 0.1337,
      "step": 1658
    },
    {
      "epoch": 3.882972498537156,
      "grad_norm": 0.27082252502441406,
      "learning_rate": 0.00020382513661202183,
      "loss": 0.1425,
      "step": 1659
    },
    {
      "epoch": 3.885313048566413,
      "grad_norm": 0.3352137804031372,
      "learning_rate": 0.00020374707259953158,
      "loss": 0.1749,
      "step": 1660
    },
    {
      "epoch": 3.88765359859567,
      "grad_norm": 0.34152939915657043,
      "learning_rate": 0.00020366900858704136,
      "loss": 0.1422,
      "step": 1661
    },
    {
      "epoch": 3.8899941486249268,
      "grad_norm": 0.36010968685150146,
      "learning_rate": 0.0002035909445745511,
      "loss": 0.1884,
      "step": 1662
    },
    {
      "epoch": 3.892334698654184,
      "grad_norm": 0.31685903668403625,
      "learning_rate": 0.00020351288056206086,
      "loss": 0.0999,
      "step": 1663
    },
    {
      "epoch": 3.8946752486834404,
      "grad_norm": 0.2502603828907013,
      "learning_rate": 0.00020343481654957064,
      "loss": 0.1331,
      "step": 1664
    },
    {
      "epoch": 3.8970157987126974,
      "grad_norm": 0.30726343393325806,
      "learning_rate": 0.0002033567525370804,
      "loss": 0.124,
      "step": 1665
    },
    {
      "epoch": 3.8993563487419545,
      "grad_norm": 0.33119502663612366,
      "learning_rate": 0.00020327868852459014,
      "loss": 0.1646,
      "step": 1666
    },
    {
      "epoch": 3.9016968987712115,
      "grad_norm": 0.3709515333175659,
      "learning_rate": 0.00020320062451209992,
      "loss": 0.1855,
      "step": 1667
    },
    {
      "epoch": 3.904037448800468,
      "grad_norm": 0.29318058490753174,
      "learning_rate": 0.00020312256049960967,
      "loss": 0.1552,
      "step": 1668
    },
    {
      "epoch": 3.906377998829725,
      "grad_norm": 0.4451057016849518,
      "learning_rate": 0.00020304449648711942,
      "loss": 0.1542,
      "step": 1669
    },
    {
      "epoch": 3.9087185488589817,
      "grad_norm": 0.27683544158935547,
      "learning_rate": 0.0002029664324746292,
      "loss": 0.1148,
      "step": 1670
    },
    {
      "epoch": 3.9110590988882388,
      "grad_norm": 0.37465736269950867,
      "learning_rate": 0.00020288836846213895,
      "loss": 0.1807,
      "step": 1671
    },
    {
      "epoch": 3.913399648917496,
      "grad_norm": 0.3734848201274872,
      "learning_rate": 0.0002028103044496487,
      "loss": 0.1731,
      "step": 1672
    },
    {
      "epoch": 3.9157401989467524,
      "grad_norm": 0.29611021280288696,
      "learning_rate": 0.00020273224043715847,
      "loss": 0.137,
      "step": 1673
    },
    {
      "epoch": 3.9180807489760094,
      "grad_norm": 0.3437877893447876,
      "learning_rate": 0.00020265417642466822,
      "loss": 0.1418,
      "step": 1674
    },
    {
      "epoch": 3.920421299005266,
      "grad_norm": 0.3181118965148926,
      "learning_rate": 0.00020257611241217795,
      "loss": 0.1248,
      "step": 1675
    },
    {
      "epoch": 3.922761849034523,
      "grad_norm": 0.26856401562690735,
      "learning_rate": 0.00020249804839968775,
      "loss": 0.1062,
      "step": 1676
    },
    {
      "epoch": 3.92510239906378,
      "grad_norm": 0.37035199999809265,
      "learning_rate": 0.00020241998438719748,
      "loss": 0.1438,
      "step": 1677
    },
    {
      "epoch": 3.9274429490930367,
      "grad_norm": 0.28479963541030884,
      "learning_rate": 0.00020234192037470723,
      "loss": 0.1392,
      "step": 1678
    },
    {
      "epoch": 3.9297834991222937,
      "grad_norm": 0.37093859910964966,
      "learning_rate": 0.000202263856362217,
      "loss": 0.1343,
      "step": 1679
    },
    {
      "epoch": 3.9321240491515503,
      "grad_norm": 0.4188712239265442,
      "learning_rate": 0.00020218579234972675,
      "loss": 0.1624,
      "step": 1680
    },
    {
      "epoch": 3.9344645991808074,
      "grad_norm": 0.2986404299736023,
      "learning_rate": 0.0002021077283372365,
      "loss": 0.1291,
      "step": 1681
    },
    {
      "epoch": 3.9368051492100644,
      "grad_norm": 0.33328157663345337,
      "learning_rate": 0.00020202966432474628,
      "loss": 0.0953,
      "step": 1682
    },
    {
      "epoch": 3.9391456992393215,
      "grad_norm": 0.3273293077945709,
      "learning_rate": 0.00020195160031225603,
      "loss": 0.1356,
      "step": 1683
    },
    {
      "epoch": 3.941486249268578,
      "grad_norm": 0.4642309546470642,
      "learning_rate": 0.00020187353629976578,
      "loss": 0.1662,
      "step": 1684
    },
    {
      "epoch": 3.943826799297835,
      "grad_norm": 0.43405023217201233,
      "learning_rate": 0.00020179547228727556,
      "loss": 0.2049,
      "step": 1685
    },
    {
      "epoch": 3.9461673493270917,
      "grad_norm": 0.32871463894844055,
      "learning_rate": 0.0002017174082747853,
      "loss": 0.1155,
      "step": 1686
    },
    {
      "epoch": 3.9485078993563487,
      "grad_norm": 0.2781261205673218,
      "learning_rate": 0.00020163934426229506,
      "loss": 0.1378,
      "step": 1687
    },
    {
      "epoch": 3.9508484493856058,
      "grad_norm": 0.2876938581466675,
      "learning_rate": 0.00020156128024980484,
      "loss": 0.1264,
      "step": 1688
    },
    {
      "epoch": 3.9531889994148623,
      "grad_norm": 0.40765854716300964,
      "learning_rate": 0.0002014832162373146,
      "loss": 0.1302,
      "step": 1689
    },
    {
      "epoch": 3.9555295494441194,
      "grad_norm": 0.2857882082462311,
      "learning_rate": 0.00020140515222482434,
      "loss": 0.1783,
      "step": 1690
    },
    {
      "epoch": 3.957870099473376,
      "grad_norm": 0.3002341091632843,
      "learning_rate": 0.00020132708821233412,
      "loss": 0.16,
      "step": 1691
    },
    {
      "epoch": 3.960210649502633,
      "grad_norm": 0.3965288996696472,
      "learning_rate": 0.00020124902419984387,
      "loss": 0.159,
      "step": 1692
    },
    {
      "epoch": 3.96255119953189,
      "grad_norm": 0.35279691219329834,
      "learning_rate": 0.0002011709601873536,
      "loss": 0.1562,
      "step": 1693
    },
    {
      "epoch": 3.964891749561147,
      "grad_norm": 0.26922374963760376,
      "learning_rate": 0.0002010928961748634,
      "loss": 0.1364,
      "step": 1694
    },
    {
      "epoch": 3.9672322995904037,
      "grad_norm": 0.4029492437839508,
      "learning_rate": 0.00020101483216237312,
      "loss": 0.1625,
      "step": 1695
    },
    {
      "epoch": 3.9695728496196607,
      "grad_norm": 0.30846139788627625,
      "learning_rate": 0.00020093676814988287,
      "loss": 0.1176,
      "step": 1696
    },
    {
      "epoch": 3.9719133996489173,
      "grad_norm": 0.2408073991537094,
      "learning_rate": 0.00020085870413739265,
      "loss": 0.0999,
      "step": 1697
    },
    {
      "epoch": 3.9742539496781744,
      "grad_norm": 0.35472971200942993,
      "learning_rate": 0.0002007806401249024,
      "loss": 0.1465,
      "step": 1698
    },
    {
      "epoch": 3.9765944997074314,
      "grad_norm": 0.353672057390213,
      "learning_rate": 0.00020070257611241215,
      "loss": 0.1328,
      "step": 1699
    },
    {
      "epoch": 3.978935049736688,
      "grad_norm": 0.3733687400817871,
      "learning_rate": 0.00020062451209992192,
      "loss": 0.1575,
      "step": 1700
    },
    {
      "epoch": 3.981275599765945,
      "grad_norm": 0.4715711176395416,
      "learning_rate": 0.00020054644808743168,
      "loss": 0.2611,
      "step": 1701
    },
    {
      "epoch": 3.9836161497952016,
      "grad_norm": 0.3158032298088074,
      "learning_rate": 0.00020046838407494143,
      "loss": 0.1699,
      "step": 1702
    },
    {
      "epoch": 3.9859566998244587,
      "grad_norm": 0.40478605031967163,
      "learning_rate": 0.0002003903200624512,
      "loss": 0.1722,
      "step": 1703
    },
    {
      "epoch": 3.9882972498537157,
      "grad_norm": 0.18799607455730438,
      "learning_rate": 0.00020031225604996095,
      "loss": 0.0798,
      "step": 1704
    },
    {
      "epoch": 3.9906377998829727,
      "grad_norm": 0.37990802526474,
      "learning_rate": 0.0002002341920374707,
      "loss": 0.1343,
      "step": 1705
    },
    {
      "epoch": 3.9929783499122293,
      "grad_norm": 0.4571920335292816,
      "learning_rate": 0.00020015612802498048,
      "loss": 0.2257,
      "step": 1706
    },
    {
      "epoch": 3.9953188999414864,
      "grad_norm": 0.3130652904510498,
      "learning_rate": 0.00020007806401249023,
      "loss": 0.144,
      "step": 1707
    },
    {
      "epoch": 3.997659449970743,
      "grad_norm": 0.3781755566596985,
      "learning_rate": 0.00019999999999999998,
      "loss": 0.1692,
      "step": 1708
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.3372117877006531,
      "learning_rate": 0.00019992193598750976,
      "loss": 0.1684,
      "step": 1709
    },
    {
      "epoch": 4.0,
      "eval_loss": 0.2214934378862381,
      "eval_runtime": 128.3614,
      "eval_samples_per_second": 4.3,
      "eval_steps_per_second": 0.538,
      "step": 1709
    }
  ],
  "logging_steps": 1,
  "max_steps": 4270,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 4.7435689200490906e+17,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
