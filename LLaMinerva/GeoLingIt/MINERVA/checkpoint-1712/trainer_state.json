{
  "best_metric": 0.22446826100349426,
  "best_model_checkpoint": "LLaMinerva/GeoLingIt/MINERVA/checkpoint-1712",
  "epoch": 8.0,
  "eval_steps": 500,
  "global_step": 1712,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.004672897196261682,
      "grad_norm": 1.8815052509307861,
      "learning_rate": 1.4018691588785045e-06,
      "loss": 3.3003,
      "step": 1
    },
    {
      "epoch": 0.009345794392523364,
      "grad_norm": 1.8835419416427612,
      "learning_rate": 2.803738317757009e-06,
      "loss": 3.331,
      "step": 2
    },
    {
      "epoch": 0.014018691588785047,
      "grad_norm": 1.9395310878753662,
      "learning_rate": 4.205607476635513e-06,
      "loss": 3.3541,
      "step": 3
    },
    {
      "epoch": 0.018691588785046728,
      "grad_norm": 1.8656501770019531,
      "learning_rate": 5.607476635514018e-06,
      "loss": 3.3223,
      "step": 4
    },
    {
      "epoch": 0.02336448598130841,
      "grad_norm": 1.8692481517791748,
      "learning_rate": 7.0093457943925225e-06,
      "loss": 3.3132,
      "step": 5
    },
    {
      "epoch": 0.028037383177570093,
      "grad_norm": 1.8971928358078003,
      "learning_rate": 8.411214953271026e-06,
      "loss": 3.2866,
      "step": 6
    },
    {
      "epoch": 0.03271028037383177,
      "grad_norm": 1.8366154432296753,
      "learning_rate": 9.813084112149531e-06,
      "loss": 3.33,
      "step": 7
    },
    {
      "epoch": 0.037383177570093455,
      "grad_norm": 1.9098163843154907,
      "learning_rate": 1.1214953271028036e-05,
      "loss": 3.3148,
      "step": 8
    },
    {
      "epoch": 0.04205607476635514,
      "grad_norm": 1.8914649486541748,
      "learning_rate": 1.261682242990654e-05,
      "loss": 3.3455,
      "step": 9
    },
    {
      "epoch": 0.04672897196261682,
      "grad_norm": 1.8792775869369507,
      "learning_rate": 1.4018691588785045e-05,
      "loss": 3.332,
      "step": 10
    },
    {
      "epoch": 0.0514018691588785,
      "grad_norm": 1.884199619293213,
      "learning_rate": 1.5420560747663548e-05,
      "loss": 3.2982,
      "step": 11
    },
    {
      "epoch": 0.056074766355140186,
      "grad_norm": 1.9098985195159912,
      "learning_rate": 1.6822429906542053e-05,
      "loss": 3.2851,
      "step": 12
    },
    {
      "epoch": 0.06074766355140187,
      "grad_norm": 1.9002035856246948,
      "learning_rate": 1.8224299065420557e-05,
      "loss": 3.2593,
      "step": 13
    },
    {
      "epoch": 0.06542056074766354,
      "grad_norm": 1.9028128385543823,
      "learning_rate": 1.9626168224299062e-05,
      "loss": 3.2265,
      "step": 14
    },
    {
      "epoch": 0.07009345794392523,
      "grad_norm": 1.8767255544662476,
      "learning_rate": 2.1028037383177567e-05,
      "loss": 3.2271,
      "step": 15
    },
    {
      "epoch": 0.07476635514018691,
      "grad_norm": 1.9281150102615356,
      "learning_rate": 2.242990654205607e-05,
      "loss": 3.1773,
      "step": 16
    },
    {
      "epoch": 0.0794392523364486,
      "grad_norm": 1.9038175344467163,
      "learning_rate": 2.3831775700934576e-05,
      "loss": 3.2183,
      "step": 17
    },
    {
      "epoch": 0.08411214953271028,
      "grad_norm": 1.8771458864212036,
      "learning_rate": 2.523364485981308e-05,
      "loss": 3.1838,
      "step": 18
    },
    {
      "epoch": 0.08878504672897196,
      "grad_norm": 1.896606206893921,
      "learning_rate": 2.6635514018691585e-05,
      "loss": 3.159,
      "step": 19
    },
    {
      "epoch": 0.09345794392523364,
      "grad_norm": 1.8939064741134644,
      "learning_rate": 2.803738317757009e-05,
      "loss": 3.0926,
      "step": 20
    },
    {
      "epoch": 0.09813084112149532,
      "grad_norm": 1.9581830501556396,
      "learning_rate": 2.9439252336448595e-05,
      "loss": 3.0537,
      "step": 21
    },
    {
      "epoch": 0.102803738317757,
      "grad_norm": 1.9243276119232178,
      "learning_rate": 3.0841121495327096e-05,
      "loss": 3.0689,
      "step": 22
    },
    {
      "epoch": 0.10747663551401869,
      "grad_norm": 1.946875810623169,
      "learning_rate": 3.22429906542056e-05,
      "loss": 3.0252,
      "step": 23
    },
    {
      "epoch": 0.11214953271028037,
      "grad_norm": 1.8874787092208862,
      "learning_rate": 3.3644859813084105e-05,
      "loss": 2.9279,
      "step": 24
    },
    {
      "epoch": 0.11682242990654206,
      "grad_norm": 1.9323413372039795,
      "learning_rate": 3.504672897196261e-05,
      "loss": 2.8756,
      "step": 25
    },
    {
      "epoch": 0.12149532710280374,
      "grad_norm": 1.8514940738677979,
      "learning_rate": 3.6448598130841115e-05,
      "loss": 2.8655,
      "step": 26
    },
    {
      "epoch": 0.1261682242990654,
      "grad_norm": 1.936355710029602,
      "learning_rate": 3.785046728971962e-05,
      "loss": 2.8499,
      "step": 27
    },
    {
      "epoch": 0.1308411214953271,
      "grad_norm": 1.9100676774978638,
      "learning_rate": 3.9252336448598124e-05,
      "loss": 2.7694,
      "step": 28
    },
    {
      "epoch": 0.13551401869158877,
      "grad_norm": 1.9439932107925415,
      "learning_rate": 4.065420560747663e-05,
      "loss": 2.7259,
      "step": 29
    },
    {
      "epoch": 0.14018691588785046,
      "grad_norm": 1.948280930519104,
      "learning_rate": 4.2056074766355134e-05,
      "loss": 2.658,
      "step": 30
    },
    {
      "epoch": 0.14485981308411214,
      "grad_norm": 1.8909847736358643,
      "learning_rate": 4.345794392523364e-05,
      "loss": 2.6031,
      "step": 31
    },
    {
      "epoch": 0.14953271028037382,
      "grad_norm": 1.9232759475708008,
      "learning_rate": 4.485981308411214e-05,
      "loss": 2.4699,
      "step": 32
    },
    {
      "epoch": 0.1542056074766355,
      "grad_norm": 1.9595930576324463,
      "learning_rate": 4.626168224299065e-05,
      "loss": 2.4444,
      "step": 33
    },
    {
      "epoch": 0.1588785046728972,
      "grad_norm": 1.9927295446395874,
      "learning_rate": 4.766355140186915e-05,
      "loss": 2.3647,
      "step": 34
    },
    {
      "epoch": 0.16355140186915887,
      "grad_norm": 1.962851643562317,
      "learning_rate": 4.906542056074766e-05,
      "loss": 2.2878,
      "step": 35
    },
    {
      "epoch": 0.16822429906542055,
      "grad_norm": 2.0136630535125732,
      "learning_rate": 5.046728971962616e-05,
      "loss": 2.1855,
      "step": 36
    },
    {
      "epoch": 0.17289719626168223,
      "grad_norm": 1.9552267789840698,
      "learning_rate": 5.1869158878504666e-05,
      "loss": 2.1234,
      "step": 37
    },
    {
      "epoch": 0.17757009345794392,
      "grad_norm": 2.0093047618865967,
      "learning_rate": 5.327102803738317e-05,
      "loss": 2.0222,
      "step": 38
    },
    {
      "epoch": 0.1822429906542056,
      "grad_norm": 2.0174577236175537,
      "learning_rate": 5.4672897196261676e-05,
      "loss": 1.9271,
      "step": 39
    },
    {
      "epoch": 0.18691588785046728,
      "grad_norm": 2.0559589862823486,
      "learning_rate": 5.607476635514018e-05,
      "loss": 1.8415,
      "step": 40
    },
    {
      "epoch": 0.19158878504672897,
      "grad_norm": 2.069730758666992,
      "learning_rate": 5.7476635514018685e-05,
      "loss": 1.7639,
      "step": 41
    },
    {
      "epoch": 0.19626168224299065,
      "grad_norm": 2.096458911895752,
      "learning_rate": 5.887850467289719e-05,
      "loss": 1.619,
      "step": 42
    },
    {
      "epoch": 0.20093457943925233,
      "grad_norm": 2.1536715030670166,
      "learning_rate": 6.0280373831775694e-05,
      "loss": 1.4844,
      "step": 43
    },
    {
      "epoch": 0.205607476635514,
      "grad_norm": 2.2939610481262207,
      "learning_rate": 6.168224299065419e-05,
      "loss": 1.3533,
      "step": 44
    },
    {
      "epoch": 0.2102803738317757,
      "grad_norm": 2.4742653369903564,
      "learning_rate": 6.30841121495327e-05,
      "loss": 1.2634,
      "step": 45
    },
    {
      "epoch": 0.21495327102803738,
      "grad_norm": 2.6355254650115967,
      "learning_rate": 6.44859813084112e-05,
      "loss": 1.0868,
      "step": 46
    },
    {
      "epoch": 0.21962616822429906,
      "grad_norm": 2.2959578037261963,
      "learning_rate": 6.58878504672897e-05,
      "loss": 1.0209,
      "step": 47
    },
    {
      "epoch": 0.22429906542056074,
      "grad_norm": 2.102672815322876,
      "learning_rate": 6.728971962616821e-05,
      "loss": 0.8744,
      "step": 48
    },
    {
      "epoch": 0.22897196261682243,
      "grad_norm": 2.1034154891967773,
      "learning_rate": 6.869158878504672e-05,
      "loss": 0.7965,
      "step": 49
    },
    {
      "epoch": 0.2336448598130841,
      "grad_norm": 1.8801497220993042,
      "learning_rate": 7.009345794392522e-05,
      "loss": 0.7543,
      "step": 50
    },
    {
      "epoch": 0.2383177570093458,
      "grad_norm": 1.4706770181655884,
      "learning_rate": 7.149532710280372e-05,
      "loss": 0.7212,
      "step": 51
    },
    {
      "epoch": 0.24299065420560748,
      "grad_norm": 1.2048588991165161,
      "learning_rate": 7.289719626168223e-05,
      "loss": 0.6856,
      "step": 52
    },
    {
      "epoch": 0.24766355140186916,
      "grad_norm": 1.0787873268127441,
      "learning_rate": 7.429906542056073e-05,
      "loss": 0.632,
      "step": 53
    },
    {
      "epoch": 0.2523364485981308,
      "grad_norm": 0.8942329287528992,
      "learning_rate": 7.570093457943924e-05,
      "loss": 0.6121,
      "step": 54
    },
    {
      "epoch": 0.2570093457943925,
      "grad_norm": 0.9066107273101807,
      "learning_rate": 7.710280373831774e-05,
      "loss": 0.6173,
      "step": 55
    },
    {
      "epoch": 0.2616822429906542,
      "grad_norm": 0.6026924848556519,
      "learning_rate": 7.850467289719625e-05,
      "loss": 0.4898,
      "step": 56
    },
    {
      "epoch": 0.26635514018691586,
      "grad_norm": 0.6421239376068115,
      "learning_rate": 7.990654205607475e-05,
      "loss": 0.499,
      "step": 57
    },
    {
      "epoch": 0.27102803738317754,
      "grad_norm": 0.5827729105949402,
      "learning_rate": 8.130841121495326e-05,
      "loss": 0.4952,
      "step": 58
    },
    {
      "epoch": 0.2757009345794392,
      "grad_norm": 0.6724992990493774,
      "learning_rate": 8.271028037383176e-05,
      "loss": 0.5179,
      "step": 59
    },
    {
      "epoch": 0.2803738317757009,
      "grad_norm": 0.5928758978843689,
      "learning_rate": 8.411214953271027e-05,
      "loss": 0.45,
      "step": 60
    },
    {
      "epoch": 0.2850467289719626,
      "grad_norm": 0.6100549101829529,
      "learning_rate": 8.551401869158877e-05,
      "loss": 0.5403,
      "step": 61
    },
    {
      "epoch": 0.2897196261682243,
      "grad_norm": 0.7373524904251099,
      "learning_rate": 8.691588785046728e-05,
      "loss": 0.4957,
      "step": 62
    },
    {
      "epoch": 0.29439252336448596,
      "grad_norm": 0.7677330374717712,
      "learning_rate": 8.831775700934578e-05,
      "loss": 0.5185,
      "step": 63
    },
    {
      "epoch": 0.29906542056074764,
      "grad_norm": 0.6263610124588013,
      "learning_rate": 8.971962616822429e-05,
      "loss": 0.4847,
      "step": 64
    },
    {
      "epoch": 0.3037383177570093,
      "grad_norm": 0.4617396891117096,
      "learning_rate": 9.112149532710279e-05,
      "loss": 0.48,
      "step": 65
    },
    {
      "epoch": 0.308411214953271,
      "grad_norm": 0.5930570960044861,
      "learning_rate": 9.25233644859813e-05,
      "loss": 0.4887,
      "step": 66
    },
    {
      "epoch": 0.3130841121495327,
      "grad_norm": 0.5264148712158203,
      "learning_rate": 9.39252336448598e-05,
      "loss": 0.4107,
      "step": 67
    },
    {
      "epoch": 0.3177570093457944,
      "grad_norm": 0.48520225286483765,
      "learning_rate": 9.53271028037383e-05,
      "loss": 0.4523,
      "step": 68
    },
    {
      "epoch": 0.32242990654205606,
      "grad_norm": 0.3983704447746277,
      "learning_rate": 9.672897196261681e-05,
      "loss": 0.4267,
      "step": 69
    },
    {
      "epoch": 0.32710280373831774,
      "grad_norm": 0.4878273904323578,
      "learning_rate": 9.813084112149531e-05,
      "loss": 0.3943,
      "step": 70
    },
    {
      "epoch": 0.3317757009345794,
      "grad_norm": 0.4177432060241699,
      "learning_rate": 9.953271028037382e-05,
      "loss": 0.4325,
      "step": 71
    },
    {
      "epoch": 0.3364485981308411,
      "grad_norm": 0.48322173953056335,
      "learning_rate": 0.00010093457943925232,
      "loss": 0.4456,
      "step": 72
    },
    {
      "epoch": 0.3411214953271028,
      "grad_norm": 0.5381689071655273,
      "learning_rate": 0.00010233644859813083,
      "loss": 0.4736,
      "step": 73
    },
    {
      "epoch": 0.34579439252336447,
      "grad_norm": 0.42288002371788025,
      "learning_rate": 0.00010373831775700933,
      "loss": 0.4112,
      "step": 74
    },
    {
      "epoch": 0.35046728971962615,
      "grad_norm": 0.5258895754814148,
      "learning_rate": 0.00010514018691588784,
      "loss": 0.4502,
      "step": 75
    },
    {
      "epoch": 0.35514018691588783,
      "grad_norm": 0.41975125670433044,
      "learning_rate": 0.00010654205607476634,
      "loss": 0.4008,
      "step": 76
    },
    {
      "epoch": 0.3598130841121495,
      "grad_norm": 0.5638459324836731,
      "learning_rate": 0.00010794392523364485,
      "loss": 0.4589,
      "step": 77
    },
    {
      "epoch": 0.3644859813084112,
      "grad_norm": 0.4262862503528595,
      "learning_rate": 0.00010934579439252335,
      "loss": 0.4137,
      "step": 78
    },
    {
      "epoch": 0.3691588785046729,
      "grad_norm": 0.41566887497901917,
      "learning_rate": 0.00011074766355140186,
      "loss": 0.3829,
      "step": 79
    },
    {
      "epoch": 0.37383177570093457,
      "grad_norm": 0.49328458309173584,
      "learning_rate": 0.00011214953271028036,
      "loss": 0.4624,
      "step": 80
    },
    {
      "epoch": 0.37850467289719625,
      "grad_norm": 0.42599380016326904,
      "learning_rate": 0.00011355140186915887,
      "loss": 0.4073,
      "step": 81
    },
    {
      "epoch": 0.38317757009345793,
      "grad_norm": 0.4146297574043274,
      "learning_rate": 0.00011495327102803737,
      "loss": 0.4256,
      "step": 82
    },
    {
      "epoch": 0.3878504672897196,
      "grad_norm": 0.37872374057769775,
      "learning_rate": 0.00011635514018691587,
      "loss": 0.3906,
      "step": 83
    },
    {
      "epoch": 0.3925233644859813,
      "grad_norm": 0.3559724688529968,
      "learning_rate": 0.00011775700934579438,
      "loss": 0.4192,
      "step": 84
    },
    {
      "epoch": 0.397196261682243,
      "grad_norm": 0.3718598484992981,
      "learning_rate": 0.00011915887850467288,
      "loss": 0.4122,
      "step": 85
    },
    {
      "epoch": 0.40186915887850466,
      "grad_norm": 0.46507275104522705,
      "learning_rate": 0.00012056074766355139,
      "loss": 0.3366,
      "step": 86
    },
    {
      "epoch": 0.40654205607476634,
      "grad_norm": 0.3829153776168823,
      "learning_rate": 0.0001219626168224299,
      "loss": 0.363,
      "step": 87
    },
    {
      "epoch": 0.411214953271028,
      "grad_norm": 0.36182287335395813,
      "learning_rate": 0.00012336448598130838,
      "loss": 0.3818,
      "step": 88
    },
    {
      "epoch": 0.4158878504672897,
      "grad_norm": 0.38517531752586365,
      "learning_rate": 0.0001247663551401869,
      "loss": 0.3609,
      "step": 89
    },
    {
      "epoch": 0.4205607476635514,
      "grad_norm": 0.37783780694007874,
      "learning_rate": 0.0001261682242990654,
      "loss": 0.3508,
      "step": 90
    },
    {
      "epoch": 0.4252336448598131,
      "grad_norm": 0.49455946683883667,
      "learning_rate": 0.0001275700934579439,
      "loss": 0.4035,
      "step": 91
    },
    {
      "epoch": 0.42990654205607476,
      "grad_norm": 0.43909820914268494,
      "learning_rate": 0.0001289719626168224,
      "loss": 0.3849,
      "step": 92
    },
    {
      "epoch": 0.43457943925233644,
      "grad_norm": 0.3391451835632324,
      "learning_rate": 0.00013037383177570092,
      "loss": 0.3431,
      "step": 93
    },
    {
      "epoch": 0.4392523364485981,
      "grad_norm": 0.34879031777381897,
      "learning_rate": 0.0001317757009345794,
      "loss": 0.3609,
      "step": 94
    },
    {
      "epoch": 0.4439252336448598,
      "grad_norm": 0.40080374479293823,
      "learning_rate": 0.00013317757009345793,
      "loss": 0.31,
      "step": 95
    },
    {
      "epoch": 0.4485981308411215,
      "grad_norm": 0.3392825126647949,
      "learning_rate": 0.00013457943925233642,
      "loss": 0.3347,
      "step": 96
    },
    {
      "epoch": 0.4532710280373832,
      "grad_norm": 0.40629932284355164,
      "learning_rate": 0.00013598130841121494,
      "loss": 0.3095,
      "step": 97
    },
    {
      "epoch": 0.45794392523364486,
      "grad_norm": 0.3899966776371002,
      "learning_rate": 0.00013738317757009343,
      "loss": 0.3475,
      "step": 98
    },
    {
      "epoch": 0.46261682242990654,
      "grad_norm": 0.5936737060546875,
      "learning_rate": 0.00013878504672897195,
      "loss": 0.3989,
      "step": 99
    },
    {
      "epoch": 0.4672897196261682,
      "grad_norm": 0.33845382928848267,
      "learning_rate": 0.00014018691588785044,
      "loss": 0.3019,
      "step": 100
    },
    {
      "epoch": 0.4719626168224299,
      "grad_norm": 0.5679856538772583,
      "learning_rate": 0.00014158878504672896,
      "loss": 0.4128,
      "step": 101
    },
    {
      "epoch": 0.4766355140186916,
      "grad_norm": 0.40489259362220764,
      "learning_rate": 0.00014299065420560745,
      "loss": 0.3138,
      "step": 102
    },
    {
      "epoch": 0.48130841121495327,
      "grad_norm": 0.3309035003185272,
      "learning_rate": 0.00014439252336448597,
      "loss": 0.3479,
      "step": 103
    },
    {
      "epoch": 0.48598130841121495,
      "grad_norm": 0.4794415235519409,
      "learning_rate": 0.00014579439252336446,
      "loss": 0.3541,
      "step": 104
    },
    {
      "epoch": 0.49065420560747663,
      "grad_norm": 0.34613633155822754,
      "learning_rate": 0.00014719626168224298,
      "loss": 0.349,
      "step": 105
    },
    {
      "epoch": 0.4953271028037383,
      "grad_norm": 0.35769641399383545,
      "learning_rate": 0.00014859813084112147,
      "loss": 0.3425,
      "step": 106
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.4028024673461914,
      "learning_rate": 0.00015,
      "loss": 0.3054,
      "step": 107
    },
    {
      "epoch": 0.5046728971962616,
      "grad_norm": 0.32482099533081055,
      "learning_rate": 0.00015140186915887848,
      "loss": 0.3021,
      "step": 108
    },
    {
      "epoch": 0.5093457943925234,
      "grad_norm": 0.45050543546676636,
      "learning_rate": 0.000152803738317757,
      "loss": 0.3237,
      "step": 109
    },
    {
      "epoch": 0.514018691588785,
      "grad_norm": 0.5504162907600403,
      "learning_rate": 0.0001542056074766355,
      "loss": 0.3445,
      "step": 110
    },
    {
      "epoch": 0.5186915887850467,
      "grad_norm": 0.5508272051811218,
      "learning_rate": 0.000155607476635514,
      "loss": 0.3178,
      "step": 111
    },
    {
      "epoch": 0.5233644859813084,
      "grad_norm": 0.4430913031101227,
      "learning_rate": 0.0001570093457943925,
      "loss": 0.3502,
      "step": 112
    },
    {
      "epoch": 0.5280373831775701,
      "grad_norm": 0.4380330741405487,
      "learning_rate": 0.00015841121495327101,
      "loss": 0.3412,
      "step": 113
    },
    {
      "epoch": 0.5327102803738317,
      "grad_norm": 0.4836913049221039,
      "learning_rate": 0.0001598130841121495,
      "loss": 0.3561,
      "step": 114
    },
    {
      "epoch": 0.5373831775700935,
      "grad_norm": 0.4211674928665161,
      "learning_rate": 0.00016121495327102802,
      "loss": 0.3205,
      "step": 115
    },
    {
      "epoch": 0.5420560747663551,
      "grad_norm": 0.6272251605987549,
      "learning_rate": 0.00016261682242990652,
      "loss": 0.2665,
      "step": 116
    },
    {
      "epoch": 0.5467289719626168,
      "grad_norm": 0.4071166217327118,
      "learning_rate": 0.00016401869158878503,
      "loss": 0.3206,
      "step": 117
    },
    {
      "epoch": 0.5514018691588785,
      "grad_norm": 0.3869965076446533,
      "learning_rate": 0.00016542056074766352,
      "loss": 0.2958,
      "step": 118
    },
    {
      "epoch": 0.5560747663551402,
      "grad_norm": 0.5137064456939697,
      "learning_rate": 0.00016682242990654204,
      "loss": 0.3689,
      "step": 119
    },
    {
      "epoch": 0.5607476635514018,
      "grad_norm": 0.495895117521286,
      "learning_rate": 0.00016822429906542053,
      "loss": 0.3472,
      "step": 120
    },
    {
      "epoch": 0.5654205607476636,
      "grad_norm": 0.47324472665786743,
      "learning_rate": 0.00016962616822429905,
      "loss": 0.3105,
      "step": 121
    },
    {
      "epoch": 0.5700934579439252,
      "grad_norm": 0.5054698586463928,
      "learning_rate": 0.00017102803738317754,
      "loss": 0.3929,
      "step": 122
    },
    {
      "epoch": 0.5747663551401869,
      "grad_norm": 0.418328195810318,
      "learning_rate": 0.00017242990654205606,
      "loss": 0.3091,
      "step": 123
    },
    {
      "epoch": 0.5794392523364486,
      "grad_norm": 0.37431827187538147,
      "learning_rate": 0.00017383177570093455,
      "loss": 0.2907,
      "step": 124
    },
    {
      "epoch": 0.5841121495327103,
      "grad_norm": 0.44804275035858154,
      "learning_rate": 0.00017523364485981307,
      "loss": 0.3294,
      "step": 125
    },
    {
      "epoch": 0.5887850467289719,
      "grad_norm": 0.4702403247356415,
      "learning_rate": 0.00017663551401869156,
      "loss": 0.3498,
      "step": 126
    },
    {
      "epoch": 0.5934579439252337,
      "grad_norm": 0.48160478472709656,
      "learning_rate": 0.00017803738317757008,
      "loss": 0.2778,
      "step": 127
    },
    {
      "epoch": 0.5981308411214953,
      "grad_norm": 0.45230987668037415,
      "learning_rate": 0.00017943925233644857,
      "loss": 0.2738,
      "step": 128
    },
    {
      "epoch": 0.602803738317757,
      "grad_norm": 0.38504210114479065,
      "learning_rate": 0.0001808411214953271,
      "loss": 0.3554,
      "step": 129
    },
    {
      "epoch": 0.6074766355140186,
      "grad_norm": 0.33053940534591675,
      "learning_rate": 0.00018224299065420558,
      "loss": 0.2893,
      "step": 130
    },
    {
      "epoch": 0.6121495327102804,
      "grad_norm": 0.42061173915863037,
      "learning_rate": 0.0001836448598130841,
      "loss": 0.2843,
      "step": 131
    },
    {
      "epoch": 0.616822429906542,
      "grad_norm": 0.4157899022102356,
      "learning_rate": 0.0001850467289719626,
      "loss": 0.3359,
      "step": 132
    },
    {
      "epoch": 0.6214953271028038,
      "grad_norm": 0.4526776075363159,
      "learning_rate": 0.0001864485981308411,
      "loss": 0.3437,
      "step": 133
    },
    {
      "epoch": 0.6261682242990654,
      "grad_norm": 0.3068268597126007,
      "learning_rate": 0.0001878504672897196,
      "loss": 0.2875,
      "step": 134
    },
    {
      "epoch": 0.6308411214953271,
      "grad_norm": 0.41558995842933655,
      "learning_rate": 0.00018925233644859812,
      "loss": 0.3566,
      "step": 135
    },
    {
      "epoch": 0.6355140186915887,
      "grad_norm": 0.32500529289245605,
      "learning_rate": 0.0001906542056074766,
      "loss": 0.3194,
      "step": 136
    },
    {
      "epoch": 0.6401869158878505,
      "grad_norm": 0.4353014826774597,
      "learning_rate": 0.00019205607476635513,
      "loss": 0.2905,
      "step": 137
    },
    {
      "epoch": 0.6448598130841121,
      "grad_norm": 0.3816524147987366,
      "learning_rate": 0.00019345794392523362,
      "loss": 0.3574,
      "step": 138
    },
    {
      "epoch": 0.6495327102803738,
      "grad_norm": 0.314840704202652,
      "learning_rate": 0.00019485981308411214,
      "loss": 0.2983,
      "step": 139
    },
    {
      "epoch": 0.6542056074766355,
      "grad_norm": 0.38703057169914246,
      "learning_rate": 0.00019626168224299063,
      "loss": 0.2823,
      "step": 140
    },
    {
      "epoch": 0.6588785046728972,
      "grad_norm": 0.4346475899219513,
      "learning_rate": 0.00019766355140186915,
      "loss": 0.3144,
      "step": 141
    },
    {
      "epoch": 0.6635514018691588,
      "grad_norm": 0.4031387269496918,
      "learning_rate": 0.00019906542056074764,
      "loss": 0.2651,
      "step": 142
    },
    {
      "epoch": 0.6682242990654206,
      "grad_norm": 0.41228628158569336,
      "learning_rate": 0.00020046728971962616,
      "loss": 0.2584,
      "step": 143
    },
    {
      "epoch": 0.6728971962616822,
      "grad_norm": 0.3748122453689575,
      "learning_rate": 0.00020186915887850465,
      "loss": 0.2954,
      "step": 144
    },
    {
      "epoch": 0.677570093457944,
      "grad_norm": 0.5753006935119629,
      "learning_rate": 0.00020327102803738316,
      "loss": 0.3585,
      "step": 145
    },
    {
      "epoch": 0.6822429906542056,
      "grad_norm": 0.5344408750534058,
      "learning_rate": 0.00020467289719626166,
      "loss": 0.3202,
      "step": 146
    },
    {
      "epoch": 0.6869158878504673,
      "grad_norm": 0.5076481699943542,
      "learning_rate": 0.00020607476635514017,
      "loss": 0.347,
      "step": 147
    },
    {
      "epoch": 0.6915887850467289,
      "grad_norm": 0.42972463369369507,
      "learning_rate": 0.00020747663551401867,
      "loss": 0.2703,
      "step": 148
    },
    {
      "epoch": 0.6962616822429907,
      "grad_norm": 0.4447568655014038,
      "learning_rate": 0.00020887850467289718,
      "loss": 0.3228,
      "step": 149
    },
    {
      "epoch": 0.7009345794392523,
      "grad_norm": 0.5766744613647461,
      "learning_rate": 0.00021028037383177567,
      "loss": 0.2971,
      "step": 150
    },
    {
      "epoch": 0.705607476635514,
      "grad_norm": 0.43084490299224854,
      "learning_rate": 0.0002116822429906542,
      "loss": 0.3335,
      "step": 151
    },
    {
      "epoch": 0.7102803738317757,
      "grad_norm": 0.45942747592926025,
      "learning_rate": 0.00021308411214953268,
      "loss": 0.3283,
      "step": 152
    },
    {
      "epoch": 0.7149532710280374,
      "grad_norm": 0.40997952222824097,
      "learning_rate": 0.0002144859813084112,
      "loss": 0.3258,
      "step": 153
    },
    {
      "epoch": 0.719626168224299,
      "grad_norm": 0.41550835967063904,
      "learning_rate": 0.0002158878504672897,
      "loss": 0.2739,
      "step": 154
    },
    {
      "epoch": 0.7242990654205608,
      "grad_norm": 0.4532230496406555,
      "learning_rate": 0.0002172897196261682,
      "loss": 0.2434,
      "step": 155
    },
    {
      "epoch": 0.7289719626168224,
      "grad_norm": 0.47736120223999023,
      "learning_rate": 0.0002186915887850467,
      "loss": 0.2975,
      "step": 156
    },
    {
      "epoch": 0.7336448598130841,
      "grad_norm": 0.4109285771846771,
      "learning_rate": 0.00022009345794392522,
      "loss": 0.2877,
      "step": 157
    },
    {
      "epoch": 0.7383177570093458,
      "grad_norm": 0.47784745693206787,
      "learning_rate": 0.0002214953271028037,
      "loss": 0.3099,
      "step": 158
    },
    {
      "epoch": 0.7429906542056075,
      "grad_norm": 0.42892763018608093,
      "learning_rate": 0.00022289719626168223,
      "loss": 0.3057,
      "step": 159
    },
    {
      "epoch": 0.7476635514018691,
      "grad_norm": 0.46420446038246155,
      "learning_rate": 0.00022429906542056072,
      "loss": 0.3022,
      "step": 160
    },
    {
      "epoch": 0.7523364485981309,
      "grad_norm": 0.4412018060684204,
      "learning_rate": 0.00022570093457943924,
      "loss": 0.265,
      "step": 161
    },
    {
      "epoch": 0.7570093457943925,
      "grad_norm": 0.5675196051597595,
      "learning_rate": 0.00022710280373831773,
      "loss": 0.3392,
      "step": 162
    },
    {
      "epoch": 0.7616822429906542,
      "grad_norm": 0.6275599002838135,
      "learning_rate": 0.00022850467289719625,
      "loss": 0.2621,
      "step": 163
    },
    {
      "epoch": 0.7663551401869159,
      "grad_norm": 0.4825347065925598,
      "learning_rate": 0.00022990654205607474,
      "loss": 0.2942,
      "step": 164
    },
    {
      "epoch": 0.7710280373831776,
      "grad_norm": 0.46182799339294434,
      "learning_rate": 0.00023130841121495326,
      "loss": 0.2852,
      "step": 165
    },
    {
      "epoch": 0.7757009345794392,
      "grad_norm": 0.5549268126487732,
      "learning_rate": 0.00023271028037383175,
      "loss": 0.3046,
      "step": 166
    },
    {
      "epoch": 0.780373831775701,
      "grad_norm": 0.48819905519485474,
      "learning_rate": 0.00023411214953271027,
      "loss": 0.2577,
      "step": 167
    },
    {
      "epoch": 0.7850467289719626,
      "grad_norm": 0.5300769805908203,
      "learning_rate": 0.00023551401869158876,
      "loss": 0.2914,
      "step": 168
    },
    {
      "epoch": 0.7897196261682243,
      "grad_norm": 0.6791442632675171,
      "learning_rate": 0.00023691588785046728,
      "loss": 0.2989,
      "step": 169
    },
    {
      "epoch": 0.794392523364486,
      "grad_norm": 0.6697086691856384,
      "learning_rate": 0.00023831775700934577,
      "loss": 0.3546,
      "step": 170
    },
    {
      "epoch": 0.7990654205607477,
      "grad_norm": 0.48630186915397644,
      "learning_rate": 0.00023971962616822429,
      "loss": 0.3197,
      "step": 171
    },
    {
      "epoch": 0.8037383177570093,
      "grad_norm": 0.5939908623695374,
      "learning_rate": 0.00024112149532710278,
      "loss": 0.3368,
      "step": 172
    },
    {
      "epoch": 0.8084112149532711,
      "grad_norm": 0.5732728838920593,
      "learning_rate": 0.0002425233644859813,
      "loss": 0.2962,
      "step": 173
    },
    {
      "epoch": 0.8130841121495327,
      "grad_norm": 0.6490129828453064,
      "learning_rate": 0.0002439252336448598,
      "loss": 0.2885,
      "step": 174
    },
    {
      "epoch": 0.8177570093457944,
      "grad_norm": 0.6987048387527466,
      "learning_rate": 0.00024532710280373833,
      "loss": 0.3082,
      "step": 175
    },
    {
      "epoch": 0.822429906542056,
      "grad_norm": 0.45700156688690186,
      "learning_rate": 0.00024672897196261677,
      "loss": 0.2872,
      "step": 176
    },
    {
      "epoch": 0.8271028037383178,
      "grad_norm": 0.5766392946243286,
      "learning_rate": 0.0002481308411214953,
      "loss": 0.3081,
      "step": 177
    },
    {
      "epoch": 0.8317757009345794,
      "grad_norm": 0.4620848000049591,
      "learning_rate": 0.0002495327102803738,
      "loss": 0.2442,
      "step": 178
    },
    {
      "epoch": 0.8364485981308412,
      "grad_norm": 0.48393914103507996,
      "learning_rate": 0.00025093457943925235,
      "loss": 0.2329,
      "step": 179
    },
    {
      "epoch": 0.8411214953271028,
      "grad_norm": 0.4725552499294281,
      "learning_rate": 0.0002523364485981308,
      "loss": 0.2679,
      "step": 180
    },
    {
      "epoch": 0.8457943925233645,
      "grad_norm": 0.44608274102211,
      "learning_rate": 0.00025373831775700933,
      "loss": 0.306,
      "step": 181
    },
    {
      "epoch": 0.8504672897196262,
      "grad_norm": 0.5795326828956604,
      "learning_rate": 0.0002551401869158878,
      "loss": 0.2752,
      "step": 182
    },
    {
      "epoch": 0.8551401869158879,
      "grad_norm": 0.47607526183128357,
      "learning_rate": 0.00025654205607476637,
      "loss": 0.2448,
      "step": 183
    },
    {
      "epoch": 0.8598130841121495,
      "grad_norm": 0.5189138650894165,
      "learning_rate": 0.0002579439252336448,
      "loss": 0.288,
      "step": 184
    },
    {
      "epoch": 0.8644859813084113,
      "grad_norm": 0.5570269227027893,
      "learning_rate": 0.00025934579439252335,
      "loss": 0.2933,
      "step": 185
    },
    {
      "epoch": 0.8691588785046729,
      "grad_norm": 0.4379851818084717,
      "learning_rate": 0.00026074766355140184,
      "loss": 0.2803,
      "step": 186
    },
    {
      "epoch": 0.8738317757009346,
      "grad_norm": 0.44580480456352234,
      "learning_rate": 0.0002621495327102804,
      "loss": 0.2809,
      "step": 187
    },
    {
      "epoch": 0.8785046728971962,
      "grad_norm": 0.5945780277252197,
      "learning_rate": 0.0002635514018691588,
      "loss": 0.3243,
      "step": 188
    },
    {
      "epoch": 0.883177570093458,
      "grad_norm": 0.37857919931411743,
      "learning_rate": 0.00026495327102803737,
      "loss": 0.2497,
      "step": 189
    },
    {
      "epoch": 0.8878504672897196,
      "grad_norm": 0.4508804380893707,
      "learning_rate": 0.00026635514018691586,
      "loss": 0.2897,
      "step": 190
    },
    {
      "epoch": 0.8925233644859814,
      "grad_norm": 0.49665653705596924,
      "learning_rate": 0.0002677570093457944,
      "loss": 0.2663,
      "step": 191
    },
    {
      "epoch": 0.897196261682243,
      "grad_norm": 0.5628969073295593,
      "learning_rate": 0.00026915887850467284,
      "loss": 0.3039,
      "step": 192
    },
    {
      "epoch": 0.9018691588785047,
      "grad_norm": 0.5298646092414856,
      "learning_rate": 0.0002705607476635514,
      "loss": 0.2715,
      "step": 193
    },
    {
      "epoch": 0.9065420560747663,
      "grad_norm": 0.8276045918464661,
      "learning_rate": 0.0002719626168224299,
      "loss": 0.3483,
      "step": 194
    },
    {
      "epoch": 0.9112149532710281,
      "grad_norm": 0.8365036845207214,
      "learning_rate": 0.0002733644859813084,
      "loss": 0.2695,
      "step": 195
    },
    {
      "epoch": 0.9158878504672897,
      "grad_norm": 0.6864085793495178,
      "learning_rate": 0.00027476635514018686,
      "loss": 0.2228,
      "step": 196
    },
    {
      "epoch": 0.9205607476635514,
      "grad_norm": 0.5175933837890625,
      "learning_rate": 0.0002761682242990654,
      "loss": 0.2906,
      "step": 197
    },
    {
      "epoch": 0.9252336448598131,
      "grad_norm": 0.5331153869628906,
      "learning_rate": 0.0002775700934579439,
      "loss": 0.31,
      "step": 198
    },
    {
      "epoch": 0.9299065420560748,
      "grad_norm": 0.43000170588493347,
      "learning_rate": 0.00027897196261682244,
      "loss": 0.3105,
      "step": 199
    },
    {
      "epoch": 0.9345794392523364,
      "grad_norm": 0.40214642882347107,
      "learning_rate": 0.0002803738317757009,
      "loss": 0.2254,
      "step": 200
    },
    {
      "epoch": 0.9392523364485982,
      "grad_norm": 0.5154314041137695,
      "learning_rate": 0.0002817757009345794,
      "loss": 0.2836,
      "step": 201
    },
    {
      "epoch": 0.9439252336448598,
      "grad_norm": 0.4046143889427185,
      "learning_rate": 0.0002831775700934579,
      "loss": 0.2732,
      "step": 202
    },
    {
      "epoch": 0.9485981308411215,
      "grad_norm": 0.37177610397338867,
      "learning_rate": 0.00028457943925233646,
      "loss": 0.2709,
      "step": 203
    },
    {
      "epoch": 0.9532710280373832,
      "grad_norm": 0.4481426477432251,
      "learning_rate": 0.0002859813084112149,
      "loss": 0.2672,
      "step": 204
    },
    {
      "epoch": 0.9579439252336449,
      "grad_norm": 0.5229674577713013,
      "learning_rate": 0.00028738317757009345,
      "loss": 0.2498,
      "step": 205
    },
    {
      "epoch": 0.9626168224299065,
      "grad_norm": 0.39455339312553406,
      "learning_rate": 0.00028878504672897194,
      "loss": 0.2457,
      "step": 206
    },
    {
      "epoch": 0.9672897196261683,
      "grad_norm": 0.4971538484096527,
      "learning_rate": 0.0002901869158878505,
      "loss": 0.2789,
      "step": 207
    },
    {
      "epoch": 0.9719626168224299,
      "grad_norm": 0.5185973048210144,
      "learning_rate": 0.0002915887850467289,
      "loss": 0.3094,
      "step": 208
    },
    {
      "epoch": 0.9766355140186916,
      "grad_norm": 0.4189246594905853,
      "learning_rate": 0.00029299065420560746,
      "loss": 0.2885,
      "step": 209
    },
    {
      "epoch": 0.9813084112149533,
      "grad_norm": 0.4556058645248413,
      "learning_rate": 0.00029439252336448596,
      "loss": 0.3259,
      "step": 210
    },
    {
      "epoch": 0.985981308411215,
      "grad_norm": 0.4249024987220764,
      "learning_rate": 0.0002957943925233645,
      "loss": 0.2514,
      "step": 211
    },
    {
      "epoch": 0.9906542056074766,
      "grad_norm": 0.46567073464393616,
      "learning_rate": 0.00029719626168224294,
      "loss": 0.2638,
      "step": 212
    },
    {
      "epoch": 0.9953271028037384,
      "grad_norm": 0.3531776964664459,
      "learning_rate": 0.0002985981308411215,
      "loss": 0.2407,
      "step": 213
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.49741289019584656,
      "learning_rate": 0.0003,
      "loss": 0.1185,
      "step": 214
    },
    {
      "epoch": 1.0,
      "eval_loss": 0.36126670241355896,
      "eval_runtime": 53.5068,
      "eval_samples_per_second": 10.316,
      "eval_steps_per_second": 1.29,
      "step": 214
    },
    {
      "epoch": 1.0046728971962617,
      "grad_norm": 0.5093424320220947,
      "learning_rate": 0.0002998442367601246,
      "loss": 0.3031,
      "step": 215
    },
    {
      "epoch": 1.0093457943925233,
      "grad_norm": 0.6222847104072571,
      "learning_rate": 0.0002996884735202492,
      "loss": 0.2524,
      "step": 216
    },
    {
      "epoch": 1.014018691588785,
      "grad_norm": 0.5250963568687439,
      "learning_rate": 0.00029953271028037383,
      "loss": 0.2782,
      "step": 217
    },
    {
      "epoch": 1.0186915887850467,
      "grad_norm": 0.44196617603302,
      "learning_rate": 0.0002993769470404984,
      "loss": 0.2854,
      "step": 218
    },
    {
      "epoch": 1.0233644859813085,
      "grad_norm": 0.42851218581199646,
      "learning_rate": 0.000299221183800623,
      "loss": 0.2027,
      "step": 219
    },
    {
      "epoch": 1.02803738317757,
      "grad_norm": 0.538623571395874,
      "learning_rate": 0.00029906542056074763,
      "loss": 0.3507,
      "step": 220
    },
    {
      "epoch": 1.0327102803738317,
      "grad_norm": 0.4848197400569916,
      "learning_rate": 0.00029890965732087225,
      "loss": 0.306,
      "step": 221
    },
    {
      "epoch": 1.0373831775700935,
      "grad_norm": 0.5137667059898376,
      "learning_rate": 0.00029875389408099686,
      "loss": 0.2642,
      "step": 222
    },
    {
      "epoch": 1.0420560747663552,
      "grad_norm": 0.5163947939872742,
      "learning_rate": 0.0002985981308411215,
      "loss": 0.3208,
      "step": 223
    },
    {
      "epoch": 1.0467289719626167,
      "grad_norm": 0.5149715542793274,
      "learning_rate": 0.00029844236760124605,
      "loss": 0.2351,
      "step": 224
    },
    {
      "epoch": 1.0514018691588785,
      "grad_norm": 0.5275676250457764,
      "learning_rate": 0.00029828660436137066,
      "loss": 0.2674,
      "step": 225
    },
    {
      "epoch": 1.0560747663551402,
      "grad_norm": 0.4760637879371643,
      "learning_rate": 0.0002981308411214953,
      "loss": 0.2729,
      "step": 226
    },
    {
      "epoch": 1.060747663551402,
      "grad_norm": 0.4720558524131775,
      "learning_rate": 0.0002979750778816199,
      "loss": 0.2571,
      "step": 227
    },
    {
      "epoch": 1.0654205607476634,
      "grad_norm": 0.6113613247871399,
      "learning_rate": 0.0002978193146417445,
      "loss": 0.2572,
      "step": 228
    },
    {
      "epoch": 1.0700934579439252,
      "grad_norm": 0.7529714107513428,
      "learning_rate": 0.00029766355140186914,
      "loss": 0.2376,
      "step": 229
    },
    {
      "epoch": 1.074766355140187,
      "grad_norm": 0.5343906283378601,
      "learning_rate": 0.00029750778816199376,
      "loss": 0.2108,
      "step": 230
    },
    {
      "epoch": 1.0794392523364487,
      "grad_norm": 0.5604724884033203,
      "learning_rate": 0.0002973520249221184,
      "loss": 0.2573,
      "step": 231
    },
    {
      "epoch": 1.0841121495327102,
      "grad_norm": 0.44978269934654236,
      "learning_rate": 0.00029719626168224294,
      "loss": 0.2474,
      "step": 232
    },
    {
      "epoch": 1.088785046728972,
      "grad_norm": 0.5430713891983032,
      "learning_rate": 0.00029704049844236756,
      "loss": 0.3224,
      "step": 233
    },
    {
      "epoch": 1.0934579439252337,
      "grad_norm": 0.5057111382484436,
      "learning_rate": 0.0002968847352024922,
      "loss": 0.2735,
      "step": 234
    },
    {
      "epoch": 1.0981308411214954,
      "grad_norm": 0.46571093797683716,
      "learning_rate": 0.0002967289719626168,
      "loss": 0.2537,
      "step": 235
    },
    {
      "epoch": 1.102803738317757,
      "grad_norm": 0.47344347834587097,
      "learning_rate": 0.0002965732087227414,
      "loss": 0.2973,
      "step": 236
    },
    {
      "epoch": 1.1074766355140186,
      "grad_norm": 0.32640522718429565,
      "learning_rate": 0.00029641744548286603,
      "loss": 0.2511,
      "step": 237
    },
    {
      "epoch": 1.1121495327102804,
      "grad_norm": 0.3538658618927002,
      "learning_rate": 0.00029626168224299065,
      "loss": 0.2182,
      "step": 238
    },
    {
      "epoch": 1.1168224299065421,
      "grad_norm": 0.3540774881839752,
      "learning_rate": 0.00029610591900311526,
      "loss": 0.2184,
      "step": 239
    },
    {
      "epoch": 1.1214953271028036,
      "grad_norm": 0.6448273658752441,
      "learning_rate": 0.0002959501557632399,
      "loss": 0.3122,
      "step": 240
    },
    {
      "epoch": 1.1261682242990654,
      "grad_norm": 0.4038638174533844,
      "learning_rate": 0.0002957943925233645,
      "loss": 0.2737,
      "step": 241
    },
    {
      "epoch": 1.1308411214953271,
      "grad_norm": 0.6227460503578186,
      "learning_rate": 0.00029563862928348906,
      "loss": 0.3128,
      "step": 242
    },
    {
      "epoch": 1.1355140186915889,
      "grad_norm": 0.5357035994529724,
      "learning_rate": 0.0002954828660436137,
      "loss": 0.3057,
      "step": 243
    },
    {
      "epoch": 1.1401869158878504,
      "grad_norm": 0.3612295985221863,
      "learning_rate": 0.0002953271028037383,
      "loss": 0.2131,
      "step": 244
    },
    {
      "epoch": 1.144859813084112,
      "grad_norm": 0.39098843932151794,
      "learning_rate": 0.0002951713395638629,
      "loss": 0.238,
      "step": 245
    },
    {
      "epoch": 1.1495327102803738,
      "grad_norm": 0.3917027413845062,
      "learning_rate": 0.00029501557632398754,
      "loss": 0.2762,
      "step": 246
    },
    {
      "epoch": 1.1542056074766356,
      "grad_norm": 0.3800537586212158,
      "learning_rate": 0.00029485981308411215,
      "loss": 0.2469,
      "step": 247
    },
    {
      "epoch": 1.158878504672897,
      "grad_norm": 0.41278818249702454,
      "learning_rate": 0.0002947040498442367,
      "loss": 0.2248,
      "step": 248
    },
    {
      "epoch": 1.1635514018691588,
      "grad_norm": 0.33948415517807007,
      "learning_rate": 0.00029454828660436134,
      "loss": 0.2237,
      "step": 249
    },
    {
      "epoch": 1.1682242990654206,
      "grad_norm": 0.3935600519180298,
      "learning_rate": 0.00029439252336448596,
      "loss": 0.2519,
      "step": 250
    },
    {
      "epoch": 1.1728971962616823,
      "grad_norm": 0.4376516044139862,
      "learning_rate": 0.0002942367601246106,
      "loss": 0.2757,
      "step": 251
    },
    {
      "epoch": 1.1775700934579438,
      "grad_norm": 0.43691226840019226,
      "learning_rate": 0.0002940809968847352,
      "loss": 0.1922,
      "step": 252
    },
    {
      "epoch": 1.1822429906542056,
      "grad_norm": 0.3568747937679291,
      "learning_rate": 0.0002939252336448598,
      "loss": 0.2052,
      "step": 253
    },
    {
      "epoch": 1.1869158878504673,
      "grad_norm": 0.37499168515205383,
      "learning_rate": 0.0002937694704049844,
      "loss": 0.1835,
      "step": 254
    },
    {
      "epoch": 1.191588785046729,
      "grad_norm": 0.41638678312301636,
      "learning_rate": 0.000293613707165109,
      "loss": 0.2415,
      "step": 255
    },
    {
      "epoch": 1.1962616822429906,
      "grad_norm": 0.5247153043746948,
      "learning_rate": 0.0002934579439252336,
      "loss": 0.263,
      "step": 256
    },
    {
      "epoch": 1.2009345794392523,
      "grad_norm": 0.39326557517051697,
      "learning_rate": 0.00029330218068535823,
      "loss": 0.2589,
      "step": 257
    },
    {
      "epoch": 1.205607476635514,
      "grad_norm": 0.3339598774909973,
      "learning_rate": 0.00029314641744548285,
      "loss": 0.1869,
      "step": 258
    },
    {
      "epoch": 1.2102803738317758,
      "grad_norm": 0.39250051975250244,
      "learning_rate": 0.00029299065420560746,
      "loss": 0.2311,
      "step": 259
    },
    {
      "epoch": 1.2149532710280373,
      "grad_norm": 0.34290841221809387,
      "learning_rate": 0.00029283489096573203,
      "loss": 0.2195,
      "step": 260
    },
    {
      "epoch": 1.219626168224299,
      "grad_norm": 0.34642598032951355,
      "learning_rate": 0.00029267912772585665,
      "loss": 0.208,
      "step": 261
    },
    {
      "epoch": 1.2242990654205608,
      "grad_norm": 0.4080093204975128,
      "learning_rate": 0.00029252336448598126,
      "loss": 0.2493,
      "step": 262
    },
    {
      "epoch": 1.2289719626168225,
      "grad_norm": 0.4412749707698822,
      "learning_rate": 0.0002923676012461059,
      "loss": 0.2483,
      "step": 263
    },
    {
      "epoch": 1.233644859813084,
      "grad_norm": 0.39980801939964294,
      "learning_rate": 0.0002922118380062305,
      "loss": 0.2674,
      "step": 264
    },
    {
      "epoch": 1.2383177570093458,
      "grad_norm": 0.41386741399765015,
      "learning_rate": 0.0002920560747663551,
      "loss": 0.2319,
      "step": 265
    },
    {
      "epoch": 1.2429906542056075,
      "grad_norm": 0.5207935571670532,
      "learning_rate": 0.00029190031152647974,
      "loss": 0.2714,
      "step": 266
    },
    {
      "epoch": 1.2476635514018692,
      "grad_norm": 0.4824307858943939,
      "learning_rate": 0.00029174454828660435,
      "loss": 0.2232,
      "step": 267
    },
    {
      "epoch": 1.2523364485981308,
      "grad_norm": 0.343248575925827,
      "learning_rate": 0.0002915887850467289,
      "loss": 0.1999,
      "step": 268
    },
    {
      "epoch": 1.2570093457943925,
      "grad_norm": 0.39994627237319946,
      "learning_rate": 0.00029143302180685354,
      "loss": 0.2043,
      "step": 269
    },
    {
      "epoch": 1.2616822429906542,
      "grad_norm": 0.41567832231521606,
      "learning_rate": 0.00029127725856697815,
      "loss": 0.2006,
      "step": 270
    },
    {
      "epoch": 1.2663551401869158,
      "grad_norm": 0.45111051201820374,
      "learning_rate": 0.0002911214953271028,
      "loss": 0.2068,
      "step": 271
    },
    {
      "epoch": 1.2710280373831775,
      "grad_norm": 0.5538960695266724,
      "learning_rate": 0.0002909657320872274,
      "loss": 0.2576,
      "step": 272
    },
    {
      "epoch": 1.2757009345794392,
      "grad_norm": 0.4920056164264679,
      "learning_rate": 0.000290809968847352,
      "loss": 0.215,
      "step": 273
    },
    {
      "epoch": 1.280373831775701,
      "grad_norm": 0.47620105743408203,
      "learning_rate": 0.00029065420560747663,
      "loss": 0.1965,
      "step": 274
    },
    {
      "epoch": 1.2850467289719627,
      "grad_norm": 0.5913344621658325,
      "learning_rate": 0.00029049844236760125,
      "loss": 0.2946,
      "step": 275
    },
    {
      "epoch": 1.2897196261682242,
      "grad_norm": 0.4218387007713318,
      "learning_rate": 0.00029034267912772586,
      "loss": 0.24,
      "step": 276
    },
    {
      "epoch": 1.294392523364486,
      "grad_norm": 0.4422839283943176,
      "learning_rate": 0.0002901869158878505,
      "loss": 0.2396,
      "step": 277
    },
    {
      "epoch": 1.2990654205607477,
      "grad_norm": 0.4000198245048523,
      "learning_rate": 0.00029003115264797505,
      "loss": 0.1949,
      "step": 278
    },
    {
      "epoch": 1.3037383177570092,
      "grad_norm": 0.463515967130661,
      "learning_rate": 0.00028987538940809966,
      "loss": 0.2744,
      "step": 279
    },
    {
      "epoch": 1.308411214953271,
      "grad_norm": 0.42384326457977295,
      "learning_rate": 0.0002897196261682243,
      "loss": 0.2725,
      "step": 280
    },
    {
      "epoch": 1.3130841121495327,
      "grad_norm": 0.43044233322143555,
      "learning_rate": 0.0002895638629283489,
      "loss": 0.2408,
      "step": 281
    },
    {
      "epoch": 1.3177570093457944,
      "grad_norm": 0.36982572078704834,
      "learning_rate": 0.0002894080996884735,
      "loss": 0.2447,
      "step": 282
    },
    {
      "epoch": 1.3224299065420562,
      "grad_norm": 0.4672091603279114,
      "learning_rate": 0.00028925233644859814,
      "loss": 0.204,
      "step": 283
    },
    {
      "epoch": 1.3271028037383177,
      "grad_norm": 0.5415518283843994,
      "learning_rate": 0.0002890965732087227,
      "loss": 0.2547,
      "step": 284
    },
    {
      "epoch": 1.3317757009345794,
      "grad_norm": 0.4068405032157898,
      "learning_rate": 0.0002889408099688473,
      "loss": 0.1963,
      "step": 285
    },
    {
      "epoch": 1.3364485981308412,
      "grad_norm": 0.3899727761745453,
      "learning_rate": 0.00028878504672897194,
      "loss": 0.2152,
      "step": 286
    },
    {
      "epoch": 1.3411214953271027,
      "grad_norm": 0.5241720080375671,
      "learning_rate": 0.00028862928348909655,
      "loss": 0.2277,
      "step": 287
    },
    {
      "epoch": 1.3457943925233644,
      "grad_norm": 0.5890369415283203,
      "learning_rate": 0.00028847352024922117,
      "loss": 0.2539,
      "step": 288
    },
    {
      "epoch": 1.3504672897196262,
      "grad_norm": 0.49259722232818604,
      "learning_rate": 0.0002883177570093458,
      "loss": 0.2563,
      "step": 289
    },
    {
      "epoch": 1.355140186915888,
      "grad_norm": 0.43029212951660156,
      "learning_rate": 0.00028816199376947035,
      "loss": 0.2527,
      "step": 290
    },
    {
      "epoch": 1.3598130841121496,
      "grad_norm": 0.4370424151420593,
      "learning_rate": 0.00028800623052959497,
      "loss": 0.2414,
      "step": 291
    },
    {
      "epoch": 1.3644859813084111,
      "grad_norm": 0.3541645407676697,
      "learning_rate": 0.0002878504672897196,
      "loss": 0.1688,
      "step": 292
    },
    {
      "epoch": 1.3691588785046729,
      "grad_norm": 0.420745313167572,
      "learning_rate": 0.0002876947040498442,
      "loss": 0.2432,
      "step": 293
    },
    {
      "epoch": 1.3738317757009346,
      "grad_norm": 0.4319837987422943,
      "learning_rate": 0.0002875389408099688,
      "loss": 0.1903,
      "step": 294
    },
    {
      "epoch": 1.3785046728971961,
      "grad_norm": 0.6152929663658142,
      "learning_rate": 0.00028738317757009345,
      "loss": 0.2918,
      "step": 295
    },
    {
      "epoch": 1.3831775700934579,
      "grad_norm": 0.5440046191215515,
      "learning_rate": 0.000287227414330218,
      "loss": 0.2523,
      "step": 296
    },
    {
      "epoch": 1.3878504672897196,
      "grad_norm": 0.5640828013420105,
      "learning_rate": 0.00028707165109034263,
      "loss": 0.2106,
      "step": 297
    },
    {
      "epoch": 1.3925233644859814,
      "grad_norm": 0.38218218088150024,
      "learning_rate": 0.00028691588785046725,
      "loss": 0.251,
      "step": 298
    },
    {
      "epoch": 1.397196261682243,
      "grad_norm": 0.39721184968948364,
      "learning_rate": 0.00028676012461059186,
      "loss": 0.1906,
      "step": 299
    },
    {
      "epoch": 1.4018691588785046,
      "grad_norm": 0.49441176652908325,
      "learning_rate": 0.0002866043613707165,
      "loss": 0.2435,
      "step": 300
    },
    {
      "epoch": 1.4065420560747663,
      "grad_norm": 0.47977566719055176,
      "learning_rate": 0.0002864485981308411,
      "loss": 0.2736,
      "step": 301
    },
    {
      "epoch": 1.411214953271028,
      "grad_norm": 0.41448959708213806,
      "learning_rate": 0.0002862928348909657,
      "loss": 0.2724,
      "step": 302
    },
    {
      "epoch": 1.4158878504672896,
      "grad_norm": 0.38549625873565674,
      "learning_rate": 0.00028613707165109034,
      "loss": 0.2399,
      "step": 303
    },
    {
      "epoch": 1.4205607476635513,
      "grad_norm": 0.4015490710735321,
      "learning_rate": 0.0002859813084112149,
      "loss": 0.2585,
      "step": 304
    },
    {
      "epoch": 1.425233644859813,
      "grad_norm": 0.32108965516090393,
      "learning_rate": 0.0002858255451713395,
      "loss": 0.1934,
      "step": 305
    },
    {
      "epoch": 1.4299065420560748,
      "grad_norm": 0.388698548078537,
      "learning_rate": 0.00028566978193146414,
      "loss": 0.215,
      "step": 306
    },
    {
      "epoch": 1.4345794392523366,
      "grad_norm": 0.35669103264808655,
      "learning_rate": 0.00028551401869158875,
      "loss": 0.1916,
      "step": 307
    },
    {
      "epoch": 1.439252336448598,
      "grad_norm": 0.4988592267036438,
      "learning_rate": 0.00028535825545171337,
      "loss": 0.2326,
      "step": 308
    },
    {
      "epoch": 1.4439252336448598,
      "grad_norm": 0.40116748213768005,
      "learning_rate": 0.000285202492211838,
      "loss": 0.2365,
      "step": 309
    },
    {
      "epoch": 1.4485981308411215,
      "grad_norm": 0.3729148209095001,
      "learning_rate": 0.0002850467289719626,
      "loss": 0.2302,
      "step": 310
    },
    {
      "epoch": 1.453271028037383,
      "grad_norm": 0.48181256651878357,
      "learning_rate": 0.0002848909657320872,
      "loss": 0.263,
      "step": 311
    },
    {
      "epoch": 1.4579439252336448,
      "grad_norm": 0.47957995533943176,
      "learning_rate": 0.00028473520249221184,
      "loss": 0.2645,
      "step": 312
    },
    {
      "epoch": 1.4626168224299065,
      "grad_norm": 0.4550630450248718,
      "learning_rate": 0.00028457943925233646,
      "loss": 0.2377,
      "step": 313
    },
    {
      "epoch": 1.4672897196261683,
      "grad_norm": 0.36463662981987,
      "learning_rate": 0.000284423676012461,
      "loss": 0.2118,
      "step": 314
    },
    {
      "epoch": 1.47196261682243,
      "grad_norm": 0.3270622789859772,
      "learning_rate": 0.00028426791277258564,
      "loss": 0.2054,
      "step": 315
    },
    {
      "epoch": 1.4766355140186915,
      "grad_norm": 0.3342922031879425,
      "learning_rate": 0.00028411214953271026,
      "loss": 0.2251,
      "step": 316
    },
    {
      "epoch": 1.4813084112149533,
      "grad_norm": 0.4121073782444,
      "learning_rate": 0.0002839563862928349,
      "loss": 0.2677,
      "step": 317
    },
    {
      "epoch": 1.485981308411215,
      "grad_norm": 0.4632221460342407,
      "learning_rate": 0.0002838006230529595,
      "loss": 0.2388,
      "step": 318
    },
    {
      "epoch": 1.4906542056074765,
      "grad_norm": 0.332771360874176,
      "learning_rate": 0.0002836448598130841,
      "loss": 0.2046,
      "step": 319
    },
    {
      "epoch": 1.4953271028037383,
      "grad_norm": 0.303480863571167,
      "learning_rate": 0.0002834890965732087,
      "loss": 0.1876,
      "step": 320
    },
    {
      "epoch": 1.5,
      "grad_norm": 0.37942972779273987,
      "learning_rate": 0.0002833333333333333,
      "loss": 0.2254,
      "step": 321
    },
    {
      "epoch": 1.5046728971962615,
      "grad_norm": 0.3193072974681854,
      "learning_rate": 0.0002831775700934579,
      "loss": 0.1938,
      "step": 322
    },
    {
      "epoch": 1.5093457943925235,
      "grad_norm": 0.43831974267959595,
      "learning_rate": 0.00028302180685358254,
      "loss": 0.2453,
      "step": 323
    },
    {
      "epoch": 1.514018691588785,
      "grad_norm": 0.42763665318489075,
      "learning_rate": 0.00028286604361370715,
      "loss": 0.235,
      "step": 324
    },
    {
      "epoch": 1.5186915887850467,
      "grad_norm": 0.48295870423316956,
      "learning_rate": 0.00028271028037383177,
      "loss": 0.2333,
      "step": 325
    },
    {
      "epoch": 1.5233644859813085,
      "grad_norm": 0.3650263249874115,
      "learning_rate": 0.00028255451713395634,
      "loss": 0.1931,
      "step": 326
    },
    {
      "epoch": 1.52803738317757,
      "grad_norm": 0.42020294070243835,
      "learning_rate": 0.00028239875389408095,
      "loss": 0.2166,
      "step": 327
    },
    {
      "epoch": 1.5327102803738317,
      "grad_norm": 0.4002986252307892,
      "learning_rate": 0.00028224299065420557,
      "loss": 0.2309,
      "step": 328
    },
    {
      "epoch": 1.5373831775700935,
      "grad_norm": 0.5297614932060242,
      "learning_rate": 0.0002820872274143302,
      "loss": 0.2317,
      "step": 329
    },
    {
      "epoch": 1.542056074766355,
      "grad_norm": 0.48632365465164185,
      "learning_rate": 0.0002819314641744548,
      "loss": 0.244,
      "step": 330
    },
    {
      "epoch": 1.546728971962617,
      "grad_norm": 0.5284860730171204,
      "learning_rate": 0.0002817757009345794,
      "loss": 0.259,
      "step": 331
    },
    {
      "epoch": 1.5514018691588785,
      "grad_norm": 0.3600948452949524,
      "learning_rate": 0.000281619937694704,
      "loss": 0.2195,
      "step": 332
    },
    {
      "epoch": 1.5560747663551402,
      "grad_norm": 0.3992065191268921,
      "learning_rate": 0.0002814641744548286,
      "loss": 0.2128,
      "step": 333
    },
    {
      "epoch": 1.560747663551402,
      "grad_norm": 0.36237436532974243,
      "learning_rate": 0.0002813084112149532,
      "loss": 0.2648,
      "step": 334
    },
    {
      "epoch": 1.5654205607476634,
      "grad_norm": 0.3519071042537689,
      "learning_rate": 0.00028115264797507784,
      "loss": 0.2046,
      "step": 335
    },
    {
      "epoch": 1.5700934579439252,
      "grad_norm": 0.385841965675354,
      "learning_rate": 0.00028099688473520246,
      "loss": 0.2397,
      "step": 336
    },
    {
      "epoch": 1.574766355140187,
      "grad_norm": 0.40591633319854736,
      "learning_rate": 0.0002808411214953271,
      "loss": 0.2512,
      "step": 337
    },
    {
      "epoch": 1.5794392523364484,
      "grad_norm": 0.3842562735080719,
      "learning_rate": 0.0002806853582554517,
      "loss": 0.1988,
      "step": 338
    },
    {
      "epoch": 1.5841121495327104,
      "grad_norm": 0.46864554286003113,
      "learning_rate": 0.0002805295950155763,
      "loss": 0.2249,
      "step": 339
    },
    {
      "epoch": 1.588785046728972,
      "grad_norm": 0.3813214600086212,
      "learning_rate": 0.0002803738317757009,
      "loss": 0.2003,
      "step": 340
    },
    {
      "epoch": 1.5934579439252337,
      "grad_norm": 0.3446866571903229,
      "learning_rate": 0.0002802180685358255,
      "loss": 0.1775,
      "step": 341
    },
    {
      "epoch": 1.5981308411214954,
      "grad_norm": 0.5303182005882263,
      "learning_rate": 0.0002800623052959501,
      "loss": 0.2773,
      "step": 342
    },
    {
      "epoch": 1.602803738317757,
      "grad_norm": 0.43938741087913513,
      "learning_rate": 0.00027990654205607474,
      "loss": 0.2321,
      "step": 343
    },
    {
      "epoch": 1.6074766355140186,
      "grad_norm": 0.3744860887527466,
      "learning_rate": 0.00027975077881619935,
      "loss": 0.193,
      "step": 344
    },
    {
      "epoch": 1.6121495327102804,
      "grad_norm": 0.41954368352890015,
      "learning_rate": 0.00027959501557632397,
      "loss": 0.2065,
      "step": 345
    },
    {
      "epoch": 1.616822429906542,
      "grad_norm": 0.40304428339004517,
      "learning_rate": 0.0002794392523364486,
      "loss": 0.2494,
      "step": 346
    },
    {
      "epoch": 1.6214953271028039,
      "grad_norm": 0.4001505672931671,
      "learning_rate": 0.0002792834890965732,
      "loss": 0.234,
      "step": 347
    },
    {
      "epoch": 1.6261682242990654,
      "grad_norm": 0.41791221499443054,
      "learning_rate": 0.0002791277258566978,
      "loss": 0.2686,
      "step": 348
    },
    {
      "epoch": 1.6308411214953271,
      "grad_norm": 0.41074225306510925,
      "learning_rate": 0.00027897196261682244,
      "loss": 0.257,
      "step": 349
    },
    {
      "epoch": 1.6355140186915889,
      "grad_norm": 0.3681872487068176,
      "learning_rate": 0.000278816199376947,
      "loss": 0.2232,
      "step": 350
    },
    {
      "epoch": 1.6401869158878504,
      "grad_norm": 0.3680969774723053,
      "learning_rate": 0.0002786604361370716,
      "loss": 0.2048,
      "step": 351
    },
    {
      "epoch": 1.644859813084112,
      "grad_norm": 0.40555763244628906,
      "learning_rate": 0.00027850467289719624,
      "loss": 0.3206,
      "step": 352
    },
    {
      "epoch": 1.6495327102803738,
      "grad_norm": 0.29944083094596863,
      "learning_rate": 0.00027834890965732086,
      "loss": 0.2268,
      "step": 353
    },
    {
      "epoch": 1.6542056074766354,
      "grad_norm": 0.33577683568000793,
      "learning_rate": 0.0002781931464174455,
      "loss": 0.2273,
      "step": 354
    },
    {
      "epoch": 1.6588785046728973,
      "grad_norm": 0.36659350991249084,
      "learning_rate": 0.0002780373831775701,
      "loss": 0.2493,
      "step": 355
    },
    {
      "epoch": 1.6635514018691588,
      "grad_norm": 0.3279997408390045,
      "learning_rate": 0.00027788161993769466,
      "loss": 0.1832,
      "step": 356
    },
    {
      "epoch": 1.6682242990654206,
      "grad_norm": 0.41193851828575134,
      "learning_rate": 0.0002777258566978193,
      "loss": 0.2311,
      "step": 357
    },
    {
      "epoch": 1.6728971962616823,
      "grad_norm": 0.46701332926750183,
      "learning_rate": 0.0002775700934579439,
      "loss": 0.2639,
      "step": 358
    },
    {
      "epoch": 1.6775700934579438,
      "grad_norm": 0.3226991891860962,
      "learning_rate": 0.0002774143302180685,
      "loss": 0.1782,
      "step": 359
    },
    {
      "epoch": 1.6822429906542056,
      "grad_norm": 0.3602573871612549,
      "learning_rate": 0.00027725856697819314,
      "loss": 0.2048,
      "step": 360
    },
    {
      "epoch": 1.6869158878504673,
      "grad_norm": 0.5532739162445068,
      "learning_rate": 0.00027710280373831775,
      "loss": 0.25,
      "step": 361
    },
    {
      "epoch": 1.6915887850467288,
      "grad_norm": 0.5148209929466248,
      "learning_rate": 0.0002769470404984423,
      "loss": 0.2137,
      "step": 362
    },
    {
      "epoch": 1.6962616822429908,
      "grad_norm": 0.47944512963294983,
      "learning_rate": 0.00027679127725856694,
      "loss": 0.2751,
      "step": 363
    },
    {
      "epoch": 1.7009345794392523,
      "grad_norm": 0.33936363458633423,
      "learning_rate": 0.00027663551401869155,
      "loss": 0.217,
      "step": 364
    },
    {
      "epoch": 1.705607476635514,
      "grad_norm": 0.400328129529953,
      "learning_rate": 0.00027647975077881617,
      "loss": 0.1863,
      "step": 365
    },
    {
      "epoch": 1.7102803738317758,
      "grad_norm": 0.3694053888320923,
      "learning_rate": 0.0002763239875389408,
      "loss": 0.2478,
      "step": 366
    },
    {
      "epoch": 1.7149532710280373,
      "grad_norm": 0.3460296392440796,
      "learning_rate": 0.0002761682242990654,
      "loss": 0.2413,
      "step": 367
    },
    {
      "epoch": 1.719626168224299,
      "grad_norm": 0.3522711396217346,
      "learning_rate": 0.00027601246105918997,
      "loss": 0.2061,
      "step": 368
    },
    {
      "epoch": 1.7242990654205608,
      "grad_norm": 0.4550352096557617,
      "learning_rate": 0.0002758566978193146,
      "loss": 0.2897,
      "step": 369
    },
    {
      "epoch": 1.7289719626168223,
      "grad_norm": 0.354705810546875,
      "learning_rate": 0.0002757009345794392,
      "loss": 0.204,
      "step": 370
    },
    {
      "epoch": 1.7336448598130842,
      "grad_norm": 0.40673360228538513,
      "learning_rate": 0.0002755451713395638,
      "loss": 0.2452,
      "step": 371
    },
    {
      "epoch": 1.7383177570093458,
      "grad_norm": 0.5174711346626282,
      "learning_rate": 0.00027538940809968844,
      "loss": 0.2226,
      "step": 372
    },
    {
      "epoch": 1.7429906542056075,
      "grad_norm": 0.45989328622817993,
      "learning_rate": 0.00027523364485981306,
      "loss": 0.2599,
      "step": 373
    },
    {
      "epoch": 1.7476635514018692,
      "grad_norm": 0.364676296710968,
      "learning_rate": 0.0002750778816199377,
      "loss": 0.2161,
      "step": 374
    },
    {
      "epoch": 1.7523364485981308,
      "grad_norm": 0.5044504404067993,
      "learning_rate": 0.0002749221183800623,
      "loss": 0.2633,
      "step": 375
    },
    {
      "epoch": 1.7570093457943925,
      "grad_norm": 0.3480493426322937,
      "learning_rate": 0.00027476635514018686,
      "loss": 0.2415,
      "step": 376
    },
    {
      "epoch": 1.7616822429906542,
      "grad_norm": 0.47333407402038574,
      "learning_rate": 0.0002746105919003115,
      "loss": 0.2664,
      "step": 377
    },
    {
      "epoch": 1.7663551401869158,
      "grad_norm": 0.3448392450809479,
      "learning_rate": 0.0002744548286604361,
      "loss": 0.2183,
      "step": 378
    },
    {
      "epoch": 1.7710280373831777,
      "grad_norm": 0.32243019342422485,
      "learning_rate": 0.0002742990654205607,
      "loss": 0.157,
      "step": 379
    },
    {
      "epoch": 1.7757009345794392,
      "grad_norm": 0.3497414290904999,
      "learning_rate": 0.00027414330218068533,
      "loss": 0.2581,
      "step": 380
    },
    {
      "epoch": 1.780373831775701,
      "grad_norm": 0.3072507977485657,
      "learning_rate": 0.00027398753894080995,
      "loss": 0.1932,
      "step": 381
    },
    {
      "epoch": 1.7850467289719627,
      "grad_norm": 0.35146984457969666,
      "learning_rate": 0.00027383177570093457,
      "loss": 0.2434,
      "step": 382
    },
    {
      "epoch": 1.7897196261682242,
      "grad_norm": 0.365144819021225,
      "learning_rate": 0.0002736760124610592,
      "loss": 0.1727,
      "step": 383
    },
    {
      "epoch": 1.794392523364486,
      "grad_norm": 0.3754303455352783,
      "learning_rate": 0.0002735202492211838,
      "loss": 0.1992,
      "step": 384
    },
    {
      "epoch": 1.7990654205607477,
      "grad_norm": 0.34230414032936096,
      "learning_rate": 0.0002733644859813084,
      "loss": 0.1924,
      "step": 385
    },
    {
      "epoch": 1.8037383177570092,
      "grad_norm": 0.4177423119544983,
      "learning_rate": 0.000273208722741433,
      "loss": 0.2077,
      "step": 386
    },
    {
      "epoch": 1.8084112149532712,
      "grad_norm": 0.30092620849609375,
      "learning_rate": 0.0002730529595015576,
      "loss": 0.1877,
      "step": 387
    },
    {
      "epoch": 1.8130841121495327,
      "grad_norm": 0.5308588743209839,
      "learning_rate": 0.0002728971962616822,
      "loss": 0.2688,
      "step": 388
    },
    {
      "epoch": 1.8177570093457944,
      "grad_norm": 0.44551214575767517,
      "learning_rate": 0.00027274143302180684,
      "loss": 0.2084,
      "step": 389
    },
    {
      "epoch": 1.8224299065420562,
      "grad_norm": 0.3938150703907013,
      "learning_rate": 0.00027258566978193146,
      "loss": 0.2109,
      "step": 390
    },
    {
      "epoch": 1.8271028037383177,
      "grad_norm": 0.3645317256450653,
      "learning_rate": 0.0002724299065420561,
      "loss": 0.2253,
      "step": 391
    },
    {
      "epoch": 1.8317757009345794,
      "grad_norm": 0.4510645568370819,
      "learning_rate": 0.00027227414330218064,
      "loss": 0.2171,
      "step": 392
    },
    {
      "epoch": 1.8364485981308412,
      "grad_norm": 0.3717730641365051,
      "learning_rate": 0.00027211838006230526,
      "loss": 0.2592,
      "step": 393
    },
    {
      "epoch": 1.8411214953271027,
      "grad_norm": 0.35035836696624756,
      "learning_rate": 0.0002719626168224299,
      "loss": 0.2363,
      "step": 394
    },
    {
      "epoch": 1.8457943925233646,
      "grad_norm": 0.5343219637870789,
      "learning_rate": 0.0002718068535825545,
      "loss": 0.2465,
      "step": 395
    },
    {
      "epoch": 1.8504672897196262,
      "grad_norm": 0.3508366048336029,
      "learning_rate": 0.0002716510903426791,
      "loss": 0.2207,
      "step": 396
    },
    {
      "epoch": 1.855140186915888,
      "grad_norm": 0.40803295373916626,
      "learning_rate": 0.00027149532710280373,
      "loss": 0.2033,
      "step": 397
    },
    {
      "epoch": 1.8598130841121496,
      "grad_norm": 0.415950208902359,
      "learning_rate": 0.0002713395638629283,
      "loss": 0.2092,
      "step": 398
    },
    {
      "epoch": 1.8644859813084111,
      "grad_norm": 0.37734276056289673,
      "learning_rate": 0.0002711838006230529,
      "loss": 0.2121,
      "step": 399
    },
    {
      "epoch": 1.8691588785046729,
      "grad_norm": 0.41017231345176697,
      "learning_rate": 0.00027102803738317753,
      "loss": 0.2372,
      "step": 400
    },
    {
      "epoch": 1.8738317757009346,
      "grad_norm": 0.3648647665977478,
      "learning_rate": 0.00027087227414330215,
      "loss": 0.1923,
      "step": 401
    },
    {
      "epoch": 1.8785046728971961,
      "grad_norm": 0.3919427692890167,
      "learning_rate": 0.00027071651090342677,
      "loss": 0.2223,
      "step": 402
    },
    {
      "epoch": 1.883177570093458,
      "grad_norm": 0.43950653076171875,
      "learning_rate": 0.0002705607476635514,
      "loss": 0.2317,
      "step": 403
    },
    {
      "epoch": 1.8878504672897196,
      "grad_norm": 0.4605214297771454,
      "learning_rate": 0.00027040498442367595,
      "loss": 0.2037,
      "step": 404
    },
    {
      "epoch": 1.8925233644859814,
      "grad_norm": 0.3971187174320221,
      "learning_rate": 0.00027024922118380057,
      "loss": 0.2305,
      "step": 405
    },
    {
      "epoch": 1.897196261682243,
      "grad_norm": 0.30557548999786377,
      "learning_rate": 0.0002700934579439252,
      "loss": 0.2003,
      "step": 406
    },
    {
      "epoch": 1.9018691588785046,
      "grad_norm": 0.3724663555622101,
      "learning_rate": 0.0002699376947040498,
      "loss": 0.2241,
      "step": 407
    },
    {
      "epoch": 1.9065420560747663,
      "grad_norm": 0.45371201634407043,
      "learning_rate": 0.0002697819314641744,
      "loss": 0.2325,
      "step": 408
    },
    {
      "epoch": 1.911214953271028,
      "grad_norm": 0.3375653922557831,
      "learning_rate": 0.00026962616822429904,
      "loss": 0.1724,
      "step": 409
    },
    {
      "epoch": 1.9158878504672896,
      "grad_norm": 0.43217116594314575,
      "learning_rate": 0.00026947040498442366,
      "loss": 0.3044,
      "step": 410
    },
    {
      "epoch": 1.9205607476635516,
      "grad_norm": 0.3853517174720764,
      "learning_rate": 0.0002693146417445483,
      "loss": 0.2378,
      "step": 411
    },
    {
      "epoch": 1.925233644859813,
      "grad_norm": 0.37825676798820496,
      "learning_rate": 0.00026915887850467284,
      "loss": 0.2509,
      "step": 412
    },
    {
      "epoch": 1.9299065420560748,
      "grad_norm": 0.43532007932662964,
      "learning_rate": 0.00026900311526479746,
      "loss": 0.2189,
      "step": 413
    },
    {
      "epoch": 1.9345794392523366,
      "grad_norm": 0.41252219676971436,
      "learning_rate": 0.0002688473520249221,
      "loss": 0.2203,
      "step": 414
    },
    {
      "epoch": 1.939252336448598,
      "grad_norm": 0.3778564929962158,
      "learning_rate": 0.0002686915887850467,
      "loss": 0.2323,
      "step": 415
    },
    {
      "epoch": 1.9439252336448598,
      "grad_norm": 0.42601001262664795,
      "learning_rate": 0.0002685358255451713,
      "loss": 0.2119,
      "step": 416
    },
    {
      "epoch": 1.9485981308411215,
      "grad_norm": 0.3528680205345154,
      "learning_rate": 0.00026838006230529593,
      "loss": 0.1996,
      "step": 417
    },
    {
      "epoch": 1.953271028037383,
      "grad_norm": 0.37343570590019226,
      "learning_rate": 0.00026822429906542055,
      "loss": 0.1997,
      "step": 418
    },
    {
      "epoch": 1.957943925233645,
      "grad_norm": 0.49211204051971436,
      "learning_rate": 0.00026806853582554517,
      "loss": 0.237,
      "step": 419
    },
    {
      "epoch": 1.9626168224299065,
      "grad_norm": 0.4218895733356476,
      "learning_rate": 0.0002679127725856698,
      "loss": 0.2211,
      "step": 420
    },
    {
      "epoch": 1.9672897196261683,
      "grad_norm": 0.38691869378089905,
      "learning_rate": 0.0002677570093457944,
      "loss": 0.1604,
      "step": 421
    },
    {
      "epoch": 1.97196261682243,
      "grad_norm": 0.555326521396637,
      "learning_rate": 0.00026760124610591897,
      "loss": 0.2519,
      "step": 422
    },
    {
      "epoch": 1.9766355140186915,
      "grad_norm": 0.3835749626159668,
      "learning_rate": 0.0002674454828660436,
      "loss": 0.2284,
      "step": 423
    },
    {
      "epoch": 1.9813084112149533,
      "grad_norm": 0.3387444317340851,
      "learning_rate": 0.0002672897196261682,
      "loss": 0.2108,
      "step": 424
    },
    {
      "epoch": 1.985981308411215,
      "grad_norm": 0.36571264266967773,
      "learning_rate": 0.0002671339563862928,
      "loss": 0.2397,
      "step": 425
    },
    {
      "epoch": 1.9906542056074765,
      "grad_norm": 0.37170353531837463,
      "learning_rate": 0.00026697819314641744,
      "loss": 0.2428,
      "step": 426
    },
    {
      "epoch": 1.9953271028037385,
      "grad_norm": 0.33383795619010925,
      "learning_rate": 0.00026682242990654206,
      "loss": 0.1744,
      "step": 427
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.6764259934425354,
      "learning_rate": 0.0002666666666666666,
      "loss": 0.253,
      "step": 428
    },
    {
      "epoch": 2.0,
      "eval_loss": 0.28423210978507996,
      "eval_runtime": 53.5414,
      "eval_samples_per_second": 10.31,
      "eval_steps_per_second": 1.289,
      "step": 428
    },
    {
      "epoch": 2.0046728971962615,
      "grad_norm": 0.30848491191864014,
      "learning_rate": 0.00026651090342679124,
      "loss": 0.1815,
      "step": 429
    },
    {
      "epoch": 2.0093457943925235,
      "grad_norm": 0.4630880653858185,
      "learning_rate": 0.00026635514018691586,
      "loss": 0.2323,
      "step": 430
    },
    {
      "epoch": 2.014018691588785,
      "grad_norm": 0.33351439237594604,
      "learning_rate": 0.0002661993769470405,
      "loss": 0.2245,
      "step": 431
    },
    {
      "epoch": 2.0186915887850465,
      "grad_norm": 0.4194774925708771,
      "learning_rate": 0.0002660436137071651,
      "loss": 0.2274,
      "step": 432
    },
    {
      "epoch": 2.0233644859813085,
      "grad_norm": 0.3426246643066406,
      "learning_rate": 0.0002658878504672897,
      "loss": 0.2174,
      "step": 433
    },
    {
      "epoch": 2.02803738317757,
      "grad_norm": 0.33265215158462524,
      "learning_rate": 0.0002657320872274143,
      "loss": 0.2131,
      "step": 434
    },
    {
      "epoch": 2.032710280373832,
      "grad_norm": 0.29884928464889526,
      "learning_rate": 0.0002655763239875389,
      "loss": 0.1917,
      "step": 435
    },
    {
      "epoch": 2.0373831775700935,
      "grad_norm": 0.4915352165699005,
      "learning_rate": 0.0002654205607476635,
      "loss": 0.216,
      "step": 436
    },
    {
      "epoch": 2.042056074766355,
      "grad_norm": 0.3620762825012207,
      "learning_rate": 0.00026526479750778813,
      "loss": 0.2138,
      "step": 437
    },
    {
      "epoch": 2.046728971962617,
      "grad_norm": 0.403283029794693,
      "learning_rate": 0.00026510903426791275,
      "loss": 0.2115,
      "step": 438
    },
    {
      "epoch": 2.0514018691588785,
      "grad_norm": 0.4159157872200012,
      "learning_rate": 0.00026495327102803737,
      "loss": 0.2091,
      "step": 439
    },
    {
      "epoch": 2.05607476635514,
      "grad_norm": 0.43345943093299866,
      "learning_rate": 0.00026479750778816193,
      "loss": 0.197,
      "step": 440
    },
    {
      "epoch": 2.060747663551402,
      "grad_norm": 0.41201111674308777,
      "learning_rate": 0.00026464174454828655,
      "loss": 0.1773,
      "step": 441
    },
    {
      "epoch": 2.0654205607476634,
      "grad_norm": 0.3741145133972168,
      "learning_rate": 0.00026448598130841117,
      "loss": 0.1968,
      "step": 442
    },
    {
      "epoch": 2.0700934579439254,
      "grad_norm": 0.3727463185787201,
      "learning_rate": 0.0002643302180685358,
      "loss": 0.1885,
      "step": 443
    },
    {
      "epoch": 2.074766355140187,
      "grad_norm": 0.4526873528957367,
      "learning_rate": 0.0002641744548286604,
      "loss": 0.1732,
      "step": 444
    },
    {
      "epoch": 2.0794392523364484,
      "grad_norm": 0.4288298189640045,
      "learning_rate": 0.000264018691588785,
      "loss": 0.2113,
      "step": 445
    },
    {
      "epoch": 2.0841121495327104,
      "grad_norm": 0.3982926309108734,
      "learning_rate": 0.00026386292834890964,
      "loss": 0.1991,
      "step": 446
    },
    {
      "epoch": 2.088785046728972,
      "grad_norm": 0.3690636456012726,
      "learning_rate": 0.00026370716510903426,
      "loss": 0.1677,
      "step": 447
    },
    {
      "epoch": 2.0934579439252334,
      "grad_norm": 0.3491738736629486,
      "learning_rate": 0.0002635514018691588,
      "loss": 0.1948,
      "step": 448
    },
    {
      "epoch": 2.0981308411214954,
      "grad_norm": 0.41284728050231934,
      "learning_rate": 0.00026339563862928344,
      "loss": 0.2085,
      "step": 449
    },
    {
      "epoch": 2.102803738317757,
      "grad_norm": 0.5461464524269104,
      "learning_rate": 0.00026323987538940806,
      "loss": 0.206,
      "step": 450
    },
    {
      "epoch": 2.107476635514019,
      "grad_norm": 0.3487900495529175,
      "learning_rate": 0.0002630841121495327,
      "loss": 0.185,
      "step": 451
    },
    {
      "epoch": 2.1121495327102804,
      "grad_norm": 0.38645780086517334,
      "learning_rate": 0.0002629283489096573,
      "loss": 0.1905,
      "step": 452
    },
    {
      "epoch": 2.116822429906542,
      "grad_norm": 0.3230155408382416,
      "learning_rate": 0.0002627725856697819,
      "loss": 0.1772,
      "step": 453
    },
    {
      "epoch": 2.121495327102804,
      "grad_norm": 0.4194455146789551,
      "learning_rate": 0.00026261682242990653,
      "loss": 0.227,
      "step": 454
    },
    {
      "epoch": 2.1261682242990654,
      "grad_norm": 0.3327924311161041,
      "learning_rate": 0.00026246105919003115,
      "loss": 0.1987,
      "step": 455
    },
    {
      "epoch": 2.130841121495327,
      "grad_norm": 0.28721076250076294,
      "learning_rate": 0.00026230529595015577,
      "loss": 0.1917,
      "step": 456
    },
    {
      "epoch": 2.135514018691589,
      "grad_norm": 0.4798516631126404,
      "learning_rate": 0.0002621495327102804,
      "loss": 0.2463,
      "step": 457
    },
    {
      "epoch": 2.1401869158878504,
      "grad_norm": 0.3499351143836975,
      "learning_rate": 0.00026199376947040495,
      "loss": 0.1668,
      "step": 458
    },
    {
      "epoch": 2.1448598130841123,
      "grad_norm": 0.35131970047950745,
      "learning_rate": 0.00026183800623052957,
      "loss": 0.1944,
      "step": 459
    },
    {
      "epoch": 2.149532710280374,
      "grad_norm": 0.4113864004611969,
      "learning_rate": 0.0002616822429906542,
      "loss": 0.2278,
      "step": 460
    },
    {
      "epoch": 2.1542056074766354,
      "grad_norm": 0.39774250984191895,
      "learning_rate": 0.0002615264797507788,
      "loss": 0.173,
      "step": 461
    },
    {
      "epoch": 2.1588785046728973,
      "grad_norm": 0.474223792552948,
      "learning_rate": 0.0002613707165109034,
      "loss": 0.2247,
      "step": 462
    },
    {
      "epoch": 2.163551401869159,
      "grad_norm": 0.5004827380180359,
      "learning_rate": 0.00026121495327102804,
      "loss": 0.2098,
      "step": 463
    },
    {
      "epoch": 2.1682242990654204,
      "grad_norm": 0.3595128357410431,
      "learning_rate": 0.0002610591900311526,
      "loss": 0.1798,
      "step": 464
    },
    {
      "epoch": 2.1728971962616823,
      "grad_norm": 0.4086713194847107,
      "learning_rate": 0.0002609034267912772,
      "loss": 0.2159,
      "step": 465
    },
    {
      "epoch": 2.177570093457944,
      "grad_norm": 0.40674281120300293,
      "learning_rate": 0.00026074766355140184,
      "loss": 0.174,
      "step": 466
    },
    {
      "epoch": 2.182242990654206,
      "grad_norm": 0.373685359954834,
      "learning_rate": 0.00026059190031152646,
      "loss": 0.231,
      "step": 467
    },
    {
      "epoch": 2.1869158878504673,
      "grad_norm": 0.5248479843139648,
      "learning_rate": 0.0002604361370716511,
      "loss": 0.1953,
      "step": 468
    },
    {
      "epoch": 2.191588785046729,
      "grad_norm": 0.586233377456665,
      "learning_rate": 0.0002602803738317757,
      "loss": 0.2651,
      "step": 469
    },
    {
      "epoch": 2.196261682242991,
      "grad_norm": 0.442749947309494,
      "learning_rate": 0.00026012461059190026,
      "loss": 0.1898,
      "step": 470
    },
    {
      "epoch": 2.2009345794392523,
      "grad_norm": 0.39599645137786865,
      "learning_rate": 0.0002599688473520249,
      "loss": 0.2217,
      "step": 471
    },
    {
      "epoch": 2.205607476635514,
      "grad_norm": 0.3362051546573639,
      "learning_rate": 0.0002598130841121495,
      "loss": 0.1975,
      "step": 472
    },
    {
      "epoch": 2.210280373831776,
      "grad_norm": 0.39142459630966187,
      "learning_rate": 0.0002596573208722741,
      "loss": 0.2384,
      "step": 473
    },
    {
      "epoch": 2.2149532710280373,
      "grad_norm": 0.4178955554962158,
      "learning_rate": 0.00025950155763239873,
      "loss": 0.2193,
      "step": 474
    },
    {
      "epoch": 2.2196261682242993,
      "grad_norm": 0.3608540892601013,
      "learning_rate": 0.00025934579439252335,
      "loss": 0.2103,
      "step": 475
    },
    {
      "epoch": 2.2242990654205608,
      "grad_norm": 0.3731110095977783,
      "learning_rate": 0.0002591900311526479,
      "loss": 0.2214,
      "step": 476
    },
    {
      "epoch": 2.2289719626168223,
      "grad_norm": 0.309736430644989,
      "learning_rate": 0.00025903426791277253,
      "loss": 0.1596,
      "step": 477
    },
    {
      "epoch": 2.2336448598130842,
      "grad_norm": 0.34517958760261536,
      "learning_rate": 0.00025887850467289715,
      "loss": 0.2241,
      "step": 478
    },
    {
      "epoch": 2.2383177570093458,
      "grad_norm": 0.5002716779708862,
      "learning_rate": 0.00025872274143302177,
      "loss": 0.2119,
      "step": 479
    },
    {
      "epoch": 2.2429906542056073,
      "grad_norm": 0.540239691734314,
      "learning_rate": 0.0002585669781931464,
      "loss": 0.2249,
      "step": 480
    },
    {
      "epoch": 2.2476635514018692,
      "grad_norm": 0.382313072681427,
      "learning_rate": 0.000258411214953271,
      "loss": 0.1773,
      "step": 481
    },
    {
      "epoch": 2.2523364485981308,
      "grad_norm": 0.43948304653167725,
      "learning_rate": 0.0002582554517133956,
      "loss": 0.235,
      "step": 482
    },
    {
      "epoch": 2.2570093457943923,
      "grad_norm": 0.41170915961265564,
      "learning_rate": 0.00025809968847352024,
      "loss": 0.2149,
      "step": 483
    },
    {
      "epoch": 2.2616822429906542,
      "grad_norm": 0.457354873418808,
      "learning_rate": 0.0002579439252336448,
      "loss": 0.2374,
      "step": 484
    },
    {
      "epoch": 2.2663551401869158,
      "grad_norm": 0.4163548946380615,
      "learning_rate": 0.0002577881619937694,
      "loss": 0.1866,
      "step": 485
    },
    {
      "epoch": 2.2710280373831777,
      "grad_norm": 0.3730331063270569,
      "learning_rate": 0.00025763239875389404,
      "loss": 0.2164,
      "step": 486
    },
    {
      "epoch": 2.2757009345794392,
      "grad_norm": 0.2889437675476074,
      "learning_rate": 0.00025747663551401866,
      "loss": 0.1771,
      "step": 487
    },
    {
      "epoch": 2.2803738317757007,
      "grad_norm": 0.3572808802127838,
      "learning_rate": 0.0002573208722741433,
      "loss": 0.2124,
      "step": 488
    },
    {
      "epoch": 2.2850467289719627,
      "grad_norm": 0.37784743309020996,
      "learning_rate": 0.0002571651090342679,
      "loss": 0.1819,
      "step": 489
    },
    {
      "epoch": 2.289719626168224,
      "grad_norm": 0.3769161105155945,
      "learning_rate": 0.0002570093457943925,
      "loss": 0.2265,
      "step": 490
    },
    {
      "epoch": 2.294392523364486,
      "grad_norm": 0.3563615083694458,
      "learning_rate": 0.00025685358255451713,
      "loss": 0.2084,
      "step": 491
    },
    {
      "epoch": 2.2990654205607477,
      "grad_norm": 0.36401134729385376,
      "learning_rate": 0.00025669781931464175,
      "loss": 0.185,
      "step": 492
    },
    {
      "epoch": 2.303738317757009,
      "grad_norm": 0.32314831018447876,
      "learning_rate": 0.00025654205607476637,
      "loss": 0.1793,
      "step": 493
    },
    {
      "epoch": 2.308411214953271,
      "grad_norm": 0.36853697896003723,
      "learning_rate": 0.00025638629283489093,
      "loss": 0.1982,
      "step": 494
    },
    {
      "epoch": 2.3130841121495327,
      "grad_norm": 0.36208146810531616,
      "learning_rate": 0.00025623052959501555,
      "loss": 0.2071,
      "step": 495
    },
    {
      "epoch": 2.317757009345794,
      "grad_norm": 0.34242916107177734,
      "learning_rate": 0.00025607476635514017,
      "loss": 0.1775,
      "step": 496
    },
    {
      "epoch": 2.322429906542056,
      "grad_norm": 0.37880516052246094,
      "learning_rate": 0.0002559190031152648,
      "loss": 0.2022,
      "step": 497
    },
    {
      "epoch": 2.3271028037383177,
      "grad_norm": 0.43341925740242004,
      "learning_rate": 0.0002557632398753894,
      "loss": 0.2297,
      "step": 498
    },
    {
      "epoch": 2.331775700934579,
      "grad_norm": 0.3978525996208191,
      "learning_rate": 0.000255607476635514,
      "loss": 0.2193,
      "step": 499
    },
    {
      "epoch": 2.336448598130841,
      "grad_norm": 0.42696765065193176,
      "learning_rate": 0.0002554517133956386,
      "loss": 0.2213,
      "step": 500
    },
    {
      "epoch": 2.3411214953271027,
      "grad_norm": 0.3818460702896118,
      "learning_rate": 0.0002552959501557632,
      "loss": 0.1817,
      "step": 501
    },
    {
      "epoch": 2.3457943925233646,
      "grad_norm": 0.4678801894187927,
      "learning_rate": 0.0002551401869158878,
      "loss": 0.1966,
      "step": 502
    },
    {
      "epoch": 2.350467289719626,
      "grad_norm": 0.40393465757369995,
      "learning_rate": 0.00025498442367601244,
      "loss": 0.2172,
      "step": 503
    },
    {
      "epoch": 2.3551401869158877,
      "grad_norm": 0.5411710739135742,
      "learning_rate": 0.00025482866043613706,
      "loss": 0.2417,
      "step": 504
    },
    {
      "epoch": 2.3598130841121496,
      "grad_norm": 0.391727089881897,
      "learning_rate": 0.0002546728971962617,
      "loss": 0.1689,
      "step": 505
    },
    {
      "epoch": 2.364485981308411,
      "grad_norm": 0.3635815978050232,
      "learning_rate": 0.00025451713395638624,
      "loss": 0.1795,
      "step": 506
    },
    {
      "epoch": 2.369158878504673,
      "grad_norm": 0.4612976014614105,
      "learning_rate": 0.00025436137071651086,
      "loss": 0.1986,
      "step": 507
    },
    {
      "epoch": 2.3738317757009346,
      "grad_norm": 0.41424116492271423,
      "learning_rate": 0.0002542056074766355,
      "loss": 0.1815,
      "step": 508
    },
    {
      "epoch": 2.378504672897196,
      "grad_norm": 0.3846572935581207,
      "learning_rate": 0.0002540498442367601,
      "loss": 0.1845,
      "step": 509
    },
    {
      "epoch": 2.383177570093458,
      "grad_norm": 0.3952149748802185,
      "learning_rate": 0.0002538940809968847,
      "loss": 0.1681,
      "step": 510
    },
    {
      "epoch": 2.3878504672897196,
      "grad_norm": 0.4043794572353363,
      "learning_rate": 0.00025373831775700933,
      "loss": 0.2204,
      "step": 511
    },
    {
      "epoch": 2.392523364485981,
      "grad_norm": 0.42396238446235657,
      "learning_rate": 0.0002535825545171339,
      "loss": 0.1851,
      "step": 512
    },
    {
      "epoch": 2.397196261682243,
      "grad_norm": 0.3651803433895111,
      "learning_rate": 0.0002534267912772585,
      "loss": 0.1884,
      "step": 513
    },
    {
      "epoch": 2.4018691588785046,
      "grad_norm": 0.40461891889572144,
      "learning_rate": 0.00025327102803738313,
      "loss": 0.1826,
      "step": 514
    },
    {
      "epoch": 2.406542056074766,
      "grad_norm": 0.3056260347366333,
      "learning_rate": 0.00025311526479750775,
      "loss": 0.1692,
      "step": 515
    },
    {
      "epoch": 2.411214953271028,
      "grad_norm": 0.409395694732666,
      "learning_rate": 0.00025295950155763237,
      "loss": 0.2091,
      "step": 516
    },
    {
      "epoch": 2.4158878504672896,
      "grad_norm": 0.39761316776275635,
      "learning_rate": 0.000252803738317757,
      "loss": 0.2058,
      "step": 517
    },
    {
      "epoch": 2.4205607476635516,
      "grad_norm": 0.3810979127883911,
      "learning_rate": 0.0002526479750778816,
      "loss": 0.2067,
      "step": 518
    },
    {
      "epoch": 2.425233644859813,
      "grad_norm": 0.3566889762878418,
      "learning_rate": 0.0002524922118380062,
      "loss": 0.1724,
      "step": 519
    },
    {
      "epoch": 2.4299065420560746,
      "grad_norm": 0.36350810527801514,
      "learning_rate": 0.0002523364485981308,
      "loss": 0.168,
      "step": 520
    },
    {
      "epoch": 2.4345794392523366,
      "grad_norm": 0.34000474214553833,
      "learning_rate": 0.0002521806853582554,
      "loss": 0.1898,
      "step": 521
    },
    {
      "epoch": 2.439252336448598,
      "grad_norm": 0.29644882678985596,
      "learning_rate": 0.00025202492211838,
      "loss": 0.1579,
      "step": 522
    },
    {
      "epoch": 2.44392523364486,
      "grad_norm": 0.40236204862594604,
      "learning_rate": 0.00025186915887850464,
      "loss": 0.2193,
      "step": 523
    },
    {
      "epoch": 2.4485981308411215,
      "grad_norm": 0.4390479624271393,
      "learning_rate": 0.00025171339563862926,
      "loss": 0.1996,
      "step": 524
    },
    {
      "epoch": 2.453271028037383,
      "grad_norm": 0.47422516345977783,
      "learning_rate": 0.0002515576323987539,
      "loss": 0.2233,
      "step": 525
    },
    {
      "epoch": 2.457943925233645,
      "grad_norm": 0.4673263430595398,
      "learning_rate": 0.0002514018691588785,
      "loss": 0.1931,
      "step": 526
    },
    {
      "epoch": 2.4626168224299065,
      "grad_norm": 0.3515952229499817,
      "learning_rate": 0.0002512461059190031,
      "loss": 0.1474,
      "step": 527
    },
    {
      "epoch": 2.467289719626168,
      "grad_norm": 0.4271216094493866,
      "learning_rate": 0.00025109034267912773,
      "loss": 0.1838,
      "step": 528
    },
    {
      "epoch": 2.47196261682243,
      "grad_norm": 0.48724427819252014,
      "learning_rate": 0.00025093457943925235,
      "loss": 0.2163,
      "step": 529
    },
    {
      "epoch": 2.4766355140186915,
      "grad_norm": 0.31273090839385986,
      "learning_rate": 0.0002507788161993769,
      "loss": 0.1895,
      "step": 530
    },
    {
      "epoch": 2.481308411214953,
      "grad_norm": 0.3513011336326599,
      "learning_rate": 0.00025062305295950153,
      "loss": 0.1844,
      "step": 531
    },
    {
      "epoch": 2.485981308411215,
      "grad_norm": 0.4077201783657074,
      "learning_rate": 0.00025046728971962615,
      "loss": 0.2089,
      "step": 532
    },
    {
      "epoch": 2.4906542056074765,
      "grad_norm": 0.28535786271095276,
      "learning_rate": 0.00025031152647975077,
      "loss": 0.1634,
      "step": 533
    },
    {
      "epoch": 2.4953271028037385,
      "grad_norm": 0.4766232371330261,
      "learning_rate": 0.0002501557632398754,
      "loss": 0.1932,
      "step": 534
    },
    {
      "epoch": 2.5,
      "grad_norm": 0.5915123224258423,
      "learning_rate": 0.00025,
      "loss": 0.2226,
      "step": 535
    },
    {
      "epoch": 2.5046728971962615,
      "grad_norm": 0.4864995777606964,
      "learning_rate": 0.0002498442367601246,
      "loss": 0.2095,
      "step": 536
    },
    {
      "epoch": 2.5093457943925235,
      "grad_norm": 0.4180038571357727,
      "learning_rate": 0.0002496884735202492,
      "loss": 0.2235,
      "step": 537
    },
    {
      "epoch": 2.514018691588785,
      "grad_norm": 0.34920215606689453,
      "learning_rate": 0.0002495327102803738,
      "loss": 0.2059,
      "step": 538
    },
    {
      "epoch": 2.518691588785047,
      "grad_norm": 0.3633234202861786,
      "learning_rate": 0.0002493769470404984,
      "loss": 0.2328,
      "step": 539
    },
    {
      "epoch": 2.5233644859813085,
      "grad_norm": 0.295169472694397,
      "learning_rate": 0.00024922118380062304,
      "loss": 0.1681,
      "step": 540
    },
    {
      "epoch": 2.52803738317757,
      "grad_norm": 0.3667091429233551,
      "learning_rate": 0.00024906542056074766,
      "loss": 0.1919,
      "step": 541
    },
    {
      "epoch": 2.5327102803738315,
      "grad_norm": 0.34053122997283936,
      "learning_rate": 0.0002489096573208723,
      "loss": 0.1905,
      "step": 542
    },
    {
      "epoch": 2.5373831775700935,
      "grad_norm": 0.3181303143501282,
      "learning_rate": 0.00024875389408099684,
      "loss": 0.1628,
      "step": 543
    },
    {
      "epoch": 2.542056074766355,
      "grad_norm": 0.3309215009212494,
      "learning_rate": 0.00024859813084112146,
      "loss": 0.211,
      "step": 544
    },
    {
      "epoch": 2.546728971962617,
      "grad_norm": 0.31804949045181274,
      "learning_rate": 0.0002484423676012461,
      "loss": 0.1683,
      "step": 545
    },
    {
      "epoch": 2.5514018691588785,
      "grad_norm": 0.4120226204395294,
      "learning_rate": 0.0002482866043613707,
      "loss": 0.1769,
      "step": 546
    },
    {
      "epoch": 2.55607476635514,
      "grad_norm": 0.3970593810081482,
      "learning_rate": 0.0002481308411214953,
      "loss": 0.1981,
      "step": 547
    },
    {
      "epoch": 2.560747663551402,
      "grad_norm": 0.43201902508735657,
      "learning_rate": 0.00024797507788161993,
      "loss": 0.1943,
      "step": 548
    },
    {
      "epoch": 2.5654205607476634,
      "grad_norm": 0.4246949553489685,
      "learning_rate": 0.0002478193146417445,
      "loss": 0.1866,
      "step": 549
    },
    {
      "epoch": 2.5700934579439254,
      "grad_norm": 0.45139023661613464,
      "learning_rate": 0.0002476635514018691,
      "loss": 0.1883,
      "step": 550
    },
    {
      "epoch": 2.574766355140187,
      "grad_norm": 0.5094509720802307,
      "learning_rate": 0.00024750778816199373,
      "loss": 0.1894,
      "step": 551
    },
    {
      "epoch": 2.5794392523364484,
      "grad_norm": 0.3100448548793793,
      "learning_rate": 0.00024735202492211835,
      "loss": 0.1439,
      "step": 552
    },
    {
      "epoch": 2.5841121495327104,
      "grad_norm": 0.37431108951568604,
      "learning_rate": 0.00024719626168224297,
      "loss": 0.1893,
      "step": 553
    },
    {
      "epoch": 2.588785046728972,
      "grad_norm": 0.35644108057022095,
      "learning_rate": 0.0002470404984423676,
      "loss": 0.2,
      "step": 554
    },
    {
      "epoch": 2.593457943925234,
      "grad_norm": 0.3816941976547241,
      "learning_rate": 0.0002468847352024922,
      "loss": 0.222,
      "step": 555
    },
    {
      "epoch": 2.5981308411214954,
      "grad_norm": 0.3095436990261078,
      "learning_rate": 0.00024672897196261677,
      "loss": 0.1911,
      "step": 556
    },
    {
      "epoch": 2.602803738317757,
      "grad_norm": 0.37515130639076233,
      "learning_rate": 0.0002465732087227414,
      "loss": 0.2603,
      "step": 557
    },
    {
      "epoch": 2.6074766355140184,
      "grad_norm": 0.3191685378551483,
      "learning_rate": 0.000246417445482866,
      "loss": 0.1837,
      "step": 558
    },
    {
      "epoch": 2.6121495327102804,
      "grad_norm": 0.31546205282211304,
      "learning_rate": 0.0002462616822429906,
      "loss": 0.1852,
      "step": 559
    },
    {
      "epoch": 2.616822429906542,
      "grad_norm": 0.3057706356048584,
      "learning_rate": 0.00024610591900311524,
      "loss": 0.1768,
      "step": 560
    },
    {
      "epoch": 2.621495327102804,
      "grad_norm": 0.2908453345298767,
      "learning_rate": 0.00024595015576323986,
      "loss": 0.158,
      "step": 561
    },
    {
      "epoch": 2.6261682242990654,
      "grad_norm": 0.2642555832862854,
      "learning_rate": 0.0002457943925233645,
      "loss": 0.1919,
      "step": 562
    },
    {
      "epoch": 2.630841121495327,
      "grad_norm": 0.31570568680763245,
      "learning_rate": 0.0002456386292834891,
      "loss": 0.1696,
      "step": 563
    },
    {
      "epoch": 2.635514018691589,
      "grad_norm": 0.4295428395271301,
      "learning_rate": 0.0002454828660436137,
      "loss": 0.2411,
      "step": 564
    },
    {
      "epoch": 2.6401869158878504,
      "grad_norm": 0.4044080376625061,
      "learning_rate": 0.00024532710280373833,
      "loss": 0.1826,
      "step": 565
    },
    {
      "epoch": 2.6448598130841123,
      "grad_norm": 0.4006812274456024,
      "learning_rate": 0.0002451713395638629,
      "loss": 0.1967,
      "step": 566
    },
    {
      "epoch": 2.649532710280374,
      "grad_norm": 0.3637782633304596,
      "learning_rate": 0.0002450155763239875,
      "loss": 0.1782,
      "step": 567
    },
    {
      "epoch": 2.6542056074766354,
      "grad_norm": 0.372733473777771,
      "learning_rate": 0.00024485981308411213,
      "loss": 0.182,
      "step": 568
    },
    {
      "epoch": 2.6588785046728973,
      "grad_norm": 0.4145485460758209,
      "learning_rate": 0.00024470404984423675,
      "loss": 0.1832,
      "step": 569
    },
    {
      "epoch": 2.663551401869159,
      "grad_norm": 0.3601810336112976,
      "learning_rate": 0.00024454828660436137,
      "loss": 0.2228,
      "step": 570
    },
    {
      "epoch": 2.668224299065421,
      "grad_norm": 0.3618566691875458,
      "learning_rate": 0.000244392523364486,
      "loss": 0.1988,
      "step": 571
    },
    {
      "epoch": 2.6728971962616823,
      "grad_norm": 0.42586037516593933,
      "learning_rate": 0.0002442367601246106,
      "loss": 0.2038,
      "step": 572
    },
    {
      "epoch": 2.677570093457944,
      "grad_norm": 0.31931164860725403,
      "learning_rate": 0.00024408099688473517,
      "loss": 0.179,
      "step": 573
    },
    {
      "epoch": 2.6822429906542054,
      "grad_norm": 0.3358354866504669,
      "learning_rate": 0.0002439252336448598,
      "loss": 0.2186,
      "step": 574
    },
    {
      "epoch": 2.6869158878504673,
      "grad_norm": 0.4202830195426941,
      "learning_rate": 0.0002437694704049844,
      "loss": 0.2244,
      "step": 575
    },
    {
      "epoch": 2.691588785046729,
      "grad_norm": 0.29565444588661194,
      "learning_rate": 0.00024361370716510902,
      "loss": 0.169,
      "step": 576
    },
    {
      "epoch": 2.696261682242991,
      "grad_norm": 0.310234934091568,
      "learning_rate": 0.00024345794392523364,
      "loss": 0.1952,
      "step": 577
    },
    {
      "epoch": 2.7009345794392523,
      "grad_norm": 0.3704071044921875,
      "learning_rate": 0.00024330218068535826,
      "loss": 0.1885,
      "step": 578
    },
    {
      "epoch": 2.705607476635514,
      "grad_norm": 0.42314526438713074,
      "learning_rate": 0.00024314641744548282,
      "loss": 0.1964,
      "step": 579
    },
    {
      "epoch": 2.710280373831776,
      "grad_norm": 0.36664310097694397,
      "learning_rate": 0.00024299065420560744,
      "loss": 0.2065,
      "step": 580
    },
    {
      "epoch": 2.7149532710280373,
      "grad_norm": 0.32071128487586975,
      "learning_rate": 0.00024283489096573206,
      "loss": 0.1722,
      "step": 581
    },
    {
      "epoch": 2.7196261682242993,
      "grad_norm": 0.445947527885437,
      "learning_rate": 0.00024267912772585668,
      "loss": 0.2304,
      "step": 582
    },
    {
      "epoch": 2.7242990654205608,
      "grad_norm": 0.4003125727176666,
      "learning_rate": 0.0002425233644859813,
      "loss": 0.2085,
      "step": 583
    },
    {
      "epoch": 2.7289719626168223,
      "grad_norm": 0.3776102364063263,
      "learning_rate": 0.00024236760124610591,
      "loss": 0.1907,
      "step": 584
    },
    {
      "epoch": 2.7336448598130842,
      "grad_norm": 0.4303711950778961,
      "learning_rate": 0.0002422118380062305,
      "loss": 0.2052,
      "step": 585
    },
    {
      "epoch": 2.7383177570093458,
      "grad_norm": 0.4105670750141144,
      "learning_rate": 0.00024205607476635512,
      "loss": 0.1862,
      "step": 586
    },
    {
      "epoch": 2.7429906542056077,
      "grad_norm": 0.31944605708122253,
      "learning_rate": 0.00024190031152647974,
      "loss": 0.2029,
      "step": 587
    },
    {
      "epoch": 2.7476635514018692,
      "grad_norm": 0.4471835792064667,
      "learning_rate": 0.00024174454828660433,
      "loss": 0.2413,
      "step": 588
    },
    {
      "epoch": 2.7523364485981308,
      "grad_norm": 0.35526880621910095,
      "learning_rate": 0.00024158878504672895,
      "loss": 0.1996,
      "step": 589
    },
    {
      "epoch": 2.7570093457943923,
      "grad_norm": 0.4014016389846802,
      "learning_rate": 0.00024143302180685357,
      "loss": 0.1762,
      "step": 590
    },
    {
      "epoch": 2.7616822429906542,
      "grad_norm": 0.40513017773628235,
      "learning_rate": 0.00024127725856697816,
      "loss": 0.2205,
      "step": 591
    },
    {
      "epoch": 2.7663551401869158,
      "grad_norm": 0.36617887020111084,
      "learning_rate": 0.00024112149532710278,
      "loss": 0.1972,
      "step": 592
    },
    {
      "epoch": 2.7710280373831777,
      "grad_norm": 0.29587793350219727,
      "learning_rate": 0.0002409657320872274,
      "loss": 0.1651,
      "step": 593
    },
    {
      "epoch": 2.7757009345794392,
      "grad_norm": 0.4104922115802765,
      "learning_rate": 0.000240809968847352,
      "loss": 0.1957,
      "step": 594
    },
    {
      "epoch": 2.7803738317757007,
      "grad_norm": 0.49861904978752136,
      "learning_rate": 0.00024065420560747663,
      "loss": 0.231,
      "step": 595
    },
    {
      "epoch": 2.7850467289719627,
      "grad_norm": 0.4135231375694275,
      "learning_rate": 0.00024049844236760125,
      "loss": 0.1859,
      "step": 596
    },
    {
      "epoch": 2.789719626168224,
      "grad_norm": 0.3851621150970459,
      "learning_rate": 0.00024034267912772581,
      "loss": 0.1691,
      "step": 597
    },
    {
      "epoch": 2.794392523364486,
      "grad_norm": 0.4107542634010315,
      "learning_rate": 0.00024018691588785043,
      "loss": 0.185,
      "step": 598
    },
    {
      "epoch": 2.7990654205607477,
      "grad_norm": 0.3764370083808899,
      "learning_rate": 0.00024003115264797505,
      "loss": 0.1818,
      "step": 599
    },
    {
      "epoch": 2.803738317757009,
      "grad_norm": 0.3493020236492157,
      "learning_rate": 0.00023987538940809967,
      "loss": 0.1711,
      "step": 600
    },
    {
      "epoch": 2.808411214953271,
      "grad_norm": 0.3724783957004547,
      "learning_rate": 0.00023971962616822429,
      "loss": 0.1823,
      "step": 601
    },
    {
      "epoch": 2.8130841121495327,
      "grad_norm": 0.39888671040534973,
      "learning_rate": 0.0002395638629283489,
      "loss": 0.1938,
      "step": 602
    },
    {
      "epoch": 2.8177570093457946,
      "grad_norm": 0.32124194502830505,
      "learning_rate": 0.0002394080996884735,
      "loss": 0.1918,
      "step": 603
    },
    {
      "epoch": 2.822429906542056,
      "grad_norm": 0.3693210482597351,
      "learning_rate": 0.0002392523364485981,
      "loss": 0.1988,
      "step": 604
    },
    {
      "epoch": 2.8271028037383177,
      "grad_norm": 0.37851154804229736,
      "learning_rate": 0.00023909657320872273,
      "loss": 0.2303,
      "step": 605
    },
    {
      "epoch": 2.831775700934579,
      "grad_norm": 0.3938215374946594,
      "learning_rate": 0.00023894080996884732,
      "loss": 0.1949,
      "step": 606
    },
    {
      "epoch": 2.836448598130841,
      "grad_norm": 0.3231286108493805,
      "learning_rate": 0.00023878504672897194,
      "loss": 0.1687,
      "step": 607
    },
    {
      "epoch": 2.8411214953271027,
      "grad_norm": 0.31910017132759094,
      "learning_rate": 0.00023862928348909656,
      "loss": 0.1907,
      "step": 608
    },
    {
      "epoch": 2.8457943925233646,
      "grad_norm": 0.3885522484779358,
      "learning_rate": 0.00023847352024922115,
      "loss": 0.2193,
      "step": 609
    },
    {
      "epoch": 2.850467289719626,
      "grad_norm": 0.3637753427028656,
      "learning_rate": 0.00023831775700934577,
      "loss": 0.2121,
      "step": 610
    },
    {
      "epoch": 2.8551401869158877,
      "grad_norm": 0.3482663035392761,
      "learning_rate": 0.00023816199376947039,
      "loss": 0.1936,
      "step": 611
    },
    {
      "epoch": 2.8598130841121496,
      "grad_norm": 0.31490465998649597,
      "learning_rate": 0.000238006230529595,
      "loss": 0.1642,
      "step": 612
    },
    {
      "epoch": 2.864485981308411,
      "grad_norm": 0.31150099635124207,
      "learning_rate": 0.00023785046728971962,
      "loss": 0.1871,
      "step": 613
    },
    {
      "epoch": 2.869158878504673,
      "grad_norm": 0.2803860604763031,
      "learning_rate": 0.00023769470404984424,
      "loss": 0.1654,
      "step": 614
    },
    {
      "epoch": 2.8738317757009346,
      "grad_norm": 0.34240028262138367,
      "learning_rate": 0.0002375389408099688,
      "loss": 0.17,
      "step": 615
    },
    {
      "epoch": 2.878504672897196,
      "grad_norm": 0.44071146845817566,
      "learning_rate": 0.00023738317757009342,
      "loss": 0.206,
      "step": 616
    },
    {
      "epoch": 2.883177570093458,
      "grad_norm": 0.2671719193458557,
      "learning_rate": 0.00023722741433021804,
      "loss": 0.1854,
      "step": 617
    },
    {
      "epoch": 2.8878504672897196,
      "grad_norm": 0.3370571434497833,
      "learning_rate": 0.00023707165109034266,
      "loss": 0.1737,
      "step": 618
    },
    {
      "epoch": 2.8925233644859816,
      "grad_norm": 0.3948157727718353,
      "learning_rate": 0.00023691588785046728,
      "loss": 0.2064,
      "step": 619
    },
    {
      "epoch": 2.897196261682243,
      "grad_norm": 0.42205023765563965,
      "learning_rate": 0.0002367601246105919,
      "loss": 0.1797,
      "step": 620
    },
    {
      "epoch": 2.9018691588785046,
      "grad_norm": 0.3393915593624115,
      "learning_rate": 0.00023660436137071649,
      "loss": 0.1979,
      "step": 621
    },
    {
      "epoch": 2.906542056074766,
      "grad_norm": 0.3443976938724518,
      "learning_rate": 0.0002364485981308411,
      "loss": 0.2227,
      "step": 622
    },
    {
      "epoch": 2.911214953271028,
      "grad_norm": 0.3129746913909912,
      "learning_rate": 0.00023629283489096572,
      "loss": 0.1928,
      "step": 623
    },
    {
      "epoch": 2.9158878504672896,
      "grad_norm": 0.3575300872325897,
      "learning_rate": 0.0002361370716510903,
      "loss": 0.2009,
      "step": 624
    },
    {
      "epoch": 2.9205607476635516,
      "grad_norm": 0.34107089042663574,
      "learning_rate": 0.00023598130841121493,
      "loss": 0.2055,
      "step": 625
    },
    {
      "epoch": 2.925233644859813,
      "grad_norm": 0.3644324839115143,
      "learning_rate": 0.00023582554517133955,
      "loss": 0.2037,
      "step": 626
    },
    {
      "epoch": 2.9299065420560746,
      "grad_norm": 0.45130106806755066,
      "learning_rate": 0.00023566978193146414,
      "loss": 0.2489,
      "step": 627
    },
    {
      "epoch": 2.9345794392523366,
      "grad_norm": 0.4294106066226959,
      "learning_rate": 0.00023551401869158876,
      "loss": 0.1893,
      "step": 628
    },
    {
      "epoch": 2.939252336448598,
      "grad_norm": 0.3887840509414673,
      "learning_rate": 0.00023535825545171338,
      "loss": 0.1717,
      "step": 629
    },
    {
      "epoch": 2.94392523364486,
      "grad_norm": 0.40433362126350403,
      "learning_rate": 0.000235202492211838,
      "loss": 0.2119,
      "step": 630
    },
    {
      "epoch": 2.9485981308411215,
      "grad_norm": 0.36495983600616455,
      "learning_rate": 0.0002350467289719626,
      "loss": 0.2311,
      "step": 631
    },
    {
      "epoch": 2.953271028037383,
      "grad_norm": 0.33857932686805725,
      "learning_rate": 0.00023489096573208723,
      "loss": 0.1899,
      "step": 632
    },
    {
      "epoch": 2.957943925233645,
      "grad_norm": 0.39512649178504944,
      "learning_rate": 0.0002347352024922118,
      "loss": 0.2104,
      "step": 633
    },
    {
      "epoch": 2.9626168224299065,
      "grad_norm": 0.37362104654312134,
      "learning_rate": 0.0002345794392523364,
      "loss": 0.2235,
      "step": 634
    },
    {
      "epoch": 2.9672897196261685,
      "grad_norm": 0.3572041988372803,
      "learning_rate": 0.00023442367601246103,
      "loss": 0.1936,
      "step": 635
    },
    {
      "epoch": 2.97196261682243,
      "grad_norm": 0.36547738313674927,
      "learning_rate": 0.00023426791277258565,
      "loss": 0.2003,
      "step": 636
    },
    {
      "epoch": 2.9766355140186915,
      "grad_norm": 0.2625688910484314,
      "learning_rate": 0.00023411214953271027,
      "loss": 0.1878,
      "step": 637
    },
    {
      "epoch": 2.981308411214953,
      "grad_norm": 0.3597560524940491,
      "learning_rate": 0.00023395638629283489,
      "loss": 0.2079,
      "step": 638
    },
    {
      "epoch": 2.985981308411215,
      "grad_norm": 0.2710864245891571,
      "learning_rate": 0.00023380062305295948,
      "loss": 0.1257,
      "step": 639
    },
    {
      "epoch": 2.9906542056074765,
      "grad_norm": 0.4278910756111145,
      "learning_rate": 0.0002336448598130841,
      "loss": 0.2291,
      "step": 640
    },
    {
      "epoch": 2.9953271028037385,
      "grad_norm": 0.4489861726760864,
      "learning_rate": 0.0002334890965732087,
      "loss": 0.1884,
      "step": 641
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.9259746074676514,
      "learning_rate": 0.0002333333333333333,
      "loss": 0.2919,
      "step": 642
    },
    {
      "epoch": 3.0,
      "eval_loss": 0.2605590224266052,
      "eval_runtime": 53.6146,
      "eval_samples_per_second": 10.296,
      "eval_steps_per_second": 1.287,
      "step": 642
    },
    {
      "epoch": 3.0046728971962615,
      "grad_norm": 0.282850980758667,
      "learning_rate": 0.00023317757009345792,
      "loss": 0.1869,
      "step": 643
    },
    {
      "epoch": 3.0093457943925235,
      "grad_norm": 0.3389245569705963,
      "learning_rate": 0.00023302180685358254,
      "loss": 0.2007,
      "step": 644
    },
    {
      "epoch": 3.014018691588785,
      "grad_norm": 0.31299734115600586,
      "learning_rate": 0.00023286604361370713,
      "loss": 0.1686,
      "step": 645
    },
    {
      "epoch": 3.0186915887850465,
      "grad_norm": 0.2966921031475067,
      "learning_rate": 0.00023271028037383175,
      "loss": 0.1584,
      "step": 646
    },
    {
      "epoch": 3.0233644859813085,
      "grad_norm": 0.3834257125854492,
      "learning_rate": 0.00023255451713395637,
      "loss": 0.1914,
      "step": 647
    },
    {
      "epoch": 3.02803738317757,
      "grad_norm": 0.4180598556995392,
      "learning_rate": 0.00023239875389408099,
      "loss": 0.1857,
      "step": 648
    },
    {
      "epoch": 3.032710280373832,
      "grad_norm": 0.32902461290359497,
      "learning_rate": 0.0002322429906542056,
      "loss": 0.1707,
      "step": 649
    },
    {
      "epoch": 3.0373831775700935,
      "grad_norm": 0.31696203351020813,
      "learning_rate": 0.00023208722741433022,
      "loss": 0.162,
      "step": 650
    },
    {
      "epoch": 3.042056074766355,
      "grad_norm": 0.3945046663284302,
      "learning_rate": 0.00023193146417445479,
      "loss": 0.19,
      "step": 651
    },
    {
      "epoch": 3.046728971962617,
      "grad_norm": 0.4688479006290436,
      "learning_rate": 0.0002317757009345794,
      "loss": 0.2157,
      "step": 652
    },
    {
      "epoch": 3.0514018691588785,
      "grad_norm": 0.6584779024124146,
      "learning_rate": 0.00023161993769470402,
      "loss": 0.1914,
      "step": 653
    },
    {
      "epoch": 3.05607476635514,
      "grad_norm": 0.37987810373306274,
      "learning_rate": 0.00023146417445482864,
      "loss": 0.158,
      "step": 654
    },
    {
      "epoch": 3.060747663551402,
      "grad_norm": 0.3942280113697052,
      "learning_rate": 0.00023130841121495326,
      "loss": 0.1901,
      "step": 655
    },
    {
      "epoch": 3.0654205607476634,
      "grad_norm": 0.45164570212364197,
      "learning_rate": 0.00023115264797507788,
      "loss": 0.1961,
      "step": 656
    },
    {
      "epoch": 3.0700934579439254,
      "grad_norm": 0.4462888836860657,
      "learning_rate": 0.00023099688473520247,
      "loss": 0.1996,
      "step": 657
    },
    {
      "epoch": 3.074766355140187,
      "grad_norm": 0.3487391471862793,
      "learning_rate": 0.00023084112149532709,
      "loss": 0.1601,
      "step": 658
    },
    {
      "epoch": 3.0794392523364484,
      "grad_norm": 0.34169816970825195,
      "learning_rate": 0.0002306853582554517,
      "loss": 0.1561,
      "step": 659
    },
    {
      "epoch": 3.0841121495327104,
      "grad_norm": 0.4418788254261017,
      "learning_rate": 0.0002305295950155763,
      "loss": 0.2126,
      "step": 660
    },
    {
      "epoch": 3.088785046728972,
      "grad_norm": 0.31313809752464294,
      "learning_rate": 0.0002303738317757009,
      "loss": 0.1559,
      "step": 661
    },
    {
      "epoch": 3.0934579439252334,
      "grad_norm": 0.4222319424152374,
      "learning_rate": 0.00023021806853582553,
      "loss": 0.205,
      "step": 662
    },
    {
      "epoch": 3.0981308411214954,
      "grad_norm": 0.39460331201553345,
      "learning_rate": 0.00023006230529595012,
      "loss": 0.1902,
      "step": 663
    },
    {
      "epoch": 3.102803738317757,
      "grad_norm": 0.32761478424072266,
      "learning_rate": 0.00022990654205607474,
      "loss": 0.158,
      "step": 664
    },
    {
      "epoch": 3.107476635514019,
      "grad_norm": 0.4303731918334961,
      "learning_rate": 0.00022975077881619936,
      "loss": 0.1881,
      "step": 665
    },
    {
      "epoch": 3.1121495327102804,
      "grad_norm": 0.36430224776268005,
      "learning_rate": 0.00022959501557632398,
      "loss": 0.1717,
      "step": 666
    },
    {
      "epoch": 3.116822429906542,
      "grad_norm": 0.43767398595809937,
      "learning_rate": 0.0002294392523364486,
      "loss": 0.2315,
      "step": 667
    },
    {
      "epoch": 3.121495327102804,
      "grad_norm": 0.3051854372024536,
      "learning_rate": 0.0002292834890965732,
      "loss": 0.1372,
      "step": 668
    },
    {
      "epoch": 3.1261682242990654,
      "grad_norm": 0.3692663013935089,
      "learning_rate": 0.00022912772585669778,
      "loss": 0.1785,
      "step": 669
    },
    {
      "epoch": 3.130841121495327,
      "grad_norm": 0.3385402262210846,
      "learning_rate": 0.0002289719626168224,
      "loss": 0.186,
      "step": 670
    },
    {
      "epoch": 3.135514018691589,
      "grad_norm": 0.38115403056144714,
      "learning_rate": 0.000228816199376947,
      "loss": 0.2291,
      "step": 671
    },
    {
      "epoch": 3.1401869158878504,
      "grad_norm": 0.34238842129707336,
      "learning_rate": 0.00022866043613707163,
      "loss": 0.1616,
      "step": 672
    },
    {
      "epoch": 3.1448598130841123,
      "grad_norm": 0.32167282700538635,
      "learning_rate": 0.00022850467289719625,
      "loss": 0.1497,
      "step": 673
    },
    {
      "epoch": 3.149532710280374,
      "grad_norm": 0.3440501391887665,
      "learning_rate": 0.00022834890965732087,
      "loss": 0.1549,
      "step": 674
    },
    {
      "epoch": 3.1542056074766354,
      "grad_norm": 0.3309871256351471,
      "learning_rate": 0.00022819314641744546,
      "loss": 0.1628,
      "step": 675
    },
    {
      "epoch": 3.1588785046728973,
      "grad_norm": 0.4510149657726288,
      "learning_rate": 0.00022803738317757008,
      "loss": 0.2183,
      "step": 676
    },
    {
      "epoch": 3.163551401869159,
      "grad_norm": 0.3204292058944702,
      "learning_rate": 0.0002278816199376947,
      "loss": 0.1638,
      "step": 677
    },
    {
      "epoch": 3.1682242990654204,
      "grad_norm": 0.38698098063468933,
      "learning_rate": 0.00022772585669781929,
      "loss": 0.2109,
      "step": 678
    },
    {
      "epoch": 3.1728971962616823,
      "grad_norm": 0.4064953029155731,
      "learning_rate": 0.0002275700934579439,
      "loss": 0.2061,
      "step": 679
    },
    {
      "epoch": 3.177570093457944,
      "grad_norm": 0.3282099664211273,
      "learning_rate": 0.00022741433021806852,
      "loss": 0.1939,
      "step": 680
    },
    {
      "epoch": 3.182242990654206,
      "grad_norm": 0.3466319143772125,
      "learning_rate": 0.0002272585669781931,
      "loss": 0.1773,
      "step": 681
    },
    {
      "epoch": 3.1869158878504673,
      "grad_norm": 0.315966933965683,
      "learning_rate": 0.00022710280373831773,
      "loss": 0.1499,
      "step": 682
    },
    {
      "epoch": 3.191588785046729,
      "grad_norm": 0.3472583293914795,
      "learning_rate": 0.00022694704049844235,
      "loss": 0.1689,
      "step": 683
    },
    {
      "epoch": 3.196261682242991,
      "grad_norm": 0.3477707505226135,
      "learning_rate": 0.00022679127725856697,
      "loss": 0.1917,
      "step": 684
    },
    {
      "epoch": 3.2009345794392523,
      "grad_norm": 0.3892492949962616,
      "learning_rate": 0.00022663551401869158,
      "loss": 0.1742,
      "step": 685
    },
    {
      "epoch": 3.205607476635514,
      "grad_norm": 0.30523064732551575,
      "learning_rate": 0.0002264797507788162,
      "loss": 0.1345,
      "step": 686
    },
    {
      "epoch": 3.210280373831776,
      "grad_norm": 0.405577152967453,
      "learning_rate": 0.00022632398753894077,
      "loss": 0.1964,
      "step": 687
    },
    {
      "epoch": 3.2149532710280373,
      "grad_norm": 0.35258013010025024,
      "learning_rate": 0.00022616822429906539,
      "loss": 0.1585,
      "step": 688
    },
    {
      "epoch": 3.2196261682242993,
      "grad_norm": 0.3575587272644043,
      "learning_rate": 0.00022601246105919,
      "loss": 0.1608,
      "step": 689
    },
    {
      "epoch": 3.2242990654205608,
      "grad_norm": 0.3528509736061096,
      "learning_rate": 0.00022585669781931462,
      "loss": 0.2151,
      "step": 690
    },
    {
      "epoch": 3.2289719626168223,
      "grad_norm": 0.37046048045158386,
      "learning_rate": 0.00022570093457943924,
      "loss": 0.2062,
      "step": 691
    },
    {
      "epoch": 3.2336448598130842,
      "grad_norm": 0.42139530181884766,
      "learning_rate": 0.00022554517133956386,
      "loss": 0.1811,
      "step": 692
    },
    {
      "epoch": 3.2383177570093458,
      "grad_norm": 0.3655394911766052,
      "learning_rate": 0.00022538940809968845,
      "loss": 0.2125,
      "step": 693
    },
    {
      "epoch": 3.2429906542056073,
      "grad_norm": 0.2951563596725464,
      "learning_rate": 0.00022523364485981307,
      "loss": 0.1701,
      "step": 694
    },
    {
      "epoch": 3.2476635514018692,
      "grad_norm": 0.3229837119579315,
      "learning_rate": 0.00022507788161993768,
      "loss": 0.2052,
      "step": 695
    },
    {
      "epoch": 3.2523364485981308,
      "grad_norm": 0.3430992662906647,
      "learning_rate": 0.00022492211838006228,
      "loss": 0.1721,
      "step": 696
    },
    {
      "epoch": 3.2570093457943923,
      "grad_norm": 0.3262476921081543,
      "learning_rate": 0.0002247663551401869,
      "loss": 0.1849,
      "step": 697
    },
    {
      "epoch": 3.2616822429906542,
      "grad_norm": 0.4323742389678955,
      "learning_rate": 0.0002246105919003115,
      "loss": 0.2033,
      "step": 698
    },
    {
      "epoch": 3.2663551401869158,
      "grad_norm": 0.36065495014190674,
      "learning_rate": 0.0002244548286604361,
      "loss": 0.1755,
      "step": 699
    },
    {
      "epoch": 3.2710280373831777,
      "grad_norm": 0.36936476826667786,
      "learning_rate": 0.00022429906542056072,
      "loss": 0.1989,
      "step": 700
    },
    {
      "epoch": 3.2757009345794392,
      "grad_norm": 0.3122652471065521,
      "learning_rate": 0.00022414330218068534,
      "loss": 0.1752,
      "step": 701
    },
    {
      "epoch": 3.2803738317757007,
      "grad_norm": 0.3346720039844513,
      "learning_rate": 0.00022398753894080996,
      "loss": 0.1646,
      "step": 702
    },
    {
      "epoch": 3.2850467289719627,
      "grad_norm": 0.47559869289398193,
      "learning_rate": 0.00022383177570093458,
      "loss": 0.2116,
      "step": 703
    },
    {
      "epoch": 3.289719626168224,
      "grad_norm": 0.33474671840667725,
      "learning_rate": 0.0002236760124610592,
      "loss": 0.1737,
      "step": 704
    },
    {
      "epoch": 3.294392523364486,
      "grad_norm": 0.32320043444633484,
      "learning_rate": 0.00022352024922118376,
      "loss": 0.1679,
      "step": 705
    },
    {
      "epoch": 3.2990654205607477,
      "grad_norm": 0.4519348442554474,
      "learning_rate": 0.00022336448598130838,
      "loss": 0.2168,
      "step": 706
    },
    {
      "epoch": 3.303738317757009,
      "grad_norm": 0.33773332834243774,
      "learning_rate": 0.000223208722741433,
      "loss": 0.166,
      "step": 707
    },
    {
      "epoch": 3.308411214953271,
      "grad_norm": 0.38274967670440674,
      "learning_rate": 0.0002230529595015576,
      "loss": 0.2277,
      "step": 708
    },
    {
      "epoch": 3.3130841121495327,
      "grad_norm": 0.39729568362236023,
      "learning_rate": 0.00022289719626168223,
      "loss": 0.2034,
      "step": 709
    },
    {
      "epoch": 3.317757009345794,
      "grad_norm": 0.38252192735671997,
      "learning_rate": 0.00022274143302180685,
      "loss": 0.1904,
      "step": 710
    },
    {
      "epoch": 3.322429906542056,
      "grad_norm": 0.42210572957992554,
      "learning_rate": 0.00022258566978193144,
      "loss": 0.1981,
      "step": 711
    },
    {
      "epoch": 3.3271028037383177,
      "grad_norm": 0.33118030428886414,
      "learning_rate": 0.00022242990654205606,
      "loss": 0.1656,
      "step": 712
    },
    {
      "epoch": 3.331775700934579,
      "grad_norm": 0.36424675583839417,
      "learning_rate": 0.00022227414330218068,
      "loss": 0.1925,
      "step": 713
    },
    {
      "epoch": 3.336448598130841,
      "grad_norm": 0.5048614740371704,
      "learning_rate": 0.00022211838006230527,
      "loss": 0.1953,
      "step": 714
    },
    {
      "epoch": 3.3411214953271027,
      "grad_norm": 0.40364643931388855,
      "learning_rate": 0.00022196261682242988,
      "loss": 0.1846,
      "step": 715
    },
    {
      "epoch": 3.3457943925233646,
      "grad_norm": 0.30982106924057007,
      "learning_rate": 0.0002218068535825545,
      "loss": 0.1263,
      "step": 716
    },
    {
      "epoch": 3.350467289719626,
      "grad_norm": 0.3953969478607178,
      "learning_rate": 0.0002216510903426791,
      "loss": 0.1524,
      "step": 717
    },
    {
      "epoch": 3.3551401869158877,
      "grad_norm": 0.26265016198158264,
      "learning_rate": 0.0002214953271028037,
      "loss": 0.1456,
      "step": 718
    },
    {
      "epoch": 3.3598130841121496,
      "grad_norm": 0.35032302141189575,
      "learning_rate": 0.00022133956386292833,
      "loss": 0.1538,
      "step": 719
    },
    {
      "epoch": 3.364485981308411,
      "grad_norm": 0.35639116168022156,
      "learning_rate": 0.00022118380062305295,
      "loss": 0.1874,
      "step": 720
    },
    {
      "epoch": 3.369158878504673,
      "grad_norm": 0.28465238213539124,
      "learning_rate": 0.00022102803738317757,
      "loss": 0.1545,
      "step": 721
    },
    {
      "epoch": 3.3738317757009346,
      "grad_norm": 0.31795430183410645,
      "learning_rate": 0.00022087227414330218,
      "loss": 0.1664,
      "step": 722
    },
    {
      "epoch": 3.378504672897196,
      "grad_norm": 0.3779538869857788,
      "learning_rate": 0.00022071651090342675,
      "loss": 0.1743,
      "step": 723
    },
    {
      "epoch": 3.383177570093458,
      "grad_norm": 0.3157060146331787,
      "learning_rate": 0.00022056074766355137,
      "loss": 0.137,
      "step": 724
    },
    {
      "epoch": 3.3878504672897196,
      "grad_norm": 0.3200058043003082,
      "learning_rate": 0.00022040498442367598,
      "loss": 0.158,
      "step": 725
    },
    {
      "epoch": 3.392523364485981,
      "grad_norm": 0.3970724642276764,
      "learning_rate": 0.0002202492211838006,
      "loss": 0.1624,
      "step": 726
    },
    {
      "epoch": 3.397196261682243,
      "grad_norm": 0.38098567724227905,
      "learning_rate": 0.00022009345794392522,
      "loss": 0.1889,
      "step": 727
    },
    {
      "epoch": 3.4018691588785046,
      "grad_norm": 0.4278014302253723,
      "learning_rate": 0.00021993769470404984,
      "loss": 0.1998,
      "step": 728
    },
    {
      "epoch": 3.406542056074766,
      "grad_norm": 0.44658246636390686,
      "learning_rate": 0.00021978193146417443,
      "loss": 0.205,
      "step": 729
    },
    {
      "epoch": 3.411214953271028,
      "grad_norm": 0.4098937511444092,
      "learning_rate": 0.00021962616822429905,
      "loss": 0.1838,
      "step": 730
    },
    {
      "epoch": 3.4158878504672896,
      "grad_norm": 0.480593204498291,
      "learning_rate": 0.00021947040498442367,
      "loss": 0.2126,
      "step": 731
    },
    {
      "epoch": 3.4205607476635516,
      "grad_norm": 0.30447039008140564,
      "learning_rate": 0.00021931464174454826,
      "loss": 0.1327,
      "step": 732
    },
    {
      "epoch": 3.425233644859813,
      "grad_norm": 0.3417087197303772,
      "learning_rate": 0.00021915887850467288,
      "loss": 0.1561,
      "step": 733
    },
    {
      "epoch": 3.4299065420560746,
      "grad_norm": 0.387686550617218,
      "learning_rate": 0.0002190031152647975,
      "loss": 0.1956,
      "step": 734
    },
    {
      "epoch": 3.4345794392523366,
      "grad_norm": 0.3840639889240265,
      "learning_rate": 0.00021884735202492208,
      "loss": 0.1708,
      "step": 735
    },
    {
      "epoch": 3.439252336448598,
      "grad_norm": 0.41180896759033203,
      "learning_rate": 0.0002186915887850467,
      "loss": 0.1792,
      "step": 736
    },
    {
      "epoch": 3.44392523364486,
      "grad_norm": 0.4981573820114136,
      "learning_rate": 0.00021853582554517132,
      "loss": 0.185,
      "step": 737
    },
    {
      "epoch": 3.4485981308411215,
      "grad_norm": 0.3597899079322815,
      "learning_rate": 0.00021838006230529594,
      "loss": 0.1722,
      "step": 738
    },
    {
      "epoch": 3.453271028037383,
      "grad_norm": 0.40692138671875,
      "learning_rate": 0.00021822429906542056,
      "loss": 0.2114,
      "step": 739
    },
    {
      "epoch": 3.457943925233645,
      "grad_norm": 0.2586742341518402,
      "learning_rate": 0.00021806853582554517,
      "loss": 0.144,
      "step": 740
    },
    {
      "epoch": 3.4626168224299065,
      "grad_norm": 0.4116852879524231,
      "learning_rate": 0.00021791277258566974,
      "loss": 0.2157,
      "step": 741
    },
    {
      "epoch": 3.467289719626168,
      "grad_norm": 0.42505335807800293,
      "learning_rate": 0.00021775700934579436,
      "loss": 0.2418,
      "step": 742
    },
    {
      "epoch": 3.47196261682243,
      "grad_norm": 0.3168327212333679,
      "learning_rate": 0.00021760124610591898,
      "loss": 0.1782,
      "step": 743
    },
    {
      "epoch": 3.4766355140186915,
      "grad_norm": 0.3328588306903839,
      "learning_rate": 0.0002174454828660436,
      "loss": 0.1911,
      "step": 744
    },
    {
      "epoch": 3.481308411214953,
      "grad_norm": 0.302956223487854,
      "learning_rate": 0.0002172897196261682,
      "loss": 0.1604,
      "step": 745
    },
    {
      "epoch": 3.485981308411215,
      "grad_norm": 0.3242064416408539,
      "learning_rate": 0.00021713395638629283,
      "loss": 0.1847,
      "step": 746
    },
    {
      "epoch": 3.4906542056074765,
      "grad_norm": 0.39044541120529175,
      "learning_rate": 0.00021697819314641742,
      "loss": 0.1936,
      "step": 747
    },
    {
      "epoch": 3.4953271028037385,
      "grad_norm": 0.31760138273239136,
      "learning_rate": 0.00021682242990654204,
      "loss": 0.1495,
      "step": 748
    },
    {
      "epoch": 3.5,
      "grad_norm": 0.44463926553726196,
      "learning_rate": 0.00021666666666666666,
      "loss": 0.2073,
      "step": 749
    },
    {
      "epoch": 3.5046728971962615,
      "grad_norm": 0.424992173910141,
      "learning_rate": 0.00021651090342679125,
      "loss": 0.1672,
      "step": 750
    },
    {
      "epoch": 3.5093457943925235,
      "grad_norm": 0.45945191383361816,
      "learning_rate": 0.00021635514018691587,
      "loss": 0.1819,
      "step": 751
    },
    {
      "epoch": 3.514018691588785,
      "grad_norm": 0.321653813123703,
      "learning_rate": 0.00021619937694704048,
      "loss": 0.139,
      "step": 752
    },
    {
      "epoch": 3.518691588785047,
      "grad_norm": 0.31055504083633423,
      "learning_rate": 0.00021604361370716507,
      "loss": 0.1664,
      "step": 753
    },
    {
      "epoch": 3.5233644859813085,
      "grad_norm": 0.49269866943359375,
      "learning_rate": 0.0002158878504672897,
      "loss": 0.1979,
      "step": 754
    },
    {
      "epoch": 3.52803738317757,
      "grad_norm": 0.40640076994895935,
      "learning_rate": 0.0002157320872274143,
      "loss": 0.1724,
      "step": 755
    },
    {
      "epoch": 3.5327102803738315,
      "grad_norm": 0.3599317669868469,
      "learning_rate": 0.00021557632398753893,
      "loss": 0.1572,
      "step": 756
    },
    {
      "epoch": 3.5373831775700935,
      "grad_norm": 0.458209753036499,
      "learning_rate": 0.00021542056074766355,
      "loss": 0.1394,
      "step": 757
    },
    {
      "epoch": 3.542056074766355,
      "grad_norm": 0.3651861846446991,
      "learning_rate": 0.00021526479750778817,
      "loss": 0.1791,
      "step": 758
    },
    {
      "epoch": 3.546728971962617,
      "grad_norm": 0.35594096779823303,
      "learning_rate": 0.00021510903426791273,
      "loss": 0.1744,
      "step": 759
    },
    {
      "epoch": 3.5514018691588785,
      "grad_norm": 0.34880760312080383,
      "learning_rate": 0.00021495327102803735,
      "loss": 0.171,
      "step": 760
    },
    {
      "epoch": 3.55607476635514,
      "grad_norm": 0.3107152581214905,
      "learning_rate": 0.00021479750778816197,
      "loss": 0.1324,
      "step": 761
    },
    {
      "epoch": 3.560747663551402,
      "grad_norm": 0.4565580189228058,
      "learning_rate": 0.00021464174454828658,
      "loss": 0.1958,
      "step": 762
    },
    {
      "epoch": 3.5654205607476634,
      "grad_norm": 0.3850518465042114,
      "learning_rate": 0.0002144859813084112,
      "loss": 0.1575,
      "step": 763
    },
    {
      "epoch": 3.5700934579439254,
      "grad_norm": 0.4449690580368042,
      "learning_rate": 0.00021433021806853582,
      "loss": 0.1816,
      "step": 764
    },
    {
      "epoch": 3.574766355140187,
      "grad_norm": 0.41196519136428833,
      "learning_rate": 0.0002141744548286604,
      "loss": 0.1755,
      "step": 765
    },
    {
      "epoch": 3.5794392523364484,
      "grad_norm": 0.41677623987197876,
      "learning_rate": 0.00021401869158878503,
      "loss": 0.2028,
      "step": 766
    },
    {
      "epoch": 3.5841121495327104,
      "grad_norm": 0.42949432134628296,
      "learning_rate": 0.00021386292834890965,
      "loss": 0.2241,
      "step": 767
    },
    {
      "epoch": 3.588785046728972,
      "grad_norm": 0.36219021677970886,
      "learning_rate": 0.00021370716510903424,
      "loss": 0.1536,
      "step": 768
    },
    {
      "epoch": 3.593457943925234,
      "grad_norm": 0.3046724498271942,
      "learning_rate": 0.00021355140186915886,
      "loss": 0.1875,
      "step": 769
    },
    {
      "epoch": 3.5981308411214954,
      "grad_norm": 0.358603835105896,
      "learning_rate": 0.00021339563862928347,
      "loss": 0.1743,
      "step": 770
    },
    {
      "epoch": 3.602803738317757,
      "grad_norm": 0.3370297849178314,
      "learning_rate": 0.00021323987538940807,
      "loss": 0.1571,
      "step": 771
    },
    {
      "epoch": 3.6074766355140184,
      "grad_norm": 0.2905347943305969,
      "learning_rate": 0.00021308411214953268,
      "loss": 0.1633,
      "step": 772
    },
    {
      "epoch": 3.6121495327102804,
      "grad_norm": 0.42640385031700134,
      "learning_rate": 0.0002129283489096573,
      "loss": 0.2057,
      "step": 773
    },
    {
      "epoch": 3.616822429906542,
      "grad_norm": 0.33640149235725403,
      "learning_rate": 0.00021277258566978192,
      "loss": 0.1432,
      "step": 774
    },
    {
      "epoch": 3.621495327102804,
      "grad_norm": 0.3367312550544739,
      "learning_rate": 0.00021261682242990654,
      "loss": 0.1949,
      "step": 775
    },
    {
      "epoch": 3.6261682242990654,
      "grad_norm": 0.3477689027786255,
      "learning_rate": 0.00021246105919003116,
      "loss": 0.1727,
      "step": 776
    },
    {
      "epoch": 3.630841121495327,
      "grad_norm": 0.44364771246910095,
      "learning_rate": 0.00021230529595015572,
      "loss": 0.1902,
      "step": 777
    },
    {
      "epoch": 3.635514018691589,
      "grad_norm": 0.3780875504016876,
      "learning_rate": 0.00021214953271028034,
      "loss": 0.1641,
      "step": 778
    },
    {
      "epoch": 3.6401869158878504,
      "grad_norm": 0.3351004719734192,
      "learning_rate": 0.00021199376947040496,
      "loss": 0.1712,
      "step": 779
    },
    {
      "epoch": 3.6448598130841123,
      "grad_norm": 0.41376519203186035,
      "learning_rate": 0.00021183800623052957,
      "loss": 0.1884,
      "step": 780
    },
    {
      "epoch": 3.649532710280374,
      "grad_norm": 0.4685673713684082,
      "learning_rate": 0.0002116822429906542,
      "loss": 0.1832,
      "step": 781
    },
    {
      "epoch": 3.6542056074766354,
      "grad_norm": 0.31954851746559143,
      "learning_rate": 0.0002115264797507788,
      "loss": 0.1601,
      "step": 782
    },
    {
      "epoch": 3.6588785046728973,
      "grad_norm": 0.4388374984264374,
      "learning_rate": 0.0002113707165109034,
      "loss": 0.1818,
      "step": 783
    },
    {
      "epoch": 3.663551401869159,
      "grad_norm": 0.4135851263999939,
      "learning_rate": 0.00021121495327102802,
      "loss": 0.2213,
      "step": 784
    },
    {
      "epoch": 3.668224299065421,
      "grad_norm": 0.3515441417694092,
      "learning_rate": 0.00021105919003115264,
      "loss": 0.1867,
      "step": 785
    },
    {
      "epoch": 3.6728971962616823,
      "grad_norm": 0.33308520913124084,
      "learning_rate": 0.00021090342679127723,
      "loss": 0.1846,
      "step": 786
    },
    {
      "epoch": 3.677570093457944,
      "grad_norm": 0.36877909302711487,
      "learning_rate": 0.00021074766355140185,
      "loss": 0.1847,
      "step": 787
    },
    {
      "epoch": 3.6822429906542054,
      "grad_norm": 0.3229413330554962,
      "learning_rate": 0.00021059190031152647,
      "loss": 0.1871,
      "step": 788
    },
    {
      "epoch": 3.6869158878504673,
      "grad_norm": 0.3204028904438019,
      "learning_rate": 0.00021043613707165106,
      "loss": 0.1663,
      "step": 789
    },
    {
      "epoch": 3.691588785046729,
      "grad_norm": 0.4152712821960449,
      "learning_rate": 0.00021028037383177567,
      "loss": 0.1854,
      "step": 790
    },
    {
      "epoch": 3.696261682242991,
      "grad_norm": 0.32532066106796265,
      "learning_rate": 0.0002101246105919003,
      "loss": 0.2138,
      "step": 791
    },
    {
      "epoch": 3.7009345794392523,
      "grad_norm": 0.2990092933177948,
      "learning_rate": 0.0002099688473520249,
      "loss": 0.1746,
      "step": 792
    },
    {
      "epoch": 3.705607476635514,
      "grad_norm": 0.3711606562137604,
      "learning_rate": 0.00020981308411214953,
      "loss": 0.207,
      "step": 793
    },
    {
      "epoch": 3.710280373831776,
      "grad_norm": 0.2966114282608032,
      "learning_rate": 0.00020965732087227415,
      "loss": 0.1581,
      "step": 794
    },
    {
      "epoch": 3.7149532710280373,
      "grad_norm": 0.43656590580940247,
      "learning_rate": 0.0002095015576323987,
      "loss": 0.1839,
      "step": 795
    },
    {
      "epoch": 3.7196261682242993,
      "grad_norm": 0.48855337500572205,
      "learning_rate": 0.00020934579439252333,
      "loss": 0.245,
      "step": 796
    },
    {
      "epoch": 3.7242990654205608,
      "grad_norm": 0.38540175557136536,
      "learning_rate": 0.00020919003115264795,
      "loss": 0.1926,
      "step": 797
    },
    {
      "epoch": 3.7289719626168223,
      "grad_norm": 0.3093784749507904,
      "learning_rate": 0.00020903426791277257,
      "loss": 0.146,
      "step": 798
    },
    {
      "epoch": 3.7336448598130842,
      "grad_norm": 0.4242874085903168,
      "learning_rate": 0.00020887850467289718,
      "loss": 0.1607,
      "step": 799
    },
    {
      "epoch": 3.7383177570093458,
      "grad_norm": 0.32642892003059387,
      "learning_rate": 0.0002087227414330218,
      "loss": 0.1606,
      "step": 800
    },
    {
      "epoch": 3.7429906542056077,
      "grad_norm": 0.4151886999607086,
      "learning_rate": 0.0002085669781931464,
      "loss": 0.1969,
      "step": 801
    },
    {
      "epoch": 3.7476635514018692,
      "grad_norm": 0.5235384702682495,
      "learning_rate": 0.000208411214953271,
      "loss": 0.2024,
      "step": 802
    },
    {
      "epoch": 3.7523364485981308,
      "grad_norm": 0.46788737177848816,
      "learning_rate": 0.00020825545171339563,
      "loss": 0.2006,
      "step": 803
    },
    {
      "epoch": 3.7570093457943923,
      "grad_norm": 0.3332003653049469,
      "learning_rate": 0.00020809968847352022,
      "loss": 0.1854,
      "step": 804
    },
    {
      "epoch": 3.7616822429906542,
      "grad_norm": 0.29773879051208496,
      "learning_rate": 0.00020794392523364484,
      "loss": 0.1649,
      "step": 805
    },
    {
      "epoch": 3.7663551401869158,
      "grad_norm": 0.3131445348262787,
      "learning_rate": 0.00020778816199376946,
      "loss": 0.1526,
      "step": 806
    },
    {
      "epoch": 3.7710280373831777,
      "grad_norm": 0.34968677163124084,
      "learning_rate": 0.00020763239875389405,
      "loss": 0.1596,
      "step": 807
    },
    {
      "epoch": 3.7757009345794392,
      "grad_norm": 0.36934253573417664,
      "learning_rate": 0.00020747663551401867,
      "loss": 0.1817,
      "step": 808
    },
    {
      "epoch": 3.7803738317757007,
      "grad_norm": 0.38317805528640747,
      "learning_rate": 0.00020732087227414328,
      "loss": 0.1787,
      "step": 809
    },
    {
      "epoch": 3.7850467289719627,
      "grad_norm": 0.34369221329689026,
      "learning_rate": 0.0002071651090342679,
      "loss": 0.1727,
      "step": 810
    },
    {
      "epoch": 3.789719626168224,
      "grad_norm": 0.31090080738067627,
      "learning_rate": 0.00020700934579439252,
      "loss": 0.155,
      "step": 811
    },
    {
      "epoch": 3.794392523364486,
      "grad_norm": 0.31126660108566284,
      "learning_rate": 0.00020685358255451714,
      "loss": 0.1588,
      "step": 812
    },
    {
      "epoch": 3.7990654205607477,
      "grad_norm": 0.44528311491012573,
      "learning_rate": 0.0002066978193146417,
      "loss": 0.2051,
      "step": 813
    },
    {
      "epoch": 3.803738317757009,
      "grad_norm": 0.32972466945648193,
      "learning_rate": 0.00020654205607476632,
      "loss": 0.1682,
      "step": 814
    },
    {
      "epoch": 3.808411214953271,
      "grad_norm": 0.4222649931907654,
      "learning_rate": 0.00020638629283489094,
      "loss": 0.1998,
      "step": 815
    },
    {
      "epoch": 3.8130841121495327,
      "grad_norm": 0.44521620869636536,
      "learning_rate": 0.00020623052959501556,
      "loss": 0.2051,
      "step": 816
    },
    {
      "epoch": 3.8177570093457946,
      "grad_norm": 0.4309295117855072,
      "learning_rate": 0.00020607476635514017,
      "loss": 0.1835,
      "step": 817
    },
    {
      "epoch": 3.822429906542056,
      "grad_norm": 0.3112013041973114,
      "learning_rate": 0.0002059190031152648,
      "loss": 0.1482,
      "step": 818
    },
    {
      "epoch": 3.8271028037383177,
      "grad_norm": 0.38798895478248596,
      "learning_rate": 0.00020576323987538938,
      "loss": 0.1859,
      "step": 819
    },
    {
      "epoch": 3.831775700934579,
      "grad_norm": 0.3125342130661011,
      "learning_rate": 0.000205607476635514,
      "loss": 0.1509,
      "step": 820
    },
    {
      "epoch": 3.836448598130841,
      "grad_norm": 0.39797869324684143,
      "learning_rate": 0.00020545171339563862,
      "loss": 0.1856,
      "step": 821
    },
    {
      "epoch": 3.8411214953271027,
      "grad_norm": 0.378061980009079,
      "learning_rate": 0.0002052959501557632,
      "loss": 0.2073,
      "step": 822
    },
    {
      "epoch": 3.8457943925233646,
      "grad_norm": 0.3996480107307434,
      "learning_rate": 0.00020514018691588783,
      "loss": 0.1634,
      "step": 823
    },
    {
      "epoch": 3.850467289719626,
      "grad_norm": 0.33105072379112244,
      "learning_rate": 0.00020498442367601245,
      "loss": 0.1332,
      "step": 824
    },
    {
      "epoch": 3.8551401869158877,
      "grad_norm": 0.44665926694869995,
      "learning_rate": 0.00020482866043613704,
      "loss": 0.2071,
      "step": 825
    },
    {
      "epoch": 3.8598130841121496,
      "grad_norm": 0.3053632080554962,
      "learning_rate": 0.00020467289719626166,
      "loss": 0.1874,
      "step": 826
    },
    {
      "epoch": 3.864485981308411,
      "grad_norm": 0.3004792034626007,
      "learning_rate": 0.00020451713395638627,
      "loss": 0.1592,
      "step": 827
    },
    {
      "epoch": 3.869158878504673,
      "grad_norm": 0.3812083303928375,
      "learning_rate": 0.0002043613707165109,
      "loss": 0.1688,
      "step": 828
    },
    {
      "epoch": 3.8738317757009346,
      "grad_norm": 0.27141642570495605,
      "learning_rate": 0.0002042056074766355,
      "loss": 0.1518,
      "step": 829
    },
    {
      "epoch": 3.878504672897196,
      "grad_norm": 0.31242647767066956,
      "learning_rate": 0.00020404984423676013,
      "loss": 0.1661,
      "step": 830
    },
    {
      "epoch": 3.883177570093458,
      "grad_norm": 0.39815375208854675,
      "learning_rate": 0.0002038940809968847,
      "loss": 0.1993,
      "step": 831
    },
    {
      "epoch": 3.8878504672897196,
      "grad_norm": 0.2982756495475769,
      "learning_rate": 0.0002037383177570093,
      "loss": 0.1786,
      "step": 832
    },
    {
      "epoch": 3.8925233644859816,
      "grad_norm": 0.2876741588115692,
      "learning_rate": 0.00020358255451713393,
      "loss": 0.1489,
      "step": 833
    },
    {
      "epoch": 3.897196261682243,
      "grad_norm": 0.31847459077835083,
      "learning_rate": 0.00020342679127725855,
      "loss": 0.1942,
      "step": 834
    },
    {
      "epoch": 3.9018691588785046,
      "grad_norm": 0.3531244099140167,
      "learning_rate": 0.00020327102803738316,
      "loss": 0.1675,
      "step": 835
    },
    {
      "epoch": 3.906542056074766,
      "grad_norm": 0.532646894454956,
      "learning_rate": 0.00020311526479750778,
      "loss": 0.1747,
      "step": 836
    },
    {
      "epoch": 3.911214953271028,
      "grad_norm": 0.37494704127311707,
      "learning_rate": 0.00020295950155763237,
      "loss": 0.1623,
      "step": 837
    },
    {
      "epoch": 3.9158878504672896,
      "grad_norm": 0.2763575613498688,
      "learning_rate": 0.000202803738317757,
      "loss": 0.1393,
      "step": 838
    },
    {
      "epoch": 3.9205607476635516,
      "grad_norm": 0.3928026556968689,
      "learning_rate": 0.0002026479750778816,
      "loss": 0.1678,
      "step": 839
    },
    {
      "epoch": 3.925233644859813,
      "grad_norm": 0.3430684804916382,
      "learning_rate": 0.0002024922118380062,
      "loss": 0.1327,
      "step": 840
    },
    {
      "epoch": 3.9299065420560746,
      "grad_norm": 0.3564482033252716,
      "learning_rate": 0.00020233644859813082,
      "loss": 0.155,
      "step": 841
    },
    {
      "epoch": 3.9345794392523366,
      "grad_norm": 0.3207632005214691,
      "learning_rate": 0.00020218068535825544,
      "loss": 0.1609,
      "step": 842
    },
    {
      "epoch": 3.939252336448598,
      "grad_norm": 0.3627873659133911,
      "learning_rate": 0.00020202492211838003,
      "loss": 0.1713,
      "step": 843
    },
    {
      "epoch": 3.94392523364486,
      "grad_norm": 0.3256559371948242,
      "learning_rate": 0.00020186915887850465,
      "loss": 0.1494,
      "step": 844
    },
    {
      "epoch": 3.9485981308411215,
      "grad_norm": 0.49343425035476685,
      "learning_rate": 0.00020171339563862926,
      "loss": 0.1819,
      "step": 845
    },
    {
      "epoch": 3.953271028037383,
      "grad_norm": 0.34913933277130127,
      "learning_rate": 0.00020155763239875388,
      "loss": 0.1525,
      "step": 846
    },
    {
      "epoch": 3.957943925233645,
      "grad_norm": 0.4432277977466583,
      "learning_rate": 0.0002014018691588785,
      "loss": 0.1791,
      "step": 847
    },
    {
      "epoch": 3.9626168224299065,
      "grad_norm": 0.4195820689201355,
      "learning_rate": 0.00020124610591900312,
      "loss": 0.196,
      "step": 848
    },
    {
      "epoch": 3.9672897196261685,
      "grad_norm": 0.43731316924095154,
      "learning_rate": 0.00020109034267912768,
      "loss": 0.2091,
      "step": 849
    },
    {
      "epoch": 3.97196261682243,
      "grad_norm": 0.5249904990196228,
      "learning_rate": 0.0002009345794392523,
      "loss": 0.2062,
      "step": 850
    },
    {
      "epoch": 3.9766355140186915,
      "grad_norm": 0.308659166097641,
      "learning_rate": 0.00020077881619937692,
      "loss": 0.1423,
      "step": 851
    },
    {
      "epoch": 3.981308411214953,
      "grad_norm": 0.4168187081813812,
      "learning_rate": 0.00020062305295950154,
      "loss": 0.2174,
      "step": 852
    },
    {
      "epoch": 3.985981308411215,
      "grad_norm": 0.32331383228302,
      "learning_rate": 0.00020046728971962616,
      "loss": 0.1555,
      "step": 853
    },
    {
      "epoch": 3.9906542056074765,
      "grad_norm": 0.3590245842933655,
      "learning_rate": 0.00020031152647975077,
      "loss": 0.1434,
      "step": 854
    },
    {
      "epoch": 3.9953271028037385,
      "grad_norm": 0.3636554181575775,
      "learning_rate": 0.00020015576323987536,
      "loss": 0.1499,
      "step": 855
    },
    {
      "epoch": 4.0,
      "grad_norm": 1.2763363122940063,
      "learning_rate": 0.00019999999999999998,
      "loss": 0.1884,
      "step": 856
    },
    {
      "epoch": 4.0,
      "eval_loss": 0.24838538467884064,
      "eval_runtime": 52.9661,
      "eval_samples_per_second": 10.422,
      "eval_steps_per_second": 1.303,
      "step": 856
    },
    {
      "epoch": 4.0046728971962615,
      "grad_norm": 0.34739506244659424,
      "learning_rate": 0.0001998442367601246,
      "loss": 0.2053,
      "step": 857
    },
    {
      "epoch": 4.009345794392523,
      "grad_norm": 0.2730407416820526,
      "learning_rate": 0.0001996884735202492,
      "loss": 0.142,
      "step": 858
    },
    {
      "epoch": 4.014018691588785,
      "grad_norm": 0.30525895953178406,
      "learning_rate": 0.0001995327102803738,
      "loss": 0.1426,
      "step": 859
    },
    {
      "epoch": 4.018691588785047,
      "grad_norm": 0.33819499611854553,
      "learning_rate": 0.00019937694704049843,
      "loss": 0.1837,
      "step": 860
    },
    {
      "epoch": 4.0233644859813085,
      "grad_norm": 0.369003564119339,
      "learning_rate": 0.00019922118380062305,
      "loss": 0.2244,
      "step": 861
    },
    {
      "epoch": 4.02803738317757,
      "grad_norm": 0.37331971526145935,
      "learning_rate": 0.00019906542056074764,
      "loss": 0.1766,
      "step": 862
    },
    {
      "epoch": 4.0327102803738315,
      "grad_norm": 0.32495737075805664,
      "learning_rate": 0.00019890965732087226,
      "loss": 0.1699,
      "step": 863
    },
    {
      "epoch": 4.037383177570093,
      "grad_norm": 0.32966145873069763,
      "learning_rate": 0.00019875389408099687,
      "loss": 0.1482,
      "step": 864
    },
    {
      "epoch": 4.042056074766355,
      "grad_norm": 0.42269086837768555,
      "learning_rate": 0.0001985981308411215,
      "loss": 0.1584,
      "step": 865
    },
    {
      "epoch": 4.046728971962617,
      "grad_norm": 0.35768967866897583,
      "learning_rate": 0.0001984423676012461,
      "loss": 0.1536,
      "step": 866
    },
    {
      "epoch": 4.0514018691588785,
      "grad_norm": 0.3696902096271515,
      "learning_rate": 0.00019828660436137073,
      "loss": 0.1626,
      "step": 867
    },
    {
      "epoch": 4.05607476635514,
      "grad_norm": 0.41267675161361694,
      "learning_rate": 0.0001981308411214953,
      "loss": 0.1631,
      "step": 868
    },
    {
      "epoch": 4.0607476635514015,
      "grad_norm": 0.4474998116493225,
      "learning_rate": 0.0001979750778816199,
      "loss": 0.1737,
      "step": 869
    },
    {
      "epoch": 4.065420560747664,
      "grad_norm": 0.40339919924736023,
      "learning_rate": 0.00019781931464174453,
      "loss": 0.1356,
      "step": 870
    },
    {
      "epoch": 4.070093457943925,
      "grad_norm": 0.5019205212593079,
      "learning_rate": 0.00019766355140186915,
      "loss": 0.1488,
      "step": 871
    },
    {
      "epoch": 4.074766355140187,
      "grad_norm": 0.4135969579219818,
      "learning_rate": 0.00019750778816199376,
      "loss": 0.1719,
      "step": 872
    },
    {
      "epoch": 4.079439252336448,
      "grad_norm": 0.4622306525707245,
      "learning_rate": 0.00019735202492211838,
      "loss": 0.1709,
      "step": 873
    },
    {
      "epoch": 4.08411214953271,
      "grad_norm": 0.42673027515411377,
      "learning_rate": 0.00019719626168224297,
      "loss": 0.1799,
      "step": 874
    },
    {
      "epoch": 4.088785046728972,
      "grad_norm": 0.4208630919456482,
      "learning_rate": 0.0001970404984423676,
      "loss": 0.1755,
      "step": 875
    },
    {
      "epoch": 4.093457943925234,
      "grad_norm": 0.43369996547698975,
      "learning_rate": 0.00019688473520249218,
      "loss": 0.1973,
      "step": 876
    },
    {
      "epoch": 4.098130841121495,
      "grad_norm": 0.41398584842681885,
      "learning_rate": 0.0001967289719626168,
      "loss": 0.1677,
      "step": 877
    },
    {
      "epoch": 4.102803738317757,
      "grad_norm": 0.4117114543914795,
      "learning_rate": 0.00019657320872274142,
      "loss": 0.171,
      "step": 878
    },
    {
      "epoch": 4.107476635514018,
      "grad_norm": 0.3718746304512024,
      "learning_rate": 0.00019641744548286604,
      "loss": 0.1947,
      "step": 879
    },
    {
      "epoch": 4.11214953271028,
      "grad_norm": 0.33576416969299316,
      "learning_rate": 0.00019626168224299063,
      "loss": 0.1418,
      "step": 880
    },
    {
      "epoch": 4.116822429906542,
      "grad_norm": 0.29116490483283997,
      "learning_rate": 0.00019610591900311525,
      "loss": 0.1343,
      "step": 881
    },
    {
      "epoch": 4.121495327102804,
      "grad_norm": 0.30176860094070435,
      "learning_rate": 0.00019595015576323986,
      "loss": 0.1242,
      "step": 882
    },
    {
      "epoch": 4.126168224299065,
      "grad_norm": 0.38680627942085266,
      "learning_rate": 0.00019579439252336448,
      "loss": 0.1745,
      "step": 883
    },
    {
      "epoch": 4.130841121495327,
      "grad_norm": 0.3346724510192871,
      "learning_rate": 0.0001956386292834891,
      "loss": 0.1717,
      "step": 884
    },
    {
      "epoch": 4.135514018691588,
      "grad_norm": 0.3079819083213806,
      "learning_rate": 0.00019548286604361372,
      "loss": 0.1644,
      "step": 885
    },
    {
      "epoch": 4.140186915887851,
      "grad_norm": 0.42686647176742554,
      "learning_rate": 0.00019532710280373828,
      "loss": 0.1798,
      "step": 886
    },
    {
      "epoch": 4.144859813084112,
      "grad_norm": 0.4064069092273712,
      "learning_rate": 0.0001951713395638629,
      "loss": 0.1423,
      "step": 887
    },
    {
      "epoch": 4.149532710280374,
      "grad_norm": 0.40536046028137207,
      "learning_rate": 0.00019501557632398752,
      "loss": 0.1696,
      "step": 888
    },
    {
      "epoch": 4.154205607476635,
      "grad_norm": 0.4617767035961151,
      "learning_rate": 0.00019485981308411214,
      "loss": 0.1822,
      "step": 889
    },
    {
      "epoch": 4.158878504672897,
      "grad_norm": 0.37310025095939636,
      "learning_rate": 0.00019470404984423675,
      "loss": 0.1672,
      "step": 890
    },
    {
      "epoch": 4.163551401869159,
      "grad_norm": 0.34881696105003357,
      "learning_rate": 0.00019454828660436137,
      "loss": 0.141,
      "step": 891
    },
    {
      "epoch": 4.168224299065421,
      "grad_norm": 0.3536577522754669,
      "learning_rate": 0.00019439252336448596,
      "loss": 0.1655,
      "step": 892
    },
    {
      "epoch": 4.172897196261682,
      "grad_norm": 0.40264371037483215,
      "learning_rate": 0.00019423676012461058,
      "loss": 0.192,
      "step": 893
    },
    {
      "epoch": 4.177570093457944,
      "grad_norm": 0.4162615239620209,
      "learning_rate": 0.00019408099688473517,
      "loss": 0.1452,
      "step": 894
    },
    {
      "epoch": 4.182242990654205,
      "grad_norm": 0.41703835129737854,
      "learning_rate": 0.0001939252336448598,
      "loss": 0.1526,
      "step": 895
    },
    {
      "epoch": 4.186915887850467,
      "grad_norm": 0.3270564079284668,
      "learning_rate": 0.0001937694704049844,
      "loss": 0.1328,
      "step": 896
    },
    {
      "epoch": 4.191588785046729,
      "grad_norm": 0.3481740355491638,
      "learning_rate": 0.00019361370716510903,
      "loss": 0.1638,
      "step": 897
    },
    {
      "epoch": 4.196261682242991,
      "grad_norm": 0.4418979287147522,
      "learning_rate": 0.00019345794392523362,
      "loss": 0.1962,
      "step": 898
    },
    {
      "epoch": 4.200934579439252,
      "grad_norm": 0.31934723258018494,
      "learning_rate": 0.00019330218068535824,
      "loss": 0.1528,
      "step": 899
    },
    {
      "epoch": 4.205607476635514,
      "grad_norm": 0.42992520332336426,
      "learning_rate": 0.00019314641744548285,
      "loss": 0.1423,
      "step": 900
    },
    {
      "epoch": 4.210280373831775,
      "grad_norm": 0.40096205472946167,
      "learning_rate": 0.00019299065420560747,
      "loss": 0.164,
      "step": 901
    },
    {
      "epoch": 4.214953271028038,
      "grad_norm": 0.30620303750038147,
      "learning_rate": 0.0001928348909657321,
      "loss": 0.1472,
      "step": 902
    },
    {
      "epoch": 4.219626168224299,
      "grad_norm": 0.3091743290424347,
      "learning_rate": 0.0001926791277258567,
      "loss": 0.1501,
      "step": 903
    },
    {
      "epoch": 4.224299065420561,
      "grad_norm": 0.41016945242881775,
      "learning_rate": 0.00019252336448598127,
      "loss": 0.1946,
      "step": 904
    },
    {
      "epoch": 4.228971962616822,
      "grad_norm": 0.37961846590042114,
      "learning_rate": 0.0001923676012461059,
      "loss": 0.1621,
      "step": 905
    },
    {
      "epoch": 4.233644859813084,
      "grad_norm": 0.25823548436164856,
      "learning_rate": 0.0001922118380062305,
      "loss": 0.1179,
      "step": 906
    },
    {
      "epoch": 4.238317757009346,
      "grad_norm": 0.3589861989021301,
      "learning_rate": 0.00019205607476635513,
      "loss": 0.1537,
      "step": 907
    },
    {
      "epoch": 4.242990654205608,
      "grad_norm": 0.39249446988105774,
      "learning_rate": 0.00019190031152647975,
      "loss": 0.1499,
      "step": 908
    },
    {
      "epoch": 4.247663551401869,
      "grad_norm": 0.3960876762866974,
      "learning_rate": 0.00019174454828660436,
      "loss": 0.1426,
      "step": 909
    },
    {
      "epoch": 4.252336448598131,
      "grad_norm": 0.3313557803630829,
      "learning_rate": 0.00019158878504672895,
      "loss": 0.1506,
      "step": 910
    },
    {
      "epoch": 4.257009345794392,
      "grad_norm": 0.3029380440711975,
      "learning_rate": 0.00019143302180685357,
      "loss": 0.1447,
      "step": 911
    },
    {
      "epoch": 4.261682242990654,
      "grad_norm": 0.29482167959213257,
      "learning_rate": 0.00019127725856697816,
      "loss": 0.1401,
      "step": 912
    },
    {
      "epoch": 4.266355140186916,
      "grad_norm": 0.404742032289505,
      "learning_rate": 0.00019112149532710278,
      "loss": 0.1727,
      "step": 913
    },
    {
      "epoch": 4.271028037383178,
      "grad_norm": 0.32308119535446167,
      "learning_rate": 0.0001909657320872274,
      "loss": 0.1622,
      "step": 914
    },
    {
      "epoch": 4.275700934579439,
      "grad_norm": 0.3348313868045807,
      "learning_rate": 0.00019080996884735202,
      "loss": 0.1764,
      "step": 915
    },
    {
      "epoch": 4.280373831775701,
      "grad_norm": 0.3448529839515686,
      "learning_rate": 0.0001906542056074766,
      "loss": 0.1576,
      "step": 916
    },
    {
      "epoch": 4.285046728971962,
      "grad_norm": 0.3992690145969391,
      "learning_rate": 0.00019049844236760123,
      "loss": 0.1505,
      "step": 917
    },
    {
      "epoch": 4.289719626168225,
      "grad_norm": 0.4447070360183716,
      "learning_rate": 0.00019034267912772585,
      "loss": 0.1784,
      "step": 918
    },
    {
      "epoch": 4.294392523364486,
      "grad_norm": 0.3217145800590515,
      "learning_rate": 0.00019018691588785046,
      "loss": 0.1496,
      "step": 919
    },
    {
      "epoch": 4.299065420560748,
      "grad_norm": 0.35135388374328613,
      "learning_rate": 0.00019003115264797508,
      "loss": 0.1794,
      "step": 920
    },
    {
      "epoch": 4.303738317757009,
      "grad_norm": 0.3651643693447113,
      "learning_rate": 0.0001898753894080997,
      "loss": 0.1613,
      "step": 921
    },
    {
      "epoch": 4.308411214953271,
      "grad_norm": 0.3845970034599304,
      "learning_rate": 0.00018971962616822426,
      "loss": 0.1637,
      "step": 922
    },
    {
      "epoch": 4.313084112149532,
      "grad_norm": 0.4410553276538849,
      "learning_rate": 0.00018956386292834888,
      "loss": 0.1956,
      "step": 923
    },
    {
      "epoch": 4.317757009345795,
      "grad_norm": 0.35467633605003357,
      "learning_rate": 0.0001894080996884735,
      "loss": 0.1813,
      "step": 924
    },
    {
      "epoch": 4.322429906542056,
      "grad_norm": 0.3303235173225403,
      "learning_rate": 0.00018925233644859812,
      "loss": 0.1444,
      "step": 925
    },
    {
      "epoch": 4.327102803738318,
      "grad_norm": 0.4050770103931427,
      "learning_rate": 0.00018909657320872274,
      "loss": 0.1737,
      "step": 926
    },
    {
      "epoch": 4.331775700934579,
      "grad_norm": 0.3493598699569702,
      "learning_rate": 0.00018894080996884735,
      "loss": 0.1887,
      "step": 927
    },
    {
      "epoch": 4.336448598130841,
      "grad_norm": 0.3519497215747833,
      "learning_rate": 0.00018878504672897195,
      "loss": 0.1718,
      "step": 928
    },
    {
      "epoch": 4.341121495327103,
      "grad_norm": 0.3544370234012604,
      "learning_rate": 0.00018862928348909656,
      "loss": 0.1553,
      "step": 929
    },
    {
      "epoch": 4.345794392523365,
      "grad_norm": 0.3493083417415619,
      "learning_rate": 0.00018847352024922115,
      "loss": 0.1541,
      "step": 930
    },
    {
      "epoch": 4.350467289719626,
      "grad_norm": 0.455143541097641,
      "learning_rate": 0.00018831775700934577,
      "loss": 0.1694,
      "step": 931
    },
    {
      "epoch": 4.355140186915888,
      "grad_norm": 0.37033045291900635,
      "learning_rate": 0.0001881619937694704,
      "loss": 0.1675,
      "step": 932
    },
    {
      "epoch": 4.359813084112149,
      "grad_norm": 0.3117184638977051,
      "learning_rate": 0.000188006230529595,
      "loss": 0.1629,
      "step": 933
    },
    {
      "epoch": 4.364485981308412,
      "grad_norm": 0.4067097008228302,
      "learning_rate": 0.0001878504672897196,
      "loss": 0.1537,
      "step": 934
    },
    {
      "epoch": 4.369158878504673,
      "grad_norm": 0.34433963894844055,
      "learning_rate": 0.00018769470404984422,
      "loss": 0.1589,
      "step": 935
    },
    {
      "epoch": 4.373831775700935,
      "grad_norm": 0.4590274691581726,
      "learning_rate": 0.00018753894080996884,
      "loss": 0.1516,
      "step": 936
    },
    {
      "epoch": 4.378504672897196,
      "grad_norm": 0.4060811996459961,
      "learning_rate": 0.00018738317757009345,
      "loss": 0.1642,
      "step": 937
    },
    {
      "epoch": 4.383177570093458,
      "grad_norm": 0.456302672624588,
      "learning_rate": 0.00018722741433021807,
      "loss": 0.1442,
      "step": 938
    },
    {
      "epoch": 4.38785046728972,
      "grad_norm": 0.3033100664615631,
      "learning_rate": 0.0001870716510903427,
      "loss": 0.1337,
      "step": 939
    },
    {
      "epoch": 4.392523364485982,
      "grad_norm": 0.44003647565841675,
      "learning_rate": 0.00018691588785046725,
      "loss": 0.131,
      "step": 940
    },
    {
      "epoch": 4.397196261682243,
      "grad_norm": 0.4001340866088867,
      "learning_rate": 0.00018676012461059187,
      "loss": 0.166,
      "step": 941
    },
    {
      "epoch": 4.401869158878505,
      "grad_norm": 0.3921581208705902,
      "learning_rate": 0.0001866043613707165,
      "loss": 0.1572,
      "step": 942
    },
    {
      "epoch": 4.406542056074766,
      "grad_norm": 0.3277539908885956,
      "learning_rate": 0.0001864485981308411,
      "loss": 0.1519,
      "step": 943
    },
    {
      "epoch": 4.411214953271028,
      "grad_norm": 0.3739788830280304,
      "learning_rate": 0.00018629283489096573,
      "loss": 0.1465,
      "step": 944
    },
    {
      "epoch": 4.41588785046729,
      "grad_norm": 0.32589492201805115,
      "learning_rate": 0.00018613707165109034,
      "loss": 0.1351,
      "step": 945
    },
    {
      "epoch": 4.420560747663552,
      "grad_norm": 0.39931750297546387,
      "learning_rate": 0.00018598130841121494,
      "loss": 0.1829,
      "step": 946
    },
    {
      "epoch": 4.425233644859813,
      "grad_norm": 0.4129701554775238,
      "learning_rate": 0.00018582554517133955,
      "loss": 0.1897,
      "step": 947
    },
    {
      "epoch": 4.429906542056075,
      "grad_norm": 0.3519931137561798,
      "learning_rate": 0.00018566978193146414,
      "loss": 0.1658,
      "step": 948
    },
    {
      "epoch": 4.434579439252336,
      "grad_norm": 0.37075355648994446,
      "learning_rate": 0.00018551401869158876,
      "loss": 0.1713,
      "step": 949
    },
    {
      "epoch": 4.4392523364485985,
      "grad_norm": 0.27445873618125916,
      "learning_rate": 0.00018535825545171338,
      "loss": 0.1498,
      "step": 950
    },
    {
      "epoch": 4.44392523364486,
      "grad_norm": 0.41424912214279175,
      "learning_rate": 0.000185202492211838,
      "loss": 0.2085,
      "step": 951
    },
    {
      "epoch": 4.4485981308411215,
      "grad_norm": 0.315450519323349,
      "learning_rate": 0.0001850467289719626,
      "loss": 0.1751,
      "step": 952
    },
    {
      "epoch": 4.453271028037383,
      "grad_norm": 0.3715207278728485,
      "learning_rate": 0.0001848909657320872,
      "loss": 0.1542,
      "step": 953
    },
    {
      "epoch": 4.457943925233645,
      "grad_norm": 0.39358285069465637,
      "learning_rate": 0.00018473520249221183,
      "loss": 0.1822,
      "step": 954
    },
    {
      "epoch": 4.462616822429906,
      "grad_norm": 0.38519442081451416,
      "learning_rate": 0.00018457943925233644,
      "loss": 0.162,
      "step": 955
    },
    {
      "epoch": 4.4672897196261685,
      "grad_norm": 0.37722471356391907,
      "learning_rate": 0.00018442367601246106,
      "loss": 0.1636,
      "step": 956
    },
    {
      "epoch": 4.47196261682243,
      "grad_norm": 0.33294758200645447,
      "learning_rate": 0.00018426791277258568,
      "loss": 0.1539,
      "step": 957
    },
    {
      "epoch": 4.4766355140186915,
      "grad_norm": 0.37193164229393005,
      "learning_rate": 0.00018411214953271024,
      "loss": 0.1509,
      "step": 958
    },
    {
      "epoch": 4.481308411214953,
      "grad_norm": 0.3272717297077179,
      "learning_rate": 0.00018395638629283486,
      "loss": 0.1596,
      "step": 959
    },
    {
      "epoch": 4.485981308411215,
      "grad_norm": 0.4075641632080078,
      "learning_rate": 0.00018380062305295948,
      "loss": 0.1658,
      "step": 960
    },
    {
      "epoch": 4.490654205607477,
      "grad_norm": 0.481773316860199,
      "learning_rate": 0.0001836448598130841,
      "loss": 0.1693,
      "step": 961
    },
    {
      "epoch": 4.4953271028037385,
      "grad_norm": 0.4442426860332489,
      "learning_rate": 0.00018348909657320872,
      "loss": 0.1995,
      "step": 962
    },
    {
      "epoch": 4.5,
      "grad_norm": 0.37774205207824707,
      "learning_rate": 0.00018333333333333334,
      "loss": 0.1445,
      "step": 963
    },
    {
      "epoch": 4.5046728971962615,
      "grad_norm": 0.4127858579158783,
      "learning_rate": 0.00018317757009345793,
      "loss": 0.1896,
      "step": 964
    },
    {
      "epoch": 4.509345794392523,
      "grad_norm": 0.35105839371681213,
      "learning_rate": 0.00018302180685358254,
      "loss": 0.1482,
      "step": 965
    },
    {
      "epoch": 4.5140186915887845,
      "grad_norm": 0.34053701162338257,
      "learning_rate": 0.00018286604361370714,
      "loss": 0.1612,
      "step": 966
    },
    {
      "epoch": 4.518691588785047,
      "grad_norm": 0.3440059423446655,
      "learning_rate": 0.00018271028037383175,
      "loss": 0.1515,
      "step": 967
    },
    {
      "epoch": 4.5233644859813085,
      "grad_norm": 0.3459179401397705,
      "learning_rate": 0.00018255451713395637,
      "loss": 0.1388,
      "step": 968
    },
    {
      "epoch": 4.52803738317757,
      "grad_norm": 0.4168221652507782,
      "learning_rate": 0.000182398753894081,
      "loss": 0.192,
      "step": 969
    },
    {
      "epoch": 4.5327102803738315,
      "grad_norm": 0.29284366965293884,
      "learning_rate": 0.00018224299065420558,
      "loss": 0.1353,
      "step": 970
    },
    {
      "epoch": 4.537383177570094,
      "grad_norm": 0.39688023924827576,
      "learning_rate": 0.0001820872274143302,
      "loss": 0.1572,
      "step": 971
    },
    {
      "epoch": 4.542056074766355,
      "grad_norm": 0.4218137562274933,
      "learning_rate": 0.00018193146417445482,
      "loss": 0.181,
      "step": 972
    },
    {
      "epoch": 4.546728971962617,
      "grad_norm": 0.3064042627811432,
      "learning_rate": 0.00018177570093457944,
      "loss": 0.155,
      "step": 973
    },
    {
      "epoch": 4.5514018691588785,
      "grad_norm": 0.345797598361969,
      "learning_rate": 0.00018161993769470405,
      "loss": 0.1491,
      "step": 974
    },
    {
      "epoch": 4.55607476635514,
      "grad_norm": 0.3539612293243408,
      "learning_rate": 0.00018146417445482867,
      "loss": 0.1465,
      "step": 975
    },
    {
      "epoch": 4.5607476635514015,
      "grad_norm": 0.36414384841918945,
      "learning_rate": 0.00018130841121495324,
      "loss": 0.1573,
      "step": 976
    },
    {
      "epoch": 4.565420560747664,
      "grad_norm": 0.37043556571006775,
      "learning_rate": 0.00018115264797507785,
      "loss": 0.1779,
      "step": 977
    },
    {
      "epoch": 4.570093457943925,
      "grad_norm": 0.3924807906150818,
      "learning_rate": 0.00018099688473520247,
      "loss": 0.1497,
      "step": 978
    },
    {
      "epoch": 4.574766355140187,
      "grad_norm": 0.4232722520828247,
      "learning_rate": 0.0001808411214953271,
      "loss": 0.1785,
      "step": 979
    },
    {
      "epoch": 4.579439252336448,
      "grad_norm": 0.31639671325683594,
      "learning_rate": 0.0001806853582554517,
      "loss": 0.1358,
      "step": 980
    },
    {
      "epoch": 4.58411214953271,
      "grad_norm": 0.37205028533935547,
      "learning_rate": 0.00018052959501557633,
      "loss": 0.1869,
      "step": 981
    },
    {
      "epoch": 4.588785046728972,
      "grad_norm": 0.38575780391693115,
      "learning_rate": 0.00018037383177570092,
      "loss": 0.1801,
      "step": 982
    },
    {
      "epoch": 4.593457943925234,
      "grad_norm": 0.31444376707077026,
      "learning_rate": 0.00018021806853582554,
      "loss": 0.1706,
      "step": 983
    },
    {
      "epoch": 4.598130841121495,
      "grad_norm": 0.39968740940093994,
      "learning_rate": 0.00018006230529595013,
      "loss": 0.1809,
      "step": 984
    },
    {
      "epoch": 4.602803738317757,
      "grad_norm": 0.3664064407348633,
      "learning_rate": 0.00017990654205607474,
      "loss": 0.1772,
      "step": 985
    },
    {
      "epoch": 4.607476635514018,
      "grad_norm": 0.34500405192375183,
      "learning_rate": 0.00017975077881619936,
      "loss": 0.1857,
      "step": 986
    },
    {
      "epoch": 4.61214953271028,
      "grad_norm": 0.34149715304374695,
      "learning_rate": 0.00017959501557632398,
      "loss": 0.193,
      "step": 987
    },
    {
      "epoch": 4.616822429906542,
      "grad_norm": 0.31908276677131653,
      "learning_rate": 0.00017943925233644857,
      "loss": 0.1662,
      "step": 988
    },
    {
      "epoch": 4.621495327102804,
      "grad_norm": 0.36149391531944275,
      "learning_rate": 0.0001792834890965732,
      "loss": 0.1602,
      "step": 989
    },
    {
      "epoch": 4.626168224299065,
      "grad_norm": 0.34562358260154724,
      "learning_rate": 0.0001791277258566978,
      "loss": 0.1813,
      "step": 990
    },
    {
      "epoch": 4.630841121495327,
      "grad_norm": 0.31548595428466797,
      "learning_rate": 0.00017897196261682243,
      "loss": 0.1468,
      "step": 991
    },
    {
      "epoch": 4.635514018691588,
      "grad_norm": 0.3365189731121063,
      "learning_rate": 0.00017881619937694704,
      "loss": 0.1786,
      "step": 992
    },
    {
      "epoch": 4.640186915887851,
      "grad_norm": 0.3202178478240967,
      "learning_rate": 0.00017866043613707166,
      "loss": 0.1729,
      "step": 993
    },
    {
      "epoch": 4.644859813084112,
      "grad_norm": 0.23484420776367188,
      "learning_rate": 0.00017850467289719623,
      "loss": 0.123,
      "step": 994
    },
    {
      "epoch": 4.649532710280374,
      "grad_norm": 0.3546544909477234,
      "learning_rate": 0.00017834890965732084,
      "loss": 0.1593,
      "step": 995
    },
    {
      "epoch": 4.654205607476635,
      "grad_norm": 0.31212157011032104,
      "learning_rate": 0.00017819314641744546,
      "loss": 0.1342,
      "step": 996
    },
    {
      "epoch": 4.658878504672897,
      "grad_norm": 0.4468859136104584,
      "learning_rate": 0.00017803738317757008,
      "loss": 0.1688,
      "step": 997
    },
    {
      "epoch": 4.663551401869158,
      "grad_norm": 0.3433874249458313,
      "learning_rate": 0.0001778816199376947,
      "loss": 0.1398,
      "step": 998
    },
    {
      "epoch": 4.668224299065421,
      "grad_norm": 0.5031092166900635,
      "learning_rate": 0.00017772585669781932,
      "loss": 0.1736,
      "step": 999
    },
    {
      "epoch": 4.672897196261682,
      "grad_norm": 0.49412909150123596,
      "learning_rate": 0.0001775700934579439,
      "loss": 0.2235,
      "step": 1000
    },
    {
      "epoch": 4.677570093457944,
      "grad_norm": 0.37751707434654236,
      "learning_rate": 0.00017741433021806853,
      "loss": 0.1681,
      "step": 1001
    },
    {
      "epoch": 4.682242990654205,
      "grad_norm": 0.37734517455101013,
      "learning_rate": 0.00017725856697819312,
      "loss": 0.1834,
      "step": 1002
    },
    {
      "epoch": 4.686915887850468,
      "grad_norm": 0.3423713445663452,
      "learning_rate": 0.00017710280373831773,
      "loss": 0.15,
      "step": 1003
    },
    {
      "epoch": 4.691588785046729,
      "grad_norm": 0.3423613905906677,
      "learning_rate": 0.00017694704049844235,
      "loss": 0.1792,
      "step": 1004
    },
    {
      "epoch": 4.696261682242991,
      "grad_norm": 0.35343971848487854,
      "learning_rate": 0.00017679127725856697,
      "loss": 0.1792,
      "step": 1005
    },
    {
      "epoch": 4.700934579439252,
      "grad_norm": 0.31654712557792664,
      "learning_rate": 0.00017663551401869156,
      "loss": 0.157,
      "step": 1006
    },
    {
      "epoch": 4.705607476635514,
      "grad_norm": 0.43035393953323364,
      "learning_rate": 0.00017647975077881618,
      "loss": 0.1795,
      "step": 1007
    },
    {
      "epoch": 4.710280373831775,
      "grad_norm": 0.31845971941947937,
      "learning_rate": 0.0001763239875389408,
      "loss": 0.144,
      "step": 1008
    },
    {
      "epoch": 4.714953271028038,
      "grad_norm": 0.3587392568588257,
      "learning_rate": 0.00017616822429906542,
      "loss": 0.1871,
      "step": 1009
    },
    {
      "epoch": 4.719626168224299,
      "grad_norm": 0.29605409502983093,
      "learning_rate": 0.00017601246105919003,
      "loss": 0.1219,
      "step": 1010
    },
    {
      "epoch": 4.724299065420561,
      "grad_norm": 0.3654855191707611,
      "learning_rate": 0.00017585669781931465,
      "loss": 0.1658,
      "step": 1011
    },
    {
      "epoch": 4.728971962616822,
      "grad_norm": 0.41348782181739807,
      "learning_rate": 0.00017570093457943922,
      "loss": 0.1633,
      "step": 1012
    },
    {
      "epoch": 4.733644859813084,
      "grad_norm": 0.5043564438819885,
      "learning_rate": 0.00017554517133956383,
      "loss": 0.1781,
      "step": 1013
    },
    {
      "epoch": 4.738317757009346,
      "grad_norm": 0.48574408888816833,
      "learning_rate": 0.00017538940809968845,
      "loss": 0.1934,
      "step": 1014
    },
    {
      "epoch": 4.742990654205608,
      "grad_norm": 0.4257358908653259,
      "learning_rate": 0.00017523364485981307,
      "loss": 0.1666,
      "step": 1015
    },
    {
      "epoch": 4.747663551401869,
      "grad_norm": 0.45419007539749146,
      "learning_rate": 0.0001750778816199377,
      "loss": 0.1955,
      "step": 1016
    },
    {
      "epoch": 4.752336448598131,
      "grad_norm": 0.43215620517730713,
      "learning_rate": 0.0001749221183800623,
      "loss": 0.1681,
      "step": 1017
    },
    {
      "epoch": 4.757009345794392,
      "grad_norm": 0.3391821086406708,
      "learning_rate": 0.0001747663551401869,
      "loss": 0.1519,
      "step": 1018
    },
    {
      "epoch": 4.761682242990654,
      "grad_norm": 0.38205686211586,
      "learning_rate": 0.0001746105919003115,
      "loss": 0.1625,
      "step": 1019
    },
    {
      "epoch": 4.766355140186916,
      "grad_norm": 0.39281779527664185,
      "learning_rate": 0.0001744548286604361,
      "loss": 0.1802,
      "step": 1020
    },
    {
      "epoch": 4.771028037383178,
      "grad_norm": 0.351272314786911,
      "learning_rate": 0.00017429906542056073,
      "loss": 0.1754,
      "step": 1021
    },
    {
      "epoch": 4.775700934579439,
      "grad_norm": 0.3040449917316437,
      "learning_rate": 0.00017414330218068534,
      "loss": 0.1607,
      "step": 1022
    },
    {
      "epoch": 4.780373831775701,
      "grad_norm": 0.3780585825443268,
      "learning_rate": 0.00017398753894080996,
      "loss": 0.1672,
      "step": 1023
    },
    {
      "epoch": 4.785046728971962,
      "grad_norm": 0.360993891954422,
      "learning_rate": 0.00017383177570093455,
      "loss": 0.1794,
      "step": 1024
    },
    {
      "epoch": 4.789719626168225,
      "grad_norm": 0.4410978853702545,
      "learning_rate": 0.00017367601246105917,
      "loss": 0.1964,
      "step": 1025
    },
    {
      "epoch": 4.794392523364486,
      "grad_norm": 0.3811183273792267,
      "learning_rate": 0.0001735202492211838,
      "loss": 0.1549,
      "step": 1026
    },
    {
      "epoch": 4.799065420560748,
      "grad_norm": 0.27967649698257446,
      "learning_rate": 0.0001733644859813084,
      "loss": 0.1556,
      "step": 1027
    },
    {
      "epoch": 4.803738317757009,
      "grad_norm": 0.3347359597682953,
      "learning_rate": 0.00017320872274143303,
      "loss": 0.1771,
      "step": 1028
    },
    {
      "epoch": 4.808411214953271,
      "grad_norm": 0.39211949706077576,
      "learning_rate": 0.00017305295950155764,
      "loss": 0.2096,
      "step": 1029
    },
    {
      "epoch": 4.813084112149532,
      "grad_norm": 0.44938424229621887,
      "learning_rate": 0.0001728971962616822,
      "loss": 0.1521,
      "step": 1030
    },
    {
      "epoch": 4.817757009345795,
      "grad_norm": 0.317869633436203,
      "learning_rate": 0.00017274143302180683,
      "loss": 0.1369,
      "step": 1031
    },
    {
      "epoch": 4.822429906542056,
      "grad_norm": 0.3158996105194092,
      "learning_rate": 0.00017258566978193144,
      "loss": 0.1322,
      "step": 1032
    },
    {
      "epoch": 4.827102803738318,
      "grad_norm": 0.41171666979789734,
      "learning_rate": 0.00017242990654205606,
      "loss": 0.1586,
      "step": 1033
    },
    {
      "epoch": 4.831775700934579,
      "grad_norm": 0.2974456548690796,
      "learning_rate": 0.00017227414330218068,
      "loss": 0.1364,
      "step": 1034
    },
    {
      "epoch": 4.836448598130842,
      "grad_norm": 0.3902209401130676,
      "learning_rate": 0.0001721183800623053,
      "loss": 0.1593,
      "step": 1035
    },
    {
      "epoch": 4.841121495327103,
      "grad_norm": 0.3558551073074341,
      "learning_rate": 0.0001719626168224299,
      "loss": 0.1526,
      "step": 1036
    },
    {
      "epoch": 4.845794392523365,
      "grad_norm": 0.5647827982902527,
      "learning_rate": 0.00017180685358255448,
      "loss": 0.1642,
      "step": 1037
    },
    {
      "epoch": 4.850467289719626,
      "grad_norm": 0.3957686126232147,
      "learning_rate": 0.0001716510903426791,
      "loss": 0.1796,
      "step": 1038
    },
    {
      "epoch": 4.855140186915888,
      "grad_norm": 0.4249538779258728,
      "learning_rate": 0.00017149532710280372,
      "loss": 0.1844,
      "step": 1039
    },
    {
      "epoch": 4.859813084112149,
      "grad_norm": 0.48849305510520935,
      "learning_rate": 0.00017133956386292833,
      "loss": 0.2163,
      "step": 1040
    },
    {
      "epoch": 4.864485981308412,
      "grad_norm": 0.3327171206474304,
      "learning_rate": 0.00017118380062305295,
      "loss": 0.157,
      "step": 1041
    },
    {
      "epoch": 4.869158878504673,
      "grad_norm": 0.29771801829338074,
      "learning_rate": 0.00017102803738317754,
      "loss": 0.1084,
      "step": 1042
    },
    {
      "epoch": 4.873831775700935,
      "grad_norm": 0.36569488048553467,
      "learning_rate": 0.00017087227414330216,
      "loss": 0.1806,
      "step": 1043
    },
    {
      "epoch": 4.878504672897196,
      "grad_norm": 0.31847286224365234,
      "learning_rate": 0.00017071651090342678,
      "loss": 0.1525,
      "step": 1044
    },
    {
      "epoch": 4.883177570093458,
      "grad_norm": 0.30491992831230164,
      "learning_rate": 0.0001705607476635514,
      "loss": 0.1583,
      "step": 1045
    },
    {
      "epoch": 4.88785046728972,
      "grad_norm": 0.30772796273231506,
      "learning_rate": 0.00017040498442367602,
      "loss": 0.1432,
      "step": 1046
    },
    {
      "epoch": 4.892523364485982,
      "grad_norm": 0.32460343837738037,
      "learning_rate": 0.00017024922118380063,
      "loss": 0.1421,
      "step": 1047
    },
    {
      "epoch": 4.897196261682243,
      "grad_norm": 0.2550308406352997,
      "learning_rate": 0.0001700934579439252,
      "loss": 0.1257,
      "step": 1048
    },
    {
      "epoch": 4.901869158878505,
      "grad_norm": 0.36809951066970825,
      "learning_rate": 0.00016993769470404982,
      "loss": 0.1478,
      "step": 1049
    },
    {
      "epoch": 4.906542056074766,
      "grad_norm": 0.39371243119239807,
      "learning_rate": 0.00016978193146417443,
      "loss": 0.1381,
      "step": 1050
    },
    {
      "epoch": 4.911214953271028,
      "grad_norm": 0.4576844871044159,
      "learning_rate": 0.00016962616822429905,
      "loss": 0.1576,
      "step": 1051
    },
    {
      "epoch": 4.91588785046729,
      "grad_norm": 0.4509245753288269,
      "learning_rate": 0.00016947040498442367,
      "loss": 0.172,
      "step": 1052
    },
    {
      "epoch": 4.920560747663552,
      "grad_norm": 0.5088894367218018,
      "learning_rate": 0.0001693146417445483,
      "loss": 0.1712,
      "step": 1053
    },
    {
      "epoch": 4.925233644859813,
      "grad_norm": 0.3347131013870239,
      "learning_rate": 0.00016915887850467288,
      "loss": 0.1574,
      "step": 1054
    },
    {
      "epoch": 4.929906542056075,
      "grad_norm": 0.3810819983482361,
      "learning_rate": 0.00016900311526479747,
      "loss": 0.1673,
      "step": 1055
    },
    {
      "epoch": 4.934579439252336,
      "grad_norm": 0.39034032821655273,
      "learning_rate": 0.0001688473520249221,
      "loss": 0.1737,
      "step": 1056
    },
    {
      "epoch": 4.9392523364485985,
      "grad_norm": 0.3849758505821228,
      "learning_rate": 0.0001686915887850467,
      "loss": 0.1916,
      "step": 1057
    },
    {
      "epoch": 4.94392523364486,
      "grad_norm": 0.31557440757751465,
      "learning_rate": 0.00016853582554517132,
      "loss": 0.1382,
      "step": 1058
    },
    {
      "epoch": 4.9485981308411215,
      "grad_norm": 0.4021877944469452,
      "learning_rate": 0.00016838006230529594,
      "loss": 0.1679,
      "step": 1059
    },
    {
      "epoch": 4.953271028037383,
      "grad_norm": 0.3669017255306244,
      "learning_rate": 0.00016822429906542053,
      "loss": 0.1576,
      "step": 1060
    },
    {
      "epoch": 4.957943925233645,
      "grad_norm": 0.31172099709510803,
      "learning_rate": 0.00016806853582554515,
      "loss": 0.1646,
      "step": 1061
    },
    {
      "epoch": 4.962616822429906,
      "grad_norm": 0.3925687372684479,
      "learning_rate": 0.00016791277258566977,
      "loss": 0.1754,
      "step": 1062
    },
    {
      "epoch": 4.9672897196261685,
      "grad_norm": 0.28753137588500977,
      "learning_rate": 0.0001677570093457944,
      "loss": 0.1311,
      "step": 1063
    },
    {
      "epoch": 4.97196261682243,
      "grad_norm": 0.3392430245876312,
      "learning_rate": 0.000167601246105919,
      "loss": 0.1642,
      "step": 1064
    },
    {
      "epoch": 4.9766355140186915,
      "grad_norm": 0.3385106325149536,
      "learning_rate": 0.00016744548286604362,
      "loss": 0.1529,
      "step": 1065
    },
    {
      "epoch": 4.981308411214953,
      "grad_norm": 0.5121025443077087,
      "learning_rate": 0.0001672897196261682,
      "loss": 0.1795,
      "step": 1066
    },
    {
      "epoch": 4.9859813084112155,
      "grad_norm": 0.5081642866134644,
      "learning_rate": 0.0001671339563862928,
      "loss": 0.1807,
      "step": 1067
    },
    {
      "epoch": 4.990654205607477,
      "grad_norm": 0.3979562819004059,
      "learning_rate": 0.00016697819314641742,
      "loss": 0.1834,
      "step": 1068
    },
    {
      "epoch": 4.9953271028037385,
      "grad_norm": 0.461385577917099,
      "learning_rate": 0.00016682242990654204,
      "loss": 0.1774,
      "step": 1069
    },
    {
      "epoch": 5.0,
      "grad_norm": 0.6589238047599792,
      "learning_rate": 0.00016666666666666666,
      "loss": 0.1501,
      "step": 1070
    },
    {
      "epoch": 5.0,
      "eval_loss": 0.2342846840620041,
      "eval_runtime": 52.9997,
      "eval_samples_per_second": 10.415,
      "eval_steps_per_second": 1.302,
      "step": 1070
    },
    {
      "epoch": 5.0046728971962615,
      "grad_norm": 0.31749245524406433,
      "learning_rate": 0.00016651090342679128,
      "loss": 0.1643,
      "step": 1071
    },
    {
      "epoch": 5.009345794392523,
      "grad_norm": 0.2651839256286621,
      "learning_rate": 0.00016635514018691587,
      "loss": 0.1456,
      "step": 1072
    },
    {
      "epoch": 5.014018691588785,
      "grad_norm": 0.3662033677101135,
      "learning_rate": 0.00016619937694704046,
      "loss": 0.1695,
      "step": 1073
    },
    {
      "epoch": 5.018691588785047,
      "grad_norm": 0.41626203060150146,
      "learning_rate": 0.00016604361370716508,
      "loss": 0.1509,
      "step": 1074
    },
    {
      "epoch": 5.0233644859813085,
      "grad_norm": 0.3065464198589325,
      "learning_rate": 0.0001658878504672897,
      "loss": 0.1351,
      "step": 1075
    },
    {
      "epoch": 5.02803738317757,
      "grad_norm": 0.30088651180267334,
      "learning_rate": 0.00016573208722741432,
      "loss": 0.1261,
      "step": 1076
    },
    {
      "epoch": 5.0327102803738315,
      "grad_norm": 0.3188228905200958,
      "learning_rate": 0.00016557632398753893,
      "loss": 0.147,
      "step": 1077
    },
    {
      "epoch": 5.037383177570093,
      "grad_norm": 0.35744723677635193,
      "learning_rate": 0.00016542056074766352,
      "loss": 0.1382,
      "step": 1078
    },
    {
      "epoch": 5.042056074766355,
      "grad_norm": 0.3329566419124603,
      "learning_rate": 0.00016526479750778814,
      "loss": 0.1353,
      "step": 1079
    },
    {
      "epoch": 5.046728971962617,
      "grad_norm": 0.37419193983078003,
      "learning_rate": 0.00016510903426791276,
      "loss": 0.1687,
      "step": 1080
    },
    {
      "epoch": 5.0514018691588785,
      "grad_norm": 0.3616548776626587,
      "learning_rate": 0.00016495327102803738,
      "loss": 0.1538,
      "step": 1081
    },
    {
      "epoch": 5.05607476635514,
      "grad_norm": 0.2606242001056671,
      "learning_rate": 0.000164797507788162,
      "loss": 0.1336,
      "step": 1082
    },
    {
      "epoch": 5.0607476635514015,
      "grad_norm": 0.4130910336971283,
      "learning_rate": 0.00016464174454828662,
      "loss": 0.1741,
      "step": 1083
    },
    {
      "epoch": 5.065420560747664,
      "grad_norm": 0.3839578330516815,
      "learning_rate": 0.00016448598130841118,
      "loss": 0.1654,
      "step": 1084
    },
    {
      "epoch": 5.070093457943925,
      "grad_norm": 0.45272713899612427,
      "learning_rate": 0.0001643302180685358,
      "loss": 0.1801,
      "step": 1085
    },
    {
      "epoch": 5.074766355140187,
      "grad_norm": 0.35722512006759644,
      "learning_rate": 0.00016417445482866042,
      "loss": 0.1731,
      "step": 1086
    },
    {
      "epoch": 5.079439252336448,
      "grad_norm": 0.39995694160461426,
      "learning_rate": 0.00016401869158878503,
      "loss": 0.161,
      "step": 1087
    },
    {
      "epoch": 5.08411214953271,
      "grad_norm": 0.3963228762149811,
      "learning_rate": 0.00016386292834890965,
      "loss": 0.1502,
      "step": 1088
    },
    {
      "epoch": 5.088785046728972,
      "grad_norm": 0.40370118618011475,
      "learning_rate": 0.00016370716510903427,
      "loss": 0.1907,
      "step": 1089
    },
    {
      "epoch": 5.093457943925234,
      "grad_norm": 0.3921113610267639,
      "learning_rate": 0.00016355140186915886,
      "loss": 0.171,
      "step": 1090
    },
    {
      "epoch": 5.098130841121495,
      "grad_norm": 0.24623103439807892,
      "learning_rate": 0.00016339563862928345,
      "loss": 0.1289,
      "step": 1091
    },
    {
      "epoch": 5.102803738317757,
      "grad_norm": 0.34638118743896484,
      "learning_rate": 0.00016323987538940807,
      "loss": 0.1731,
      "step": 1092
    },
    {
      "epoch": 5.107476635514018,
      "grad_norm": 0.28228822350502014,
      "learning_rate": 0.0001630841121495327,
      "loss": 0.1192,
      "step": 1093
    },
    {
      "epoch": 5.11214953271028,
      "grad_norm": 0.3389583230018616,
      "learning_rate": 0.0001629283489096573,
      "loss": 0.1586,
      "step": 1094
    },
    {
      "epoch": 5.116822429906542,
      "grad_norm": 0.45791786909103394,
      "learning_rate": 0.00016277258566978192,
      "loss": 0.148,
      "step": 1095
    },
    {
      "epoch": 5.121495327102804,
      "grad_norm": 0.41057294607162476,
      "learning_rate": 0.00016261682242990652,
      "loss": 0.1696,
      "step": 1096
    },
    {
      "epoch": 5.126168224299065,
      "grad_norm": 0.3466396629810333,
      "learning_rate": 0.00016246105919003113,
      "loss": 0.1427,
      "step": 1097
    },
    {
      "epoch": 5.130841121495327,
      "grad_norm": 0.31289640069007874,
      "learning_rate": 0.00016230529595015575,
      "loss": 0.1384,
      "step": 1098
    },
    {
      "epoch": 5.135514018691588,
      "grad_norm": 0.3299732804298401,
      "learning_rate": 0.00016214953271028037,
      "loss": 0.1275,
      "step": 1099
    },
    {
      "epoch": 5.140186915887851,
      "grad_norm": 0.4844009280204773,
      "learning_rate": 0.000161993769470405,
      "loss": 0.2016,
      "step": 1100
    },
    {
      "epoch": 5.144859813084112,
      "grad_norm": 0.35526150465011597,
      "learning_rate": 0.0001618380062305296,
      "loss": 0.1374,
      "step": 1101
    },
    {
      "epoch": 5.149532710280374,
      "grad_norm": 0.34931910037994385,
      "learning_rate": 0.00016168224299065417,
      "loss": 0.115,
      "step": 1102
    },
    {
      "epoch": 5.154205607476635,
      "grad_norm": 0.3837710916996002,
      "learning_rate": 0.0001615264797507788,
      "loss": 0.1399,
      "step": 1103
    },
    {
      "epoch": 5.158878504672897,
      "grad_norm": 0.4009993374347687,
      "learning_rate": 0.0001613707165109034,
      "loss": 0.1451,
      "step": 1104
    },
    {
      "epoch": 5.163551401869159,
      "grad_norm": 0.3653769791126251,
      "learning_rate": 0.00016121495327102802,
      "loss": 0.1341,
      "step": 1105
    },
    {
      "epoch": 5.168224299065421,
      "grad_norm": 0.4355066418647766,
      "learning_rate": 0.00016105919003115264,
      "loss": 0.1611,
      "step": 1106
    },
    {
      "epoch": 5.172897196261682,
      "grad_norm": 0.3498646914958954,
      "learning_rate": 0.00016090342679127726,
      "loss": 0.142,
      "step": 1107
    },
    {
      "epoch": 5.177570093457944,
      "grad_norm": 0.3332652449607849,
      "learning_rate": 0.00016074766355140185,
      "loss": 0.1164,
      "step": 1108
    },
    {
      "epoch": 5.182242990654205,
      "grad_norm": 0.40865039825439453,
      "learning_rate": 0.00016059190031152644,
      "loss": 0.1449,
      "step": 1109
    },
    {
      "epoch": 5.186915887850467,
      "grad_norm": 0.3368442952632904,
      "learning_rate": 0.00016043613707165106,
      "loss": 0.1649,
      "step": 1110
    },
    {
      "epoch": 5.191588785046729,
      "grad_norm": 0.3867655396461487,
      "learning_rate": 0.00016028037383177568,
      "loss": 0.1702,
      "step": 1111
    },
    {
      "epoch": 5.196261682242991,
      "grad_norm": 0.3494774401187897,
      "learning_rate": 0.0001601246105919003,
      "loss": 0.1593,
      "step": 1112
    },
    {
      "epoch": 5.200934579439252,
      "grad_norm": 0.29425063729286194,
      "learning_rate": 0.00015996884735202492,
      "loss": 0.1172,
      "step": 1113
    },
    {
      "epoch": 5.205607476635514,
      "grad_norm": 0.3717508912086487,
      "learning_rate": 0.0001598130841121495,
      "loss": 0.1454,
      "step": 1114
    },
    {
      "epoch": 5.210280373831775,
      "grad_norm": 0.3447628915309906,
      "learning_rate": 0.00015965732087227412,
      "loss": 0.1616,
      "step": 1115
    },
    {
      "epoch": 5.214953271028038,
      "grad_norm": 0.325333833694458,
      "learning_rate": 0.00015950155763239874,
      "loss": 0.1414,
      "step": 1116
    },
    {
      "epoch": 5.219626168224299,
      "grad_norm": 0.39910563826560974,
      "learning_rate": 0.00015934579439252336,
      "loss": 0.1389,
      "step": 1117
    },
    {
      "epoch": 5.224299065420561,
      "grad_norm": 0.35903695225715637,
      "learning_rate": 0.00015919003115264798,
      "loss": 0.1558,
      "step": 1118
    },
    {
      "epoch": 5.228971962616822,
      "grad_norm": 0.44036784768104553,
      "learning_rate": 0.0001590342679127726,
      "loss": 0.1869,
      "step": 1119
    },
    {
      "epoch": 5.233644859813084,
      "grad_norm": 0.3136916160583496,
      "learning_rate": 0.00015887850467289716,
      "loss": 0.1429,
      "step": 1120
    },
    {
      "epoch": 5.238317757009346,
      "grad_norm": 0.4339526891708374,
      "learning_rate": 0.00015872274143302178,
      "loss": 0.1588,
      "step": 1121
    },
    {
      "epoch": 5.242990654205608,
      "grad_norm": 0.3696800768375397,
      "learning_rate": 0.0001585669781931464,
      "loss": 0.1321,
      "step": 1122
    },
    {
      "epoch": 5.247663551401869,
      "grad_norm": 0.3819759786128998,
      "learning_rate": 0.00015841121495327101,
      "loss": 0.1602,
      "step": 1123
    },
    {
      "epoch": 5.252336448598131,
      "grad_norm": 0.4230780601501465,
      "learning_rate": 0.00015825545171339563,
      "loss": 0.1694,
      "step": 1124
    },
    {
      "epoch": 5.257009345794392,
      "grad_norm": 0.36612778902053833,
      "learning_rate": 0.00015809968847352025,
      "loss": 0.1303,
      "step": 1125
    },
    {
      "epoch": 5.261682242990654,
      "grad_norm": 0.3493597209453583,
      "learning_rate": 0.00015794392523364484,
      "loss": 0.1558,
      "step": 1126
    },
    {
      "epoch": 5.266355140186916,
      "grad_norm": 0.49357926845550537,
      "learning_rate": 0.00015778816199376943,
      "loss": 0.1523,
      "step": 1127
    },
    {
      "epoch": 5.271028037383178,
      "grad_norm": 0.4051114320755005,
      "learning_rate": 0.00015763239875389405,
      "loss": 0.175,
      "step": 1128
    },
    {
      "epoch": 5.275700934579439,
      "grad_norm": 0.3334469497203827,
      "learning_rate": 0.00015747663551401867,
      "loss": 0.1388,
      "step": 1129
    },
    {
      "epoch": 5.280373831775701,
      "grad_norm": 0.33959609270095825,
      "learning_rate": 0.0001573208722741433,
      "loss": 0.1297,
      "step": 1130
    },
    {
      "epoch": 5.285046728971962,
      "grad_norm": 0.3644874095916748,
      "learning_rate": 0.0001571651090342679,
      "loss": 0.1632,
      "step": 1131
    },
    {
      "epoch": 5.289719626168225,
      "grad_norm": 0.3946158289909363,
      "learning_rate": 0.0001570093457943925,
      "loss": 0.1562,
      "step": 1132
    },
    {
      "epoch": 5.294392523364486,
      "grad_norm": 0.3573911190032959,
      "learning_rate": 0.00015685358255451711,
      "loss": 0.1408,
      "step": 1133
    },
    {
      "epoch": 5.299065420560748,
      "grad_norm": 0.30008506774902344,
      "learning_rate": 0.00015669781931464173,
      "loss": 0.1558,
      "step": 1134
    },
    {
      "epoch": 5.303738317757009,
      "grad_norm": 0.28273749351501465,
      "learning_rate": 0.00015654205607476635,
      "loss": 0.1335,
      "step": 1135
    },
    {
      "epoch": 5.308411214953271,
      "grad_norm": 0.36890465021133423,
      "learning_rate": 0.00015638629283489097,
      "loss": 0.1621,
      "step": 1136
    },
    {
      "epoch": 5.313084112149532,
      "grad_norm": 0.28798335790634155,
      "learning_rate": 0.0001562305295950156,
      "loss": 0.122,
      "step": 1137
    },
    {
      "epoch": 5.317757009345795,
      "grad_norm": 0.3668012320995331,
      "learning_rate": 0.00015607476635514015,
      "loss": 0.173,
      "step": 1138
    },
    {
      "epoch": 5.322429906542056,
      "grad_norm": 0.276591956615448,
      "learning_rate": 0.00015591900311526477,
      "loss": 0.1139,
      "step": 1139
    },
    {
      "epoch": 5.327102803738318,
      "grad_norm": 0.3030489981174469,
      "learning_rate": 0.0001557632398753894,
      "loss": 0.133,
      "step": 1140
    },
    {
      "epoch": 5.331775700934579,
      "grad_norm": 0.4300738573074341,
      "learning_rate": 0.000155607476635514,
      "loss": 0.1802,
      "step": 1141
    },
    {
      "epoch": 5.336448598130841,
      "grad_norm": 0.500671923160553,
      "learning_rate": 0.00015545171339563862,
      "loss": 0.1644,
      "step": 1142
    },
    {
      "epoch": 5.341121495327103,
      "grad_norm": 0.4886089265346527,
      "learning_rate": 0.00015529595015576324,
      "loss": 0.1403,
      "step": 1143
    },
    {
      "epoch": 5.345794392523365,
      "grad_norm": 0.3556120991706848,
      "learning_rate": 0.00015514018691588783,
      "loss": 0.1459,
      "step": 1144
    },
    {
      "epoch": 5.350467289719626,
      "grad_norm": 0.3724754750728607,
      "learning_rate": 0.00015498442367601242,
      "loss": 0.1582,
      "step": 1145
    },
    {
      "epoch": 5.355140186915888,
      "grad_norm": 0.4003771245479584,
      "learning_rate": 0.00015482866043613704,
      "loss": 0.1323,
      "step": 1146
    },
    {
      "epoch": 5.359813084112149,
      "grad_norm": 0.4260466396808624,
      "learning_rate": 0.00015467289719626166,
      "loss": 0.1372,
      "step": 1147
    },
    {
      "epoch": 5.364485981308412,
      "grad_norm": 0.2667592465877533,
      "learning_rate": 0.00015451713395638628,
      "loss": 0.1146,
      "step": 1148
    },
    {
      "epoch": 5.369158878504673,
      "grad_norm": 0.33398327231407166,
      "learning_rate": 0.0001543613707165109,
      "loss": 0.1494,
      "step": 1149
    },
    {
      "epoch": 5.373831775700935,
      "grad_norm": 0.38543933629989624,
      "learning_rate": 0.0001542056074766355,
      "loss": 0.143,
      "step": 1150
    },
    {
      "epoch": 5.378504672897196,
      "grad_norm": 0.4155537486076355,
      "learning_rate": 0.0001540498442367601,
      "loss": 0.1719,
      "step": 1151
    },
    {
      "epoch": 5.383177570093458,
      "grad_norm": 0.41160812973976135,
      "learning_rate": 0.00015389408099688472,
      "loss": 0.1552,
      "step": 1152
    },
    {
      "epoch": 5.38785046728972,
      "grad_norm": 0.4083704650402069,
      "learning_rate": 0.00015373831775700934,
      "loss": 0.1532,
      "step": 1153
    },
    {
      "epoch": 5.392523364485982,
      "grad_norm": 0.4396953284740448,
      "learning_rate": 0.00015358255451713396,
      "loss": 0.1532,
      "step": 1154
    },
    {
      "epoch": 5.397196261682243,
      "grad_norm": 0.43398165702819824,
      "learning_rate": 0.00015342679127725858,
      "loss": 0.168,
      "step": 1155
    },
    {
      "epoch": 5.401869158878505,
      "grad_norm": 0.3541688323020935,
      "learning_rate": 0.00015327102803738314,
      "loss": 0.1309,
      "step": 1156
    },
    {
      "epoch": 5.406542056074766,
      "grad_norm": 0.36191362142562866,
      "learning_rate": 0.00015311526479750776,
      "loss": 0.1525,
      "step": 1157
    },
    {
      "epoch": 5.411214953271028,
      "grad_norm": 0.42731326818466187,
      "learning_rate": 0.00015295950155763238,
      "loss": 0.1806,
      "step": 1158
    },
    {
      "epoch": 5.41588785046729,
      "grad_norm": 0.4410472512245178,
      "learning_rate": 0.000152803738317757,
      "loss": 0.1679,
      "step": 1159
    },
    {
      "epoch": 5.420560747663552,
      "grad_norm": 0.32090333104133606,
      "learning_rate": 0.00015264797507788161,
      "loss": 0.1402,
      "step": 1160
    },
    {
      "epoch": 5.425233644859813,
      "grad_norm": 0.3852922022342682,
      "learning_rate": 0.00015249221183800623,
      "loss": 0.1656,
      "step": 1161
    },
    {
      "epoch": 5.429906542056075,
      "grad_norm": 0.398063063621521,
      "learning_rate": 0.00015233644859813082,
      "loss": 0.1473,
      "step": 1162
    },
    {
      "epoch": 5.434579439252336,
      "grad_norm": 0.3418773114681244,
      "learning_rate": 0.00015218068535825541,
      "loss": 0.1485,
      "step": 1163
    },
    {
      "epoch": 5.4392523364485985,
      "grad_norm": 0.29981526732444763,
      "learning_rate": 0.00015202492211838003,
      "loss": 0.1153,
      "step": 1164
    },
    {
      "epoch": 5.44392523364486,
      "grad_norm": 0.33937591314315796,
      "learning_rate": 0.00015186915887850465,
      "loss": 0.1356,
      "step": 1165
    },
    {
      "epoch": 5.4485981308411215,
      "grad_norm": 0.35130542516708374,
      "learning_rate": 0.00015171339563862927,
      "loss": 0.1651,
      "step": 1166
    },
    {
      "epoch": 5.453271028037383,
      "grad_norm": 0.3933780789375305,
      "learning_rate": 0.0001515576323987539,
      "loss": 0.1697,
      "step": 1167
    },
    {
      "epoch": 5.457943925233645,
      "grad_norm": 0.3276248276233673,
      "learning_rate": 0.00015140186915887848,
      "loss": 0.1349,
      "step": 1168
    },
    {
      "epoch": 5.462616822429906,
      "grad_norm": 0.40222764015197754,
      "learning_rate": 0.0001512461059190031,
      "loss": 0.1506,
      "step": 1169
    },
    {
      "epoch": 5.4672897196261685,
      "grad_norm": 0.3287102282047272,
      "learning_rate": 0.00015109034267912771,
      "loss": 0.1369,
      "step": 1170
    },
    {
      "epoch": 5.47196261682243,
      "grad_norm": 0.4123481810092926,
      "learning_rate": 0.00015093457943925233,
      "loss": 0.1618,
      "step": 1171
    },
    {
      "epoch": 5.4766355140186915,
      "grad_norm": 0.3320179283618927,
      "learning_rate": 0.00015077881619937695,
      "loss": 0.1509,
      "step": 1172
    },
    {
      "epoch": 5.481308411214953,
      "grad_norm": 0.4943808913230896,
      "learning_rate": 0.00015062305295950157,
      "loss": 0.1629,
      "step": 1173
    },
    {
      "epoch": 5.485981308411215,
      "grad_norm": 0.4109898805618286,
      "learning_rate": 0.00015046728971962613,
      "loss": 0.1529,
      "step": 1174
    },
    {
      "epoch": 5.490654205607477,
      "grad_norm": 0.4578470289707184,
      "learning_rate": 0.00015031152647975075,
      "loss": 0.1688,
      "step": 1175
    },
    {
      "epoch": 5.4953271028037385,
      "grad_norm": 0.5028976798057556,
      "learning_rate": 0.00015015576323987537,
      "loss": 0.1799,
      "step": 1176
    },
    {
      "epoch": 5.5,
      "grad_norm": 0.3562828004360199,
      "learning_rate": 0.00015,
      "loss": 0.1434,
      "step": 1177
    },
    {
      "epoch": 5.5046728971962615,
      "grad_norm": 0.41982874274253845,
      "learning_rate": 0.0001498442367601246,
      "loss": 0.1659,
      "step": 1178
    },
    {
      "epoch": 5.509345794392523,
      "grad_norm": 0.33159470558166504,
      "learning_rate": 0.0001496884735202492,
      "loss": 0.1401,
      "step": 1179
    },
    {
      "epoch": 5.5140186915887845,
      "grad_norm": 0.40040335059165955,
      "learning_rate": 0.00014953271028037381,
      "loss": 0.1696,
      "step": 1180
    },
    {
      "epoch": 5.518691588785047,
      "grad_norm": 0.43327796459198,
      "learning_rate": 0.00014937694704049843,
      "loss": 0.1448,
      "step": 1181
    },
    {
      "epoch": 5.5233644859813085,
      "grad_norm": 0.39794686436653137,
      "learning_rate": 0.00014922118380062302,
      "loss": 0.1672,
      "step": 1182
    },
    {
      "epoch": 5.52803738317757,
      "grad_norm": 0.2996193766593933,
      "learning_rate": 0.00014906542056074764,
      "loss": 0.1325,
      "step": 1183
    },
    {
      "epoch": 5.5327102803738315,
      "grad_norm": 0.3644041121006012,
      "learning_rate": 0.00014890965732087226,
      "loss": 0.1247,
      "step": 1184
    },
    {
      "epoch": 5.537383177570094,
      "grad_norm": 0.4320167303085327,
      "learning_rate": 0.00014875389408099688,
      "loss": 0.1631,
      "step": 1185
    },
    {
      "epoch": 5.542056074766355,
      "grad_norm": 0.3322708308696747,
      "learning_rate": 0.00014859813084112147,
      "loss": 0.1346,
      "step": 1186
    },
    {
      "epoch": 5.546728971962617,
      "grad_norm": 0.34335920214653015,
      "learning_rate": 0.0001484423676012461,
      "loss": 0.1319,
      "step": 1187
    },
    {
      "epoch": 5.5514018691588785,
      "grad_norm": 0.29924654960632324,
      "learning_rate": 0.0001482866043613707,
      "loss": 0.1247,
      "step": 1188
    },
    {
      "epoch": 5.55607476635514,
      "grad_norm": 0.47307276725769043,
      "learning_rate": 0.00014813084112149532,
      "loss": 0.1652,
      "step": 1189
    },
    {
      "epoch": 5.5607476635514015,
      "grad_norm": 0.3890373408794403,
      "learning_rate": 0.00014797507788161994,
      "loss": 0.1498,
      "step": 1190
    },
    {
      "epoch": 5.565420560747664,
      "grad_norm": 0.5596181750297546,
      "learning_rate": 0.00014781931464174453,
      "loss": 0.1865,
      "step": 1191
    },
    {
      "epoch": 5.570093457943925,
      "grad_norm": 0.3337388038635254,
      "learning_rate": 0.00014766355140186915,
      "loss": 0.1388,
      "step": 1192
    },
    {
      "epoch": 5.574766355140187,
      "grad_norm": 0.3931508958339691,
      "learning_rate": 0.00014750778816199377,
      "loss": 0.1566,
      "step": 1193
    },
    {
      "epoch": 5.579439252336448,
      "grad_norm": 0.41332265734672546,
      "learning_rate": 0.00014735202492211836,
      "loss": 0.14,
      "step": 1194
    },
    {
      "epoch": 5.58411214953271,
      "grad_norm": 0.40505826473236084,
      "learning_rate": 0.00014719626168224298,
      "loss": 0.1515,
      "step": 1195
    },
    {
      "epoch": 5.588785046728972,
      "grad_norm": 0.49696457386016846,
      "learning_rate": 0.0001470404984423676,
      "loss": 0.177,
      "step": 1196
    },
    {
      "epoch": 5.593457943925234,
      "grad_norm": 0.31277140974998474,
      "learning_rate": 0.0001468847352024922,
      "loss": 0.1364,
      "step": 1197
    },
    {
      "epoch": 5.598130841121495,
      "grad_norm": 0.2983926236629486,
      "learning_rate": 0.0001467289719626168,
      "loss": 0.1205,
      "step": 1198
    },
    {
      "epoch": 5.602803738317757,
      "grad_norm": 0.3164847195148468,
      "learning_rate": 0.00014657320872274142,
      "loss": 0.1367,
      "step": 1199
    },
    {
      "epoch": 5.607476635514018,
      "grad_norm": 0.38613611459732056,
      "learning_rate": 0.00014641744548286601,
      "loss": 0.1891,
      "step": 1200
    },
    {
      "epoch": 5.61214953271028,
      "grad_norm": 0.3613464832305908,
      "learning_rate": 0.00014626168224299063,
      "loss": 0.1593,
      "step": 1201
    },
    {
      "epoch": 5.616822429906542,
      "grad_norm": 0.2954808175563812,
      "learning_rate": 0.00014610591900311525,
      "loss": 0.1365,
      "step": 1202
    },
    {
      "epoch": 5.621495327102804,
      "grad_norm": 0.3266771733760834,
      "learning_rate": 0.00014595015576323987,
      "loss": 0.1434,
      "step": 1203
    },
    {
      "epoch": 5.626168224299065,
      "grad_norm": 0.38050803542137146,
      "learning_rate": 0.00014579439252336446,
      "loss": 0.1557,
      "step": 1204
    },
    {
      "epoch": 5.630841121495327,
      "grad_norm": 0.3764829635620117,
      "learning_rate": 0.00014563862928348908,
      "loss": 0.1461,
      "step": 1205
    },
    {
      "epoch": 5.635514018691588,
      "grad_norm": 0.35239556431770325,
      "learning_rate": 0.0001454828660436137,
      "loss": 0.1526,
      "step": 1206
    },
    {
      "epoch": 5.640186915887851,
      "grad_norm": 0.46271684765815735,
      "learning_rate": 0.00014532710280373831,
      "loss": 0.2014,
      "step": 1207
    },
    {
      "epoch": 5.644859813084112,
      "grad_norm": 0.3265685439109802,
      "learning_rate": 0.00014517133956386293,
      "loss": 0.111,
      "step": 1208
    },
    {
      "epoch": 5.649532710280374,
      "grad_norm": 0.3547116816043854,
      "learning_rate": 0.00014501557632398752,
      "loss": 0.1495,
      "step": 1209
    },
    {
      "epoch": 5.654205607476635,
      "grad_norm": 0.35789403319358826,
      "learning_rate": 0.00014485981308411214,
      "loss": 0.1504,
      "step": 1210
    },
    {
      "epoch": 5.658878504672897,
      "grad_norm": 0.31200310587882996,
      "learning_rate": 0.00014470404984423676,
      "loss": 0.1444,
      "step": 1211
    },
    {
      "epoch": 5.663551401869158,
      "grad_norm": 0.40628954768180847,
      "learning_rate": 0.00014454828660436135,
      "loss": 0.1624,
      "step": 1212
    },
    {
      "epoch": 5.668224299065421,
      "grad_norm": 0.3476491868495941,
      "learning_rate": 0.00014439252336448597,
      "loss": 0.1334,
      "step": 1213
    },
    {
      "epoch": 5.672897196261682,
      "grad_norm": 0.35718193650245667,
      "learning_rate": 0.00014423676012461059,
      "loss": 0.1213,
      "step": 1214
    },
    {
      "epoch": 5.677570093457944,
      "grad_norm": 0.30355706810951233,
      "learning_rate": 0.00014408099688473518,
      "loss": 0.1412,
      "step": 1215
    },
    {
      "epoch": 5.682242990654205,
      "grad_norm": 0.3934764564037323,
      "learning_rate": 0.0001439252336448598,
      "loss": 0.1797,
      "step": 1216
    },
    {
      "epoch": 5.686915887850468,
      "grad_norm": 0.453714519739151,
      "learning_rate": 0.0001437694704049844,
      "loss": 0.1479,
      "step": 1217
    },
    {
      "epoch": 5.691588785046729,
      "grad_norm": 0.2872597575187683,
      "learning_rate": 0.000143613707165109,
      "loss": 0.117,
      "step": 1218
    },
    {
      "epoch": 5.696261682242991,
      "grad_norm": 0.4583859443664551,
      "learning_rate": 0.00014345794392523362,
      "loss": 0.1583,
      "step": 1219
    },
    {
      "epoch": 5.700934579439252,
      "grad_norm": 0.5193559527397156,
      "learning_rate": 0.00014330218068535824,
      "loss": 0.1903,
      "step": 1220
    },
    {
      "epoch": 5.705607476635514,
      "grad_norm": 0.4466177523136139,
      "learning_rate": 0.00014314641744548286,
      "loss": 0.1719,
      "step": 1221
    },
    {
      "epoch": 5.710280373831775,
      "grad_norm": 0.3743816614151001,
      "learning_rate": 0.00014299065420560745,
      "loss": 0.1489,
      "step": 1222
    },
    {
      "epoch": 5.714953271028038,
      "grad_norm": 0.46605077385902405,
      "learning_rate": 0.00014283489096573207,
      "loss": 0.1732,
      "step": 1223
    },
    {
      "epoch": 5.719626168224299,
      "grad_norm": 0.3884493410587311,
      "learning_rate": 0.00014267912772585669,
      "loss": 0.1868,
      "step": 1224
    },
    {
      "epoch": 5.724299065420561,
      "grad_norm": 0.312119722366333,
      "learning_rate": 0.0001425233644859813,
      "loss": 0.1629,
      "step": 1225
    },
    {
      "epoch": 5.728971962616822,
      "grad_norm": 0.34736350178718567,
      "learning_rate": 0.00014236760124610592,
      "loss": 0.1426,
      "step": 1226
    },
    {
      "epoch": 5.733644859813084,
      "grad_norm": 0.4214232861995697,
      "learning_rate": 0.0001422118380062305,
      "loss": 0.1592,
      "step": 1227
    },
    {
      "epoch": 5.738317757009346,
      "grad_norm": 0.2777852714061737,
      "learning_rate": 0.00014205607476635513,
      "loss": 0.1328,
      "step": 1228
    },
    {
      "epoch": 5.742990654205608,
      "grad_norm": 0.3428678512573242,
      "learning_rate": 0.00014190031152647975,
      "loss": 0.1484,
      "step": 1229
    },
    {
      "epoch": 5.747663551401869,
      "grad_norm": 0.3589861989021301,
      "learning_rate": 0.00014174454828660434,
      "loss": 0.1218,
      "step": 1230
    },
    {
      "epoch": 5.752336448598131,
      "grad_norm": 0.2919571101665497,
      "learning_rate": 0.00014158878504672896,
      "loss": 0.1208,
      "step": 1231
    },
    {
      "epoch": 5.757009345794392,
      "grad_norm": 0.4836157262325287,
      "learning_rate": 0.00014143302180685358,
      "loss": 0.2029,
      "step": 1232
    },
    {
      "epoch": 5.761682242990654,
      "grad_norm": 0.4080827534198761,
      "learning_rate": 0.00014127725856697817,
      "loss": 0.171,
      "step": 1233
    },
    {
      "epoch": 5.766355140186916,
      "grad_norm": 0.3679051697254181,
      "learning_rate": 0.00014112149532710279,
      "loss": 0.1471,
      "step": 1234
    },
    {
      "epoch": 5.771028037383178,
      "grad_norm": 0.32493582367897034,
      "learning_rate": 0.0001409657320872274,
      "loss": 0.1464,
      "step": 1235
    },
    {
      "epoch": 5.775700934579439,
      "grad_norm": 0.36336538195610046,
      "learning_rate": 0.000140809968847352,
      "loss": 0.1605,
      "step": 1236
    },
    {
      "epoch": 5.780373831775701,
      "grad_norm": 0.3966571092605591,
      "learning_rate": 0.0001406542056074766,
      "loss": 0.1353,
      "step": 1237
    },
    {
      "epoch": 5.785046728971962,
      "grad_norm": 0.5422837734222412,
      "learning_rate": 0.00014049844236760123,
      "loss": 0.1851,
      "step": 1238
    },
    {
      "epoch": 5.789719626168225,
      "grad_norm": 0.31799742579460144,
      "learning_rate": 0.00014034267912772585,
      "loss": 0.1185,
      "step": 1239
    },
    {
      "epoch": 5.794392523364486,
      "grad_norm": 0.33003973960876465,
      "learning_rate": 0.00014018691588785044,
      "loss": 0.1366,
      "step": 1240
    },
    {
      "epoch": 5.799065420560748,
      "grad_norm": 0.3706476390361786,
      "learning_rate": 0.00014003115264797506,
      "loss": 0.1525,
      "step": 1241
    },
    {
      "epoch": 5.803738317757009,
      "grad_norm": 0.34959831833839417,
      "learning_rate": 0.00013987538940809968,
      "loss": 0.1321,
      "step": 1242
    },
    {
      "epoch": 5.808411214953271,
      "grad_norm": 0.2766478359699249,
      "learning_rate": 0.0001397196261682243,
      "loss": 0.1113,
      "step": 1243
    },
    {
      "epoch": 5.813084112149532,
      "grad_norm": 0.3617749810218811,
      "learning_rate": 0.0001395638629283489,
      "loss": 0.1251,
      "step": 1244
    },
    {
      "epoch": 5.817757009345795,
      "grad_norm": 0.3763011693954468,
      "learning_rate": 0.0001394080996884735,
      "loss": 0.1325,
      "step": 1245
    },
    {
      "epoch": 5.822429906542056,
      "grad_norm": 0.4620795249938965,
      "learning_rate": 0.00013925233644859812,
      "loss": 0.1707,
      "step": 1246
    },
    {
      "epoch": 5.827102803738318,
      "grad_norm": 0.4169250726699829,
      "learning_rate": 0.00013909657320872274,
      "loss": 0.1906,
      "step": 1247
    },
    {
      "epoch": 5.831775700934579,
      "grad_norm": 0.3438999354839325,
      "learning_rate": 0.00013894080996884733,
      "loss": 0.134,
      "step": 1248
    },
    {
      "epoch": 5.836448598130842,
      "grad_norm": 0.42500001192092896,
      "learning_rate": 0.00013878504672897195,
      "loss": 0.1506,
      "step": 1249
    },
    {
      "epoch": 5.841121495327103,
      "grad_norm": 0.32269391417503357,
      "learning_rate": 0.00013862928348909657,
      "loss": 0.1383,
      "step": 1250
    },
    {
      "epoch": 5.845794392523365,
      "grad_norm": 0.3538845181465149,
      "learning_rate": 0.00013847352024922116,
      "loss": 0.1478,
      "step": 1251
    },
    {
      "epoch": 5.850467289719626,
      "grad_norm": 0.33687886595726013,
      "learning_rate": 0.00013831775700934578,
      "loss": 0.1359,
      "step": 1252
    },
    {
      "epoch": 5.855140186915888,
      "grad_norm": 0.3915136456489563,
      "learning_rate": 0.0001381619937694704,
      "loss": 0.178,
      "step": 1253
    },
    {
      "epoch": 5.859813084112149,
      "grad_norm": 0.3632078468799591,
      "learning_rate": 0.00013800623052959499,
      "loss": 0.1632,
      "step": 1254
    },
    {
      "epoch": 5.864485981308412,
      "grad_norm": 0.3658123314380646,
      "learning_rate": 0.0001378504672897196,
      "loss": 0.1236,
      "step": 1255
    },
    {
      "epoch": 5.869158878504673,
      "grad_norm": 0.4115709662437439,
      "learning_rate": 0.00013769470404984422,
      "loss": 0.1763,
      "step": 1256
    },
    {
      "epoch": 5.873831775700935,
      "grad_norm": 0.3696330189704895,
      "learning_rate": 0.00013753894080996884,
      "loss": 0.1607,
      "step": 1257
    },
    {
      "epoch": 5.878504672897196,
      "grad_norm": 0.3469567894935608,
      "learning_rate": 0.00013738317757009343,
      "loss": 0.143,
      "step": 1258
    },
    {
      "epoch": 5.883177570093458,
      "grad_norm": 0.4418412744998932,
      "learning_rate": 0.00013722741433021805,
      "loss": 0.1696,
      "step": 1259
    },
    {
      "epoch": 5.88785046728972,
      "grad_norm": 0.32690826058387756,
      "learning_rate": 0.00013707165109034267,
      "loss": 0.1186,
      "step": 1260
    },
    {
      "epoch": 5.892523364485982,
      "grad_norm": 0.35330113768577576,
      "learning_rate": 0.00013691588785046729,
      "loss": 0.1595,
      "step": 1261
    },
    {
      "epoch": 5.897196261682243,
      "grad_norm": 0.36959108710289,
      "learning_rate": 0.0001367601246105919,
      "loss": 0.1694,
      "step": 1262
    },
    {
      "epoch": 5.901869158878505,
      "grad_norm": 0.3103733956813812,
      "learning_rate": 0.0001366043613707165,
      "loss": 0.1408,
      "step": 1263
    },
    {
      "epoch": 5.906542056074766,
      "grad_norm": 0.44491252303123474,
      "learning_rate": 0.0001364485981308411,
      "loss": 0.1836,
      "step": 1264
    },
    {
      "epoch": 5.911214953271028,
      "grad_norm": 0.3647441864013672,
      "learning_rate": 0.00013629283489096573,
      "loss": 0.1307,
      "step": 1265
    },
    {
      "epoch": 5.91588785046729,
      "grad_norm": 0.3223067820072174,
      "learning_rate": 0.00013613707165109032,
      "loss": 0.1377,
      "step": 1266
    },
    {
      "epoch": 5.920560747663552,
      "grad_norm": 0.3445090055465698,
      "learning_rate": 0.00013598130841121494,
      "loss": 0.1582,
      "step": 1267
    },
    {
      "epoch": 5.925233644859813,
      "grad_norm": 0.363878458738327,
      "learning_rate": 0.00013582554517133956,
      "loss": 0.167,
      "step": 1268
    },
    {
      "epoch": 5.929906542056075,
      "grad_norm": 0.4236294627189636,
      "learning_rate": 0.00013566978193146415,
      "loss": 0.1461,
      "step": 1269
    },
    {
      "epoch": 5.934579439252336,
      "grad_norm": 0.25060293078422546,
      "learning_rate": 0.00013551401869158877,
      "loss": 0.1151,
      "step": 1270
    },
    {
      "epoch": 5.9392523364485985,
      "grad_norm": 0.32384803891181946,
      "learning_rate": 0.00013535825545171339,
      "loss": 0.1568,
      "step": 1271
    },
    {
      "epoch": 5.94392523364486,
      "grad_norm": 0.39375239610671997,
      "learning_rate": 0.00013520249221183798,
      "loss": 0.1517,
      "step": 1272
    },
    {
      "epoch": 5.9485981308411215,
      "grad_norm": 0.3888261616230011,
      "learning_rate": 0.0001350467289719626,
      "loss": 0.1471,
      "step": 1273
    },
    {
      "epoch": 5.953271028037383,
      "grad_norm": 0.356557697057724,
      "learning_rate": 0.0001348909657320872,
      "loss": 0.1274,
      "step": 1274
    },
    {
      "epoch": 5.957943925233645,
      "grad_norm": 0.3961380422115326,
      "learning_rate": 0.00013473520249221183,
      "loss": 0.1287,
      "step": 1275
    },
    {
      "epoch": 5.962616822429906,
      "grad_norm": 0.3607034683227539,
      "learning_rate": 0.00013457943925233642,
      "loss": 0.1503,
      "step": 1276
    },
    {
      "epoch": 5.9672897196261685,
      "grad_norm": 0.31159645318984985,
      "learning_rate": 0.00013442367601246104,
      "loss": 0.1181,
      "step": 1277
    },
    {
      "epoch": 5.97196261682243,
      "grad_norm": 0.3247295320034027,
      "learning_rate": 0.00013426791277258566,
      "loss": 0.1313,
      "step": 1278
    },
    {
      "epoch": 5.9766355140186915,
      "grad_norm": 0.33622482419013977,
      "learning_rate": 0.00013411214953271028,
      "loss": 0.1289,
      "step": 1279
    },
    {
      "epoch": 5.981308411214953,
      "grad_norm": 0.4282669126987457,
      "learning_rate": 0.0001339563862928349,
      "loss": 0.175,
      "step": 1280
    },
    {
      "epoch": 5.9859813084112155,
      "grad_norm": 0.40615442395210266,
      "learning_rate": 0.00013380062305295949,
      "loss": 0.1465,
      "step": 1281
    },
    {
      "epoch": 5.990654205607477,
      "grad_norm": 0.35518211126327515,
      "learning_rate": 0.0001336448598130841,
      "loss": 0.1502,
      "step": 1282
    },
    {
      "epoch": 5.9953271028037385,
      "grad_norm": 0.3890893757343292,
      "learning_rate": 0.00013348909657320872,
      "loss": 0.1699,
      "step": 1283
    },
    {
      "epoch": 6.0,
      "grad_norm": 0.7758963704109192,
      "learning_rate": 0.0001333333333333333,
      "loss": 0.2187,
      "step": 1284
    },
    {
      "epoch": 6.0,
      "eval_loss": 0.2299443781375885,
      "eval_runtime": 53.1157,
      "eval_samples_per_second": 10.392,
      "eval_steps_per_second": 1.299,
      "step": 1284
    },
    {
      "epoch": 6.0046728971962615,
      "grad_norm": 0.3233133852481842,
      "learning_rate": 0.00013317757009345793,
      "loss": 0.1271,
      "step": 1285
    },
    {
      "epoch": 6.009345794392523,
      "grad_norm": 0.329794704914093,
      "learning_rate": 0.00013302180685358255,
      "loss": 0.1537,
      "step": 1286
    },
    {
      "epoch": 6.014018691588785,
      "grad_norm": 0.35253265500068665,
      "learning_rate": 0.00013286604361370714,
      "loss": 0.1306,
      "step": 1287
    },
    {
      "epoch": 6.018691588785047,
      "grad_norm": 0.3276570737361908,
      "learning_rate": 0.00013271028037383176,
      "loss": 0.1287,
      "step": 1288
    },
    {
      "epoch": 6.0233644859813085,
      "grad_norm": 0.3356238007545471,
      "learning_rate": 0.00013255451713395638,
      "loss": 0.1357,
      "step": 1289
    },
    {
      "epoch": 6.02803738317757,
      "grad_norm": 0.39671963453292847,
      "learning_rate": 0.00013239875389408097,
      "loss": 0.1566,
      "step": 1290
    },
    {
      "epoch": 6.0327102803738315,
      "grad_norm": 0.37464040517807007,
      "learning_rate": 0.00013224299065420559,
      "loss": 0.1438,
      "step": 1291
    },
    {
      "epoch": 6.037383177570093,
      "grad_norm": 0.32288721203804016,
      "learning_rate": 0.0001320872274143302,
      "loss": 0.1346,
      "step": 1292
    },
    {
      "epoch": 6.042056074766355,
      "grad_norm": 0.32436448335647583,
      "learning_rate": 0.00013193146417445482,
      "loss": 0.136,
      "step": 1293
    },
    {
      "epoch": 6.046728971962617,
      "grad_norm": 0.39201968908309937,
      "learning_rate": 0.0001317757009345794,
      "loss": 0.1598,
      "step": 1294
    },
    {
      "epoch": 6.0514018691588785,
      "grad_norm": 0.36952632665634155,
      "learning_rate": 0.00013161993769470403,
      "loss": 0.1355,
      "step": 1295
    },
    {
      "epoch": 6.05607476635514,
      "grad_norm": 0.4367992579936981,
      "learning_rate": 0.00013146417445482865,
      "loss": 0.1452,
      "step": 1296
    },
    {
      "epoch": 6.0607476635514015,
      "grad_norm": 0.34225785732269287,
      "learning_rate": 0.00013130841121495327,
      "loss": 0.1368,
      "step": 1297
    },
    {
      "epoch": 6.065420560747664,
      "grad_norm": 0.36695581674575806,
      "learning_rate": 0.00013115264797507788,
      "loss": 0.116,
      "step": 1298
    },
    {
      "epoch": 6.070093457943925,
      "grad_norm": 0.3387726843357086,
      "learning_rate": 0.00013099688473520248,
      "loss": 0.1538,
      "step": 1299
    },
    {
      "epoch": 6.074766355140187,
      "grad_norm": 0.3468376696109772,
      "learning_rate": 0.0001308411214953271,
      "loss": 0.1351,
      "step": 1300
    },
    {
      "epoch": 6.079439252336448,
      "grad_norm": 0.3000911474227905,
      "learning_rate": 0.0001306853582554517,
      "loss": 0.1037,
      "step": 1301
    },
    {
      "epoch": 6.08411214953271,
      "grad_norm": 0.351893812417984,
      "learning_rate": 0.0001305295950155763,
      "loss": 0.1267,
      "step": 1302
    },
    {
      "epoch": 6.088785046728972,
      "grad_norm": 0.4186249077320099,
      "learning_rate": 0.00013037383177570092,
      "loss": 0.1496,
      "step": 1303
    },
    {
      "epoch": 6.093457943925234,
      "grad_norm": 0.39493095874786377,
      "learning_rate": 0.00013021806853582554,
      "loss": 0.1173,
      "step": 1304
    },
    {
      "epoch": 6.098130841121495,
      "grad_norm": 0.29849404096603394,
      "learning_rate": 0.00013006230529595013,
      "loss": 0.1169,
      "step": 1305
    },
    {
      "epoch": 6.102803738317757,
      "grad_norm": 0.33611389994621277,
      "learning_rate": 0.00012990654205607475,
      "loss": 0.1254,
      "step": 1306
    },
    {
      "epoch": 6.107476635514018,
      "grad_norm": 0.3405843675136566,
      "learning_rate": 0.00012975077881619937,
      "loss": 0.1376,
      "step": 1307
    },
    {
      "epoch": 6.11214953271028,
      "grad_norm": 0.37813130021095276,
      "learning_rate": 0.00012959501557632396,
      "loss": 0.1533,
      "step": 1308
    },
    {
      "epoch": 6.116822429906542,
      "grad_norm": 0.3993242681026459,
      "learning_rate": 0.00012943925233644858,
      "loss": 0.127,
      "step": 1309
    },
    {
      "epoch": 6.121495327102804,
      "grad_norm": 0.35559511184692383,
      "learning_rate": 0.0001292834890965732,
      "loss": 0.1443,
      "step": 1310
    },
    {
      "epoch": 6.126168224299065,
      "grad_norm": 0.30900701880455017,
      "learning_rate": 0.0001291277258566978,
      "loss": 0.1109,
      "step": 1311
    },
    {
      "epoch": 6.130841121495327,
      "grad_norm": 0.3662550151348114,
      "learning_rate": 0.0001289719626168224,
      "loss": 0.1341,
      "step": 1312
    },
    {
      "epoch": 6.135514018691588,
      "grad_norm": 0.38741353154182434,
      "learning_rate": 0.00012881619937694702,
      "loss": 0.1391,
      "step": 1313
    },
    {
      "epoch": 6.140186915887851,
      "grad_norm": 0.3532293438911438,
      "learning_rate": 0.00012866043613707164,
      "loss": 0.1361,
      "step": 1314
    },
    {
      "epoch": 6.144859813084112,
      "grad_norm": 0.424978107213974,
      "learning_rate": 0.00012850467289719626,
      "loss": 0.1308,
      "step": 1315
    },
    {
      "epoch": 6.149532710280374,
      "grad_norm": 0.359639436006546,
      "learning_rate": 0.00012834890965732088,
      "loss": 0.1132,
      "step": 1316
    },
    {
      "epoch": 6.154205607476635,
      "grad_norm": 0.33244651556015015,
      "learning_rate": 0.00012819314641744547,
      "loss": 0.1291,
      "step": 1317
    },
    {
      "epoch": 6.158878504672897,
      "grad_norm": 0.3439841568470001,
      "learning_rate": 0.00012803738317757008,
      "loss": 0.137,
      "step": 1318
    },
    {
      "epoch": 6.163551401869159,
      "grad_norm": 0.3768748342990875,
      "learning_rate": 0.0001278816199376947,
      "loss": 0.1142,
      "step": 1319
    },
    {
      "epoch": 6.168224299065421,
      "grad_norm": 0.4323919415473938,
      "learning_rate": 0.0001277258566978193,
      "loss": 0.1569,
      "step": 1320
    },
    {
      "epoch": 6.172897196261682,
      "grad_norm": 0.3940419554710388,
      "learning_rate": 0.0001275700934579439,
      "loss": 0.1262,
      "step": 1321
    },
    {
      "epoch": 6.177570093457944,
      "grad_norm": 0.386565625667572,
      "learning_rate": 0.00012741433021806853,
      "loss": 0.1426,
      "step": 1322
    },
    {
      "epoch": 6.182242990654205,
      "grad_norm": 0.38265350461006165,
      "learning_rate": 0.00012725856697819312,
      "loss": 0.1412,
      "step": 1323
    },
    {
      "epoch": 6.186915887850467,
      "grad_norm": 0.4203249216079712,
      "learning_rate": 0.00012710280373831774,
      "loss": 0.1715,
      "step": 1324
    },
    {
      "epoch": 6.191588785046729,
      "grad_norm": 0.362034410238266,
      "learning_rate": 0.00012694704049844236,
      "loss": 0.1281,
      "step": 1325
    },
    {
      "epoch": 6.196261682242991,
      "grad_norm": 0.38413509726524353,
      "learning_rate": 0.00012679127725856695,
      "loss": 0.1293,
      "step": 1326
    },
    {
      "epoch": 6.200934579439252,
      "grad_norm": 0.3592342734336853,
      "learning_rate": 0.00012663551401869157,
      "loss": 0.1514,
      "step": 1327
    },
    {
      "epoch": 6.205607476635514,
      "grad_norm": 0.5251092910766602,
      "learning_rate": 0.00012647975077881618,
      "loss": 0.1171,
      "step": 1328
    },
    {
      "epoch": 6.210280373831775,
      "grad_norm": 0.43842071294784546,
      "learning_rate": 0.0001263239875389408,
      "loss": 0.111,
      "step": 1329
    },
    {
      "epoch": 6.214953271028038,
      "grad_norm": 0.3465244472026825,
      "learning_rate": 0.0001261682242990654,
      "loss": 0.1233,
      "step": 1330
    },
    {
      "epoch": 6.219626168224299,
      "grad_norm": 0.37310296297073364,
      "learning_rate": 0.00012601246105919,
      "loss": 0.1235,
      "step": 1331
    },
    {
      "epoch": 6.224299065420561,
      "grad_norm": 0.3472757339477539,
      "learning_rate": 0.00012585669781931463,
      "loss": 0.1039,
      "step": 1332
    },
    {
      "epoch": 6.228971962616822,
      "grad_norm": 0.38387471437454224,
      "learning_rate": 0.00012570093457943925,
      "loss": 0.1396,
      "step": 1333
    },
    {
      "epoch": 6.233644859813084,
      "grad_norm": 0.34629330039024353,
      "learning_rate": 0.00012554517133956387,
      "loss": 0.1117,
      "step": 1334
    },
    {
      "epoch": 6.238317757009346,
      "grad_norm": 0.3443394601345062,
      "learning_rate": 0.00012538940809968846,
      "loss": 0.1085,
      "step": 1335
    },
    {
      "epoch": 6.242990654205608,
      "grad_norm": 0.6192818880081177,
      "learning_rate": 0.00012523364485981308,
      "loss": 0.1975,
      "step": 1336
    },
    {
      "epoch": 6.247663551401869,
      "grad_norm": 0.47672656178474426,
      "learning_rate": 0.0001250778816199377,
      "loss": 0.1483,
      "step": 1337
    },
    {
      "epoch": 6.252336448598131,
      "grad_norm": 0.3961414396762848,
      "learning_rate": 0.0001249221183800623,
      "loss": 0.127,
      "step": 1338
    },
    {
      "epoch": 6.257009345794392,
      "grad_norm": 0.4576268792152405,
      "learning_rate": 0.0001247663551401869,
      "loss": 0.1514,
      "step": 1339
    },
    {
      "epoch": 6.261682242990654,
      "grad_norm": 0.44110867381095886,
      "learning_rate": 0.00012461059190031152,
      "loss": 0.1352,
      "step": 1340
    },
    {
      "epoch": 6.266355140186916,
      "grad_norm": 0.43475770950317383,
      "learning_rate": 0.00012445482866043614,
      "loss": 0.1485,
      "step": 1341
    },
    {
      "epoch": 6.271028037383178,
      "grad_norm": 0.4459713399410248,
      "learning_rate": 0.00012429906542056073,
      "loss": 0.1448,
      "step": 1342
    },
    {
      "epoch": 6.275700934579439,
      "grad_norm": 0.423613965511322,
      "learning_rate": 0.00012414330218068535,
      "loss": 0.1521,
      "step": 1343
    },
    {
      "epoch": 6.280373831775701,
      "grad_norm": 0.3591921031475067,
      "learning_rate": 0.00012398753894080997,
      "loss": 0.133,
      "step": 1344
    },
    {
      "epoch": 6.285046728971962,
      "grad_norm": 0.49037736654281616,
      "learning_rate": 0.00012383177570093456,
      "loss": 0.1585,
      "step": 1345
    },
    {
      "epoch": 6.289719626168225,
      "grad_norm": 0.384029358625412,
      "learning_rate": 0.00012367601246105918,
      "loss": 0.1204,
      "step": 1346
    },
    {
      "epoch": 6.294392523364486,
      "grad_norm": 0.36513856053352356,
      "learning_rate": 0.0001235202492211838,
      "loss": 0.1447,
      "step": 1347
    },
    {
      "epoch": 6.299065420560748,
      "grad_norm": 0.3230687379837036,
      "learning_rate": 0.00012336448598130838,
      "loss": 0.1254,
      "step": 1348
    },
    {
      "epoch": 6.303738317757009,
      "grad_norm": 0.3727380931377411,
      "learning_rate": 0.000123208722741433,
      "loss": 0.1492,
      "step": 1349
    },
    {
      "epoch": 6.308411214953271,
      "grad_norm": 0.3488401770591736,
      "learning_rate": 0.00012305295950155762,
      "loss": 0.1361,
      "step": 1350
    },
    {
      "epoch": 6.313084112149532,
      "grad_norm": 0.3074035942554474,
      "learning_rate": 0.00012289719626168224,
      "loss": 0.0936,
      "step": 1351
    },
    {
      "epoch": 6.317757009345795,
      "grad_norm": 0.4365180730819702,
      "learning_rate": 0.00012274143302180686,
      "loss": 0.1718,
      "step": 1352
    },
    {
      "epoch": 6.322429906542056,
      "grad_norm": 0.4722096025943756,
      "learning_rate": 0.00012258566978193145,
      "loss": 0.1666,
      "step": 1353
    },
    {
      "epoch": 6.327102803738318,
      "grad_norm": 0.5320865511894226,
      "learning_rate": 0.00012242990654205607,
      "loss": 0.1403,
      "step": 1354
    },
    {
      "epoch": 6.331775700934579,
      "grad_norm": 0.33020079135894775,
      "learning_rate": 0.00012227414330218068,
      "loss": 0.114,
      "step": 1355
    },
    {
      "epoch": 6.336448598130841,
      "grad_norm": 0.45638030767440796,
      "learning_rate": 0.0001221183800623053,
      "loss": 0.162,
      "step": 1356
    },
    {
      "epoch": 6.341121495327103,
      "grad_norm": 0.42183077335357666,
      "learning_rate": 0.0001219626168224299,
      "loss": 0.1294,
      "step": 1357
    },
    {
      "epoch": 6.345794392523365,
      "grad_norm": 0.3487566113471985,
      "learning_rate": 0.00012180685358255451,
      "loss": 0.1323,
      "step": 1358
    },
    {
      "epoch": 6.350467289719626,
      "grad_norm": 0.34353107213974,
      "learning_rate": 0.00012165109034267913,
      "loss": 0.1345,
      "step": 1359
    },
    {
      "epoch": 6.355140186915888,
      "grad_norm": 0.3214752972126007,
      "learning_rate": 0.00012149532710280372,
      "loss": 0.1172,
      "step": 1360
    },
    {
      "epoch": 6.359813084112149,
      "grad_norm": 0.41921377182006836,
      "learning_rate": 0.00012133956386292834,
      "loss": 0.1461,
      "step": 1361
    },
    {
      "epoch": 6.364485981308412,
      "grad_norm": 0.4379708468914032,
      "learning_rate": 0.00012118380062305296,
      "loss": 0.1709,
      "step": 1362
    },
    {
      "epoch": 6.369158878504673,
      "grad_norm": 0.4065922200679779,
      "learning_rate": 0.00012102803738317756,
      "loss": 0.1243,
      "step": 1363
    },
    {
      "epoch": 6.373831775700935,
      "grad_norm": 0.40558305382728577,
      "learning_rate": 0.00012087227414330217,
      "loss": 0.1603,
      "step": 1364
    },
    {
      "epoch": 6.378504672897196,
      "grad_norm": 0.30082470178604126,
      "learning_rate": 0.00012071651090342678,
      "loss": 0.1267,
      "step": 1365
    },
    {
      "epoch": 6.383177570093458,
      "grad_norm": 0.35621967911720276,
      "learning_rate": 0.00012056074766355139,
      "loss": 0.1539,
      "step": 1366
    },
    {
      "epoch": 6.38785046728972,
      "grad_norm": 0.43298229575157166,
      "learning_rate": 0.000120404984423676,
      "loss": 0.1695,
      "step": 1367
    },
    {
      "epoch": 6.392523364485982,
      "grad_norm": 0.3987944722175598,
      "learning_rate": 0.00012024922118380062,
      "loss": 0.1422,
      "step": 1368
    },
    {
      "epoch": 6.397196261682243,
      "grad_norm": 0.28614112734794617,
      "learning_rate": 0.00012009345794392522,
      "loss": 0.1083,
      "step": 1369
    },
    {
      "epoch": 6.401869158878505,
      "grad_norm": 0.36202171444892883,
      "learning_rate": 0.00011993769470404983,
      "loss": 0.1377,
      "step": 1370
    },
    {
      "epoch": 6.406542056074766,
      "grad_norm": 0.34276533126831055,
      "learning_rate": 0.00011978193146417445,
      "loss": 0.1236,
      "step": 1371
    },
    {
      "epoch": 6.411214953271028,
      "grad_norm": 0.2958078980445862,
      "learning_rate": 0.00011962616822429906,
      "loss": 0.1285,
      "step": 1372
    },
    {
      "epoch": 6.41588785046729,
      "grad_norm": 0.4617624878883362,
      "learning_rate": 0.00011947040498442366,
      "loss": 0.1369,
      "step": 1373
    },
    {
      "epoch": 6.420560747663552,
      "grad_norm": 0.3729507029056549,
      "learning_rate": 0.00011931464174454828,
      "loss": 0.1407,
      "step": 1374
    },
    {
      "epoch": 6.425233644859813,
      "grad_norm": 0.33231204748153687,
      "learning_rate": 0.00011915887850467288,
      "loss": 0.1043,
      "step": 1375
    },
    {
      "epoch": 6.429906542056075,
      "grad_norm": 0.41734617948532104,
      "learning_rate": 0.0001190031152647975,
      "loss": 0.1671,
      "step": 1376
    },
    {
      "epoch": 6.434579439252336,
      "grad_norm": 0.4488309323787689,
      "learning_rate": 0.00011884735202492212,
      "loss": 0.1609,
      "step": 1377
    },
    {
      "epoch": 6.4392523364485985,
      "grad_norm": 0.4324268102645874,
      "learning_rate": 0.00011869158878504671,
      "loss": 0.1385,
      "step": 1378
    },
    {
      "epoch": 6.44392523364486,
      "grad_norm": 0.33983922004699707,
      "learning_rate": 0.00011853582554517133,
      "loss": 0.1387,
      "step": 1379
    },
    {
      "epoch": 6.4485981308411215,
      "grad_norm": 0.3877873420715332,
      "learning_rate": 0.00011838006230529595,
      "loss": 0.1086,
      "step": 1380
    },
    {
      "epoch": 6.453271028037383,
      "grad_norm": 0.4849761724472046,
      "learning_rate": 0.00011822429906542055,
      "loss": 0.1433,
      "step": 1381
    },
    {
      "epoch": 6.457943925233645,
      "grad_norm": 0.5645804405212402,
      "learning_rate": 0.00011806853582554516,
      "loss": 0.1693,
      "step": 1382
    },
    {
      "epoch": 6.462616822429906,
      "grad_norm": 0.37264347076416016,
      "learning_rate": 0.00011791277258566977,
      "loss": 0.141,
      "step": 1383
    },
    {
      "epoch": 6.4672897196261685,
      "grad_norm": 0.374455064535141,
      "learning_rate": 0.00011775700934579438,
      "loss": 0.1361,
      "step": 1384
    },
    {
      "epoch": 6.47196261682243,
      "grad_norm": 0.4763686954975128,
      "learning_rate": 0.000117601246105919,
      "loss": 0.1346,
      "step": 1385
    },
    {
      "epoch": 6.4766355140186915,
      "grad_norm": 0.3709576427936554,
      "learning_rate": 0.00011744548286604362,
      "loss": 0.1241,
      "step": 1386
    },
    {
      "epoch": 6.481308411214953,
      "grad_norm": 0.3528410792350769,
      "learning_rate": 0.0001172897196261682,
      "loss": 0.1423,
      "step": 1387
    },
    {
      "epoch": 6.485981308411215,
      "grad_norm": 0.3863382637500763,
      "learning_rate": 0.00011713395638629282,
      "loss": 0.1184,
      "step": 1388
    },
    {
      "epoch": 6.490654205607477,
      "grad_norm": 0.35871925950050354,
      "learning_rate": 0.00011697819314641744,
      "loss": 0.1582,
      "step": 1389
    },
    {
      "epoch": 6.4953271028037385,
      "grad_norm": 0.4983059763908386,
      "learning_rate": 0.00011682242990654205,
      "loss": 0.1747,
      "step": 1390
    },
    {
      "epoch": 6.5,
      "grad_norm": 0.38831785321235657,
      "learning_rate": 0.00011666666666666665,
      "loss": 0.1261,
      "step": 1391
    },
    {
      "epoch": 6.5046728971962615,
      "grad_norm": 0.3734223544597626,
      "learning_rate": 0.00011651090342679127,
      "loss": 0.136,
      "step": 1392
    },
    {
      "epoch": 6.509345794392523,
      "grad_norm": 0.33410775661468506,
      "learning_rate": 0.00011635514018691587,
      "loss": 0.127,
      "step": 1393
    },
    {
      "epoch": 6.5140186915887845,
      "grad_norm": 0.4830968976020813,
      "learning_rate": 0.00011619937694704049,
      "loss": 0.1578,
      "step": 1394
    },
    {
      "epoch": 6.518691588785047,
      "grad_norm": 0.36528876423835754,
      "learning_rate": 0.00011604361370716511,
      "loss": 0.118,
      "step": 1395
    },
    {
      "epoch": 6.5233644859813085,
      "grad_norm": 0.4657859802246094,
      "learning_rate": 0.0001158878504672897,
      "loss": 0.1792,
      "step": 1396
    },
    {
      "epoch": 6.52803738317757,
      "grad_norm": 0.38935065269470215,
      "learning_rate": 0.00011573208722741432,
      "loss": 0.1046,
      "step": 1397
    },
    {
      "epoch": 6.5327102803738315,
      "grad_norm": 0.38880133628845215,
      "learning_rate": 0.00011557632398753894,
      "loss": 0.1438,
      "step": 1398
    },
    {
      "epoch": 6.537383177570094,
      "grad_norm": 0.3659539222717285,
      "learning_rate": 0.00011542056074766354,
      "loss": 0.1386,
      "step": 1399
    },
    {
      "epoch": 6.542056074766355,
      "grad_norm": 0.3173750936985016,
      "learning_rate": 0.00011526479750778815,
      "loss": 0.1109,
      "step": 1400
    },
    {
      "epoch": 6.546728971962617,
      "grad_norm": 0.32990655303001404,
      "learning_rate": 0.00011510903426791277,
      "loss": 0.0991,
      "step": 1401
    },
    {
      "epoch": 6.5514018691588785,
      "grad_norm": 0.3131718933582306,
      "learning_rate": 0.00011495327102803737,
      "loss": 0.1191,
      "step": 1402
    },
    {
      "epoch": 6.55607476635514,
      "grad_norm": 0.47057053446769714,
      "learning_rate": 0.00011479750778816199,
      "loss": 0.1556,
      "step": 1403
    },
    {
      "epoch": 6.5607476635514015,
      "grad_norm": 0.3381328582763672,
      "learning_rate": 0.0001146417445482866,
      "loss": 0.1092,
      "step": 1404
    },
    {
      "epoch": 6.565420560747664,
      "grad_norm": 0.5632030963897705,
      "learning_rate": 0.0001144859813084112,
      "loss": 0.1876,
      "step": 1405
    },
    {
      "epoch": 6.570093457943925,
      "grad_norm": 0.39255085587501526,
      "learning_rate": 0.00011433021806853582,
      "loss": 0.1629,
      "step": 1406
    },
    {
      "epoch": 6.574766355140187,
      "grad_norm": 0.3620837926864624,
      "learning_rate": 0.00011417445482866043,
      "loss": 0.1348,
      "step": 1407
    },
    {
      "epoch": 6.579439252336448,
      "grad_norm": 0.4054051637649536,
      "learning_rate": 0.00011401869158878504,
      "loss": 0.1323,
      "step": 1408
    },
    {
      "epoch": 6.58411214953271,
      "grad_norm": 0.44697195291519165,
      "learning_rate": 0.00011386292834890964,
      "loss": 0.1762,
      "step": 1409
    },
    {
      "epoch": 6.588785046728972,
      "grad_norm": 0.36682021617889404,
      "learning_rate": 0.00011370716510903426,
      "loss": 0.1139,
      "step": 1410
    },
    {
      "epoch": 6.593457943925234,
      "grad_norm": 0.41624167561531067,
      "learning_rate": 0.00011355140186915887,
      "loss": 0.1365,
      "step": 1411
    },
    {
      "epoch": 6.598130841121495,
      "grad_norm": 0.45027652382850647,
      "learning_rate": 0.00011339563862928348,
      "loss": 0.1594,
      "step": 1412
    },
    {
      "epoch": 6.602803738317757,
      "grad_norm": 0.40000900626182556,
      "learning_rate": 0.0001132398753894081,
      "loss": 0.1154,
      "step": 1413
    },
    {
      "epoch": 6.607476635514018,
      "grad_norm": 0.5508811473846436,
      "learning_rate": 0.00011308411214953269,
      "loss": 0.1705,
      "step": 1414
    },
    {
      "epoch": 6.61214953271028,
      "grad_norm": 0.3584301173686981,
      "learning_rate": 0.00011292834890965731,
      "loss": 0.1583,
      "step": 1415
    },
    {
      "epoch": 6.616822429906542,
      "grad_norm": 0.436618834733963,
      "learning_rate": 0.00011277258566978193,
      "loss": 0.1539,
      "step": 1416
    },
    {
      "epoch": 6.621495327102804,
      "grad_norm": 0.43208399415016174,
      "learning_rate": 0.00011261682242990653,
      "loss": 0.1449,
      "step": 1417
    },
    {
      "epoch": 6.626168224299065,
      "grad_norm": 0.3477308452129364,
      "learning_rate": 0.00011246105919003114,
      "loss": 0.1174,
      "step": 1418
    },
    {
      "epoch": 6.630841121495327,
      "grad_norm": 0.39071977138519287,
      "learning_rate": 0.00011230529595015576,
      "loss": 0.1514,
      "step": 1419
    },
    {
      "epoch": 6.635514018691588,
      "grad_norm": 0.44596895575523376,
      "learning_rate": 0.00011214953271028036,
      "loss": 0.1577,
      "step": 1420
    },
    {
      "epoch": 6.640186915887851,
      "grad_norm": 0.38096117973327637,
      "learning_rate": 0.00011199376947040498,
      "loss": 0.1394,
      "step": 1421
    },
    {
      "epoch": 6.644859813084112,
      "grad_norm": 0.41950979828834534,
      "learning_rate": 0.0001118380062305296,
      "loss": 0.127,
      "step": 1422
    },
    {
      "epoch": 6.649532710280374,
      "grad_norm": 0.41374972462654114,
      "learning_rate": 0.00011168224299065419,
      "loss": 0.1409,
      "step": 1423
    },
    {
      "epoch": 6.654205607476635,
      "grad_norm": 0.3478814363479614,
      "learning_rate": 0.0001115264797507788,
      "loss": 0.1371,
      "step": 1424
    },
    {
      "epoch": 6.658878504672897,
      "grad_norm": 0.331679105758667,
      "learning_rate": 0.00011137071651090342,
      "loss": 0.1213,
      "step": 1425
    },
    {
      "epoch": 6.663551401869158,
      "grad_norm": 0.3820091784000397,
      "learning_rate": 0.00011121495327102803,
      "loss": 0.142,
      "step": 1426
    },
    {
      "epoch": 6.668224299065421,
      "grad_norm": 0.3488684296607971,
      "learning_rate": 0.00011105919003115263,
      "loss": 0.1231,
      "step": 1427
    },
    {
      "epoch": 6.672897196261682,
      "grad_norm": 0.4313485622406006,
      "learning_rate": 0.00011090342679127725,
      "loss": 0.1309,
      "step": 1428
    },
    {
      "epoch": 6.677570093457944,
      "grad_norm": 0.4735722839832306,
      "learning_rate": 0.00011074766355140186,
      "loss": 0.155,
      "step": 1429
    },
    {
      "epoch": 6.682242990654205,
      "grad_norm": 0.3328905403614044,
      "learning_rate": 0.00011059190031152647,
      "loss": 0.1139,
      "step": 1430
    },
    {
      "epoch": 6.686915887850468,
      "grad_norm": 0.4468761086463928,
      "learning_rate": 0.00011043613707165109,
      "loss": 0.1483,
      "step": 1431
    },
    {
      "epoch": 6.691588785046729,
      "grad_norm": 0.46243035793304443,
      "learning_rate": 0.00011028037383177568,
      "loss": 0.1395,
      "step": 1432
    },
    {
      "epoch": 6.696261682242991,
      "grad_norm": 0.419142484664917,
      "learning_rate": 0.0001101246105919003,
      "loss": 0.1329,
      "step": 1433
    },
    {
      "epoch": 6.700934579439252,
      "grad_norm": 0.33868756890296936,
      "learning_rate": 0.00010996884735202492,
      "loss": 0.1226,
      "step": 1434
    },
    {
      "epoch": 6.705607476635514,
      "grad_norm": 0.3321992754936218,
      "learning_rate": 0.00010981308411214952,
      "loss": 0.1461,
      "step": 1435
    },
    {
      "epoch": 6.710280373831775,
      "grad_norm": 0.3994896709918976,
      "learning_rate": 0.00010965732087227413,
      "loss": 0.127,
      "step": 1436
    },
    {
      "epoch": 6.714953271028038,
      "grad_norm": 0.3768570125102997,
      "learning_rate": 0.00010950155763239875,
      "loss": 0.1249,
      "step": 1437
    },
    {
      "epoch": 6.719626168224299,
      "grad_norm": 0.3496008515357971,
      "learning_rate": 0.00010934579439252335,
      "loss": 0.1508,
      "step": 1438
    },
    {
      "epoch": 6.724299065420561,
      "grad_norm": 0.41284048557281494,
      "learning_rate": 0.00010919003115264797,
      "loss": 0.1521,
      "step": 1439
    },
    {
      "epoch": 6.728971962616822,
      "grad_norm": 0.3692072033882141,
      "learning_rate": 0.00010903426791277259,
      "loss": 0.1287,
      "step": 1440
    },
    {
      "epoch": 6.733644859813084,
      "grad_norm": 0.4792416989803314,
      "learning_rate": 0.00010887850467289718,
      "loss": 0.1643,
      "step": 1441
    },
    {
      "epoch": 6.738317757009346,
      "grad_norm": 0.39389750361442566,
      "learning_rate": 0.0001087227414330218,
      "loss": 0.138,
      "step": 1442
    },
    {
      "epoch": 6.742990654205608,
      "grad_norm": 0.36337488889694214,
      "learning_rate": 0.00010856697819314641,
      "loss": 0.1498,
      "step": 1443
    },
    {
      "epoch": 6.747663551401869,
      "grad_norm": 0.4608539640903473,
      "learning_rate": 0.00010841121495327102,
      "loss": 0.1696,
      "step": 1444
    },
    {
      "epoch": 6.752336448598131,
      "grad_norm": 0.4254809617996216,
      "learning_rate": 0.00010825545171339562,
      "loss": 0.135,
      "step": 1445
    },
    {
      "epoch": 6.757009345794392,
      "grad_norm": 0.3691612482070923,
      "learning_rate": 0.00010809968847352024,
      "loss": 0.1456,
      "step": 1446
    },
    {
      "epoch": 6.761682242990654,
      "grad_norm": 0.3716886341571808,
      "learning_rate": 0.00010794392523364485,
      "loss": 0.1343,
      "step": 1447
    },
    {
      "epoch": 6.766355140186916,
      "grad_norm": 0.3258104920387268,
      "learning_rate": 0.00010778816199376946,
      "loss": 0.1026,
      "step": 1448
    },
    {
      "epoch": 6.771028037383178,
      "grad_norm": 0.4327397346496582,
      "learning_rate": 0.00010763239875389408,
      "loss": 0.1434,
      "step": 1449
    },
    {
      "epoch": 6.775700934579439,
      "grad_norm": 0.5356411337852478,
      "learning_rate": 0.00010747663551401867,
      "loss": 0.1732,
      "step": 1450
    },
    {
      "epoch": 6.780373831775701,
      "grad_norm": 0.4386788606643677,
      "learning_rate": 0.00010732087227414329,
      "loss": 0.1532,
      "step": 1451
    },
    {
      "epoch": 6.785046728971962,
      "grad_norm": 0.3680860698223114,
      "learning_rate": 0.00010716510903426791,
      "loss": 0.1206,
      "step": 1452
    },
    {
      "epoch": 6.789719626168225,
      "grad_norm": 0.36226406693458557,
      "learning_rate": 0.00010700934579439251,
      "loss": 0.1208,
      "step": 1453
    },
    {
      "epoch": 6.794392523364486,
      "grad_norm": 0.575512707233429,
      "learning_rate": 0.00010685358255451712,
      "loss": 0.1574,
      "step": 1454
    },
    {
      "epoch": 6.799065420560748,
      "grad_norm": 0.33165243268013,
      "learning_rate": 0.00010669781931464174,
      "loss": 0.1106,
      "step": 1455
    },
    {
      "epoch": 6.803738317757009,
      "grad_norm": 0.41421473026275635,
      "learning_rate": 0.00010654205607476634,
      "loss": 0.1597,
      "step": 1456
    },
    {
      "epoch": 6.808411214953271,
      "grad_norm": 0.4188912510871887,
      "learning_rate": 0.00010638629283489096,
      "loss": 0.1483,
      "step": 1457
    },
    {
      "epoch": 6.813084112149532,
      "grad_norm": 0.4241938292980194,
      "learning_rate": 0.00010623052959501558,
      "loss": 0.1612,
      "step": 1458
    },
    {
      "epoch": 6.817757009345795,
      "grad_norm": 0.3999120593070984,
      "learning_rate": 0.00010607476635514017,
      "loss": 0.1315,
      "step": 1459
    },
    {
      "epoch": 6.822429906542056,
      "grad_norm": 0.41525086760520935,
      "learning_rate": 0.00010591900311526479,
      "loss": 0.1588,
      "step": 1460
    },
    {
      "epoch": 6.827102803738318,
      "grad_norm": 0.4104660153388977,
      "learning_rate": 0.0001057632398753894,
      "loss": 0.1643,
      "step": 1461
    },
    {
      "epoch": 6.831775700934579,
      "grad_norm": 0.40372198820114136,
      "learning_rate": 0.00010560747663551401,
      "loss": 0.1388,
      "step": 1462
    },
    {
      "epoch": 6.836448598130842,
      "grad_norm": 0.3864792287349701,
      "learning_rate": 0.00010545171339563861,
      "loss": 0.1538,
      "step": 1463
    },
    {
      "epoch": 6.841121495327103,
      "grad_norm": 0.3746497929096222,
      "learning_rate": 0.00010529595015576323,
      "loss": 0.1405,
      "step": 1464
    },
    {
      "epoch": 6.845794392523365,
      "grad_norm": 0.431461900472641,
      "learning_rate": 0.00010514018691588784,
      "loss": 0.1523,
      "step": 1465
    },
    {
      "epoch": 6.850467289719626,
      "grad_norm": 0.4112648665904999,
      "learning_rate": 0.00010498442367601246,
      "loss": 0.1619,
      "step": 1466
    },
    {
      "epoch": 6.855140186915888,
      "grad_norm": 0.44624605774879456,
      "learning_rate": 0.00010482866043613707,
      "loss": 0.1602,
      "step": 1467
    },
    {
      "epoch": 6.859813084112149,
      "grad_norm": 0.33643704652786255,
      "learning_rate": 0.00010467289719626166,
      "loss": 0.1165,
      "step": 1468
    },
    {
      "epoch": 6.864485981308412,
      "grad_norm": 0.34807512164115906,
      "learning_rate": 0.00010451713395638628,
      "loss": 0.1281,
      "step": 1469
    },
    {
      "epoch": 6.869158878504673,
      "grad_norm": 0.36221739649772644,
      "learning_rate": 0.0001043613707165109,
      "loss": 0.1415,
      "step": 1470
    },
    {
      "epoch": 6.873831775700935,
      "grad_norm": 0.3980187177658081,
      "learning_rate": 0.0001042056074766355,
      "loss": 0.1483,
      "step": 1471
    },
    {
      "epoch": 6.878504672897196,
      "grad_norm": 0.33101460337638855,
      "learning_rate": 0.00010404984423676011,
      "loss": 0.1183,
      "step": 1472
    },
    {
      "epoch": 6.883177570093458,
      "grad_norm": 0.3728027045726776,
      "learning_rate": 0.00010389408099688473,
      "loss": 0.1228,
      "step": 1473
    },
    {
      "epoch": 6.88785046728972,
      "grad_norm": 0.37580910325050354,
      "learning_rate": 0.00010373831775700933,
      "loss": 0.1302,
      "step": 1474
    },
    {
      "epoch": 6.892523364485982,
      "grad_norm": 0.3386140465736389,
      "learning_rate": 0.00010358255451713395,
      "loss": 0.1301,
      "step": 1475
    },
    {
      "epoch": 6.897196261682243,
      "grad_norm": 0.29675188660621643,
      "learning_rate": 0.00010342679127725857,
      "loss": 0.1061,
      "step": 1476
    },
    {
      "epoch": 6.901869158878505,
      "grad_norm": 0.37105029821395874,
      "learning_rate": 0.00010327102803738316,
      "loss": 0.1293,
      "step": 1477
    },
    {
      "epoch": 6.906542056074766,
      "grad_norm": 0.38858622312545776,
      "learning_rate": 0.00010311526479750778,
      "loss": 0.1363,
      "step": 1478
    },
    {
      "epoch": 6.911214953271028,
      "grad_norm": 0.49637937545776367,
      "learning_rate": 0.0001029595015576324,
      "loss": 0.1756,
      "step": 1479
    },
    {
      "epoch": 6.91588785046729,
      "grad_norm": 0.4257739186286926,
      "learning_rate": 0.000102803738317757,
      "loss": 0.1438,
      "step": 1480
    },
    {
      "epoch": 6.920560747663552,
      "grad_norm": 0.3095034658908844,
      "learning_rate": 0.0001026479750778816,
      "loss": 0.1147,
      "step": 1481
    },
    {
      "epoch": 6.925233644859813,
      "grad_norm": 0.396001398563385,
      "learning_rate": 0.00010249221183800622,
      "loss": 0.129,
      "step": 1482
    },
    {
      "epoch": 6.929906542056075,
      "grad_norm": 0.427374005317688,
      "learning_rate": 0.00010233644859813083,
      "loss": 0.1469,
      "step": 1483
    },
    {
      "epoch": 6.934579439252336,
      "grad_norm": 0.3819303810596466,
      "learning_rate": 0.00010218068535825545,
      "loss": 0.1288,
      "step": 1484
    },
    {
      "epoch": 6.9392523364485985,
      "grad_norm": 0.39799439907073975,
      "learning_rate": 0.00010202492211838006,
      "loss": 0.1481,
      "step": 1485
    },
    {
      "epoch": 6.94392523364486,
      "grad_norm": 0.34512099623680115,
      "learning_rate": 0.00010186915887850466,
      "loss": 0.1188,
      "step": 1486
    },
    {
      "epoch": 6.9485981308411215,
      "grad_norm": 0.32958099246025085,
      "learning_rate": 0.00010171339563862927,
      "loss": 0.1248,
      "step": 1487
    },
    {
      "epoch": 6.953271028037383,
      "grad_norm": 0.3531675934791565,
      "learning_rate": 0.00010155763239875389,
      "loss": 0.0923,
      "step": 1488
    },
    {
      "epoch": 6.957943925233645,
      "grad_norm": 0.3487885296344757,
      "learning_rate": 0.0001014018691588785,
      "loss": 0.1267,
      "step": 1489
    },
    {
      "epoch": 6.962616822429906,
      "grad_norm": 0.4585779309272766,
      "learning_rate": 0.0001012461059190031,
      "loss": 0.1633,
      "step": 1490
    },
    {
      "epoch": 6.9672897196261685,
      "grad_norm": 0.34583696722984314,
      "learning_rate": 0.00010109034267912772,
      "loss": 0.1015,
      "step": 1491
    },
    {
      "epoch": 6.97196261682243,
      "grad_norm": 0.3985649049282074,
      "learning_rate": 0.00010093457943925232,
      "loss": 0.131,
      "step": 1492
    },
    {
      "epoch": 6.9766355140186915,
      "grad_norm": 0.4145726263523102,
      "learning_rate": 0.00010077881619937694,
      "loss": 0.1272,
      "step": 1493
    },
    {
      "epoch": 6.981308411214953,
      "grad_norm": 0.3490482270717621,
      "learning_rate": 0.00010062305295950156,
      "loss": 0.1259,
      "step": 1494
    },
    {
      "epoch": 6.9859813084112155,
      "grad_norm": 0.3781934678554535,
      "learning_rate": 0.00010046728971962615,
      "loss": 0.1418,
      "step": 1495
    },
    {
      "epoch": 6.990654205607477,
      "grad_norm": 0.4491724967956543,
      "learning_rate": 0.00010031152647975077,
      "loss": 0.1318,
      "step": 1496
    },
    {
      "epoch": 6.9953271028037385,
      "grad_norm": 0.4215426445007324,
      "learning_rate": 0.00010015576323987539,
      "loss": 0.1322,
      "step": 1497
    },
    {
      "epoch": 7.0,
      "grad_norm": 0.8267582654953003,
      "learning_rate": 9.999999999999999e-05,
      "loss": 0.159,
      "step": 1498
    },
    {
      "epoch": 7.0,
      "eval_loss": 0.22685229778289795,
      "eval_runtime": 54.0102,
      "eval_samples_per_second": 10.22,
      "eval_steps_per_second": 1.278,
      "step": 1498
    },
    {
      "epoch": 7.0046728971962615,
      "grad_norm": 0.32753682136535645,
      "learning_rate": 9.98442367601246e-05,
      "loss": 0.1228,
      "step": 1499
    },
    {
      "epoch": 7.009345794392523,
      "grad_norm": 0.33296504616737366,
      "learning_rate": 9.968847352024921e-05,
      "loss": 0.1174,
      "step": 1500
    },
    {
      "epoch": 7.014018691588785,
      "grad_norm": 0.40632081031799316,
      "learning_rate": 9.953271028037382e-05,
      "loss": 0.1609,
      "step": 1501
    },
    {
      "epoch": 7.018691588785047,
      "grad_norm": 0.32473626732826233,
      "learning_rate": 9.937694704049844e-05,
      "loss": 0.1319,
      "step": 1502
    },
    {
      "epoch": 7.0233644859813085,
      "grad_norm": 0.3542482256889343,
      "learning_rate": 9.922118380062305e-05,
      "loss": 0.1409,
      "step": 1503
    },
    {
      "epoch": 7.02803738317757,
      "grad_norm": 0.31425797939300537,
      "learning_rate": 9.906542056074765e-05,
      "loss": 0.1074,
      "step": 1504
    },
    {
      "epoch": 7.0327102803738315,
      "grad_norm": 0.3676265478134155,
      "learning_rate": 9.890965732087226e-05,
      "loss": 0.126,
      "step": 1505
    },
    {
      "epoch": 7.037383177570093,
      "grad_norm": 0.36884111166000366,
      "learning_rate": 9.875389408099688e-05,
      "loss": 0.1244,
      "step": 1506
    },
    {
      "epoch": 7.042056074766355,
      "grad_norm": 0.2823101878166199,
      "learning_rate": 9.859813084112149e-05,
      "loss": 0.0951,
      "step": 1507
    },
    {
      "epoch": 7.046728971962617,
      "grad_norm": 0.28055140376091003,
      "learning_rate": 9.844236760124609e-05,
      "loss": 0.097,
      "step": 1508
    },
    {
      "epoch": 7.0514018691588785,
      "grad_norm": 0.34144824743270874,
      "learning_rate": 9.828660436137071e-05,
      "loss": 0.1207,
      "step": 1509
    },
    {
      "epoch": 7.05607476635514,
      "grad_norm": 0.37857288122177124,
      "learning_rate": 9.813084112149531e-05,
      "loss": 0.1373,
      "step": 1510
    },
    {
      "epoch": 7.0607476635514015,
      "grad_norm": 0.4277968108654022,
      "learning_rate": 9.797507788161993e-05,
      "loss": 0.1063,
      "step": 1511
    },
    {
      "epoch": 7.065420560747664,
      "grad_norm": 0.43927910923957825,
      "learning_rate": 9.781931464174455e-05,
      "loss": 0.1157,
      "step": 1512
    },
    {
      "epoch": 7.070093457943925,
      "grad_norm": 0.4735512137413025,
      "learning_rate": 9.766355140186914e-05,
      "loss": 0.1255,
      "step": 1513
    },
    {
      "epoch": 7.074766355140187,
      "grad_norm": 0.5089333057403564,
      "learning_rate": 9.750778816199376e-05,
      "loss": 0.1357,
      "step": 1514
    },
    {
      "epoch": 7.079439252336448,
      "grad_norm": 0.566139817237854,
      "learning_rate": 9.735202492211838e-05,
      "loss": 0.1371,
      "step": 1515
    },
    {
      "epoch": 7.08411214953271,
      "grad_norm": 0.40172532200813293,
      "learning_rate": 9.719626168224298e-05,
      "loss": 0.1001,
      "step": 1516
    },
    {
      "epoch": 7.088785046728972,
      "grad_norm": 0.36594852805137634,
      "learning_rate": 9.704049844236759e-05,
      "loss": 0.0998,
      "step": 1517
    },
    {
      "epoch": 7.093457943925234,
      "grad_norm": 0.4212909936904907,
      "learning_rate": 9.68847352024922e-05,
      "loss": 0.1357,
      "step": 1518
    },
    {
      "epoch": 7.098130841121495,
      "grad_norm": 0.40156373381614685,
      "learning_rate": 9.672897196261681e-05,
      "loss": 0.0945,
      "step": 1519
    },
    {
      "epoch": 7.102803738317757,
      "grad_norm": 0.411682665348053,
      "learning_rate": 9.657320872274143e-05,
      "loss": 0.108,
      "step": 1520
    },
    {
      "epoch": 7.107476635514018,
      "grad_norm": 0.4497240483760834,
      "learning_rate": 9.641744548286605e-05,
      "loss": 0.1378,
      "step": 1521
    },
    {
      "epoch": 7.11214953271028,
      "grad_norm": 0.45082730054855347,
      "learning_rate": 9.626168224299064e-05,
      "loss": 0.1329,
      "step": 1522
    },
    {
      "epoch": 7.116822429906542,
      "grad_norm": 0.43879011273384094,
      "learning_rate": 9.610591900311525e-05,
      "loss": 0.1364,
      "step": 1523
    },
    {
      "epoch": 7.121495327102804,
      "grad_norm": 0.39586904644966125,
      "learning_rate": 9.595015576323987e-05,
      "loss": 0.1161,
      "step": 1524
    },
    {
      "epoch": 7.126168224299065,
      "grad_norm": 0.4505971670150757,
      "learning_rate": 9.579439252336448e-05,
      "loss": 0.1332,
      "step": 1525
    },
    {
      "epoch": 7.130841121495327,
      "grad_norm": 0.35954156517982483,
      "learning_rate": 9.563862928348908e-05,
      "loss": 0.1268,
      "step": 1526
    },
    {
      "epoch": 7.135514018691588,
      "grad_norm": 0.36255142092704773,
      "learning_rate": 9.54828660436137e-05,
      "loss": 0.0985,
      "step": 1527
    },
    {
      "epoch": 7.140186915887851,
      "grad_norm": 0.41335269808769226,
      "learning_rate": 9.53271028037383e-05,
      "loss": 0.1155,
      "step": 1528
    },
    {
      "epoch": 7.144859813084112,
      "grad_norm": 0.4425705075263977,
      "learning_rate": 9.517133956386292e-05,
      "loss": 0.1268,
      "step": 1529
    },
    {
      "epoch": 7.149532710280374,
      "grad_norm": 0.33991512656211853,
      "learning_rate": 9.501557632398754e-05,
      "loss": 0.1314,
      "step": 1530
    },
    {
      "epoch": 7.154205607476635,
      "grad_norm": 0.38730400800704956,
      "learning_rate": 9.485981308411213e-05,
      "loss": 0.1201,
      "step": 1531
    },
    {
      "epoch": 7.158878504672897,
      "grad_norm": 0.47401368618011475,
      "learning_rate": 9.470404984423675e-05,
      "loss": 0.1633,
      "step": 1532
    },
    {
      "epoch": 7.163551401869159,
      "grad_norm": 0.37295714020729065,
      "learning_rate": 9.454828660436137e-05,
      "loss": 0.1206,
      "step": 1533
    },
    {
      "epoch": 7.168224299065421,
      "grad_norm": 0.4017827808856964,
      "learning_rate": 9.439252336448597e-05,
      "loss": 0.1134,
      "step": 1534
    },
    {
      "epoch": 7.172897196261682,
      "grad_norm": 0.41572049260139465,
      "learning_rate": 9.423676012461058e-05,
      "loss": 0.1359,
      "step": 1535
    },
    {
      "epoch": 7.177570093457944,
      "grad_norm": 0.36712539196014404,
      "learning_rate": 9.40809968847352e-05,
      "loss": 0.1109,
      "step": 1536
    },
    {
      "epoch": 7.182242990654205,
      "grad_norm": 0.3748783767223358,
      "learning_rate": 9.39252336448598e-05,
      "loss": 0.1192,
      "step": 1537
    },
    {
      "epoch": 7.186915887850467,
      "grad_norm": 0.378540962934494,
      "learning_rate": 9.376947040498442e-05,
      "loss": 0.1129,
      "step": 1538
    },
    {
      "epoch": 7.191588785046729,
      "grad_norm": 0.2919341027736664,
      "learning_rate": 9.361370716510904e-05,
      "loss": 0.0811,
      "step": 1539
    },
    {
      "epoch": 7.196261682242991,
      "grad_norm": 0.3544880747795105,
      "learning_rate": 9.345794392523363e-05,
      "loss": 0.1133,
      "step": 1540
    },
    {
      "epoch": 7.200934579439252,
      "grad_norm": 0.4169521629810333,
      "learning_rate": 9.330218068535825e-05,
      "loss": 0.1085,
      "step": 1541
    },
    {
      "epoch": 7.205607476635514,
      "grad_norm": 0.4226028621196747,
      "learning_rate": 9.314641744548286e-05,
      "loss": 0.1278,
      "step": 1542
    },
    {
      "epoch": 7.210280373831775,
      "grad_norm": 0.49492454528808594,
      "learning_rate": 9.299065420560747e-05,
      "loss": 0.1614,
      "step": 1543
    },
    {
      "epoch": 7.214953271028038,
      "grad_norm": 0.35013750195503235,
      "learning_rate": 9.283489096573207e-05,
      "loss": 0.112,
      "step": 1544
    },
    {
      "epoch": 7.219626168224299,
      "grad_norm": 0.4128197729587555,
      "learning_rate": 9.267912772585669e-05,
      "loss": 0.1187,
      "step": 1545
    },
    {
      "epoch": 7.224299065420561,
      "grad_norm": 0.31627601385116577,
      "learning_rate": 9.25233644859813e-05,
      "loss": 0.1073,
      "step": 1546
    },
    {
      "epoch": 7.228971962616822,
      "grad_norm": 0.4326860308647156,
      "learning_rate": 9.236760124610591e-05,
      "loss": 0.1393,
      "step": 1547
    },
    {
      "epoch": 7.233644859813084,
      "grad_norm": 0.3588762879371643,
      "learning_rate": 9.221183800623053e-05,
      "loss": 0.097,
      "step": 1548
    },
    {
      "epoch": 7.238317757009346,
      "grad_norm": 0.414302796125412,
      "learning_rate": 9.205607476635512e-05,
      "loss": 0.1239,
      "step": 1549
    },
    {
      "epoch": 7.242990654205608,
      "grad_norm": 0.40102601051330566,
      "learning_rate": 9.190031152647974e-05,
      "loss": 0.1155,
      "step": 1550
    },
    {
      "epoch": 7.247663551401869,
      "grad_norm": 0.35119014978408813,
      "learning_rate": 9.174454828660436e-05,
      "loss": 0.1146,
      "step": 1551
    },
    {
      "epoch": 7.252336448598131,
      "grad_norm": 0.4738229811191559,
      "learning_rate": 9.158878504672896e-05,
      "loss": 0.1349,
      "step": 1552
    },
    {
      "epoch": 7.257009345794392,
      "grad_norm": 0.3735959529876709,
      "learning_rate": 9.143302180685357e-05,
      "loss": 0.1024,
      "step": 1553
    },
    {
      "epoch": 7.261682242990654,
      "grad_norm": 0.4898603856563568,
      "learning_rate": 9.127725856697819e-05,
      "loss": 0.1419,
      "step": 1554
    },
    {
      "epoch": 7.266355140186916,
      "grad_norm": 0.3668118417263031,
      "learning_rate": 9.112149532710279e-05,
      "loss": 0.1272,
      "step": 1555
    },
    {
      "epoch": 7.271028037383178,
      "grad_norm": 0.4262004792690277,
      "learning_rate": 9.096573208722741e-05,
      "loss": 0.1408,
      "step": 1556
    },
    {
      "epoch": 7.275700934579439,
      "grad_norm": 0.4947028160095215,
      "learning_rate": 9.080996884735203e-05,
      "loss": 0.1339,
      "step": 1557
    },
    {
      "epoch": 7.280373831775701,
      "grad_norm": 0.4353959560394287,
      "learning_rate": 9.065420560747662e-05,
      "loss": 0.1322,
      "step": 1558
    },
    {
      "epoch": 7.285046728971962,
      "grad_norm": 0.37327373027801514,
      "learning_rate": 9.049844236760124e-05,
      "loss": 0.1136,
      "step": 1559
    },
    {
      "epoch": 7.289719626168225,
      "grad_norm": 0.4477998614311218,
      "learning_rate": 9.034267912772585e-05,
      "loss": 0.1463,
      "step": 1560
    },
    {
      "epoch": 7.294392523364486,
      "grad_norm": 0.39704006910324097,
      "learning_rate": 9.018691588785046e-05,
      "loss": 0.1335,
      "step": 1561
    },
    {
      "epoch": 7.299065420560748,
      "grad_norm": 0.3943304419517517,
      "learning_rate": 9.003115264797506e-05,
      "loss": 0.1329,
      "step": 1562
    },
    {
      "epoch": 7.303738317757009,
      "grad_norm": 0.3352386951446533,
      "learning_rate": 8.987538940809968e-05,
      "loss": 0.1023,
      "step": 1563
    },
    {
      "epoch": 7.308411214953271,
      "grad_norm": 0.514626681804657,
      "learning_rate": 8.971962616822429e-05,
      "loss": 0.1072,
      "step": 1564
    },
    {
      "epoch": 7.313084112149532,
      "grad_norm": 0.43998414278030396,
      "learning_rate": 8.95638629283489e-05,
      "loss": 0.135,
      "step": 1565
    },
    {
      "epoch": 7.317757009345795,
      "grad_norm": 0.45332059264183044,
      "learning_rate": 8.940809968847352e-05,
      "loss": 0.1473,
      "step": 1566
    },
    {
      "epoch": 7.322429906542056,
      "grad_norm": 0.4215964078903198,
      "learning_rate": 8.925233644859811e-05,
      "loss": 0.1295,
      "step": 1567
    },
    {
      "epoch": 7.327102803738318,
      "grad_norm": 0.4838808476924896,
      "learning_rate": 8.909657320872273e-05,
      "loss": 0.1479,
      "step": 1568
    },
    {
      "epoch": 7.331775700934579,
      "grad_norm": 0.38279175758361816,
      "learning_rate": 8.894080996884735e-05,
      "loss": 0.1248,
      "step": 1569
    },
    {
      "epoch": 7.336448598130841,
      "grad_norm": 0.42061755061149597,
      "learning_rate": 8.878504672897195e-05,
      "loss": 0.1103,
      "step": 1570
    },
    {
      "epoch": 7.341121495327103,
      "grad_norm": 0.4072108566761017,
      "learning_rate": 8.862928348909656e-05,
      "loss": 0.1356,
      "step": 1571
    },
    {
      "epoch": 7.345794392523365,
      "grad_norm": 0.4202394485473633,
      "learning_rate": 8.847352024922118e-05,
      "loss": 0.1411,
      "step": 1572
    },
    {
      "epoch": 7.350467289719626,
      "grad_norm": 0.37324783205986023,
      "learning_rate": 8.831775700934578e-05,
      "loss": 0.1289,
      "step": 1573
    },
    {
      "epoch": 7.355140186915888,
      "grad_norm": 0.35451310873031616,
      "learning_rate": 8.81619937694704e-05,
      "loss": 0.111,
      "step": 1574
    },
    {
      "epoch": 7.359813084112149,
      "grad_norm": 0.3295401632785797,
      "learning_rate": 8.800623052959502e-05,
      "loss": 0.099,
      "step": 1575
    },
    {
      "epoch": 7.364485981308412,
      "grad_norm": 0.33327382802963257,
      "learning_rate": 8.785046728971961e-05,
      "loss": 0.1132,
      "step": 1576
    },
    {
      "epoch": 7.369158878504673,
      "grad_norm": 0.4068683087825775,
      "learning_rate": 8.769470404984423e-05,
      "loss": 0.1493,
      "step": 1577
    },
    {
      "epoch": 7.373831775700935,
      "grad_norm": 0.50435471534729,
      "learning_rate": 8.753894080996884e-05,
      "loss": 0.1464,
      "step": 1578
    },
    {
      "epoch": 7.378504672897196,
      "grad_norm": 0.41482076048851013,
      "learning_rate": 8.738317757009345e-05,
      "loss": 0.1367,
      "step": 1579
    },
    {
      "epoch": 7.383177570093458,
      "grad_norm": 0.4246578812599182,
      "learning_rate": 8.722741433021805e-05,
      "loss": 0.1321,
      "step": 1580
    },
    {
      "epoch": 7.38785046728972,
      "grad_norm": 0.4399435520172119,
      "learning_rate": 8.707165109034267e-05,
      "loss": 0.1407,
      "step": 1581
    },
    {
      "epoch": 7.392523364485982,
      "grad_norm": 0.36167436838150024,
      "learning_rate": 8.691588785046728e-05,
      "loss": 0.1142,
      "step": 1582
    },
    {
      "epoch": 7.397196261682243,
      "grad_norm": 0.4060228168964386,
      "learning_rate": 8.67601246105919e-05,
      "loss": 0.13,
      "step": 1583
    },
    {
      "epoch": 7.401869158878505,
      "grad_norm": 0.3781508505344391,
      "learning_rate": 8.660436137071651e-05,
      "loss": 0.1208,
      "step": 1584
    },
    {
      "epoch": 7.406542056074766,
      "grad_norm": 0.34870195388793945,
      "learning_rate": 8.64485981308411e-05,
      "loss": 0.1146,
      "step": 1585
    },
    {
      "epoch": 7.411214953271028,
      "grad_norm": 0.4056077301502228,
      "learning_rate": 8.629283489096572e-05,
      "loss": 0.1523,
      "step": 1586
    },
    {
      "epoch": 7.41588785046729,
      "grad_norm": 0.3947487473487854,
      "learning_rate": 8.613707165109034e-05,
      "loss": 0.1433,
      "step": 1587
    },
    {
      "epoch": 7.420560747663552,
      "grad_norm": 0.37104547023773193,
      "learning_rate": 8.598130841121494e-05,
      "loss": 0.1162,
      "step": 1588
    },
    {
      "epoch": 7.425233644859813,
      "grad_norm": 0.3825664818286896,
      "learning_rate": 8.582554517133955e-05,
      "loss": 0.117,
      "step": 1589
    },
    {
      "epoch": 7.429906542056075,
      "grad_norm": 0.372943639755249,
      "learning_rate": 8.566978193146417e-05,
      "loss": 0.1267,
      "step": 1590
    },
    {
      "epoch": 7.434579439252336,
      "grad_norm": 0.3522340953350067,
      "learning_rate": 8.551401869158877e-05,
      "loss": 0.1009,
      "step": 1591
    },
    {
      "epoch": 7.4392523364485985,
      "grad_norm": 0.49095815420150757,
      "learning_rate": 8.535825545171339e-05,
      "loss": 0.1727,
      "step": 1592
    },
    {
      "epoch": 7.44392523364486,
      "grad_norm": 0.4400705397129059,
      "learning_rate": 8.520249221183801e-05,
      "loss": 0.1521,
      "step": 1593
    },
    {
      "epoch": 7.4485981308411215,
      "grad_norm": 0.4140131175518036,
      "learning_rate": 8.50467289719626e-05,
      "loss": 0.1509,
      "step": 1594
    },
    {
      "epoch": 7.453271028037383,
      "grad_norm": 0.3689199984073639,
      "learning_rate": 8.489096573208722e-05,
      "loss": 0.111,
      "step": 1595
    },
    {
      "epoch": 7.457943925233645,
      "grad_norm": 0.4041788876056671,
      "learning_rate": 8.473520249221184e-05,
      "loss": 0.1301,
      "step": 1596
    },
    {
      "epoch": 7.462616822429906,
      "grad_norm": 0.4061855375766754,
      "learning_rate": 8.457943925233644e-05,
      "loss": 0.1382,
      "step": 1597
    },
    {
      "epoch": 7.4672897196261685,
      "grad_norm": 0.47884508967399597,
      "learning_rate": 8.442367601246104e-05,
      "loss": 0.1477,
      "step": 1598
    },
    {
      "epoch": 7.47196261682243,
      "grad_norm": 0.3988182246685028,
      "learning_rate": 8.426791277258566e-05,
      "loss": 0.1392,
      "step": 1599
    },
    {
      "epoch": 7.4766355140186915,
      "grad_norm": 0.3837360739707947,
      "learning_rate": 8.411214953271027e-05,
      "loss": 0.1113,
      "step": 1600
    },
    {
      "epoch": 7.481308411214953,
      "grad_norm": 0.409254252910614,
      "learning_rate": 8.395638629283489e-05,
      "loss": 0.1307,
      "step": 1601
    },
    {
      "epoch": 7.485981308411215,
      "grad_norm": 0.41327738761901855,
      "learning_rate": 8.38006230529595e-05,
      "loss": 0.1464,
      "step": 1602
    },
    {
      "epoch": 7.490654205607477,
      "grad_norm": 0.4787958264350891,
      "learning_rate": 8.36448598130841e-05,
      "loss": 0.1531,
      "step": 1603
    },
    {
      "epoch": 7.4953271028037385,
      "grad_norm": 0.3917415738105774,
      "learning_rate": 8.348909657320871e-05,
      "loss": 0.1244,
      "step": 1604
    },
    {
      "epoch": 7.5,
      "grad_norm": 0.46279624104499817,
      "learning_rate": 8.333333333333333e-05,
      "loss": 0.1533,
      "step": 1605
    },
    {
      "epoch": 7.5046728971962615,
      "grad_norm": 0.4296838045120239,
      "learning_rate": 8.317757009345794e-05,
      "loss": 0.1468,
      "step": 1606
    },
    {
      "epoch": 7.509345794392523,
      "grad_norm": 0.32959380745887756,
      "learning_rate": 8.302180685358254e-05,
      "loss": 0.1154,
      "step": 1607
    },
    {
      "epoch": 7.5140186915887845,
      "grad_norm": 0.46779099106788635,
      "learning_rate": 8.286604361370716e-05,
      "loss": 0.1324,
      "step": 1608
    },
    {
      "epoch": 7.518691588785047,
      "grad_norm": 0.3900592625141144,
      "learning_rate": 8.271028037383176e-05,
      "loss": 0.1225,
      "step": 1609
    },
    {
      "epoch": 7.5233644859813085,
      "grad_norm": 0.4294288158416748,
      "learning_rate": 8.255451713395638e-05,
      "loss": 0.1464,
      "step": 1610
    },
    {
      "epoch": 7.52803738317757,
      "grad_norm": 0.3428072929382324,
      "learning_rate": 8.2398753894081e-05,
      "loss": 0.1053,
      "step": 1611
    },
    {
      "epoch": 7.5327102803738315,
      "grad_norm": 0.4213067889213562,
      "learning_rate": 8.224299065420559e-05,
      "loss": 0.1266,
      "step": 1612
    },
    {
      "epoch": 7.537383177570094,
      "grad_norm": 0.34788650274276733,
      "learning_rate": 8.208722741433021e-05,
      "loss": 0.1044,
      "step": 1613
    },
    {
      "epoch": 7.542056074766355,
      "grad_norm": 0.41542431712150574,
      "learning_rate": 8.193146417445483e-05,
      "loss": 0.1401,
      "step": 1614
    },
    {
      "epoch": 7.546728971962617,
      "grad_norm": 0.455986887216568,
      "learning_rate": 8.177570093457943e-05,
      "loss": 0.1473,
      "step": 1615
    },
    {
      "epoch": 7.5514018691588785,
      "grad_norm": 0.4297446012496948,
      "learning_rate": 8.161993769470404e-05,
      "loss": 0.1444,
      "step": 1616
    },
    {
      "epoch": 7.55607476635514,
      "grad_norm": 0.31357961893081665,
      "learning_rate": 8.146417445482865e-05,
      "loss": 0.0835,
      "step": 1617
    },
    {
      "epoch": 7.5607476635514015,
      "grad_norm": 0.3831125795841217,
      "learning_rate": 8.130841121495326e-05,
      "loss": 0.109,
      "step": 1618
    },
    {
      "epoch": 7.565420560747664,
      "grad_norm": 0.50676429271698,
      "learning_rate": 8.115264797507788e-05,
      "loss": 0.15,
      "step": 1619
    },
    {
      "epoch": 7.570093457943925,
      "grad_norm": 0.3794625997543335,
      "learning_rate": 8.09968847352025e-05,
      "loss": 0.1295,
      "step": 1620
    },
    {
      "epoch": 7.574766355140187,
      "grad_norm": 0.42788052558898926,
      "learning_rate": 8.084112149532708e-05,
      "loss": 0.1268,
      "step": 1621
    },
    {
      "epoch": 7.579439252336448,
      "grad_norm": 0.4382689595222473,
      "learning_rate": 8.06853582554517e-05,
      "loss": 0.1428,
      "step": 1622
    },
    {
      "epoch": 7.58411214953271,
      "grad_norm": 0.5126932859420776,
      "learning_rate": 8.052959501557632e-05,
      "loss": 0.1526,
      "step": 1623
    },
    {
      "epoch": 7.588785046728972,
      "grad_norm": 0.3621457517147064,
      "learning_rate": 8.037383177570093e-05,
      "loss": 0.1035,
      "step": 1624
    },
    {
      "epoch": 7.593457943925234,
      "grad_norm": 0.40951040387153625,
      "learning_rate": 8.021806853582553e-05,
      "loss": 0.1053,
      "step": 1625
    },
    {
      "epoch": 7.598130841121495,
      "grad_norm": 0.4130201041698456,
      "learning_rate": 8.006230529595015e-05,
      "loss": 0.1254,
      "step": 1626
    },
    {
      "epoch": 7.602803738317757,
      "grad_norm": 0.4105347692966461,
      "learning_rate": 7.990654205607475e-05,
      "loss": 0.1489,
      "step": 1627
    },
    {
      "epoch": 7.607476635514018,
      "grad_norm": 0.32969093322753906,
      "learning_rate": 7.975077881619937e-05,
      "loss": 0.1148,
      "step": 1628
    },
    {
      "epoch": 7.61214953271028,
      "grad_norm": 0.35137856006622314,
      "learning_rate": 7.959501557632399e-05,
      "loss": 0.1132,
      "step": 1629
    },
    {
      "epoch": 7.616822429906542,
      "grad_norm": 0.49614810943603516,
      "learning_rate": 7.943925233644858e-05,
      "loss": 0.1376,
      "step": 1630
    },
    {
      "epoch": 7.621495327102804,
      "grad_norm": 0.4521335959434509,
      "learning_rate": 7.92834890965732e-05,
      "loss": 0.132,
      "step": 1631
    },
    {
      "epoch": 7.626168224299065,
      "grad_norm": 0.3816913366317749,
      "learning_rate": 7.912772585669782e-05,
      "loss": 0.1233,
      "step": 1632
    },
    {
      "epoch": 7.630841121495327,
      "grad_norm": 0.31684473156929016,
      "learning_rate": 7.897196261682242e-05,
      "loss": 0.101,
      "step": 1633
    },
    {
      "epoch": 7.635514018691588,
      "grad_norm": 0.44790884852409363,
      "learning_rate": 7.881619937694703e-05,
      "loss": 0.1283,
      "step": 1634
    },
    {
      "epoch": 7.640186915887851,
      "grad_norm": 0.3797135353088379,
      "learning_rate": 7.866043613707164e-05,
      "loss": 0.1294,
      "step": 1635
    },
    {
      "epoch": 7.644859813084112,
      "grad_norm": 0.3065931797027588,
      "learning_rate": 7.850467289719625e-05,
      "loss": 0.0818,
      "step": 1636
    },
    {
      "epoch": 7.649532710280374,
      "grad_norm": 0.4281034469604492,
      "learning_rate": 7.834890965732087e-05,
      "loss": 0.1507,
      "step": 1637
    },
    {
      "epoch": 7.654205607476635,
      "grad_norm": 0.3566446304321289,
      "learning_rate": 7.819314641744548e-05,
      "loss": 0.117,
      "step": 1638
    },
    {
      "epoch": 7.658878504672897,
      "grad_norm": 0.38565659523010254,
      "learning_rate": 7.803738317757008e-05,
      "loss": 0.121,
      "step": 1639
    },
    {
      "epoch": 7.663551401869158,
      "grad_norm": 0.41113898158073425,
      "learning_rate": 7.78816199376947e-05,
      "loss": 0.1166,
      "step": 1640
    },
    {
      "epoch": 7.668224299065421,
      "grad_norm": 0.41554227471351624,
      "learning_rate": 7.772585669781931e-05,
      "loss": 0.1288,
      "step": 1641
    },
    {
      "epoch": 7.672897196261682,
      "grad_norm": 0.43422016501426697,
      "learning_rate": 7.757009345794392e-05,
      "loss": 0.1292,
      "step": 1642
    },
    {
      "epoch": 7.677570093457944,
      "grad_norm": 0.37660497426986694,
      "learning_rate": 7.741433021806852e-05,
      "loss": 0.1118,
      "step": 1643
    },
    {
      "epoch": 7.682242990654205,
      "grad_norm": 0.3676287531852722,
      "learning_rate": 7.725856697819314e-05,
      "loss": 0.1197,
      "step": 1644
    },
    {
      "epoch": 7.686915887850468,
      "grad_norm": 0.40530043840408325,
      "learning_rate": 7.710280373831774e-05,
      "loss": 0.1125,
      "step": 1645
    },
    {
      "epoch": 7.691588785046729,
      "grad_norm": 0.42863336205482483,
      "learning_rate": 7.694704049844236e-05,
      "loss": 0.137,
      "step": 1646
    },
    {
      "epoch": 7.696261682242991,
      "grad_norm": 0.47388121485710144,
      "learning_rate": 7.679127725856698e-05,
      "loss": 0.1496,
      "step": 1647
    },
    {
      "epoch": 7.700934579439252,
      "grad_norm": 0.4583084285259247,
      "learning_rate": 7.663551401869157e-05,
      "loss": 0.1444,
      "step": 1648
    },
    {
      "epoch": 7.705607476635514,
      "grad_norm": 0.379762202501297,
      "learning_rate": 7.647975077881619e-05,
      "loss": 0.1204,
      "step": 1649
    },
    {
      "epoch": 7.710280373831775,
      "grad_norm": 0.5156470537185669,
      "learning_rate": 7.632398753894081e-05,
      "loss": 0.1428,
      "step": 1650
    },
    {
      "epoch": 7.714953271028038,
      "grad_norm": 0.43558934330940247,
      "learning_rate": 7.616822429906541e-05,
      "loss": 0.1265,
      "step": 1651
    },
    {
      "epoch": 7.719626168224299,
      "grad_norm": 0.428328275680542,
      "learning_rate": 7.601246105919002e-05,
      "loss": 0.1318,
      "step": 1652
    },
    {
      "epoch": 7.724299065420561,
      "grad_norm": 0.4174804389476776,
      "learning_rate": 7.585669781931463e-05,
      "loss": 0.1345,
      "step": 1653
    },
    {
      "epoch": 7.728971962616822,
      "grad_norm": 0.45726636052131653,
      "learning_rate": 7.570093457943924e-05,
      "loss": 0.1261,
      "step": 1654
    },
    {
      "epoch": 7.733644859813084,
      "grad_norm": 0.43506285548210144,
      "learning_rate": 7.554517133956386e-05,
      "loss": 0.1137,
      "step": 1655
    },
    {
      "epoch": 7.738317757009346,
      "grad_norm": 0.42708995938301086,
      "learning_rate": 7.538940809968848e-05,
      "loss": 0.1151,
      "step": 1656
    },
    {
      "epoch": 7.742990654205608,
      "grad_norm": 0.41427043080329895,
      "learning_rate": 7.523364485981307e-05,
      "loss": 0.1318,
      "step": 1657
    },
    {
      "epoch": 7.747663551401869,
      "grad_norm": 0.4058382213115692,
      "learning_rate": 7.507788161993768e-05,
      "loss": 0.1143,
      "step": 1658
    },
    {
      "epoch": 7.752336448598131,
      "grad_norm": 0.3981271982192993,
      "learning_rate": 7.49221183800623e-05,
      "loss": 0.1029,
      "step": 1659
    },
    {
      "epoch": 7.757009345794392,
      "grad_norm": 0.4674298167228699,
      "learning_rate": 7.476635514018691e-05,
      "loss": 0.1368,
      "step": 1660
    },
    {
      "epoch": 7.761682242990654,
      "grad_norm": 0.4404541254043579,
      "learning_rate": 7.461059190031151e-05,
      "loss": 0.1358,
      "step": 1661
    },
    {
      "epoch": 7.766355140186916,
      "grad_norm": 0.38080504536628723,
      "learning_rate": 7.445482866043613e-05,
      "loss": 0.1263,
      "step": 1662
    },
    {
      "epoch": 7.771028037383178,
      "grad_norm": 0.4722754657268524,
      "learning_rate": 7.429906542056073e-05,
      "loss": 0.1379,
      "step": 1663
    },
    {
      "epoch": 7.775700934579439,
      "grad_norm": 0.4857088029384613,
      "learning_rate": 7.414330218068535e-05,
      "loss": 0.1436,
      "step": 1664
    },
    {
      "epoch": 7.780373831775701,
      "grad_norm": 0.43743568658828735,
      "learning_rate": 7.398753894080997e-05,
      "loss": 0.1433,
      "step": 1665
    },
    {
      "epoch": 7.785046728971962,
      "grad_norm": 0.4048311114311218,
      "learning_rate": 7.383177570093458e-05,
      "loss": 0.1134,
      "step": 1666
    },
    {
      "epoch": 7.789719626168225,
      "grad_norm": 0.42299726605415344,
      "learning_rate": 7.367601246105918e-05,
      "loss": 0.1236,
      "step": 1667
    },
    {
      "epoch": 7.794392523364486,
      "grad_norm": 0.44015562534332275,
      "learning_rate": 7.35202492211838e-05,
      "loss": 0.13,
      "step": 1668
    },
    {
      "epoch": 7.799065420560748,
      "grad_norm": 0.39026379585266113,
      "learning_rate": 7.33644859813084e-05,
      "loss": 0.1095,
      "step": 1669
    },
    {
      "epoch": 7.803738317757009,
      "grad_norm": 0.5138530135154724,
      "learning_rate": 7.320872274143301e-05,
      "loss": 0.1378,
      "step": 1670
    },
    {
      "epoch": 7.808411214953271,
      "grad_norm": 0.47225406765937805,
      "learning_rate": 7.305295950155763e-05,
      "loss": 0.1295,
      "step": 1671
    },
    {
      "epoch": 7.813084112149532,
      "grad_norm": 0.35950133204460144,
      "learning_rate": 7.289719626168223e-05,
      "loss": 0.1281,
      "step": 1672
    },
    {
      "epoch": 7.817757009345795,
      "grad_norm": 0.3276474177837372,
      "learning_rate": 7.274143302180685e-05,
      "loss": 0.104,
      "step": 1673
    },
    {
      "epoch": 7.822429906542056,
      "grad_norm": 0.4656187891960144,
      "learning_rate": 7.258566978193147e-05,
      "loss": 0.1519,
      "step": 1674
    },
    {
      "epoch": 7.827102803738318,
      "grad_norm": 0.39280784130096436,
      "learning_rate": 7.242990654205607e-05,
      "loss": 0.1067,
      "step": 1675
    },
    {
      "epoch": 7.831775700934579,
      "grad_norm": 0.463266521692276,
      "learning_rate": 7.227414330218068e-05,
      "loss": 0.1381,
      "step": 1676
    },
    {
      "epoch": 7.836448598130842,
      "grad_norm": 0.41007885336875916,
      "learning_rate": 7.211838006230529e-05,
      "loss": 0.1273,
      "step": 1677
    },
    {
      "epoch": 7.841121495327103,
      "grad_norm": 0.48095327615737915,
      "learning_rate": 7.19626168224299e-05,
      "loss": 0.1702,
      "step": 1678
    },
    {
      "epoch": 7.845794392523365,
      "grad_norm": 0.38017407059669495,
      "learning_rate": 7.18068535825545e-05,
      "loss": 0.1031,
      "step": 1679
    },
    {
      "epoch": 7.850467289719626,
      "grad_norm": 0.3446693420410156,
      "learning_rate": 7.165109034267912e-05,
      "loss": 0.1081,
      "step": 1680
    },
    {
      "epoch": 7.855140186915888,
      "grad_norm": 0.40197083353996277,
      "learning_rate": 7.149532710280372e-05,
      "loss": 0.1237,
      "step": 1681
    },
    {
      "epoch": 7.859813084112149,
      "grad_norm": 0.39584633708000183,
      "learning_rate": 7.133956386292834e-05,
      "loss": 0.1088,
      "step": 1682
    },
    {
      "epoch": 7.864485981308412,
      "grad_norm": 0.3823072910308838,
      "learning_rate": 7.118380062305296e-05,
      "loss": 0.1109,
      "step": 1683
    },
    {
      "epoch": 7.869158878504673,
      "grad_norm": 0.3652031719684601,
      "learning_rate": 7.102803738317757e-05,
      "loss": 0.123,
      "step": 1684
    },
    {
      "epoch": 7.873831775700935,
      "grad_norm": 0.4614955484867096,
      "learning_rate": 7.087227414330217e-05,
      "loss": 0.1154,
      "step": 1685
    },
    {
      "epoch": 7.878504672897196,
      "grad_norm": 0.535062849521637,
      "learning_rate": 7.071651090342679e-05,
      "loss": 0.1248,
      "step": 1686
    },
    {
      "epoch": 7.883177570093458,
      "grad_norm": 0.367807000875473,
      "learning_rate": 7.056074766355139e-05,
      "loss": 0.1198,
      "step": 1687
    },
    {
      "epoch": 7.88785046728972,
      "grad_norm": 0.3644523620605469,
      "learning_rate": 7.0404984423676e-05,
      "loss": 0.1044,
      "step": 1688
    },
    {
      "epoch": 7.892523364485982,
      "grad_norm": 0.41284656524658203,
      "learning_rate": 7.024922118380062e-05,
      "loss": 0.1303,
      "step": 1689
    },
    {
      "epoch": 7.897196261682243,
      "grad_norm": 0.44680607318878174,
      "learning_rate": 7.009345794392522e-05,
      "loss": 0.1186,
      "step": 1690
    },
    {
      "epoch": 7.901869158878505,
      "grad_norm": 0.3539596199989319,
      "learning_rate": 6.993769470404984e-05,
      "loss": 0.1065,
      "step": 1691
    },
    {
      "epoch": 7.906542056074766,
      "grad_norm": 0.4314512610435486,
      "learning_rate": 6.978193146417446e-05,
      "loss": 0.1383,
      "step": 1692
    },
    {
      "epoch": 7.911214953271028,
      "grad_norm": 0.39839696884155273,
      "learning_rate": 6.962616822429906e-05,
      "loss": 0.1267,
      "step": 1693
    },
    {
      "epoch": 7.91588785046729,
      "grad_norm": 0.38782525062561035,
      "learning_rate": 6.947040498442367e-05,
      "loss": 0.1087,
      "step": 1694
    },
    {
      "epoch": 7.920560747663552,
      "grad_norm": 0.46452879905700684,
      "learning_rate": 6.931464174454828e-05,
      "loss": 0.1175,
      "step": 1695
    },
    {
      "epoch": 7.925233644859813,
      "grad_norm": 0.4488690197467804,
      "learning_rate": 6.915887850467289e-05,
      "loss": 0.1404,
      "step": 1696
    },
    {
      "epoch": 7.929906542056075,
      "grad_norm": 0.5626158714294434,
      "learning_rate": 6.900311526479749e-05,
      "loss": 0.1313,
      "step": 1697
    },
    {
      "epoch": 7.934579439252336,
      "grad_norm": 0.4547991156578064,
      "learning_rate": 6.884735202492211e-05,
      "loss": 0.1377,
      "step": 1698
    },
    {
      "epoch": 7.9392523364485985,
      "grad_norm": 0.6773298978805542,
      "learning_rate": 6.869158878504672e-05,
      "loss": 0.1328,
      "step": 1699
    },
    {
      "epoch": 7.94392523364486,
      "grad_norm": 0.4333645701408386,
      "learning_rate": 6.853582554517133e-05,
      "loss": 0.1356,
      "step": 1700
    },
    {
      "epoch": 7.9485981308411215,
      "grad_norm": 0.4315149188041687,
      "learning_rate": 6.838006230529595e-05,
      "loss": 0.1266,
      "step": 1701
    },
    {
      "epoch": 7.953271028037383,
      "grad_norm": 0.27042680978775024,
      "learning_rate": 6.822429906542056e-05,
      "loss": 0.0745,
      "step": 1702
    },
    {
      "epoch": 7.957943925233645,
      "grad_norm": 0.3888159692287445,
      "learning_rate": 6.806853582554516e-05,
      "loss": 0.1217,
      "step": 1703
    },
    {
      "epoch": 7.962616822429906,
      "grad_norm": 0.3525131940841675,
      "learning_rate": 6.791277258566978e-05,
      "loss": 0.1118,
      "step": 1704
    },
    {
      "epoch": 7.9672897196261685,
      "grad_norm": 0.3972553610801697,
      "learning_rate": 6.775700934579438e-05,
      "loss": 0.1157,
      "step": 1705
    },
    {
      "epoch": 7.97196261682243,
      "grad_norm": 0.39781099557876587,
      "learning_rate": 6.760124610591899e-05,
      "loss": 0.1289,
      "step": 1706
    },
    {
      "epoch": 7.9766355140186915,
      "grad_norm": 0.43140825629234314,
      "learning_rate": 6.74454828660436e-05,
      "loss": 0.139,
      "step": 1707
    },
    {
      "epoch": 7.981308411214953,
      "grad_norm": 0.37024518847465515,
      "learning_rate": 6.728971962616821e-05,
      "loss": 0.1265,
      "step": 1708
    },
    {
      "epoch": 7.9859813084112155,
      "grad_norm": 0.5370610356330872,
      "learning_rate": 6.713395638629283e-05,
      "loss": 0.155,
      "step": 1709
    },
    {
      "epoch": 7.990654205607477,
      "grad_norm": 0.3757244646549225,
      "learning_rate": 6.697819314641745e-05,
      "loss": 0.1224,
      "step": 1710
    },
    {
      "epoch": 7.9953271028037385,
      "grad_norm": 0.40104109048843384,
      "learning_rate": 6.682242990654205e-05,
      "loss": 0.1399,
      "step": 1711
    },
    {
      "epoch": 8.0,
      "grad_norm": 0.4097554385662079,
      "learning_rate": 6.666666666666666e-05,
      "loss": 0.0961,
      "step": 1712
    },
    {
      "epoch": 8.0,
      "eval_loss": 0.22446826100349426,
      "eval_runtime": 54.1581,
      "eval_samples_per_second": 10.192,
      "eval_steps_per_second": 1.274,
      "step": 1712
    }
  ],
  "logging_steps": 1,
  "max_steps": 2140,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 3.885813978207437e+17,
  "train_batch_size": 32,
  "trial_name": null,
  "trial_params": null
}
