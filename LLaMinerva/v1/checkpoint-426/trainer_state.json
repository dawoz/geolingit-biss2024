{
  "best_metric": 0.27231791615486145,
  "best_model_checkpoint": "LLaMinerva/checkpoint-426",
  "epoch": 1.9929824561403509,
  "eval_steps": 500,
  "global_step": 426,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.004678362573099415,
      "grad_norm": 1.3241361379623413,
      "learning_rate": 6.976744186046511e-06,
      "loss": 2.4112,
      "step": 1
    },
    {
      "epoch": 0.00935672514619883,
      "grad_norm": 1.3309026956558228,
      "learning_rate": 1.3953488372093022e-05,
      "loss": 2.3949,
      "step": 2
    },
    {
      "epoch": 0.014035087719298246,
      "grad_norm": 1.3665930032730103,
      "learning_rate": 2.0930232558139532e-05,
      "loss": 2.3673,
      "step": 3
    },
    {
      "epoch": 0.01871345029239766,
      "grad_norm": 1.3776658773422241,
      "learning_rate": 2.7906976744186044e-05,
      "loss": 2.3681,
      "step": 4
    },
    {
      "epoch": 0.023391812865497075,
      "grad_norm": 1.4351457357406616,
      "learning_rate": 3.4883720930232556e-05,
      "loss": 2.3447,
      "step": 5
    },
    {
      "epoch": 0.028070175438596492,
      "grad_norm": 1.3860231637954712,
      "learning_rate": 4.1860465116279065e-05,
      "loss": 2.3057,
      "step": 6
    },
    {
      "epoch": 0.03274853801169591,
      "grad_norm": 1.4375429153442383,
      "learning_rate": 4.883720930232558e-05,
      "loss": 2.2124,
      "step": 7
    },
    {
      "epoch": 0.03742690058479532,
      "grad_norm": 1.4509488344192505,
      "learning_rate": 5.581395348837209e-05,
      "loss": 2.1939,
      "step": 8
    },
    {
      "epoch": 0.042105263157894736,
      "grad_norm": 1.396327257156372,
      "learning_rate": 6.27906976744186e-05,
      "loss": 2.0819,
      "step": 9
    },
    {
      "epoch": 0.04678362573099415,
      "grad_norm": 1.4750782251358032,
      "learning_rate": 6.976744186046511e-05,
      "loss": 2.0077,
      "step": 10
    },
    {
      "epoch": 0.05146198830409357,
      "grad_norm": 1.498728632926941,
      "learning_rate": 7.674418604651162e-05,
      "loss": 1.8966,
      "step": 11
    },
    {
      "epoch": 0.056140350877192984,
      "grad_norm": 1.5695933103561401,
      "learning_rate": 8.372093023255813e-05,
      "loss": 1.7629,
      "step": 12
    },
    {
      "epoch": 0.0608187134502924,
      "grad_norm": 1.618862271308899,
      "learning_rate": 9.069767441860464e-05,
      "loss": 1.6048,
      "step": 13
    },
    {
      "epoch": 0.06549707602339182,
      "grad_norm": 1.789753794670105,
      "learning_rate": 9.767441860465116e-05,
      "loss": 1.4786,
      "step": 14
    },
    {
      "epoch": 0.07017543859649122,
      "grad_norm": 1.7458181381225586,
      "learning_rate": 0.00010465116279069767,
      "loss": 1.336,
      "step": 15
    },
    {
      "epoch": 0.07485380116959064,
      "grad_norm": 1.6542575359344482,
      "learning_rate": 0.00011162790697674418,
      "loss": 1.1164,
      "step": 16
    },
    {
      "epoch": 0.07953216374269007,
      "grad_norm": 1.5377473831176758,
      "learning_rate": 0.00011860465116279069,
      "loss": 0.9016,
      "step": 17
    },
    {
      "epoch": 0.08421052631578947,
      "grad_norm": 1.3835731744766235,
      "learning_rate": 0.0001255813953488372,
      "loss": 0.7309,
      "step": 18
    },
    {
      "epoch": 0.08888888888888889,
      "grad_norm": 1.2073580026626587,
      "learning_rate": 0.00013255813953488372,
      "loss": 0.6186,
      "step": 19
    },
    {
      "epoch": 0.0935672514619883,
      "grad_norm": 1.0136940479278564,
      "learning_rate": 0.00013953488372093022,
      "loss": 0.518,
      "step": 20
    },
    {
      "epoch": 0.09824561403508772,
      "grad_norm": 0.9117522835731506,
      "learning_rate": 0.00014651162790697673,
      "loss": 0.4796,
      "step": 21
    },
    {
      "epoch": 0.10292397660818714,
      "grad_norm": 0.6205695271492004,
      "learning_rate": 0.00015348837209302324,
      "loss": 0.4823,
      "step": 22
    },
    {
      "epoch": 0.10760233918128655,
      "grad_norm": 0.7212193012237549,
      "learning_rate": 0.00016046511627906975,
      "loss": 0.4486,
      "step": 23
    },
    {
      "epoch": 0.11228070175438597,
      "grad_norm": 1.1288676261901855,
      "learning_rate": 0.00016744186046511626,
      "loss": 0.495,
      "step": 24
    },
    {
      "epoch": 0.11695906432748537,
      "grad_norm": 1.0187925100326538,
      "learning_rate": 0.0001744186046511628,
      "loss": 0.4594,
      "step": 25
    },
    {
      "epoch": 0.1216374269005848,
      "grad_norm": 0.8887890577316284,
      "learning_rate": 0.00018139534883720928,
      "loss": 0.4201,
      "step": 26
    },
    {
      "epoch": 0.12631578947368421,
      "grad_norm": 0.48650288581848145,
      "learning_rate": 0.0001883720930232558,
      "loss": 0.4148,
      "step": 27
    },
    {
      "epoch": 0.13099415204678364,
      "grad_norm": 0.5619785189628601,
      "learning_rate": 0.00019534883720930232,
      "loss": 0.399,
      "step": 28
    },
    {
      "epoch": 0.13567251461988303,
      "grad_norm": 0.682427704334259,
      "learning_rate": 0.0002023255813953488,
      "loss": 0.3505,
      "step": 29
    },
    {
      "epoch": 0.14035087719298245,
      "grad_norm": 0.578461766242981,
      "learning_rate": 0.00020930232558139534,
      "loss": 0.3958,
      "step": 30
    },
    {
      "epoch": 0.14502923976608187,
      "grad_norm": 0.4517197012901306,
      "learning_rate": 0.00021627906976744182,
      "loss": 0.3625,
      "step": 31
    },
    {
      "epoch": 0.1497076023391813,
      "grad_norm": 0.45554202795028687,
      "learning_rate": 0.00022325581395348835,
      "loss": 0.4004,
      "step": 32
    },
    {
      "epoch": 0.1543859649122807,
      "grad_norm": 0.34832876920700073,
      "learning_rate": 0.00023023255813953486,
      "loss": 0.2981,
      "step": 33
    },
    {
      "epoch": 0.15906432748538013,
      "grad_norm": 0.4668924808502197,
      "learning_rate": 0.00023720930232558137,
      "loss": 0.41,
      "step": 34
    },
    {
      "epoch": 0.16374269005847952,
      "grad_norm": 0.37630653381347656,
      "learning_rate": 0.0002441860465116279,
      "loss": 0.3274,
      "step": 35
    },
    {
      "epoch": 0.16842105263157894,
      "grad_norm": 0.34758278727531433,
      "learning_rate": 0.0002511627906976744,
      "loss": 0.397,
      "step": 36
    },
    {
      "epoch": 0.17309941520467836,
      "grad_norm": 0.3303459882736206,
      "learning_rate": 0.0002581395348837209,
      "loss": 0.3533,
      "step": 37
    },
    {
      "epoch": 0.17777777777777778,
      "grad_norm": 0.3016093671321869,
      "learning_rate": 0.00026511627906976743,
      "loss": 0.3822,
      "step": 38
    },
    {
      "epoch": 0.1824561403508772,
      "grad_norm": 0.3179210126399994,
      "learning_rate": 0.00027209302325581394,
      "loss": 0.3164,
      "step": 39
    },
    {
      "epoch": 0.1871345029239766,
      "grad_norm": 0.3086426854133606,
      "learning_rate": 0.00027906976744186045,
      "loss": 0.3868,
      "step": 40
    },
    {
      "epoch": 0.19181286549707602,
      "grad_norm": 0.3192501664161682,
      "learning_rate": 0.00028604651162790696,
      "loss": 0.322,
      "step": 41
    },
    {
      "epoch": 0.19649122807017544,
      "grad_norm": 0.3186642825603485,
      "learning_rate": 0.00029302325581395347,
      "loss": 0.3942,
      "step": 42
    },
    {
      "epoch": 0.20116959064327486,
      "grad_norm": 0.3111198842525482,
      "learning_rate": 0.0003,
      "loss": 0.344,
      "step": 43
    },
    {
      "epoch": 0.20584795321637428,
      "grad_norm": 0.33464181423187256,
      "learning_rate": 0.0002992167101827676,
      "loss": 0.3417,
      "step": 44
    },
    {
      "epoch": 0.21052631578947367,
      "grad_norm": 0.3189530670642853,
      "learning_rate": 0.0002984334203655352,
      "loss": 0.3274,
      "step": 45
    },
    {
      "epoch": 0.2152046783625731,
      "grad_norm": 0.2801547944545746,
      "learning_rate": 0.00029765013054830285,
      "loss": 0.2858,
      "step": 46
    },
    {
      "epoch": 0.2198830409356725,
      "grad_norm": 0.2858394682407379,
      "learning_rate": 0.00029686684073107044,
      "loss": 0.2758,
      "step": 47
    },
    {
      "epoch": 0.22456140350877193,
      "grad_norm": 0.28483447432518005,
      "learning_rate": 0.0002960835509138381,
      "loss": 0.3516,
      "step": 48
    },
    {
      "epoch": 0.22923976608187135,
      "grad_norm": 0.29165756702423096,
      "learning_rate": 0.00029530026109660573,
      "loss": 0.3325,
      "step": 49
    },
    {
      "epoch": 0.23391812865497075,
      "grad_norm": 0.286949098110199,
      "learning_rate": 0.0002945169712793734,
      "loss": 0.3312,
      "step": 50
    },
    {
      "epoch": 0.23859649122807017,
      "grad_norm": 0.26238778233528137,
      "learning_rate": 0.00029373368146214096,
      "loss": 0.3426,
      "step": 51
    },
    {
      "epoch": 0.2432748538011696,
      "grad_norm": 0.44660481810569763,
      "learning_rate": 0.0002929503916449086,
      "loss": 0.2659,
      "step": 52
    },
    {
      "epoch": 0.247953216374269,
      "grad_norm": 0.26443716883659363,
      "learning_rate": 0.0002921671018276762,
      "loss": 0.338,
      "step": 53
    },
    {
      "epoch": 0.25263157894736843,
      "grad_norm": 0.2041986733675003,
      "learning_rate": 0.00029138381201044384,
      "loss": 0.2925,
      "step": 54
    },
    {
      "epoch": 0.2573099415204678,
      "grad_norm": 0.21460139751434326,
      "learning_rate": 0.00029060052219321143,
      "loss": 0.3221,
      "step": 55
    },
    {
      "epoch": 0.26198830409356727,
      "grad_norm": 0.2533111572265625,
      "learning_rate": 0.00028981723237597913,
      "loss": 0.3096,
      "step": 56
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 0.22340229153633118,
      "learning_rate": 0.0002890339425587467,
      "loss": 0.3202,
      "step": 57
    },
    {
      "epoch": 0.27134502923976606,
      "grad_norm": 0.2518696188926697,
      "learning_rate": 0.00028825065274151436,
      "loss": 0.1925,
      "step": 58
    },
    {
      "epoch": 0.2760233918128655,
      "grad_norm": 0.2573477327823639,
      "learning_rate": 0.00028746736292428195,
      "loss": 0.3124,
      "step": 59
    },
    {
      "epoch": 0.2807017543859649,
      "grad_norm": 0.26252010464668274,
      "learning_rate": 0.0002866840731070496,
      "loss": 0.3483,
      "step": 60
    },
    {
      "epoch": 0.28538011695906434,
      "grad_norm": 0.2210385501384735,
      "learning_rate": 0.0002859007832898172,
      "loss": 0.3243,
      "step": 61
    },
    {
      "epoch": 0.29005847953216374,
      "grad_norm": 0.2829238176345825,
      "learning_rate": 0.00028511749347258483,
      "loss": 0.2914,
      "step": 62
    },
    {
      "epoch": 0.29473684210526313,
      "grad_norm": 0.2730584442615509,
      "learning_rate": 0.0002843342036553524,
      "loss": 0.3654,
      "step": 63
    },
    {
      "epoch": 0.2994152046783626,
      "grad_norm": 0.2889770269393921,
      "learning_rate": 0.00028355091383812007,
      "loss": 0.2995,
      "step": 64
    },
    {
      "epoch": 0.30409356725146197,
      "grad_norm": 0.28021547198295593,
      "learning_rate": 0.0002827676240208877,
      "loss": 0.3041,
      "step": 65
    },
    {
      "epoch": 0.3087719298245614,
      "grad_norm": 0.2747771143913269,
      "learning_rate": 0.00028198433420365535,
      "loss": 0.3115,
      "step": 66
    },
    {
      "epoch": 0.3134502923976608,
      "grad_norm": 0.20699311792850494,
      "learning_rate": 0.00028120104438642294,
      "loss": 0.2702,
      "step": 67
    },
    {
      "epoch": 0.31812865497076026,
      "grad_norm": 0.23913034796714783,
      "learning_rate": 0.0002804177545691906,
      "loss": 0.3201,
      "step": 68
    },
    {
      "epoch": 0.32280701754385965,
      "grad_norm": 0.26070258021354675,
      "learning_rate": 0.0002796344647519582,
      "loss": 0.2865,
      "step": 69
    },
    {
      "epoch": 0.32748538011695905,
      "grad_norm": 0.27273157238960266,
      "learning_rate": 0.0002788511749347258,
      "loss": 0.3216,
      "step": 70
    },
    {
      "epoch": 0.3321637426900585,
      "grad_norm": 0.31363582611083984,
      "learning_rate": 0.00027806788511749346,
      "loss": 0.3187,
      "step": 71
    },
    {
      "epoch": 0.3368421052631579,
      "grad_norm": 0.21774175763130188,
      "learning_rate": 0.00027728459530026105,
      "loss": 0.2796,
      "step": 72
    },
    {
      "epoch": 0.34152046783625734,
      "grad_norm": 0.33834531903266907,
      "learning_rate": 0.0002765013054830287,
      "loss": 0.2586,
      "step": 73
    },
    {
      "epoch": 0.34619883040935673,
      "grad_norm": 0.22769396007061005,
      "learning_rate": 0.00027571801566579634,
      "loss": 0.3357,
      "step": 74
    },
    {
      "epoch": 0.3508771929824561,
      "grad_norm": 0.2111770510673523,
      "learning_rate": 0.00027493472584856393,
      "loss": 0.2511,
      "step": 75
    },
    {
      "epoch": 0.35555555555555557,
      "grad_norm": 0.21373657882213593,
      "learning_rate": 0.0002741514360313316,
      "loss": 0.3208,
      "step": 76
    },
    {
      "epoch": 0.36023391812865496,
      "grad_norm": 0.2358919382095337,
      "learning_rate": 0.00027336814621409917,
      "loss": 0.3322,
      "step": 77
    },
    {
      "epoch": 0.3649122807017544,
      "grad_norm": 0.20544667541980743,
      "learning_rate": 0.0002725848563968668,
      "loss": 0.2678,
      "step": 78
    },
    {
      "epoch": 0.3695906432748538,
      "grad_norm": 0.211936816573143,
      "learning_rate": 0.00027180156657963445,
      "loss": 0.3325,
      "step": 79
    },
    {
      "epoch": 0.3742690058479532,
      "grad_norm": 0.2150450497865677,
      "learning_rate": 0.00027101827676240204,
      "loss": 0.2865,
      "step": 80
    },
    {
      "epoch": 0.37894736842105264,
      "grad_norm": 0.23176005482673645,
      "learning_rate": 0.0002702349869451697,
      "loss": 0.3258,
      "step": 81
    },
    {
      "epoch": 0.38362573099415204,
      "grad_norm": 0.17364636063575745,
      "learning_rate": 0.00026945169712793733,
      "loss": 0.2493,
      "step": 82
    },
    {
      "epoch": 0.3883040935672515,
      "grad_norm": 0.2256927341222763,
      "learning_rate": 0.0002686684073107049,
      "loss": 0.3685,
      "step": 83
    },
    {
      "epoch": 0.3929824561403509,
      "grad_norm": 0.27331656217575073,
      "learning_rate": 0.00026788511749347257,
      "loss": 0.3346,
      "step": 84
    },
    {
      "epoch": 0.39766081871345027,
      "grad_norm": 0.19501107931137085,
      "learning_rate": 0.0002671018276762402,
      "loss": 0.319,
      "step": 85
    },
    {
      "epoch": 0.4023391812865497,
      "grad_norm": 0.22632820904254913,
      "learning_rate": 0.0002663185378590078,
      "loss": 0.2999,
      "step": 86
    },
    {
      "epoch": 0.4070175438596491,
      "grad_norm": 0.21422924101352692,
      "learning_rate": 0.00026553524804177544,
      "loss": 0.2962,
      "step": 87
    },
    {
      "epoch": 0.41169590643274856,
      "grad_norm": 0.2732509970664978,
      "learning_rate": 0.00026475195822454303,
      "loss": 0.2986,
      "step": 88
    },
    {
      "epoch": 0.41637426900584795,
      "grad_norm": 0.21932852268218994,
      "learning_rate": 0.0002639686684073107,
      "loss": 0.339,
      "step": 89
    },
    {
      "epoch": 0.42105263157894735,
      "grad_norm": 0.25548431277275085,
      "learning_rate": 0.0002631853785900783,
      "loss": 0.2889,
      "step": 90
    },
    {
      "epoch": 0.4257309941520468,
      "grad_norm": 0.26763325929641724,
      "learning_rate": 0.00026240208877284597,
      "loss": 0.3013,
      "step": 91
    },
    {
      "epoch": 0.4304093567251462,
      "grad_norm": 0.2601314187049866,
      "learning_rate": 0.00026161879895561356,
      "loss": 0.3568,
      "step": 92
    },
    {
      "epoch": 0.43508771929824563,
      "grad_norm": 0.2792530953884125,
      "learning_rate": 0.0002608355091383812,
      "loss": 0.284,
      "step": 93
    },
    {
      "epoch": 0.439766081871345,
      "grad_norm": 0.20696145296096802,
      "learning_rate": 0.0002600522193211488,
      "loss": 0.3647,
      "step": 94
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 0.22505424916744232,
      "learning_rate": 0.00025926892950391643,
      "loss": 0.315,
      "step": 95
    },
    {
      "epoch": 0.44912280701754387,
      "grad_norm": 0.20788373053073883,
      "learning_rate": 0.000258485639686684,
      "loss": 0.2986,
      "step": 96
    },
    {
      "epoch": 0.45380116959064326,
      "grad_norm": 0.2473655790090561,
      "learning_rate": 0.00025770234986945167,
      "loss": 0.2956,
      "step": 97
    },
    {
      "epoch": 0.4584795321637427,
      "grad_norm": 0.2047998607158661,
      "learning_rate": 0.00025691906005221926,
      "loss": 0.2978,
      "step": 98
    },
    {
      "epoch": 0.4631578947368421,
      "grad_norm": 0.242078498005867,
      "learning_rate": 0.00025613577023498696,
      "loss": 0.3255,
      "step": 99
    },
    {
      "epoch": 0.4678362573099415,
      "grad_norm": 0.21453019976615906,
      "learning_rate": 0.00025535248041775455,
      "loss": 0.2603,
      "step": 100
    },
    {
      "epoch": 0.47251461988304094,
      "grad_norm": 0.24189065396785736,
      "learning_rate": 0.0002545691906005222,
      "loss": 0.2605,
      "step": 101
    },
    {
      "epoch": 0.47719298245614034,
      "grad_norm": 0.296530544757843,
      "learning_rate": 0.0002537859007832898,
      "loss": 0.2797,
      "step": 102
    },
    {
      "epoch": 0.4818713450292398,
      "grad_norm": 0.3734661340713501,
      "learning_rate": 0.0002530026109660574,
      "loss": 0.3556,
      "step": 103
    },
    {
      "epoch": 0.4865497076023392,
      "grad_norm": 0.2121235877275467,
      "learning_rate": 0.000252219321148825,
      "loss": 0.2614,
      "step": 104
    },
    {
      "epoch": 0.49122807017543857,
      "grad_norm": 0.25745096802711487,
      "learning_rate": 0.00025143603133159266,
      "loss": 0.3427,
      "step": 105
    },
    {
      "epoch": 0.495906432748538,
      "grad_norm": 0.27639657258987427,
      "learning_rate": 0.0002506527415143603,
      "loss": 0.3029,
      "step": 106
    },
    {
      "epoch": 0.5005847953216375,
      "grad_norm": 0.19176660478115082,
      "learning_rate": 0.00024986945169712795,
      "loss": 0.2748,
      "step": 107
    },
    {
      "epoch": 0.5052631578947369,
      "grad_norm": 0.25478583574295044,
      "learning_rate": 0.00024908616187989554,
      "loss": 0.3659,
      "step": 108
    },
    {
      "epoch": 0.5099415204678363,
      "grad_norm": 0.30246734619140625,
      "learning_rate": 0.0002483028720626632,
      "loss": 0.2773,
      "step": 109
    },
    {
      "epoch": 0.5146198830409356,
      "grad_norm": 0.2427903115749359,
      "learning_rate": 0.00024751958224543077,
      "loss": 0.3105,
      "step": 110
    },
    {
      "epoch": 0.519298245614035,
      "grad_norm": 0.25404971837997437,
      "learning_rate": 0.0002467362924281984,
      "loss": 0.2596,
      "step": 111
    },
    {
      "epoch": 0.5239766081871345,
      "grad_norm": 0.2740319073200226,
      "learning_rate": 0.00024595300261096606,
      "loss": 0.2201,
      "step": 112
    },
    {
      "epoch": 0.5286549707602339,
      "grad_norm": 0.2697922885417938,
      "learning_rate": 0.00024516971279373365,
      "loss": 0.2679,
      "step": 113
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 0.30009767413139343,
      "learning_rate": 0.0002443864229765013,
      "loss": 0.2954,
      "step": 114
    },
    {
      "epoch": 0.5380116959064327,
      "grad_norm": 0.36020520329475403,
      "learning_rate": 0.00024360313315926888,
      "loss": 0.3337,
      "step": 115
    },
    {
      "epoch": 0.5426900584795321,
      "grad_norm": 0.25497254729270935,
      "learning_rate": 0.00024281984334203655,
      "loss": 0.2946,
      "step": 116
    },
    {
      "epoch": 0.5473684210526316,
      "grad_norm": 0.24994750320911407,
      "learning_rate": 0.00024203655352480417,
      "loss": 0.3175,
      "step": 117
    },
    {
      "epoch": 0.552046783625731,
      "grad_norm": 0.23846426606178284,
      "learning_rate": 0.00024125326370757179,
      "loss": 0.328,
      "step": 118
    },
    {
      "epoch": 0.5567251461988304,
      "grad_norm": 0.2895345687866211,
      "learning_rate": 0.0002404699738903394,
      "loss": 0.2719,
      "step": 119
    },
    {
      "epoch": 0.5614035087719298,
      "grad_norm": 0.2592677175998688,
      "learning_rate": 0.00023968668407310702,
      "loss": 0.243,
      "step": 120
    },
    {
      "epoch": 0.5660818713450292,
      "grad_norm": 0.22330716252326965,
      "learning_rate": 0.00023890339425587464,
      "loss": 0.2483,
      "step": 121
    },
    {
      "epoch": 0.5707602339181287,
      "grad_norm": 0.24815818667411804,
      "learning_rate": 0.00023812010443864225,
      "loss": 0.2633,
      "step": 122
    },
    {
      "epoch": 0.5754385964912281,
      "grad_norm": 0.2487638145685196,
      "learning_rate": 0.0002373368146214099,
      "loss": 0.278,
      "step": 123
    },
    {
      "epoch": 0.5801169590643275,
      "grad_norm": 0.24891813099384308,
      "learning_rate": 0.00023655352480417754,
      "loss": 0.3049,
      "step": 124
    },
    {
      "epoch": 0.5847953216374269,
      "grad_norm": 0.24127408862113953,
      "learning_rate": 0.00023577023498694516,
      "loss": 0.2443,
      "step": 125
    },
    {
      "epoch": 0.5894736842105263,
      "grad_norm": 0.28364741802215576,
      "learning_rate": 0.00023498694516971278,
      "loss": 0.248,
      "step": 126
    },
    {
      "epoch": 0.5941520467836258,
      "grad_norm": 0.22733592987060547,
      "learning_rate": 0.0002342036553524804,
      "loss": 0.2537,
      "step": 127
    },
    {
      "epoch": 0.5988304093567252,
      "grad_norm": 0.2290065437555313,
      "learning_rate": 0.000233420365535248,
      "loss": 0.2254,
      "step": 128
    },
    {
      "epoch": 0.6035087719298246,
      "grad_norm": 0.31137585639953613,
      "learning_rate": 0.00023263707571801565,
      "loss": 0.2911,
      "step": 129
    },
    {
      "epoch": 0.6081871345029239,
      "grad_norm": 0.19938817620277405,
      "learning_rate": 0.00023185378590078327,
      "loss": 0.2594,
      "step": 130
    },
    {
      "epoch": 0.6128654970760234,
      "grad_norm": 0.2859763503074646,
      "learning_rate": 0.0002310704960835509,
      "loss": 0.2992,
      "step": 131
    },
    {
      "epoch": 0.6175438596491228,
      "grad_norm": 0.2581600248813629,
      "learning_rate": 0.0002302872062663185,
      "loss": 0.2792,
      "step": 132
    },
    {
      "epoch": 0.6222222222222222,
      "grad_norm": 0.2177847921848297,
      "learning_rate": 0.00022950391644908615,
      "loss": 0.2346,
      "step": 133
    },
    {
      "epoch": 0.6269005847953216,
      "grad_norm": 0.23706893622875214,
      "learning_rate": 0.00022872062663185377,
      "loss": 0.3001,
      "step": 134
    },
    {
      "epoch": 0.631578947368421,
      "grad_norm": 0.27587857842445374,
      "learning_rate": 0.00022793733681462138,
      "loss": 0.3208,
      "step": 135
    },
    {
      "epoch": 0.6362573099415205,
      "grad_norm": 0.22258973121643066,
      "learning_rate": 0.00022715404699738903,
      "loss": 0.2742,
      "step": 136
    },
    {
      "epoch": 0.6409356725146199,
      "grad_norm": 0.24578849971294403,
      "learning_rate": 0.00022637075718015664,
      "loss": 0.2611,
      "step": 137
    },
    {
      "epoch": 0.6456140350877193,
      "grad_norm": 0.21861237287521362,
      "learning_rate": 0.00022558746736292426,
      "loss": 0.2456,
      "step": 138
    },
    {
      "epoch": 0.6502923976608187,
      "grad_norm": 0.21501532196998596,
      "learning_rate": 0.00022480417754569188,
      "loss": 0.2476,
      "step": 139
    },
    {
      "epoch": 0.6549707602339181,
      "grad_norm": 0.2427855134010315,
      "learning_rate": 0.0002240208877284595,
      "loss": 0.2755,
      "step": 140
    },
    {
      "epoch": 0.6596491228070176,
      "grad_norm": 0.3790813982486725,
      "learning_rate": 0.00022323759791122714,
      "loss": 0.3094,
      "step": 141
    },
    {
      "epoch": 0.664327485380117,
      "grad_norm": 0.2384452521800995,
      "learning_rate": 0.00022245430809399478,
      "loss": 0.2292,
      "step": 142
    },
    {
      "epoch": 0.6690058479532164,
      "grad_norm": 0.2677914798259735,
      "learning_rate": 0.0002216710182767624,
      "loss": 0.3079,
      "step": 143
    },
    {
      "epoch": 0.6736842105263158,
      "grad_norm": 0.25200456380844116,
      "learning_rate": 0.00022088772845953002,
      "loss": 0.2865,
      "step": 144
    },
    {
      "epoch": 0.6783625730994152,
      "grad_norm": 0.33664393424987793,
      "learning_rate": 0.00022010443864229763,
      "loss": 0.2986,
      "step": 145
    },
    {
      "epoch": 0.6830409356725147,
      "grad_norm": 0.34141212701797485,
      "learning_rate": 0.00021932114882506525,
      "loss": 0.3164,
      "step": 146
    },
    {
      "epoch": 0.6877192982456141,
      "grad_norm": 0.2672103941440582,
      "learning_rate": 0.00021853785900783287,
      "loss": 0.2485,
      "step": 147
    },
    {
      "epoch": 0.6923976608187135,
      "grad_norm": 0.2521461844444275,
      "learning_rate": 0.00021775456919060048,
      "loss": 0.2573,
      "step": 148
    },
    {
      "epoch": 0.6970760233918128,
      "grad_norm": 0.22934138774871826,
      "learning_rate": 0.0002169712793733681,
      "loss": 0.268,
      "step": 149
    },
    {
      "epoch": 0.7017543859649122,
      "grad_norm": 0.37898167967796326,
      "learning_rate": 0.00021618798955613577,
      "loss": 0.3434,
      "step": 150
    },
    {
      "epoch": 0.7064327485380117,
      "grad_norm": 0.2533957064151764,
      "learning_rate": 0.0002154046997389034,
      "loss": 0.1994,
      "step": 151
    },
    {
      "epoch": 0.7111111111111111,
      "grad_norm": 0.2609672248363495,
      "learning_rate": 0.000214621409921671,
      "loss": 0.2618,
      "step": 152
    },
    {
      "epoch": 0.7157894736842105,
      "grad_norm": 0.3231838643550873,
      "learning_rate": 0.00021383812010443862,
      "loss": 0.3144,
      "step": 153
    },
    {
      "epoch": 0.7204678362573099,
      "grad_norm": 0.30110403895378113,
      "learning_rate": 0.00021305483028720624,
      "loss": 0.215,
      "step": 154
    },
    {
      "epoch": 0.7251461988304093,
      "grad_norm": 0.37865757942199707,
      "learning_rate": 0.00021227154046997386,
      "loss": 0.3252,
      "step": 155
    },
    {
      "epoch": 0.7298245614035088,
      "grad_norm": 0.29096391797065735,
      "learning_rate": 0.00021148825065274147,
      "loss": 0.2419,
      "step": 156
    },
    {
      "epoch": 0.7345029239766082,
      "grad_norm": 0.35364770889282227,
      "learning_rate": 0.00021070496083550912,
      "loss": 0.2914,
      "step": 157
    },
    {
      "epoch": 0.7391812865497076,
      "grad_norm": 0.2963542938232422,
      "learning_rate": 0.00020992167101827676,
      "loss": 0.2582,
      "step": 158
    },
    {
      "epoch": 0.743859649122807,
      "grad_norm": 0.27463939785957336,
      "learning_rate": 0.00020913838120104438,
      "loss": 0.2141,
      "step": 159
    },
    {
      "epoch": 0.7485380116959064,
      "grad_norm": 0.24969080090522766,
      "learning_rate": 0.000208355091383812,
      "loss": 0.2357,
      "step": 160
    },
    {
      "epoch": 0.7532163742690059,
      "grad_norm": 0.29395318031311035,
      "learning_rate": 0.0002075718015665796,
      "loss": 0.2745,
      "step": 161
    },
    {
      "epoch": 0.7578947368421053,
      "grad_norm": 0.2466636598110199,
      "learning_rate": 0.00020678851174934723,
      "loss": 0.254,
      "step": 162
    },
    {
      "epoch": 0.7625730994152047,
      "grad_norm": 0.27912116050720215,
      "learning_rate": 0.00020600522193211487,
      "loss": 0.2611,
      "step": 163
    },
    {
      "epoch": 0.7672514619883041,
      "grad_norm": 0.28236493468284607,
      "learning_rate": 0.0002052219321148825,
      "loss": 0.2567,
      "step": 164
    },
    {
      "epoch": 0.7719298245614035,
      "grad_norm": 0.33777257800102234,
      "learning_rate": 0.0002044386422976501,
      "loss": 0.2768,
      "step": 165
    },
    {
      "epoch": 0.776608187134503,
      "grad_norm": 0.3607426881790161,
      "learning_rate": 0.00020365535248041772,
      "loss": 0.2253,
      "step": 166
    },
    {
      "epoch": 0.7812865497076024,
      "grad_norm": 0.27317774295806885,
      "learning_rate": 0.00020287206266318537,
      "loss": 0.2524,
      "step": 167
    },
    {
      "epoch": 0.7859649122807018,
      "grad_norm": 0.3638685643672943,
      "learning_rate": 0.00020208877284595299,
      "loss": 0.29,
      "step": 168
    },
    {
      "epoch": 0.7906432748538011,
      "grad_norm": 0.33212950825691223,
      "learning_rate": 0.0002013054830287206,
      "loss": 0.2744,
      "step": 169
    },
    {
      "epoch": 0.7953216374269005,
      "grad_norm": 0.34307682514190674,
      "learning_rate": 0.00020052219321148825,
      "loss": 0.2909,
      "step": 170
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.36971572041511536,
      "learning_rate": 0.00019973890339425586,
      "loss": 0.2791,
      "step": 171
    },
    {
      "epoch": 0.8046783625730994,
      "grad_norm": 0.3130040764808655,
      "learning_rate": 0.00019895561357702348,
      "loss": 0.2949,
      "step": 172
    },
    {
      "epoch": 0.8093567251461988,
      "grad_norm": 0.2745920717716217,
      "learning_rate": 0.0001981723237597911,
      "loss": 0.2464,
      "step": 173
    },
    {
      "epoch": 0.8140350877192982,
      "grad_norm": 0.25954970717430115,
      "learning_rate": 0.00019738903394255871,
      "loss": 0.2263,
      "step": 174
    },
    {
      "epoch": 0.8187134502923976,
      "grad_norm": 0.2965265214443207,
      "learning_rate": 0.00019660574412532636,
      "loss": 0.2417,
      "step": 175
    },
    {
      "epoch": 0.8233918128654971,
      "grad_norm": 0.2949395477771759,
      "learning_rate": 0.000195822454308094,
      "loss": 0.2527,
      "step": 176
    },
    {
      "epoch": 0.8280701754385965,
      "grad_norm": 0.3458385169506073,
      "learning_rate": 0.00019503916449086162,
      "loss": 0.2788,
      "step": 177
    },
    {
      "epoch": 0.8327485380116959,
      "grad_norm": 0.28490325808525085,
      "learning_rate": 0.00019425587467362924,
      "loss": 0.2558,
      "step": 178
    },
    {
      "epoch": 0.8374269005847953,
      "grad_norm": 0.3123329281806946,
      "learning_rate": 0.00019347258485639685,
      "loss": 0.2893,
      "step": 179
    },
    {
      "epoch": 0.8421052631578947,
      "grad_norm": 0.26416218280792236,
      "learning_rate": 0.00019268929503916447,
      "loss": 0.2444,
      "step": 180
    },
    {
      "epoch": 0.8467836257309942,
      "grad_norm": 0.3371085226535797,
      "learning_rate": 0.0001919060052219321,
      "loss": 0.1849,
      "step": 181
    },
    {
      "epoch": 0.8514619883040936,
      "grad_norm": 0.30682268738746643,
      "learning_rate": 0.0001911227154046997,
      "loss": 0.2731,
      "step": 182
    },
    {
      "epoch": 0.856140350877193,
      "grad_norm": 0.33372190594673157,
      "learning_rate": 0.00019033942558746732,
      "loss": 0.2824,
      "step": 183
    },
    {
      "epoch": 0.8608187134502924,
      "grad_norm": 0.2942568361759186,
      "learning_rate": 0.000189556135770235,
      "loss": 0.2337,
      "step": 184
    },
    {
      "epoch": 0.8654970760233918,
      "grad_norm": 0.3447636663913727,
      "learning_rate": 0.0001887728459530026,
      "loss": 0.2829,
      "step": 185
    },
    {
      "epoch": 0.8701754385964913,
      "grad_norm": 0.3520638644695282,
      "learning_rate": 0.00018798955613577023,
      "loss": 0.2428,
      "step": 186
    },
    {
      "epoch": 0.8748538011695907,
      "grad_norm": 0.4163283109664917,
      "learning_rate": 0.00018720626631853784,
      "loss": 0.3016,
      "step": 187
    },
    {
      "epoch": 0.87953216374269,
      "grad_norm": 0.34025838971138,
      "learning_rate": 0.00018642297650130546,
      "loss": 0.3128,
      "step": 188
    },
    {
      "epoch": 0.8842105263157894,
      "grad_norm": 0.33462709188461304,
      "learning_rate": 0.00018563968668407308,
      "loss": 0.2771,
      "step": 189
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 0.32702791690826416,
      "learning_rate": 0.0001848563968668407,
      "loss": 0.2868,
      "step": 190
    },
    {
      "epoch": 0.8935672514619883,
      "grad_norm": 0.2735498547554016,
      "learning_rate": 0.00018407310704960834,
      "loss": 0.2707,
      "step": 191
    },
    {
      "epoch": 0.8982456140350877,
      "grad_norm": 0.3081477880477905,
      "learning_rate": 0.00018328981723237598,
      "loss": 0.2996,
      "step": 192
    },
    {
      "epoch": 0.9029239766081871,
      "grad_norm": 0.31048181653022766,
      "learning_rate": 0.0001825065274151436,
      "loss": 0.2354,
      "step": 193
    },
    {
      "epoch": 0.9076023391812865,
      "grad_norm": 0.32027798891067505,
      "learning_rate": 0.00018172323759791122,
      "loss": 0.2476,
      "step": 194
    },
    {
      "epoch": 0.9122807017543859,
      "grad_norm": 0.36916154623031616,
      "learning_rate": 0.00018093994778067883,
      "loss": 0.272,
      "step": 195
    },
    {
      "epoch": 0.9169590643274854,
      "grad_norm": 0.2867410480976105,
      "learning_rate": 0.00018015665796344645,
      "loss": 0.2454,
      "step": 196
    },
    {
      "epoch": 0.9216374269005848,
      "grad_norm": 0.29192161560058594,
      "learning_rate": 0.00017937336814621407,
      "loss": 0.2391,
      "step": 197
    },
    {
      "epoch": 0.9263157894736842,
      "grad_norm": 0.29950541257858276,
      "learning_rate": 0.0001785900783289817,
      "loss": 0.26,
      "step": 198
    },
    {
      "epoch": 0.9309941520467836,
      "grad_norm": 0.29537010192871094,
      "learning_rate": 0.00017780678851174933,
      "loss": 0.224,
      "step": 199
    },
    {
      "epoch": 0.935672514619883,
      "grad_norm": 0.39636072516441345,
      "learning_rate": 0.00017702349869451694,
      "loss": 0.2292,
      "step": 200
    },
    {
      "epoch": 0.9403508771929825,
      "grad_norm": 0.32127508521080017,
      "learning_rate": 0.0001762402088772846,
      "loss": 0.2812,
      "step": 201
    },
    {
      "epoch": 0.9450292397660819,
      "grad_norm": 0.2573666572570801,
      "learning_rate": 0.0001754569190600522,
      "loss": 0.179,
      "step": 202
    },
    {
      "epoch": 0.9497076023391813,
      "grad_norm": 0.4301735758781433,
      "learning_rate": 0.00017467362924281982,
      "loss": 0.2977,
      "step": 203
    },
    {
      "epoch": 0.9543859649122807,
      "grad_norm": 0.2594972252845764,
      "learning_rate": 0.00017389033942558747,
      "loss": 0.2173,
      "step": 204
    },
    {
      "epoch": 0.9590643274853801,
      "grad_norm": 0.27727827429771423,
      "learning_rate": 0.00017310704960835508,
      "loss": 0.2462,
      "step": 205
    },
    {
      "epoch": 0.9637426900584796,
      "grad_norm": 0.30286967754364014,
      "learning_rate": 0.0001723237597911227,
      "loss": 0.2828,
      "step": 206
    },
    {
      "epoch": 0.968421052631579,
      "grad_norm": 0.49924585223197937,
      "learning_rate": 0.00017154046997389032,
      "loss": 0.2798,
      "step": 207
    },
    {
      "epoch": 0.9730994152046784,
      "grad_norm": 0.32737424969673157,
      "learning_rate": 0.00017075718015665793,
      "loss": 0.2899,
      "step": 208
    },
    {
      "epoch": 0.9777777777777777,
      "grad_norm": 0.29130709171295166,
      "learning_rate": 0.00016997389033942558,
      "loss": 0.2012,
      "step": 209
    },
    {
      "epoch": 0.9824561403508771,
      "grad_norm": 0.32460060715675354,
      "learning_rate": 0.00016919060052219322,
      "loss": 0.2519,
      "step": 210
    },
    {
      "epoch": 0.9871345029239766,
      "grad_norm": 0.36065396666526794,
      "learning_rate": 0.00016840731070496084,
      "loss": 0.2729,
      "step": 211
    },
    {
      "epoch": 0.991812865497076,
      "grad_norm": 0.27394887804985046,
      "learning_rate": 0.00016762402088772846,
      "loss": 0.2471,
      "step": 212
    },
    {
      "epoch": 0.9964912280701754,
      "grad_norm": 0.26890718936920166,
      "learning_rate": 0.00016684073107049607,
      "loss": 0.1939,
      "step": 213
    },
    {
      "epoch": 0.9964912280701754,
      "eval_loss": 0.3251745104789734,
      "eval_runtime": 128.9558,
      "eval_samples_per_second": 4.281,
      "eval_steps_per_second": 0.535,
      "step": 213
    },
    {
      "epoch": 1.001169590643275,
      "grad_norm": 0.47864651679992676,
      "learning_rate": 0.0001660574412532637,
      "loss": 0.2979,
      "step": 214
    },
    {
      "epoch": 1.0058479532163742,
      "grad_norm": 0.4087766110897064,
      "learning_rate": 0.0001652741514360313,
      "loss": 0.2373,
      "step": 215
    },
    {
      "epoch": 1.0105263157894737,
      "grad_norm": 0.28931185603141785,
      "learning_rate": 0.00016449086161879892,
      "loss": 0.24,
      "step": 216
    },
    {
      "epoch": 1.015204678362573,
      "grad_norm": 0.36823251843452454,
      "learning_rate": 0.00016370757180156654,
      "loss": 0.2724,
      "step": 217
    },
    {
      "epoch": 1.0198830409356725,
      "grad_norm": 0.3195100724697113,
      "learning_rate": 0.0001629242819843342,
      "loss": 0.2439,
      "step": 218
    },
    {
      "epoch": 1.024561403508772,
      "grad_norm": 0.35398000478744507,
      "learning_rate": 0.00016214099216710183,
      "loss": 0.2383,
      "step": 219
    },
    {
      "epoch": 1.0292397660818713,
      "grad_norm": 0.32787102460861206,
      "learning_rate": 0.00016135770234986945,
      "loss": 0.2215,
      "step": 220
    },
    {
      "epoch": 1.0339181286549708,
      "grad_norm": 0.3576354682445526,
      "learning_rate": 0.00016057441253263706,
      "loss": 0.2437,
      "step": 221
    },
    {
      "epoch": 1.03859649122807,
      "grad_norm": 0.33514851331710815,
      "learning_rate": 0.00015979112271540468,
      "loss": 0.2231,
      "step": 222
    },
    {
      "epoch": 1.0432748538011696,
      "grad_norm": 0.30459219217300415,
      "learning_rate": 0.0001590078328981723,
      "loss": 0.201,
      "step": 223
    },
    {
      "epoch": 1.047953216374269,
      "grad_norm": 0.3334907591342926,
      "learning_rate": 0.00015822454308093991,
      "loss": 0.2087,
      "step": 224
    },
    {
      "epoch": 1.0526315789473684,
      "grad_norm": 0.32965296506881714,
      "learning_rate": 0.00015744125326370756,
      "loss": 0.2097,
      "step": 225
    },
    {
      "epoch": 1.0573099415204679,
      "grad_norm": 0.37934255599975586,
      "learning_rate": 0.0001566579634464752,
      "loss": 0.2464,
      "step": 226
    },
    {
      "epoch": 1.0619883040935671,
      "grad_norm": 0.40968143939971924,
      "learning_rate": 0.00015587467362924282,
      "loss": 0.2801,
      "step": 227
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 0.3816128969192505,
      "learning_rate": 0.00015509138381201044,
      "loss": 0.2349,
      "step": 228
    },
    {
      "epoch": 1.0713450292397662,
      "grad_norm": 0.3424300253391266,
      "learning_rate": 0.00015430809399477805,
      "loss": 0.213,
      "step": 229
    },
    {
      "epoch": 1.0760233918128654,
      "grad_norm": 0.286561518907547,
      "learning_rate": 0.00015352480417754567,
      "loss": 0.2276,
      "step": 230
    },
    {
      "epoch": 1.080701754385965,
      "grad_norm": 0.30346250534057617,
      "learning_rate": 0.0001527415143603133,
      "loss": 0.172,
      "step": 231
    },
    {
      "epoch": 1.0853801169590644,
      "grad_norm": 0.30811095237731934,
      "learning_rate": 0.00015195822454308093,
      "loss": 0.2291,
      "step": 232
    },
    {
      "epoch": 1.0900584795321637,
      "grad_norm": 0.3824582099914551,
      "learning_rate": 0.00015117493472584855,
      "loss": 0.2726,
      "step": 233
    },
    {
      "epoch": 1.0947368421052632,
      "grad_norm": 0.32786568999290466,
      "learning_rate": 0.00015039164490861616,
      "loss": 0.1959,
      "step": 234
    },
    {
      "epoch": 1.0994152046783625,
      "grad_norm": 0.348880410194397,
      "learning_rate": 0.0001496083550913838,
      "loss": 0.2943,
      "step": 235
    },
    {
      "epoch": 1.104093567251462,
      "grad_norm": 0.3277477025985718,
      "learning_rate": 0.00014882506527415143,
      "loss": 0.2181,
      "step": 236
    },
    {
      "epoch": 1.1087719298245613,
      "grad_norm": 0.2836081385612488,
      "learning_rate": 0.00014804177545691904,
      "loss": 0.2271,
      "step": 237
    },
    {
      "epoch": 1.1134502923976608,
      "grad_norm": 0.32830891013145447,
      "learning_rate": 0.0001472584856396867,
      "loss": 0.2353,
      "step": 238
    },
    {
      "epoch": 1.1181286549707603,
      "grad_norm": 0.3768024146556854,
      "learning_rate": 0.0001464751958224543,
      "loss": 0.2516,
      "step": 239
    },
    {
      "epoch": 1.1228070175438596,
      "grad_norm": 0.3259262144565582,
      "learning_rate": 0.00014569190600522192,
      "loss": 0.2525,
      "step": 240
    },
    {
      "epoch": 1.127485380116959,
      "grad_norm": 0.283133864402771,
      "learning_rate": 0.00014490861618798956,
      "loss": 0.1993,
      "step": 241
    },
    {
      "epoch": 1.1321637426900586,
      "grad_norm": 0.4240041673183441,
      "learning_rate": 0.00014412532637075718,
      "loss": 0.2294,
      "step": 242
    },
    {
      "epoch": 1.1368421052631579,
      "grad_norm": 0.26955336332321167,
      "learning_rate": 0.0001433420365535248,
      "loss": 0.2018,
      "step": 243
    },
    {
      "epoch": 1.1415204678362574,
      "grad_norm": 0.24795332551002502,
      "learning_rate": 0.00014255874673629242,
      "loss": 0.184,
      "step": 244
    },
    {
      "epoch": 1.1461988304093567,
      "grad_norm": 0.26709529757499695,
      "learning_rate": 0.00014177545691906003,
      "loss": 0.2012,
      "step": 245
    },
    {
      "epoch": 1.1508771929824562,
      "grad_norm": 0.2931155860424042,
      "learning_rate": 0.00014099216710182768,
      "loss": 0.1777,
      "step": 246
    },
    {
      "epoch": 1.1555555555555554,
      "grad_norm": 0.2925533354282379,
      "learning_rate": 0.0001402088772845953,
      "loss": 0.2088,
      "step": 247
    },
    {
      "epoch": 1.160233918128655,
      "grad_norm": 0.29598569869995117,
      "learning_rate": 0.0001394255874673629,
      "loss": 0.1934,
      "step": 248
    },
    {
      "epoch": 1.1649122807017545,
      "grad_norm": 0.42605283856391907,
      "learning_rate": 0.00013864229765013053,
      "loss": 0.2713,
      "step": 249
    },
    {
      "epoch": 1.1695906432748537,
      "grad_norm": 0.345916211605072,
      "learning_rate": 0.00013785900783289817,
      "loss": 0.2164,
      "step": 250
    },
    {
      "epoch": 1.1742690058479532,
      "grad_norm": 0.34757792949676514,
      "learning_rate": 0.0001370757180156658,
      "loss": 0.2659,
      "step": 251
    },
    {
      "epoch": 1.1789473684210527,
      "grad_norm": 0.296681672334671,
      "learning_rate": 0.0001362924281984334,
      "loss": 0.1773,
      "step": 252
    },
    {
      "epoch": 1.183625730994152,
      "grad_norm": 0.3245440423488617,
      "learning_rate": 0.00013550913838120102,
      "loss": 0.2418,
      "step": 253
    },
    {
      "epoch": 1.1883040935672515,
      "grad_norm": 0.3684275448322296,
      "learning_rate": 0.00013472584856396867,
      "loss": 0.1758,
      "step": 254
    },
    {
      "epoch": 1.1929824561403508,
      "grad_norm": 0.3379068076610565,
      "learning_rate": 0.00013394255874673628,
      "loss": 0.2451,
      "step": 255
    },
    {
      "epoch": 1.1976608187134503,
      "grad_norm": 0.3327611982822418,
      "learning_rate": 0.0001331592689295039,
      "loss": 0.2125,
      "step": 256
    },
    {
      "epoch": 1.2023391812865496,
      "grad_norm": 0.3268977701663971,
      "learning_rate": 0.00013237597911227152,
      "loss": 0.2105,
      "step": 257
    },
    {
      "epoch": 1.207017543859649,
      "grad_norm": 0.426891028881073,
      "learning_rate": 0.00013159268929503916,
      "loss": 0.2486,
      "step": 258
    },
    {
      "epoch": 1.2116959064327486,
      "grad_norm": 0.33537960052490234,
      "learning_rate": 0.00013080939947780678,
      "loss": 0.2378,
      "step": 259
    },
    {
      "epoch": 1.2163742690058479,
      "grad_norm": 0.3022681474685669,
      "learning_rate": 0.0001300261096605744,
      "loss": 0.2172,
      "step": 260
    },
    {
      "epoch": 1.2210526315789474,
      "grad_norm": 0.3347622752189636,
      "learning_rate": 0.000129242819843342,
      "loss": 0.212,
      "step": 261
    },
    {
      "epoch": 1.225730994152047,
      "grad_norm": 0.36888957023620605,
      "learning_rate": 0.00012845953002610963,
      "loss": 0.2619,
      "step": 262
    },
    {
      "epoch": 1.2304093567251462,
      "grad_norm": 0.33064451813697815,
      "learning_rate": 0.00012767624020887727,
      "loss": 0.2491,
      "step": 263
    },
    {
      "epoch": 1.2350877192982457,
      "grad_norm": 0.2781592309474945,
      "learning_rate": 0.0001268929503916449,
      "loss": 0.1469,
      "step": 264
    },
    {
      "epoch": 1.239766081871345,
      "grad_norm": 0.3164032995700836,
      "learning_rate": 0.0001261096605744125,
      "loss": 0.1851,
      "step": 265
    },
    {
      "epoch": 1.2444444444444445,
      "grad_norm": 0.27722832560539246,
      "learning_rate": 0.00012532637075718015,
      "loss": 0.2284,
      "step": 266
    },
    {
      "epoch": 1.2491228070175437,
      "grad_norm": 0.2780461311340332,
      "learning_rate": 0.00012454308093994777,
      "loss": 0.2437,
      "step": 267
    },
    {
      "epoch": 1.2538011695906432,
      "grad_norm": 0.3297116756439209,
      "learning_rate": 0.00012375979112271538,
      "loss": 0.2237,
      "step": 268
    },
    {
      "epoch": 1.2584795321637428,
      "grad_norm": 0.27799272537231445,
      "learning_rate": 0.00012297650130548303,
      "loss": 0.1776,
      "step": 269
    },
    {
      "epoch": 1.263157894736842,
      "grad_norm": 0.2784176468849182,
      "learning_rate": 0.00012219321148825065,
      "loss": 0.2155,
      "step": 270
    },
    {
      "epoch": 1.2678362573099415,
      "grad_norm": 0.300026535987854,
      "learning_rate": 0.00012140992167101828,
      "loss": 0.2094,
      "step": 271
    },
    {
      "epoch": 1.272514619883041,
      "grad_norm": 0.3253544867038727,
      "learning_rate": 0.00012062663185378589,
      "loss": 0.2164,
      "step": 272
    },
    {
      "epoch": 1.2771929824561403,
      "grad_norm": 0.3394622504711151,
      "learning_rate": 0.00011984334203655351,
      "loss": 0.2435,
      "step": 273
    },
    {
      "epoch": 1.2818713450292398,
      "grad_norm": 0.3456868529319763,
      "learning_rate": 0.00011906005221932113,
      "loss": 0.2226,
      "step": 274
    },
    {
      "epoch": 1.286549707602339,
      "grad_norm": 0.3763778805732727,
      "learning_rate": 0.00011827676240208877,
      "loss": 0.2466,
      "step": 275
    },
    {
      "epoch": 1.2912280701754386,
      "grad_norm": 0.2942145764827728,
      "learning_rate": 0.00011749347258485639,
      "loss": 0.193,
      "step": 276
    },
    {
      "epoch": 1.295906432748538,
      "grad_norm": 0.35192808508872986,
      "learning_rate": 0.000116710182767624,
      "loss": 0.2165,
      "step": 277
    },
    {
      "epoch": 1.3005847953216374,
      "grad_norm": 0.26470667123794556,
      "learning_rate": 0.00011592689295039164,
      "loss": 0.1782,
      "step": 278
    },
    {
      "epoch": 1.305263157894737,
      "grad_norm": 0.35745522379875183,
      "learning_rate": 0.00011514360313315925,
      "loss": 0.2365,
      "step": 279
    },
    {
      "epoch": 1.3099415204678362,
      "grad_norm": 0.3310922682285309,
      "learning_rate": 0.00011436031331592688,
      "loss": 0.1609,
      "step": 280
    },
    {
      "epoch": 1.3146198830409357,
      "grad_norm": 0.30225375294685364,
      "learning_rate": 0.00011357702349869451,
      "loss": 0.1795,
      "step": 281
    },
    {
      "epoch": 1.3192982456140352,
      "grad_norm": 0.3522951304912567,
      "learning_rate": 0.00011279373368146213,
      "loss": 0.1999,
      "step": 282
    },
    {
      "epoch": 1.3239766081871345,
      "grad_norm": 0.3027491867542267,
      "learning_rate": 0.00011201044386422975,
      "loss": 0.1785,
      "step": 283
    },
    {
      "epoch": 1.328654970760234,
      "grad_norm": 0.3386290371417999,
      "learning_rate": 0.00011122715404699739,
      "loss": 0.2143,
      "step": 284
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 0.3328944742679596,
      "learning_rate": 0.00011044386422976501,
      "loss": 0.177,
      "step": 285
    },
    {
      "epoch": 1.3380116959064328,
      "grad_norm": 0.32903677225112915,
      "learning_rate": 0.00010966057441253263,
      "loss": 0.212,
      "step": 286
    },
    {
      "epoch": 1.342690058479532,
      "grad_norm": 0.41752883791923523,
      "learning_rate": 0.00010887728459530024,
      "loss": 0.2899,
      "step": 287
    },
    {
      "epoch": 1.3473684210526315,
      "grad_norm": 0.34161877632141113,
      "learning_rate": 0.00010809399477806789,
      "loss": 0.2283,
      "step": 288
    },
    {
      "epoch": 1.352046783625731,
      "grad_norm": 0.3379518389701843,
      "learning_rate": 0.0001073107049608355,
      "loss": 0.1774,
      "step": 289
    },
    {
      "epoch": 1.3567251461988303,
      "grad_norm": 0.3521679639816284,
      "learning_rate": 0.00010652741514360312,
      "loss": 0.1999,
      "step": 290
    },
    {
      "epoch": 1.3614035087719298,
      "grad_norm": 0.36844363808631897,
      "learning_rate": 0.00010574412532637074,
      "loss": 0.2377,
      "step": 291
    },
    {
      "epoch": 1.3660818713450293,
      "grad_norm": 0.3922882676124573,
      "learning_rate": 0.00010496083550913838,
      "loss": 0.2489,
      "step": 292
    },
    {
      "epoch": 1.3707602339181286,
      "grad_norm": 0.3789832293987274,
      "learning_rate": 0.000104177545691906,
      "loss": 0.2395,
      "step": 293
    },
    {
      "epoch": 1.3754385964912281,
      "grad_norm": 0.3401392698287964,
      "learning_rate": 0.00010339425587467362,
      "loss": 0.2086,
      "step": 294
    },
    {
      "epoch": 1.3801169590643274,
      "grad_norm": 0.3572171628475189,
      "learning_rate": 0.00010261096605744125,
      "loss": 0.2077,
      "step": 295
    },
    {
      "epoch": 1.384795321637427,
      "grad_norm": 0.3946603834629059,
      "learning_rate": 0.00010182767624020886,
      "loss": 0.229,
      "step": 296
    },
    {
      "epoch": 1.3894736842105262,
      "grad_norm": 0.3405977487564087,
      "learning_rate": 0.00010104438642297649,
      "loss": 0.2274,
      "step": 297
    },
    {
      "epoch": 1.3941520467836257,
      "grad_norm": 0.34352976083755493,
      "learning_rate": 0.00010026109660574412,
      "loss": 0.176,
      "step": 298
    },
    {
      "epoch": 1.3988304093567252,
      "grad_norm": 0.33276814222335815,
      "learning_rate": 9.947780678851174e-05,
      "loss": 0.2394,
      "step": 299
    },
    {
      "epoch": 1.4035087719298245,
      "grad_norm": 0.4108532667160034,
      "learning_rate": 9.869451697127936e-05,
      "loss": 0.2468,
      "step": 300
    },
    {
      "epoch": 1.408187134502924,
      "grad_norm": 0.3485938310623169,
      "learning_rate": 9.7911227154047e-05,
      "loss": 0.1962,
      "step": 301
    },
    {
      "epoch": 1.4128654970760235,
      "grad_norm": 0.34231987595558167,
      "learning_rate": 9.712793733681462e-05,
      "loss": 0.1864,
      "step": 302
    },
    {
      "epoch": 1.4175438596491228,
      "grad_norm": 0.34098300337791443,
      "learning_rate": 9.634464751958224e-05,
      "loss": 0.2621,
      "step": 303
    },
    {
      "epoch": 1.4222222222222223,
      "grad_norm": 0.32759857177734375,
      "learning_rate": 9.556135770234985e-05,
      "loss": 0.1543,
      "step": 304
    },
    {
      "epoch": 1.4269005847953216,
      "grad_norm": 0.33690345287323,
      "learning_rate": 9.47780678851175e-05,
      "loss": 0.1879,
      "step": 305
    },
    {
      "epoch": 1.431578947368421,
      "grad_norm": 0.36911341547966003,
      "learning_rate": 9.399477806788511e-05,
      "loss": 0.2094,
      "step": 306
    },
    {
      "epoch": 1.4362573099415203,
      "grad_norm": 0.3638951778411865,
      "learning_rate": 9.321148825065273e-05,
      "loss": 0.1839,
      "step": 307
    },
    {
      "epoch": 1.4409356725146198,
      "grad_norm": 0.3131489157676697,
      "learning_rate": 9.242819843342035e-05,
      "loss": 0.2224,
      "step": 308
    },
    {
      "epoch": 1.4456140350877194,
      "grad_norm": 0.3330543637275696,
      "learning_rate": 9.164490861618799e-05,
      "loss": 0.2359,
      "step": 309
    },
    {
      "epoch": 1.4502923976608186,
      "grad_norm": 0.35975563526153564,
      "learning_rate": 9.086161879895561e-05,
      "loss": 0.2443,
      "step": 310
    },
    {
      "epoch": 1.4549707602339181,
      "grad_norm": 0.3524896204471588,
      "learning_rate": 9.007832898172323e-05,
      "loss": 0.2602,
      "step": 311
    },
    {
      "epoch": 1.4596491228070176,
      "grad_norm": 0.37578365206718445,
      "learning_rate": 8.929503916449086e-05,
      "loss": 0.2268,
      "step": 312
    },
    {
      "epoch": 1.464327485380117,
      "grad_norm": 0.3761547803878784,
      "learning_rate": 8.851174934725847e-05,
      "loss": 0.2187,
      "step": 313
    },
    {
      "epoch": 1.4690058479532164,
      "grad_norm": 0.2988968789577484,
      "learning_rate": 8.77284595300261e-05,
      "loss": 0.1944,
      "step": 314
    },
    {
      "epoch": 1.4736842105263157,
      "grad_norm": 0.3787313997745514,
      "learning_rate": 8.694516971279373e-05,
      "loss": 0.1982,
      "step": 315
    },
    {
      "epoch": 1.4783625730994152,
      "grad_norm": 0.3259766697883606,
      "learning_rate": 8.616187989556135e-05,
      "loss": 0.1979,
      "step": 316
    },
    {
      "epoch": 1.4830409356725145,
      "grad_norm": 0.35513848066329956,
      "learning_rate": 8.537859007832897e-05,
      "loss": 0.2457,
      "step": 317
    },
    {
      "epoch": 1.487719298245614,
      "grad_norm": 0.3410692811012268,
      "learning_rate": 8.459530026109661e-05,
      "loss": 0.2062,
      "step": 318
    },
    {
      "epoch": 1.4923976608187135,
      "grad_norm": 0.3280562162399292,
      "learning_rate": 8.381201044386423e-05,
      "loss": 0.2052,
      "step": 319
    },
    {
      "epoch": 1.4970760233918128,
      "grad_norm": 0.3543587028980255,
      "learning_rate": 8.302872062663185e-05,
      "loss": 0.238,
      "step": 320
    },
    {
      "epoch": 1.5017543859649123,
      "grad_norm": 0.3387645483016968,
      "learning_rate": 8.224543080939946e-05,
      "loss": 0.2099,
      "step": 321
    },
    {
      "epoch": 1.5064327485380118,
      "grad_norm": 0.3247331976890564,
      "learning_rate": 8.14621409921671e-05,
      "loss": 0.2052,
      "step": 322
    },
    {
      "epoch": 1.511111111111111,
      "grad_norm": 0.34804487228393555,
      "learning_rate": 8.067885117493472e-05,
      "loss": 0.2501,
      "step": 323
    },
    {
      "epoch": 1.5157894736842106,
      "grad_norm": 0.37038812041282654,
      "learning_rate": 7.989556135770234e-05,
      "loss": 0.2341,
      "step": 324
    },
    {
      "epoch": 1.52046783625731,
      "grad_norm": 0.4113128185272217,
      "learning_rate": 7.911227154046996e-05,
      "loss": 0.2455,
      "step": 325
    },
    {
      "epoch": 1.5251461988304094,
      "grad_norm": 0.32564428448677063,
      "learning_rate": 7.83289817232376e-05,
      "loss": 0.2283,
      "step": 326
    },
    {
      "epoch": 1.5298245614035086,
      "grad_norm": 0.35682302713394165,
      "learning_rate": 7.754569190600522e-05,
      "loss": 0.2444,
      "step": 327
    },
    {
      "epoch": 1.5345029239766081,
      "grad_norm": 0.29150667786598206,
      "learning_rate": 7.676240208877283e-05,
      "loss": 0.1732,
      "step": 328
    },
    {
      "epoch": 1.5391812865497077,
      "grad_norm": 0.3154532015323639,
      "learning_rate": 7.597911227154047e-05,
      "loss": 0.2116,
      "step": 329
    },
    {
      "epoch": 1.543859649122807,
      "grad_norm": 0.3107912838459015,
      "learning_rate": 7.519582245430808e-05,
      "loss": 0.2132,
      "step": 330
    },
    {
      "epoch": 1.5485380116959064,
      "grad_norm": 0.3499608635902405,
      "learning_rate": 7.441253263707571e-05,
      "loss": 0.2456,
      "step": 331
    },
    {
      "epoch": 1.553216374269006,
      "grad_norm": 0.3285101056098938,
      "learning_rate": 7.362924281984334e-05,
      "loss": 0.2028,
      "step": 332
    },
    {
      "epoch": 1.5578947368421052,
      "grad_norm": 0.34131646156311035,
      "learning_rate": 7.284595300261096e-05,
      "loss": 0.2022,
      "step": 333
    },
    {
      "epoch": 1.5625730994152047,
      "grad_norm": 0.34292858839035034,
      "learning_rate": 7.206266318537859e-05,
      "loss": 0.2094,
      "step": 334
    },
    {
      "epoch": 1.5672514619883042,
      "grad_norm": 0.4082040786743164,
      "learning_rate": 7.127937336814621e-05,
      "loss": 0.2254,
      "step": 335
    },
    {
      "epoch": 1.5719298245614035,
      "grad_norm": 0.29188257455825806,
      "learning_rate": 7.049608355091384e-05,
      "loss": 0.1826,
      "step": 336
    },
    {
      "epoch": 1.5766081871345028,
      "grad_norm": 0.3536394536495209,
      "learning_rate": 6.971279373368146e-05,
      "loss": 0.2154,
      "step": 337
    },
    {
      "epoch": 1.5812865497076023,
      "grad_norm": 0.2998174726963043,
      "learning_rate": 6.892950391644909e-05,
      "loss": 0.1809,
      "step": 338
    },
    {
      "epoch": 1.5859649122807018,
      "grad_norm": 0.33750003576278687,
      "learning_rate": 6.81462140992167e-05,
      "loss": 0.2313,
      "step": 339
    },
    {
      "epoch": 1.590643274853801,
      "grad_norm": 0.3651552200317383,
      "learning_rate": 6.736292428198433e-05,
      "loss": 0.2105,
      "step": 340
    },
    {
      "epoch": 1.5953216374269006,
      "grad_norm": 0.4456983506679535,
      "learning_rate": 6.657963446475195e-05,
      "loss": 0.2416,
      "step": 341
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.3395911753177643,
      "learning_rate": 6.579634464751958e-05,
      "loss": 0.1783,
      "step": 342
    },
    {
      "epoch": 1.6046783625730994,
      "grad_norm": 0.36077216267585754,
      "learning_rate": 6.50130548302872e-05,
      "loss": 0.2321,
      "step": 343
    },
    {
      "epoch": 1.6093567251461989,
      "grad_norm": 0.3595292866230011,
      "learning_rate": 6.422976501305481e-05,
      "loss": 0.2372,
      "step": 344
    },
    {
      "epoch": 1.6140350877192984,
      "grad_norm": 0.25597092509269714,
      "learning_rate": 6.344647519582244e-05,
      "loss": 0.1655,
      "step": 345
    },
    {
      "epoch": 1.6187134502923977,
      "grad_norm": 0.33444324135780334,
      "learning_rate": 6.266318537859008e-05,
      "loss": 0.2083,
      "step": 346
    },
    {
      "epoch": 1.623391812865497,
      "grad_norm": 0.3456198573112488,
      "learning_rate": 6.187989556135769e-05,
      "loss": 0.2349,
      "step": 347
    },
    {
      "epoch": 1.6280701754385964,
      "grad_norm": 0.364891916513443,
      "learning_rate": 6.109660574412532e-05,
      "loss": 0.1967,
      "step": 348
    },
    {
      "epoch": 1.632748538011696,
      "grad_norm": 0.38645410537719727,
      "learning_rate": 6.0313315926892947e-05,
      "loss": 0.2541,
      "step": 349
    },
    {
      "epoch": 1.6374269005847952,
      "grad_norm": 0.39390674233436584,
      "learning_rate": 5.9530026109660564e-05,
      "loss": 0.24,
      "step": 350
    },
    {
      "epoch": 1.6421052631578947,
      "grad_norm": 0.33374619483947754,
      "learning_rate": 5.8746736292428194e-05,
      "loss": 0.2058,
      "step": 351
    },
    {
      "epoch": 1.6467836257309942,
      "grad_norm": 0.35246360301971436,
      "learning_rate": 5.796344647519582e-05,
      "loss": 0.2179,
      "step": 352
    },
    {
      "epoch": 1.6514619883040935,
      "grad_norm": 0.34983882308006287,
      "learning_rate": 5.718015665796344e-05,
      "loss": 0.2342,
      "step": 353
    },
    {
      "epoch": 1.656140350877193,
      "grad_norm": 0.38110777735710144,
      "learning_rate": 5.6396866840731065e-05,
      "loss": 0.1944,
      "step": 354
    },
    {
      "epoch": 1.6608187134502925,
      "grad_norm": 0.3668384552001953,
      "learning_rate": 5.5613577023498696e-05,
      "loss": 0.1889,
      "step": 355
    },
    {
      "epoch": 1.6654970760233918,
      "grad_norm": 0.33782637119293213,
      "learning_rate": 5.483028720626631e-05,
      "loss": 0.1667,
      "step": 356
    },
    {
      "epoch": 1.670175438596491,
      "grad_norm": 0.3069373369216919,
      "learning_rate": 5.404699738903394e-05,
      "loss": 0.2356,
      "step": 357
    },
    {
      "epoch": 1.6748538011695906,
      "grad_norm": 0.40085291862487793,
      "learning_rate": 5.326370757180156e-05,
      "loss": 0.2484,
      "step": 358
    },
    {
      "epoch": 1.67953216374269,
      "grad_norm": 0.3321574330329895,
      "learning_rate": 5.248041775456919e-05,
      "loss": 0.1979,
      "step": 359
    },
    {
      "epoch": 1.6842105263157894,
      "grad_norm": 0.3114027678966522,
      "learning_rate": 5.169712793733681e-05,
      "loss": 0.208,
      "step": 360
    },
    {
      "epoch": 1.6888888888888889,
      "grad_norm": 0.28668585419654846,
      "learning_rate": 5.091383812010443e-05,
      "loss": 0.2011,
      "step": 361
    },
    {
      "epoch": 1.6935672514619884,
      "grad_norm": 0.3683081567287445,
      "learning_rate": 5.013054830287206e-05,
      "loss": 0.2186,
      "step": 362
    },
    {
      "epoch": 1.6982456140350877,
      "grad_norm": 0.3685920536518097,
      "learning_rate": 4.934725848563968e-05,
      "loss": 0.2465,
      "step": 363
    },
    {
      "epoch": 1.7029239766081872,
      "grad_norm": 0.41169387102127075,
      "learning_rate": 4.856396866840731e-05,
      "loss": 0.2465,
      "step": 364
    },
    {
      "epoch": 1.7076023391812867,
      "grad_norm": 0.41575515270233154,
      "learning_rate": 4.7780678851174926e-05,
      "loss": 0.2168,
      "step": 365
    },
    {
      "epoch": 1.712280701754386,
      "grad_norm": 0.3924039304256439,
      "learning_rate": 4.6997389033942557e-05,
      "loss": 0.2237,
      "step": 366
    },
    {
      "epoch": 1.7169590643274852,
      "grad_norm": 0.4359574019908905,
      "learning_rate": 4.6214099216710174e-05,
      "loss": 0.209,
      "step": 367
    },
    {
      "epoch": 1.7216374269005847,
      "grad_norm": 0.41355764865875244,
      "learning_rate": 4.5430809399477804e-05,
      "loss": 0.2047,
      "step": 368
    },
    {
      "epoch": 1.7263157894736842,
      "grad_norm": 0.41556811332702637,
      "learning_rate": 4.464751958224543e-05,
      "loss": 0.2158,
      "step": 369
    },
    {
      "epoch": 1.7309941520467835,
      "grad_norm": 0.27337971329689026,
      "learning_rate": 4.386422976501305e-05,
      "loss": 0.1711,
      "step": 370
    },
    {
      "epoch": 1.735672514619883,
      "grad_norm": 0.3295460641384125,
      "learning_rate": 4.3080939947780675e-05,
      "loss": 0.1815,
      "step": 371
    },
    {
      "epoch": 1.7403508771929825,
      "grad_norm": 0.3299092948436737,
      "learning_rate": 4.2297650130548306e-05,
      "loss": 0.2174,
      "step": 372
    },
    {
      "epoch": 1.7450292397660818,
      "grad_norm": 0.355620801448822,
      "learning_rate": 4.151436031331592e-05,
      "loss": 0.2007,
      "step": 373
    },
    {
      "epoch": 1.7497076023391813,
      "grad_norm": 0.30097195506095886,
      "learning_rate": 4.073107049608355e-05,
      "loss": 0.1725,
      "step": 374
    },
    {
      "epoch": 1.7543859649122808,
      "grad_norm": 0.29873740673065186,
      "learning_rate": 3.994778067885117e-05,
      "loss": 0.1697,
      "step": 375
    },
    {
      "epoch": 1.75906432748538,
      "grad_norm": 0.32004866003990173,
      "learning_rate": 3.91644908616188e-05,
      "loss": 0.1941,
      "step": 376
    },
    {
      "epoch": 1.7637426900584794,
      "grad_norm": 0.31824639439582825,
      "learning_rate": 3.838120104438642e-05,
      "loss": 0.1942,
      "step": 377
    },
    {
      "epoch": 1.768421052631579,
      "grad_norm": 0.3183455169200897,
      "learning_rate": 3.759791122715404e-05,
      "loss": 0.1712,
      "step": 378
    },
    {
      "epoch": 1.7730994152046784,
      "grad_norm": 0.3414574861526489,
      "learning_rate": 3.681462140992167e-05,
      "loss": 0.2045,
      "step": 379
    },
    {
      "epoch": 1.7777777777777777,
      "grad_norm": 0.35098642110824585,
      "learning_rate": 3.6031331592689295e-05,
      "loss": 0.1546,
      "step": 380
    },
    {
      "epoch": 1.7824561403508772,
      "grad_norm": 0.33511778712272644,
      "learning_rate": 3.524804177545692e-05,
      "loss": 0.2006,
      "step": 381
    },
    {
      "epoch": 1.7871345029239767,
      "grad_norm": 0.43212708830833435,
      "learning_rate": 3.446475195822454e-05,
      "loss": 0.2348,
      "step": 382
    },
    {
      "epoch": 1.791812865497076,
      "grad_norm": 0.386383056640625,
      "learning_rate": 3.3681462140992167e-05,
      "loss": 0.2329,
      "step": 383
    },
    {
      "epoch": 1.7964912280701755,
      "grad_norm": 0.38163474202156067,
      "learning_rate": 3.289817232375979e-05,
      "loss": 0.215,
      "step": 384
    },
    {
      "epoch": 1.801169590643275,
      "grad_norm": 0.32535067200660706,
      "learning_rate": 3.211488250652741e-05,
      "loss": 0.2021,
      "step": 385
    },
    {
      "epoch": 1.8058479532163743,
      "grad_norm": 0.37925317883491516,
      "learning_rate": 3.133159268929504e-05,
      "loss": 0.1879,
      "step": 386
    },
    {
      "epoch": 1.8105263157894735,
      "grad_norm": 0.3847969174385071,
      "learning_rate": 3.054830287206266e-05,
      "loss": 0.2202,
      "step": 387
    },
    {
      "epoch": 1.8152046783625733,
      "grad_norm": 0.3320363461971283,
      "learning_rate": 2.9765013054830282e-05,
      "loss": 0.1912,
      "step": 388
    },
    {
      "epoch": 1.8198830409356725,
      "grad_norm": 0.38783010840415955,
      "learning_rate": 2.898172323759791e-05,
      "loss": 0.2338,
      "step": 389
    },
    {
      "epoch": 1.8245614035087718,
      "grad_norm": 0.2960682809352875,
      "learning_rate": 2.8198433420365533e-05,
      "loss": 0.1814,
      "step": 390
    },
    {
      "epoch": 1.8292397660818713,
      "grad_norm": 0.36216381192207336,
      "learning_rate": 2.7415143603133156e-05,
      "loss": 0.2255,
      "step": 391
    },
    {
      "epoch": 1.8339181286549708,
      "grad_norm": 0.34885117411613464,
      "learning_rate": 2.663185378590078e-05,
      "loss": 0.2183,
      "step": 392
    },
    {
      "epoch": 1.8385964912280701,
      "grad_norm": 0.3236502408981323,
      "learning_rate": 2.5848563968668404e-05,
      "loss": 0.2232,
      "step": 393
    },
    {
      "epoch": 1.8432748538011696,
      "grad_norm": 0.32448524236679077,
      "learning_rate": 2.506527415143603e-05,
      "loss": 0.1964,
      "step": 394
    },
    {
      "epoch": 1.8479532163742691,
      "grad_norm": 0.313748836517334,
      "learning_rate": 2.4281984334203655e-05,
      "loss": 0.1852,
      "step": 395
    },
    {
      "epoch": 1.8526315789473684,
      "grad_norm": 0.33855879306793213,
      "learning_rate": 2.3498694516971278e-05,
      "loss": 0.2261,
      "step": 396
    },
    {
      "epoch": 1.8573099415204677,
      "grad_norm": 0.3014741837978363,
      "learning_rate": 2.2715404699738902e-05,
      "loss": 0.1838,
      "step": 397
    },
    {
      "epoch": 1.8619883040935674,
      "grad_norm": 0.3531910181045532,
      "learning_rate": 2.1932114882506526e-05,
      "loss": 0.1824,
      "step": 398
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 0.39587411284446716,
      "learning_rate": 2.1148825065274153e-05,
      "loss": 0.2401,
      "step": 399
    },
    {
      "epoch": 1.871345029239766,
      "grad_norm": 0.3619554340839386,
      "learning_rate": 2.0365535248041777e-05,
      "loss": 0.1977,
      "step": 400
    },
    {
      "epoch": 1.8760233918128655,
      "grad_norm": 0.2954935133457184,
      "learning_rate": 1.95822454308094e-05,
      "loss": 0.1663,
      "step": 401
    },
    {
      "epoch": 1.880701754385965,
      "grad_norm": 0.32834315299987793,
      "learning_rate": 1.879895561357702e-05,
      "loss": 0.2008,
      "step": 402
    },
    {
      "epoch": 1.8853801169590643,
      "grad_norm": 0.35396724939346313,
      "learning_rate": 1.8015665796344648e-05,
      "loss": 0.2302,
      "step": 403
    },
    {
      "epoch": 1.8900584795321638,
      "grad_norm": 0.3211686909198761,
      "learning_rate": 1.723237597911227e-05,
      "loss": 0.2145,
      "step": 404
    },
    {
      "epoch": 1.8947368421052633,
      "grad_norm": 0.38890671730041504,
      "learning_rate": 1.6449086161879895e-05,
      "loss": 0.2047,
      "step": 405
    },
    {
      "epoch": 1.8994152046783626,
      "grad_norm": 0.3522469401359558,
      "learning_rate": 1.566579634464752e-05,
      "loss": 0.1964,
      "step": 406
    },
    {
      "epoch": 1.9040935672514618,
      "grad_norm": 0.3185712397098541,
      "learning_rate": 1.4882506527415141e-05,
      "loss": 0.1778,
      "step": 407
    },
    {
      "epoch": 1.9087719298245616,
      "grad_norm": 0.3200305700302124,
      "learning_rate": 1.4099216710182766e-05,
      "loss": 0.1801,
      "step": 408
    },
    {
      "epoch": 1.9134502923976608,
      "grad_norm": 0.33106136322021484,
      "learning_rate": 1.331592689295039e-05,
      "loss": 0.2092,
      "step": 409
    },
    {
      "epoch": 1.9181286549707601,
      "grad_norm": 0.32985132932662964,
      "learning_rate": 1.2532637075718015e-05,
      "loss": 0.1732,
      "step": 410
    },
    {
      "epoch": 1.9228070175438596,
      "grad_norm": 0.3580392599105835,
      "learning_rate": 1.1749347258485639e-05,
      "loss": 0.2202,
      "step": 411
    },
    {
      "epoch": 1.9274853801169591,
      "grad_norm": 0.36530202627182007,
      "learning_rate": 1.0966057441253263e-05,
      "loss": 0.2285,
      "step": 412
    },
    {
      "epoch": 1.9321637426900584,
      "grad_norm": 0.328107088804245,
      "learning_rate": 1.0182767624020888e-05,
      "loss": 0.2085,
      "step": 413
    },
    {
      "epoch": 1.936842105263158,
      "grad_norm": 0.34341707825660706,
      "learning_rate": 9.39947780678851e-06,
      "loss": 0.1786,
      "step": 414
    },
    {
      "epoch": 1.9415204678362574,
      "grad_norm": 0.43782854080200195,
      "learning_rate": 8.616187989556136e-06,
      "loss": 0.2844,
      "step": 415
    },
    {
      "epoch": 1.9461988304093567,
      "grad_norm": 0.3708110749721527,
      "learning_rate": 7.83289817232376e-06,
      "loss": 0.2458,
      "step": 416
    },
    {
      "epoch": 1.950877192982456,
      "grad_norm": 0.380052387714386,
      "learning_rate": 7.049608355091383e-06,
      "loss": 0.2415,
      "step": 417
    },
    {
      "epoch": 1.9555555555555557,
      "grad_norm": 0.31808170676231384,
      "learning_rate": 6.266318537859008e-06,
      "loss": 0.1626,
      "step": 418
    },
    {
      "epoch": 1.960233918128655,
      "grad_norm": 0.3619125485420227,
      "learning_rate": 5.4830287206266314e-06,
      "loss": 0.1965,
      "step": 419
    },
    {
      "epoch": 1.9649122807017543,
      "grad_norm": 0.3461640477180481,
      "learning_rate": 4.699738903394255e-06,
      "loss": 0.1869,
      "step": 420
    },
    {
      "epoch": 1.9695906432748538,
      "grad_norm": 0.33548876643180847,
      "learning_rate": 3.91644908616188e-06,
      "loss": 0.1652,
      "step": 421
    },
    {
      "epoch": 1.9742690058479533,
      "grad_norm": 0.32583239674568176,
      "learning_rate": 3.133159268929504e-06,
      "loss": 0.1745,
      "step": 422
    },
    {
      "epoch": 1.9789473684210526,
      "grad_norm": 0.37000033259391785,
      "learning_rate": 2.3498694516971276e-06,
      "loss": 0.195,
      "step": 423
    },
    {
      "epoch": 1.983625730994152,
      "grad_norm": 0.3325011730194092,
      "learning_rate": 1.566579634464752e-06,
      "loss": 0.1912,
      "step": 424
    },
    {
      "epoch": 1.9883040935672516,
      "grad_norm": 0.33850565552711487,
      "learning_rate": 7.83289817232376e-07,
      "loss": 0.2124,
      "step": 425
    },
    {
      "epoch": 1.9929824561403509,
      "grad_norm": 0.34020423889160156,
      "learning_rate": 0.0,
      "loss": 0.2023,
      "step": 426
    },
    {
      "epoch": 1.9929824561403509,
      "eval_loss": 0.27231791615486145,
      "eval_runtime": 128.7163,
      "eval_samples_per_second": 4.289,
      "eval_steps_per_second": 0.536,
      "step": 426
    }
  ],
  "logging_steps": 1,
  "max_steps": 426,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.4886034718602035e+17,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
